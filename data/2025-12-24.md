<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 11]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study](https://arxiv.org/abs/2512.19855)
*Andrew Stirling,Mykola Lukashchuk,Dmitry Bagaev,Wouter Kouw,James R. Forbes*

Main category: cs.RO

TL;DR: 本文扩展了ESGVI算法，使其能够在矩阵李群上运行，并引入了处理重尾和偏斜噪声分布的因素，特别适用于UWB定位中的NLOS和多路径效应问题。实验表明，该方法提高了准确性并保持了一致性。


<details>
  <summary>Details</summary>
Motivation: 作者希望将ESGVI算法推广到矩阵李群上进行状态估计，并解决UWB定位中由于NLOS及多路径效应导致的重尾和偏斜噪声问题。

Method: 通过将ESGVI算法泛化至矩阵李群以正确处理包含方向分量的状态估计，并引入新因素来适应重尾和偏斜噪声分布。

Result: 所提出的方法在富含NLOS测量值的UWB定位实验中验证有效，显示出了更高的精度以及相当的一致性表现。

Conclusion: 这项工作成功地拓展了ESGVI框架的功能范围，同时保持了其稀疏性和无导数结构的特点，并且通过开源实现促进了更广泛的研究应用。

Abstract: This letter extends the exactly sparse Gaussian variational inference (ESGVI) algorithm for state estimation in two complementary directions. First, ESGVI is generalized to operate on matrix Lie groups, enabling the estimation of states with orientation components while respecting the underlying group structure. Second, factors are introduced to accommodate heavy-tailed and skewed noise distributions, as commonly encountered in ultra-wideband (UWB) localization due to non-line-of-sight (NLOS) and multipath effects. Both extensions are shown to integrate naturally within the ESGVI framework while preserving its sparse and derivative-free structure. The proposed approach is validated in a UWB localization experiment with NLOS-rich measurements, demonstrating improved accuracy and comparable consistency. Finally, a Python implementation within a factor-graph-based estimation framework is made open-source (https://github.com/decargroup/gvi_ws) to support broader research use.

</details>


### [2] [A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones](https://arxiv.org/abs/2512.19914)
*Sujan Warnakulasooriya,Andreas Willig,Xiaobing Wu*

Main category: cs.RO

TL;DR: 本文提出了一种基于优先级调度的算法，用于优化无人机群在初始形成过程中的效率。该算法根据每架无人机潜在碰撞次数及其无障碍到达目标位置的可能性来分配优先级，并计算适当的延迟以确保无碰撞路径。仿真结果显示，该算法能够为多达5000架无人机生成无碰撞轨迹，在性能和计算效率上均优于基于耦合度的启发式优先规划方法（CDH-PP）。


<details>
  <summary>Details</summary>
Motivation: 随着无人机应用领域的不断扩展，集群技术提供了增强的合作能力但也带来了初始化形成阶段的重大挑战。现有的群集算法往往难以高效且可扩展地解决当潜在碰撞迫使无人机采取次优轨迹的问题。

Method: 提出了一种时间高效的优先级调度算法，通过考虑每架无人机可能遇到的碰撞数量以及其无障碍达到指定位置的概率来为其分配优先级；然后，依据此优先级顺序让每架无人机计算出一个合适的延时，从而保证所有无人机都能沿着无碰撞路径移动。

Result: 仿真结果表明，所提出的算法能够有效地为最多包含5000架无人机的群体生成无碰撞轨迹，并且在性能和计算效率方面都优于基于耦合度的启发式优先规划方法（CDH-PP）。

Conclusion: 该研究开发的时间高效优先调度算法显著提高了无人机群在初始形成过程中避免碰撞的能力，同时保持了较高的计算效率，适用于大规模无人机编队操作。

Abstract: Drone applications continue to expand across various domains, with flocking offering enhanced cooperative capabilities but introducing significant challenges during initial formation. Existing flocking algorithms often struggle with efficiency and scalability, particularly when potential collisions force drones into suboptimal trajectories. This paper presents a time-efficient prioritised scheduling algorithm that improves the initial formation process of drone flocks. The method assigns each drone a priority based on its number of potential collisions and its likelihood of reaching its target position without permanently obstructing other drones. Using this hierarchy, each drone computes an appropriate delay to ensure a collision-free path. Simulation results show that the proposed algorithm successfully generates collision-free trajectories for flocks of up to 5000 drones and outperforms the coupling-degree-based heuristic prioritised planning method (CDH-PP) in both performance and computational efficiency.

</details>


### [3] [Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting](https://arxiv.org/abs/2512.20014)
*Sangoh Lee,Sangwoo Mo,Wook-Shin Han*

Main category: cs.RO

TL;DR: 本文提出了一种名为视觉注意提示（VAP）的方法，用于解决机器人在处理个性化指令时识别和操作特定用户对象的难题。通过将参考图像作为非参数视觉记忆，并利用开放词汇检测与嵌入式匹配来定位个人物品，然后将此定位作为视觉提示注入，以突出显示对象并重写指令。实验表明，VAP在成功率和正确物体操作方面优于通用策略和其他基准方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作(VLA)模型虽然能够很好地执行通用指令，但在面对个性化命令如“拿我的杯子”时表现不佳，这类任务要求机器人能够在视觉上相似的对象中识别并作用于某一特定实例。为了解决这一问题，研究者们探索了如何让VLA模型能够基于少量参考图片识别并控制未曾在训练过程中见过的用户特定对象。

Method: 提出了视觉注意提示(VAP)，这是一种无需额外训练的感知适配器，它使冻结状态下的VLA模型具备自顶向下的选择性注意力能力。VAP将提供的参考图片视为一种非参数化的视觉记忆库，通过开放词汇检测及基于嵌入的匹配技术来场景中定位个人物品，并通过强调该物品和改写原始指令的方式将其作为视觉提示进行注入。

Result: 通过构建两个模拟基准测试环境Personalized-SIMPLER和个人化VLABench以及一个现实世界的桌面基准测试，评估了多个机器人和任务上的个性化操控性能。实验结果显示，在成功率和正确对象操控方面，VAP持续优于通用策略以及其他基于学习令牌的基础方法。

Conclusion: 视觉注意提示(VAP)提供了一种简单而有效的方法来增强VLA模型处理个性化命令的能力，特别是当需要从视觉上类似但具有不同意义的对象中准确识别出特定个体时。这种方法有助于缩小语义理解和实例级控制之间的差距。

Abstract: While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as "bring my cup", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.

</details>


### [4] [LoLA: Long Horizon Latent Action Learning for General Robot Manipulation](https://arxiv.org/abs/2512.20166)
*Xiaofan Wang,Xingyu Gao,Jianlong Fu,Zuolei Li,Dean Fortier,Galen Mullins,Andrey Kolobov,Baining Guo*

Main category: cs.RO

TL;DR: 提出了LoLA框架，通过整合长期多视角观察和机器人本体感知来实现多步骤推理和动作生成，从而在长时序、语言引导的机器人操作任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有Vision-Language-Action（VLA）模型通常忽略了利用历史信息以及生成连贯动作序列的重要性，这对于执行长时序、语言指导下的机器人操作任务至关重要。

Method: 开发了名为LoLA（Long Horizon Latent Action Learning）的框架，该框架首先使用视觉-语言模型从历史序列和多视角观察中编码丰富的上下文特征；接着引入了一个关键模块——状态感知潜表示重构，将视觉输入和语言指令转换为可操作的机器人运动空间，并通过一个可学习的“体现锚定”潜在空间明确地将VL表征与物理尺度联系起来。

Result: LoLA在多样化的机器人预训练数据集上进行了训练，并在模拟基准测试（SIMPLER和LIBERO）及两个真实世界任务中进行了广泛评估，结果显示其在长时序操作任务中的表现显著优于现有最先进方法（如pi0）。

Conclusion: LoLA通过创新地结合长期观测数据与机器人自身感知信息，在处理需要长时间规划的语言引导型机器人操作任务方面提供了有效解决方案。

Abstract: The capability of performing long-horizon, language-guided robotic manipulation tasks critically relies on leveraging historical information and generating coherent action sequences. However, such capabilities are often overlooked by existing Vision-Language-Action (VLA) models. To solve this challenge, we propose LoLA (Long Horizon Latent Action Learning), a framework designed for robot manipulation that integrates long-term multi-view observations and robot proprioception to enable multi-step reasoning and action generation. We first employ Vision-Language Models to encode rich contextual features from historical sequences and multi-view observations. We further introduces a key module, State-Aware Latent Re-representation, which transforms visual inputs and language commands into actionable robot motion space. Unlike existing VLA approaches that merely concatenate robot proprioception (e.g., joint angles) with VL embeddings, this module leverages such robot states to explicitly ground VL representations in physical scale through a learnable "embodiment-anchored" latent space. We trained LoLA on diverse robotic pre-training datasets and conducted extensive evaluations on simulation benchmarks (SIMPLER and LIBERO), as well as two real-world tasks on Franka and Bi-Manual Aloha robots. Results show that LoLA significantly outperforms prior state-of-the-art methods (e.g., pi0), particularly in long-horizon manipulation tasks.

</details>


### [5] [Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation](https://arxiv.org/abs/2512.20188)
*Teqiang Zou,Hongliang Zeng,Yuxuan Nong,Yifan Li,Kehui Liu,Haotian Yang,Xinyang Ling,Xin Li,Lianyang Ma*

Main category: cs.RO

TL;DR: 提出了一种异步的Fast-Slow VLA框架（DuoCore-FS），通过区分高频动作生成和低频丰富的VLM推理路径，解决了传统视觉-语言-动作系统中因大模型推理速度慢而导致的控制稳定性和实时性能限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）系统由于大型视觉-语言模型（VLM）的推理速度较低，导致策略表现受限，并且在涉及更多关节、更大运动空间及动态视角变化的全身机器人操作中，同步执行严重限制了控制的稳定性和实时性。

Method: 开发了一个名为DuoCore-FS的真正异步Fast-Slow VLA架构，该架构由一个用于高频动作生成的快速路径和一个进行丰富VLM推理的慢速路径组成。利用潜变量表示缓存连接快慢两系统，以及一个全身体动作tokenizer提供紧凑统一的动作表示。尽管如此，VLM与动作专家仍可端到端联合训练，保持了统一的策略学习同时实现了异步执行。

Result: DuoCore-FS支持30亿参数级别的VLM的同时达到了约30Hz的全身体动作块生成速率，比同类大小的现有VLA模型快大约三倍。实际实验表明，在任务成功率和响应性方面均有显著提升。

Conclusion: DuoCore-FS为解决视觉-语言-动作系统中的实时性能与控制稳定性挑战提供了新方法，通过引入异步机制显著提高了全身体机器人操作的表现。

Abstract: Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.

</details>


### [6] [UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas](https://arxiv.org/abs/2512.20224)
*Qijun Qin,Ziqi Zhang,Yihan Zhong,Feng Huang,Xikun Liu,Runzhi Hu,Hang Chen,Wei Hu,Dongzhe Su,Jun Zhang,Hoi-Fung Ng,Weisong Wen*

Main category: cs.RO

TL;DR: 本文介绍了UrbanV2X，一个为支持密集城市区域智能移动应用研究而设计的多传感器数据集，它通过车辆和路侧基础设施收集数据，并提供了各种导航算法的基准测试。


<details>
  <summary>Details</summary>
Motivation: 由于单一自动驾驶车辆存在局限性，C-V2X技术通过传感器信息共享为实现完全自动驾驶提供了新的可能。然而，在复杂城市环境中支持车-基础设施协同导航的真实世界数据集仍然稀缺。为了填补这一空白，研究人员开发了UrbanV2X数据集。

Method: 本研究中，研究人员利用香港C-V2X试验场内的车辆和路边基础设施收集了一套全面的数据集UrbanV2X。车载平台提供来自多个工业相机、激光雷达、4D雷达、超宽带(UWB)、惯性测量单元(IMU)以及高精度GNSS-RTK/INS导航系统的同步数据；同时，路边设施也提供了激光雷达、GNSS和UWB测量数据。整个车-基础设施平台使用精确时间协议(PTP)进行同步，并提供了传感器校准数据。

Result: 创建了一个名为UrbanV2X的综合多感官数据集，该数据集专为支持在密集城区内进行智慧移动应用程序的研究而设计。此外，还对多种导航算法进行了基准测试以评估所收集的合作数据的质量与适用性。

Conclusion: UrbanV2X数据集旨在促进关于如何在复杂城市环境下利用C-V2X技术改善自动驾驶性能的研究工作。它不仅包括了从不同类型的传感器获取的信息，而且还实现了跨设备的时间同步，这使得它可以作为一个有价值的资源来探索未来的智能交通解决方案。

Abstract: Due to the limitations of a single autonomous vehicle, Cellular Vehicle-to-Everything (C-V2X) technology opens a new window for achieving fully autonomous driving through sensor information sharing. However, real-world datasets supporting vehicle-infrastructure cooperative navigation in complex urban environments remain rare. To address this gap, we present UrbanV2X, a comprehensive multisensory dataset collected from vehicles and roadside infrastructure in the Hong Kong C-V2X testbed, designed to support research on smart mobility applications in dense urban areas. Our onboard platform provides synchronized data from multiple industrial cameras, LiDARs, 4D radar, ultra-wideband (UWB), IMU, and high-precision GNSS-RTK/INS navigation systems. Meanwhile, our roadside infrastructure provides LiDAR, GNSS, and UWB measurements. The entire vehicle-infrastructure platform is synchronized using the Precision Time Protocol (PTP), with sensor calibration data provided. We also benchmark various navigation algorithms to evaluate the collected cooperative data. The dataset is publicly available at https://polyu-taslab.github.io/UrbanV2X/.

</details>


### [7] [KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System](https://arxiv.org/abs/2512.20299)
*Zhongyu Xia,Wenhao Chen,Yongtao Wang,Ming-Hsuan Yang*

Main category: cs.RO

TL;DR: 提出了KnowVal，一个通过整合开放世界感知和知识检索来实现视觉-语言推理的自动驾驶系统，并通过构建驾驶知识图谱和价值模型来改进决策性能，实验结果显示其在nuScenes上实现了最低碰撞率，在Bench2Drive上达到了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶方法主要依赖数据驱动的学习，难以通过模仿或有限的强化奖励捕捉到决策背后的复杂逻辑。因此，需要一种新的方式来提高自动驾驶系统的理解和决策能力。

Method: KnowVal结合了开放世界的感知能力和基于高效LLM的知识检索机制，构建了一个全面的驾驶知识图谱，编码了交通法规、防御性驾驶原则和伦理规范，并开发了一个人类偏好数据集训练的价值模型，用于指导可解释的、符合价值观的轨迹评估。

Result: 实验表明，该方法显著提高了规划性能，并且与现有架构兼容。特别地，KnowVal在nuScenes数据集上达成了最低碰撞率的成绩，在Bench2Drive评测中也取得了领先的结果。

Conclusion: 通过引入视觉-语言推理以及对驾驶知识和价值一致性的重视，KnowVal为高级自动驾驶系统提供了一种有效提升安全性和性能的新途径。

Abstract: Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.

</details>


### [8] [Pneumatic bladder links with wide range of motion joints for articulated inflatable robots](https://arxiv.org/abs/2512.20322)
*Katsu Uchiyama,Ryuma Niiyama*

Main category: cs.RO

TL;DR: 本文提出了一种由多个气动囊式连杆通过滚动接触关节（称为Hillberry关节）连接而成的关节机器人。这种囊式连杆采用双层结构，既保证了密封性也提供了形状上的灵活性。实验表明，该机制可用于移动500克的有效载荷，并且能够分别用2自由度和1自由度的手臂举起3.4千克和5千克的有效载荷。此外，还展示了单个3自由度充气腿与小车结合用于腿部运动的可能性。


<details>
  <summary>Details</summary>
Motivation: 探索充气机器人的各种应用是当前研究的一个前沿领域。本研究旨在开发一种新型的、具有更广泛活动范围的充气机器人关节解决方案。

Method: 设计并制造了一种由多段气动囊式连杆组成的关节机器人，这些连杆通过名为Hillberry关节的特殊滚动接触关节相连。每段连杆使用了双层材料来确保其密封性和柔韧性。

Result: 研究表明，所提出的机构能够在3-DoF手臂上移动500克有效载荷，并且在2-DoF和1-DoF配置下分别提升3.4千克和5千克重量。此外，单个3-DoF充气腿被证明适合于腿部行走运动。

Conclusion: 本研究所提出的基于Hillberry关节的充气机器人展示出了在承载能力和灵活度方面的显著优势，为未来充气机器人在更多应用场景中的发展奠定了基础。

Abstract: Exploration of various applications is the frontier of research on inflatable robots. We proposed an articulated robots consisting of multiple pneumatic bladder links connected by rolling contact joints called Hillberry joints. The bladder link is made of a double-layered structure of tarpaulin sheet and polyurethane sheet, which is both airtight and flexible in shape. The integration of the Hilberry joint into an inflatable robot is also a new approach. The rolling contact joint allows wide range of motion of $\pm 150 ^{\circ}$, the largest among the conventional inflatable joints. Using the proposed mechanism for inflatable robots, we demonstrated moving a 500 g payload with a 3-DoF arm and lifting 3.4 kg and 5 kg payloads with 2-DoF and 1-DoF arms, respectively. We also experimented with a single 3-DoF inflatable leg attached to a dolly to show that the proposed structure worked for legged locomotion.

</details>


### [9] [FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration](https://arxiv.org/abs/2512.20355)
*Hao Wei,Peiji Wang,Qianhao Wang,Tong Qin,Fei Gao,Yulin Si*

Main category: cs.RO

TL;DR: 本文提出了FAR-AVIO，一种基于Schur补的紧密耦合声学-视觉-惯性里程计框架，专为水下机器人设计。通过嵌入自适应权重调整和可靠性评估模块以及高效的在线校准方案，FAR-AVIO在定位精度和计算效率上优于现有技术，并且适合低功耗嵌入式平台使用。


<details>
  <summary>Details</summary>
Motivation: 水下环境对视觉-惯性里程计系统构成严重挑战，包括强光衰减、海洋雪和浑浊度等问题，加上弱激励运动，降低了惯性可观测性并导致长期操作中频繁跟踪失败。虽然紧密耦合的声学-视觉-惯性融合（通常通过声学多普勒速度记录仪DVL与视觉-惯性测量集成）可以提供准确的状态估计，但相关的图优化方法往往因计算成本高而不适用于实时部署于资源受限的平台上。

Method: 研究者们提出了一种名为FAR-AVIO的新框架，它将Schur补公式嵌入到扩展卡尔曼滤波器(EKF)中，使得能够在保持恒定时间更新的同时进行姿态-地标联合优化以提高准确性。此外，还引入了自适应权重调整与可靠性评估(AWARE)在线传感器健康模块，该模块持续评估视觉、惯性和DVL测量的可靠性，并自适应地调节它们的sigma权重。同时开发了一个无需专门校准动作就能共同估算DVL-IMU外部参数的有效在线校准方案。

Result: 数值模拟和实际水下实验一致表明，FAR-AVIO在定位准确性和计算效率方面均优于最先进的水下SLAM基线方法，使其能够在低功耗嵌入式平台上稳健运行。

Conclusion: FAR-AVIO成功地解决了传统水下视觉-惯性里程计系统面临的难题，通过创新性的方法提高了系统的可靠性和准确性，同时保证了足够的计算效率以实现实时应用。

Abstract: Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs.

</details>


### [10] [Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing](https://arxiv.org/abs/2512.20475)
*Maulana Bisyir Azhari,Donghun Han,Je In You,Sungjun Park,David Hyunchul Shim*

Main category: cs.RO

TL;DR: 本文介绍了为阿布扎比自主赛车联盟x无人机冠军联赛设计的系统，该系统通过将视觉-惯性里程计(VIO)输出与基于YOLO的门检测器得到的全局位置测量值融合，来纠正VIO漂移，并使用感知意识规划器生成平衡速度和感知系统需求的轨迹。系统在多项比赛中表现出色，取得了领奖台的成绩。


<details>
  <summary>Details</summary>
Motivation: 针对阿布扎比自主赛车联盟x无人机冠军联赛中仅使用单一摄像头和低质量惯性测量单元进行高速自主无人机竞赛的需求，研究旨在解决因传感器限制导致的视觉-惯性里程计（VIO）漂移问题，特别是在长时间、高速度以及激烈操作条件下。

Method: 采用的方法包括：1. 通过结合基于YOLO的门检测器提供的全局位置测量值与VIO输出，利用卡尔曼滤波器修正VIO漂移；2. 利用感知意识规划器生成考虑速度同时确保门对感知系统可见性的轨迹。

Result: 所开发的系统在多个类别中表现出色，包括AI大奖赛第三名（最高速度43.2公里/小时）、AI直线竞速赛第二名（超过59公里/小时）及AI多无人机竞赛第二名。

Conclusion: 本研究提出了一种有效的解决方案来改善单目视觉基础下的自主无人机飞行表现，特别是在具有挑战性的竞赛环境中。通过实验数据分析，进一步验证了该方法的有效性，并为未来相关工作提供了有价值的见解。

Abstract: The Abu Dhabi Autonomous Racing League(A2RL) x Drone Champions League competition(DCL) requires teams to perform high-speed autonomous drone racing using only a single camera and a low-quality inertial measurement unit -- a minimal sensor set that mirrors expert human drone racing pilots. This sensor limitation makes the system susceptible to drift from Visual-Inertial Odometry (VIO), particularly during long and fast flights with aggressive maneuvers. This paper presents the system developed for the championship, which achieved a competitive performance. Our approach corrected VIO drift by fusing its output with global position measurements derived from a YOLO-based gate detector using a Kalman filter. A perception-aware planner generated trajectories that balance speed with the need to keep gates visible for the perception system. The system demonstrated high performance, securing podium finishes across multiple categories: third place in the AI Grand Challenge with top speed of 43.2 km/h, second place in the AI Drag Race with over 59 km/h, and second place in the AI Multi-Drone Race. We detail the complete architecture and present a performance analysis based on experimental data from the competition, contributing our insights on building a successful system for monocular vision-based autonomous drone flight.

</details>


### [11] [LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing](https://arxiv.org/abs/2512.20591)
*Changyi Lin,Boda Huo,Mingyang Yu,Emily Ruppel,Bingqing Chen,Jonathan Francis,Ding Zhao*

Main category: cs.RO

TL;DR: LightTact, a novel visual-tactile sensor, uses an optics-based principle to detect light contact without relying on deformation, enabling robust detection of interactions with liquids and ultra-soft materials. It generates high-contrast images for accurate contact segmentation and can be used in robotic manipulation tasks, including water and thin-film interaction, as well as reasoning for robotic sorting.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to develop a tactile sensor capable of detecting light contact with surfaces that do not deform macroscopically, such as liquids or ultra-soft materials, which current tactile sensors struggle with due to their reliance on deformation for contact inference.

Method: The method involves the creation of LightTact, a visual-tactile fingertip sensor that employs an ambient-blocking optical configuration. This setup allows only diffuse light from true contacts to pass through, producing high-contrast images where non-contact areas remain dark and contact areas appear as they naturally do, facilitating pixel-level contact segmentation.

Result: LightTact successfully achieves accurate and robust contact detection across various material properties, forces, appearances, and lighting conditions. The sensor was integrated into a robotic arm and demonstrated effective use in tasks requiring extremely light touch, like handling water and facial cream, and also showed potential for value reasoning in sorting tasks by using vision-language models.

Conclusion: LightTact represents a significant advancement in tactile sensing, offering a solution for perceiving light contacts that are deformation-independent, thus expanding the capabilities of robots in delicate and precise manipulation tasks.

Abstract: Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.

</details>
