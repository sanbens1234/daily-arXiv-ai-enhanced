{"id": "2511.04758", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.04758", "abs": "https://arxiv.org/abs/2511.04758", "authors": ["Caelan Garrett", "Fabio Ramos"], "title": "ScheduleStream: Temporal Planning with Samplers for GPU-Accelerated Multi-Arm Task and Motion Planning & Scheduling", "comment": "Project website: https://schedulestream.github.io", "summary": "Bimanual and humanoid robots are appealing because of their human-like\nability to leverage multiple arms to efficiently complete tasks. However,\ncontrolling multiple arms at once is computationally challenging due to the\ngrowth in the hybrid discrete-continuous action space. Task and Motion Planning\n(TAMP) algorithms can efficiently plan in hybrid spaces but generally produce\nplans, where only one arm is moving at a time, rather than schedules that allow\nfor parallel arm motion. In order to extend TAMP to produce schedules, we\npresent ScheduleStream, the first general-purpose framework for planning &\nscheduling with sampling operations. ScheduleStream models temporal dynamics\nusing hybrid durative actions, which can be started asynchronously and persist\nfor a duration that's a function of their parameters. We propose\ndomain-independent algorithms that solve ScheduleStream problems without any\napplication-specific mechanisms. We apply ScheduleStream to Task and Motion\nPlanning & Scheduling (TAMPAS), where we use GPU acceleration within samplers\nto expedite planning. We compare ScheduleStream algorithms to several ablations\nin simulation and find that they produce more efficient solutions. We\ndemonstrate ScheduleStream on several real-world bimanual robot tasks at\nhttps://schedulestream.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ScheduleStream\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u89c4\u5212\u548c\u8c03\u5ea6\u91c7\u6837\u64cd\u4f5c\u7684\u4e00\u822c\u6846\u67b6\uff0c\u5b83\u80fd\u591f\u5904\u7406\u6df7\u5408\u6301\u7eed\u6027\u52a8\u4f5c\uff0c\u4ece\u800c\u652f\u6301\u5e76\u884c\u81c2\u8fd0\u52a8\u3002\u901a\u8fc7\u4f7f\u7528GPU\u52a0\u901f\uff0cScheduleStream\u5728\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u4e0e\u8c03\u5ea6\uff08TAMPAS\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u6bd4\u5176\u4ed6\u65b9\u6cd5\u4ea7\u751f\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u53cc\u81c2\u548c\u7c7b\u4eba\u673a\u5668\u4eba\u56e0\u5176\u7c7b\u4f3c\u4eba\u7c7b\u7684\u80fd\u529b\u800c\u53d7\u5230\u9752\u7750\uff0c\u80fd\u591f\u5229\u7528\u591a\u6761\u624b\u81c2\u9ad8\u6548\u5b8c\u6210\u4efb\u52a1\u3002\u7136\u800c\uff0c\u540c\u65f6\u63a7\u5236\u591a\u6761\u624b\u81c2\u5728\u8ba1\u7b97\u4e0a\u662f\u5177\u6709\u6311\u6218\u6027\u7684\uff0c\u56e0\u4e3a\u8fd9\u4f1a\u589e\u52a0\u6df7\u5408\u79bb\u6563-\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684\u590d\u6742\u5ea6\u3002\u5c3d\u7ba1\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\uff08TAMP\uff09\u7b97\u6cd5\u80fd\u591f\u5728\u6df7\u5408\u7a7a\u95f4\u4e2d\u6709\u6548\u89c4\u5212\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u53ea\u4ea7\u751f\u5355\u81c2\u4f9d\u6b21\u79fb\u52a8\u7684\u8ba1\u5212\uff0c\u800c\u4e0d\u662f\u5141\u8bb8\u591a\u4e2a\u624b\u81c2\u540c\u65f6\u79fb\u52a8\u7684\u8c03\u5ea6\u65b9\u6848\u3002", "method": "\u4e3a\u4e86\u6269\u5c55TAMP\u4ee5\u751f\u6210\u8c03\u5ea6\u65b9\u6848\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86ScheduleStream\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u89c4\u5212\u548c\u8c03\u5ea6\u91c7\u6837\u64cd\u4f5c\u7684\u4e00\u822c\u7528\u9014\u6846\u67b6\u3002ScheduleStream\u4f7f\u7528\u6df7\u5408\u6301\u7eed\u6027\u52a8\u4f5c\u6765\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\uff0c\u8fd9\u4e9b\u52a8\u4f5c\u53ef\u4ee5\u5f02\u6b65\u542f\u52a8\u5e76\u4e14\u6839\u636e\u5176\u53c2\u6570\u6301\u7eed\u4e00\u5b9a\u7684\u65f6\u95f4\u957f\u5ea6\u3002\u7814\u7a76\u8005\u8fd8\u63d0\u51fa\u4e86\u4e0d\u4f9d\u8d56\u4e8e\u5177\u4f53\u9886\u57df\u7684\u7b97\u6cd5\uff0c\u7528\u6765\u89e3\u51b3ScheduleStream\u95ee\u9898\u3002\u6b64\u5916\uff0c\u5728TAMPAS\u5e94\u7528\u4e2d\uff0c\u4ed6\u4eec\u91c7\u7528\u4e86GPU\u52a0\u901f\u91c7\u6837\u5668\u4ee5\u52a0\u5feb\u89c4\u5212\u8fc7\u7a0b\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u7248\u672c\u7684ScheduleStream\u7b97\u6cd5\u4e0e\u5176\u4ed6\u51e0\u79cd\u7b80\u5316\u7248\u672c\u5728\u4eff\u771f\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0ScheduleStream\u7b97\u6cd5\u4ea7\u751f\u7684\u89e3\u51b3\u65b9\u6848\u66f4\u52a0\u9ad8\u6548\u3002", "conclusion": "ScheduleStream\u4e3a\u53cc\u81c2\u673a\u5668\u4eba\u7684\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u53ca\u8c03\u5ea6\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u652f\u6301\u5e76\u884c\u81c2\u8fd0\u52a8\uff0c\u5e76\u4e14\u901a\u8fc7GPU\u52a0\u901f\u63d0\u9ad8\u4e86\u89c4\u5212\u6548\u7387\u3002"}}
{"id": "2511.04769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04769", "abs": "https://arxiv.org/abs/2511.04769", "authors": ["Phat Nguyen", "Tsun-Hsuan Wang", "Zhang-Wei Hong", "Erfan Aasi", "Andrew Silva", "Guy Rosman", "Sertac Karaman", "Daniela Rus"], "title": "ReGen: Generative Robot Simulation via Inverse Design", "comment": null, "summary": "Simulation plays a key role in scaling robot learning and validating\npolicies, but constructing simulations remains a labor-intensive process. This\npaper introduces ReGen, a generative simulation framework that automates\nsimulation design via inverse design. Given a robot's behavior -- such as a\nmotion trajectory or an objective function -- and its textual description,\nReGen infers plausible scenarios and environments that could have caused the\nbehavior. ReGen leverages large language models to synthesize scenarios by\nexpanding a directed graph that encodes cause-and-effect relationships,\nrelevant entities, and their properties. This structured graph is then\ntranslated into a symbolic program, which configures and executes a robot\nsimulation environment. Our framework supports (i) augmenting simulations based\non ego-agent behaviors, (ii) controllable, counterfactual scenario generation,\n(iii) reasoning about agent cognition and mental states, and (iv) reasoning\nwith distinct sensing modalities, such as braking due to faulty GPS signals. We\ndemonstrate ReGen in autonomous driving and robot manipulation tasks,\ngenerating more diverse, complex simulated environments compared to existing\nsimulations with high success rates, and enabling controllable generation for\ncorner cases. This approach enhances the validation of robot policies and\nsupports data or simulation augmentation, advancing scalable robot learning for\nimproved generalization and robustness. We provide code and example videos at:\nhttps://regen-sim.github.io/", "AI": {"tldr": "ReGen, a generative simulation framework, uses large language models to automatically design simulations based on robot behavior and textual descriptions, enhancing the diversity and complexity of simulated environments for better validation of robot policies.", "motivation": "The motivation is to reduce the labor-intensive process of constructing simulations for robot learning and policy validation by automating the simulation design with an inverse design approach.", "method": "ReGen employs large language models to create scenarios from a directed graph that represents cause-and-effect relationships. The graph is then converted into a symbolic program to set up and run a robot simulation environment. The method supports various features such as augmenting simulations, generating counterfactual scenarios, reasoning about agent cognition, and handling different sensing modalities.", "result": "ReGen successfully generates more diverse and complex simulated environments in autonomous driving and robot manipulation tasks, with high success rates. It also enables the controllable generation of corner cases, which are useful for validating robot policies and augmenting data or simulations.", "conclusion": "ReGen advances scalable robot learning by improving the generalization and robustness of robot policies through the creation of rich, varied, and controllable simulated environments."}}
{"id": "2511.04812", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04812", "abs": "https://arxiv.org/abs/2511.04812", "authors": ["Zixuan Huang", "Huaidian Hou", "Dmitry Berenson"], "title": "Unified Multimodal Diffusion Forcing for Forceful Manipulation", "comment": "Project website: https://unified-df.github.io", "summary": "Given a dataset of expert trajectories, standard imitation learning\napproaches typically learn a direct mapping from observations (e.g., RGB\nimages) to actions. However, such methods often overlook the rich interplay\nbetween different modalities, i.e., sensory inputs, actions, and rewards, which\nis crucial for modeling robot behavior and understanding task outcomes. In this\nwork, we propose Multimodal Diffusion Forcing, a unified framework for learning\nfrom multimodal robot trajectories that extends beyond action generation.\nRather than modeling a fixed distribution, MDF applies random partial masking\nand trains a diffusion model to reconstruct the trajectory. This training\nobjective encourages the model to learn temporal and cross-modal dependencies,\nsuch as predicting the effects of actions on force signals or inferring states\nfrom partial observations. We evaluate MDF on contact-rich, forceful\nmanipulation tasks in simulated and real-world environments. Our results show\nthat MDF not only delivers versatile functionalities, but also achieves strong\nperformance, and robustness under noisy observations. More visualizations can\nbe found on our website https://unified-df.github.io", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u6a21\u6001\u6269\u6563\u5f3a\u8feb\uff08MDF\uff09\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5305\u542b\u591a\u79cd\u6a21\u5f0f\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u4e2d\u5b66\u4e60\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u751f\u6210\u52a8\u4f5c\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u968f\u673a\u90e8\u5206\u906e\u7f69\u548c\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u6765\u91cd\u5efa\u8f68\u8ff9\uff0c\u4ece\u800c\u5b66\u4e60\u65f6\u95f4\u4e0e\u8de8\u6a21\u6001\u4f9d\u8d56\u6027\uff0c\u5e76\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u63a5\u89e6\u4e30\u5bcc\u3001\u6709\u529b\u7684\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u76f4\u63a5\u4ece\u89c2\u5bdf\u5230\u7684\u6570\u636e\uff08\u5982RGB\u56fe\u50cf\uff09\u5b66\u4e60\u5230\u52a8\u4f5c\u7684\u6620\u5c04\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u4e86\u4e0d\u540c\u6a21\u6001\uff08\u4f8b\u5982\uff0c\u611f\u5b98\u8f93\u5165\u3001\u52a8\u4f5c\u548c\u5956\u52b1\uff09\u4e4b\u95f4\u590d\u6742\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u8fd9\u5bf9\u5efa\u6a21\u673a\u5668\u4eba\u884c\u4e3a\u53ca\u7406\u89e3\u4efb\u52a1\u7ed3\u679c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6269\u6563\u5f3a\u8feb\uff08Multimodal Diffusion Forcing, MDF\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5bf9\u8f68\u8ff9\u8fdb\u884c\u968f\u673a\u90e8\u5206\u906e\u853d\u5e76\u8bad\u7ec3\u4e00\u4e2a\u6269\u6563\u6a21\u578b\u4ee5\u91cd\u5efa\u6574\u4e2a\u8f68\u8ff9\u7684\u65b9\u5f0f\uff0c\u6765\u5b66\u4e60\u65f6\u95f4\u548c\u8de8\u6a21\u6001\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u6d89\u53ca\u5927\u91cf\u63a5\u89e6\u4e14\u9700\u8981\u65bd\u52a0\u529b\u7684\u64cd\u4f5c\u4efb\u52a1\u4e0a\uff0c\u65e0\u8bba\u662f\u6a21\u62df\u73af\u5883\u8fd8\u662f\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\uff0cMDF\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u591a\u6837\u5316\u7684\u529f\u80fd\uff0c\u800c\u4e14\u8868\u73b0\u51fa\u4e86\u51fa\u8272\u7684\u6027\u80fd\u548c\u5bf9\u4e8e\u566a\u58f0\u89c2\u6d4b\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MDF\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u95f4\u7684\u590d\u6742\u8054\u7cfb\uff0c\u652f\u6301\u66f4\u5168\u9762\u5730\u7406\u89e3\u4e0e\u6267\u884c\u673a\u5668\u4eba\u4efb\u52a1\u3002"}}
{"id": "2511.04827", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.04827", "abs": "https://arxiv.org/abs/2511.04827", "authors": ["Tobias Fischer", "Wolf Vollprecht", "Bas Zalmstra", "Ruben Arts", "Tim de Jager", "Alejandro Fontan", "Adam D Hines", "Michael Milford", "Silvio Traversaro", "Daniel Claes", "Scarlett Raine"], "title": "Pixi: Unified Software Development and Distribution for Robotics and AI", "comment": "20 pages, 3 figures, 11 code snippets", "summary": "The reproducibility crisis in scientific computing constrains robotics\nresearch. Existing studies reveal that up to 70% of robotics algorithms cannot\nbe reproduced by independent teams, while many others fail to reach deployment\nbecause creating shareable software environments remains prohibitively complex.\nThese challenges stem from fragmented, multi-language, and hardware-software\ntoolchains that lead to dependency hell. We present Pixi, a unified\npackage-management framework that addresses these issues by capturing exact\ndependency states in project-level lockfiles, ensuring bit-for-bit\nreproducibility across platforms. Its high-performance SAT solver achieves up\nto 10x faster dependency resolution than comparable tools, while integration of\nthe conda-forge and PyPI ecosystems removes the need for multiple managers.\nAdopted in over 5,300 projects since 2023, Pixi reduces setup times from hours\nto minutes and lowers technical barriers for researchers worldwide. By enabling\nscalable, reproducible, collaborative research infrastructure, Pixi accelerates\nprogress in robotics and AI.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aPixi\u7684\u7edf\u4e00\u5305\u7ba1\u7406\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5728\u9879\u76ee\u7ea7\u9501\u5b9a\u6587\u4ef6\u4e2d\u6355\u83b7\u786e\u5207\u7684\u4f9d\u8d56\u72b6\u6001\u6765\u89e3\u51b3\u673a\u5668\u4eba\u7814\u7a76\u4e2d\u7684\u53ef\u91cd\u590d\u6027\u5371\u673a\uff0c\u786e\u4fdd\u8de8\u5e73\u53f0\u7684\u4e00\u81f4\u6027\u3002Pixi\u4e0d\u4ec5\u6bd4\u540c\u7c7b\u5de5\u5177\u5feb10\u500d\u5730\u89e3\u6790\u4f9d\u8d56\u5173\u7cfb\uff0c\u8fd8\u6574\u5408\u4e86conda-forge\u548cPyPI\u751f\u6001\u7cfb\u7edf\uff0c\u4ece\u800c\u51cf\u5c11\u4ece\u6570\u5c0f\u65f6\u5230\u51e0\u5206\u949f\u7684\u8bbe\u7f6e\u65f6\u95f4\uff0c\u5e76\u81ea2023\u5e74\u4ee5\u6765\u5df2\u88ab\u8d85\u8fc75,300\u4e2a\u9879\u76ee\u91c7\u7528\u3002", "motivation": "\u79d1\u5b66\u7814\u7a76\u8ba1\u7b97\u4e2d\u7684\u518d\u73b0\u6027\u5371\u673a\u9650\u5236\u4e86\u673a\u5668\u4eba\u5b66\u7814\u7a76\u7684\u53d1\u5c55\u3002\u636e\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u9ad8\u8fbe70%\u7684\u673a\u5668\u4eba\u7b97\u6cd5\u65e0\u6cd5\u88ab\u72ec\u7acb\u56e2\u961f\u518d\u73b0\uff0c\u800c\u8bb8\u591a\u5176\u4ed6\u7b97\u6cd5\u7531\u4e8e\u521b\u5efa\u53ef\u5171\u4eab\u8f6f\u4ef6\u73af\u5883\u8fc7\u4e8e\u590d\u6742\u800c\u672a\u80fd\u6295\u5165\u4f7f\u7528\u3002\u8fd9\u4e9b\u6311\u6218\u6e90\u4e8e\u788e\u7247\u5316\u3001\u591a\u8bed\u8a00\u4ee5\u53ca\u786c\u4ef6-\u8f6f\u4ef6\u5de5\u5177\u94fe\u5bfc\u81f4\u7684\u4f9d\u8d56\u5730\u72f1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPixi\u7684\u7edf\u4e00\u5305\u7ba1\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u9879\u76ee\u7ea7\u522b\u7684\u9501\u5b9a\u6587\u4ef6\u4e2d\u6355\u83b7\u786e\u5207\u7684\u4f9d\u8d56\u72b6\u6001\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u786e\u4fdd\u8de8\u5e73\u53f0\u7684\u9010\u4f4d\u53ef\u518d\u73b0\u6027\u3002\u5176\u9ad8\u6027\u80fdSAT\u6c42\u89e3\u5668\u5b9e\u73b0\u4e86\u6bd4\u540c\u7c7b\u5de5\u5177\u5feb\u8fbe10\u500d\u7684\u4f9d\u8d56\u89e3\u6790\u901f\u5ea6\uff0c\u540c\u65f6\u6574\u5408conda-forge\u548cPyPI\u751f\u6001\u7cfb\u7edf\u6d88\u9664\u4e86\u5bf9\u591a\u4e2a\u7ba1\u7406\u5668\u7684\u9700\u6c42\u3002", "result": "\u81ea2023\u5e74\u4ee5\u6765\uff0cPixi\u5df2\u7ecf\u88ab\u8d85\u8fc75,300\u4e2a\u9879\u76ee\u91c7\u7eb3\u4f7f\u7528\uff0c\u5b83\u5c06\u8bbe\u7f6e\u65f6\u95f4\u4ece\u51e0\u4e2a\u5c0f\u65f6\u7f29\u77ed\u5230\u4e86\u51e0\u5206\u949f\u5185\uff0c\u5e76\u4e3a\u5168\u7403\u7814\u7a76\u4eba\u5458\u964d\u4f4e\u4e86\u6280\u672f\u969c\u788d\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u53ef\u518d\u73b0\u7684\u5408\u4f5c\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\uff0cPixi\u52a0\u901f\u4e86\u673a\u5668\u4eba\u6280\u672f\u548c\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2511.04831", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04831", "abs": "https://arxiv.org/abs/2511.04831", "authors": ["NVIDIA", ":", "Mayank Mittal", "Pascal Roth", "James Tigue", "Antoine Richard", "Octi Zhang", "Peter Du", "Antonio Serrano-Mu\u00f1oz", "Xinjie Yao", "Ren\u00e9 Zurbr\u00fcgg", "Nikita Rudin", "Lukasz Wawrzyniak", "Milad Rakhsha", "Alain Denzler", "Eric Heiden", "Ales Borovicka", "Ossama Ahmed", "Iretiayo Akinola", "Abrar Anwar", "Mark T. Carlson", "Ji Yuan Feng", "Animesh Garg", "Renato Gasoto", "Lionel Gulich", "Yijie Guo", "M. Gussert", "Alex Hansen", "Mihir Kulkarni", "Chenran Li", "Wei Liu", "Viktor Makoviychuk", "Grzegorz Malczyk", "Hammad Mazhar", "Masoud Moghani", "Adithyavairavan Murali", "Michael Noseworthy", "Alexander Poddubny", "Nathan Ratliff", "Welf Rehberg", "Clemens Schwarke", "Ritvik Singh", "James Latham Smith", "Bingjie Tang", "Ruchik Thaker", "Matthew Trepte", "Karl Van Wyk", "Fangzhou Yu", "Alex Millane", "Vikram Ramasamy", "Remo Steiner", "Sangeeta Subramanian", "Clemens Volk", "CY Chen", "Neel Jawale", "Ashwin Varghese Kuruttukulam", "Michael A. Lin", "Ajay Mandlekar", "Karsten Patzwaldt", "John Welsh", "Huihua Zhao", "Fatima Anes", "Jean-Francois Lafleche", "Nicolas Mo\u00ebnne-Loccoz", "Soowan Park", "Rob Stepinski", "Dirk Van Gelder", "Chris Amevor", "Jan Carius", "Jumyung Chang", "Anka He Chen", "Pablo de Heras Ciechomski", "Gilles Daviet", "Mohammad Mohajerani", "Julia von Muralt", "Viktor Reutskyy", "Michael Sauter", "Simon Schirm", "Eric L. Shi", "Pierre Terdiman", "Kenny Vilella", "Tobias Widmer", "Gordon Yeoman", "Tiffany Chen", "Sergey Grizan", "Cathy Li", "Lotus Li", "Connor Smith", "Rafael Wiltz", "Kostas Alexis", "Yan Chang", "David Chu", "Linxi \"Jim\" Fan", "Farbod Farshidian", "Ankur Handa", "Spencer Huang", "Marco Hutter", "Yashraj Narang", "Soha Pouya", "Shiwei Sheng", "Yuke Zhu", "Miles Macklin", "Adam Moravanszky", "Philipp Reist", "Yunrong Guo", "David Hoeller", "Gavriel State"], "title": "Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning", "comment": "Code and documentation are available here:\n  https://github.com/isaac-sim/IsaacLab", "summary": "We present Isaac Lab, the natural successor to Isaac Gym, which extends the\nparadigm of GPU-native robotics simulation into the era of large-scale\nmulti-modal learning. Isaac Lab combines high-fidelity GPU parallel physics,\nphotorealistic rendering, and a modular, composable architecture for designing\nenvironments and training robot policies. Beyond physics and rendering, the\nframework integrates actuator models, multi-frequency sensor simulation, data\ncollection pipelines, and domain randomization tools, unifying best practices\nfor reinforcement and imitation learning at scale within a single extensible\nplatform. We highlight its application to a diverse set of challenges,\nincluding whole-body control, cross-embodiment mobility, contact-rich and\ndexterous manipulation, and the integration of human demonstrations for skill\nacquisition. Finally, we discuss upcoming integration with the differentiable,\nGPU-accelerated Newton physics engine, which promises new opportunities for\nscalable, data-efficient, and gradient-based approaches to robot learning. We\nbelieve Isaac Lab's combination of advanced simulation capabilities, rich\nsensing, and data-center scale execution will help unlock the next generation\nof breakthroughs in robotics research.", "AI": {"tldr": "Isaac Lab, \u4f5c\u4e3aIsaac Gym\u7684\u540e\u7eed\uff0c\u662f\u4e00\u4e2a\u96c6\u6210\u4e86\u9ad8\u4fdd\u771fGPU\u5e76\u884c\u7269\u7406\u8ba1\u7b97\u3001\u903c\u771f\u6e32\u67d3\u548c\u6a21\u5757\u5316\u67b6\u6784\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u8bbe\u8ba1\u73af\u5883\u548c\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u3002\u5b83\u652f\u6301\u5927\u89c4\u6a21\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u5e76\u6574\u5408\u4e86\u6267\u884c\u5668\u6a21\u578b\u3001\u591a\u9891\u7387\u4f20\u611f\u5668\u6a21\u62df\u3001\u6570\u636e\u6536\u96c6\u7ba1\u9053\u548c\u9886\u57df\u968f\u673a\u5316\u5de5\u5177\uff0c\u7edf\u4e00\u4e86\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "motivation": "\u4e3a\u4e86\u63a8\u8fdb\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u901a\u8fc7\u7ed3\u5408\u9ad8\u8d28\u91cf\u7684\u4eff\u771f\u80fd\u529b\u3001\u4e30\u5bcc\u7684\u611f\u77e5\u80fd\u529b\u548c\u6570\u636e\u4e2d\u5fc3\u7ea7\u522b\u7684\u6267\u884c\u89c4\u6a21\u6765\u5b9e\u73b0\u4e0b\u4e00\u4ee3\u673a\u5668\u4eba\u7814\u7a76\u7684\u7a81\u7834\u3002", "method": "Isaac Lab \u63d0\u4f9b\u4e86\u4e00\u4e2a\u6269\u5c55\u6027\u5f3a\u7684\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u7ed3\u5408\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684GPU\u5e76\u884c\u7269\u7406\u6a21\u62df\u3001\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u6e32\u67d3\u4ee5\u53ca\u7528\u4e8e\u8bbe\u8ba1\u73af\u5883\u548c\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u7684\u6a21\u5757\u5316\u53ef\u7ec4\u5408\u67b6\u6784\u3002\u6b64\u5916\uff0c\u8fd8\u878d\u5165\u4e86\u6267\u884c\u5668\u6a21\u578b\u3001\u591a\u9891\u4f20\u611f\u5668\u6a21\u62df\u3001\u6570\u636e\u6536\u96c6\u6d41\u7a0b\u53ca\u57df\u968f\u673a\u5316\u5de5\u5177\u7b49\u7279\u6027\u3002", "result": "Isaac Lab \u88ab\u6210\u529f\u5e94\u7528\u4e8e\u591a\u79cd\u6311\u6218\u4e2d\uff0c\u5305\u62ec\u5168\u8eab\u63a7\u5236\u3001\u8de8\u5b9e\u4f53\u79fb\u52a8\u6027\u3001\u63a5\u89e6\u4e30\u5bcc\u4e14\u7075\u5de7\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u4ee5\u53ca\u5229\u7528\u4eba\u7c7b\u6f14\u793a\u8fdb\u884c\u6280\u80fd\u83b7\u53d6\u7b49\u65b9\u9762\u3002", "conclusion": "Isaac Lab \u7684\u5148\u8fdb\u4eff\u771f\u80fd\u529b\u3001\u4e30\u5bcc\u7684\u611f\u89c9\u4fe1\u606f\u5904\u7406\u529f\u80fd\u53ca\u5176\u5728\u6570\u636e\u4e2d\u5fc3\u89c4\u6a21\u4e0a\u7684\u6267\u884c\u6f5c\u529b\u6709\u671b\u63a8\u52a8\u673a\u5668\u4eba\u5b66\u7814\u7a76\u4e2d\u7684\u65b0\u7a81\u7834\u3002"}}
{"id": "2511.04835", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04835", "abs": "https://arxiv.org/abs/2511.04835", "authors": ["Shubham Natraj", "Bruno Sinopoli", "Yiannis Kantaros"], "title": "Conformalized Non-uniform Sampling Strategies for Accelerated Sampling-based Motion Planning", "comment": null, "summary": "Sampling-based motion planners (SBMPs) are widely used to compute dynamically\nfeasible robot paths. However, their reliance on uniform sampling often leads\nto poor efficiency and slow planning in complex environments. We introduce a\nnovel non-uniform sampling strategy that integrates into existing SBMPs by\nbiasing sampling toward `certified' regions. These regions are constructed by\n(i) generating an initial, possibly infeasible, path using any heuristic path\npredictor (e.g., A* or vision-language models) and (ii) applying conformal\nprediction to quantify the predictor's uncertainty. This process yields\nprediction sets around the initial-guess path that are guaranteed, with\nuser-specified probability, to contain the optimal solution. To our knowledge,\nthis is the first non-uniform sampling approach for SBMPs that provides such\nprobabilistically correct guarantees on the sampling regions. Extensive\nevaluations demonstrate that our method consistently finds feasible paths\nfaster and generalizes better to unseen environments than existing baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u5747\u5300\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u504f\u5411\u4e8e'\u8ba4\u8bc1'\u533a\u57df\u6765\u63d0\u9ad8\u57fa\u4e8e\u91c7\u6837\u7684\u8fd0\u52a8\u89c4\u5212\u5668(SBMPs)\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6548\u7387\u548c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u91c7\u6837\u7684\u8fd0\u52a8\u89c4\u5212\u5668(SBMPs)\u4f9d\u8d56\u4e8e\u5747\u5300\u91c7\u6837\uff0c\u8fd9\u5f80\u5f80\u5bfc\u81f4\u5728\u590d\u6742\u73af\u5883\u4e2d\u6548\u7387\u4f4e\u4e0b\u3001\u89c4\u5212\u7f13\u6162\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u5747\u5300\u91c7\u6837\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u5c06\u91c7\u6837\u504f\u5411\u4e8e'\u8ba4\u8bc1'\u533a\u57df\u800c\u4e0e\u73b0\u6709SBMP\u96c6\u6210\u3002\u8fd9\u4e9b\u533a\u57df\u662f\u901a\u8fc7\u4f7f\u7528\u4efb\u4f55\u542f\u53d1\u5f0f\u8def\u5f84\u9884\u6d4b\u5668\uff08\u5982A*\u6216\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff09\u751f\u6210\u521d\u59cb\u53ef\u80fd\u4e0d\u53ef\u884c\u7684\u8def\u5f84\uff0c\u5e76\u5e94\u7528\u4e00\u81f4\u6027\u9884\u6d4b\u6765\u91cf\u5316\u9884\u6d4b\u5668\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6784\u5efa\u7684\u3002\u6b64\u8fc7\u7a0b\u4ea7\u751f\u56f4\u7ed5\u521d\u59cb\u731c\u6d4b\u8def\u5f84\u7684\u9884\u6d4b\u96c6\uff0c\u4fdd\u8bc1\u4ee5\u7528\u6237\u6307\u5b9a\u7684\u6982\u7387\u5305\u542b\u6700\u4f18\u89e3\u3002", "result": "\u5e7f\u6cdb\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u57fa\u7ebf\u66f4\u5feb\u901f\u5730\u627e\u5230\u53ef\u884c\u8def\u5f84\uff0c\u5e76\u4e14\u5bf9\u4e8e\u672a\u89c1\u73af\u5883\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4e3aSBMPs\u63d0\u4f9b\u5173\u4e8e\u91c7\u6837\u533a\u57df\u6982\u7387\u6b63\u786e\u6027\u4fdd\u8bc1\u7684\u975e\u5747\u5300\u91c7\u6837\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u52a8\u6001\u53ef\u884c\u673a\u5668\u4eba\u8def\u5f84\u8ba1\u7b97\u7684\u901f\u5ea6\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2511.04837", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04837", "abs": "https://arxiv.org/abs/2511.04837", "authors": ["Cameron Robinson", "Ganghee Jang"], "title": "Design Exploration for Protection and Cleaning of Solar Panels with Case Studies for Space Missions", "comment": "4 pages, 3 figures (5 assets)", "summary": "Solar energy is used for many mission-critical applications including space\nexploration, sensor systems to monitor wildfires, etc. Their operation can be\nlimited or even terminated if solar panels are covered with dust or hit by\nspace debris. To address this issue, we designed panel cleaning mechanisms and\ntested protective materials. For cleaning mechanisms, we designed and compared\na wiper system and a rail system. For protective materials, we found through\ncollision tests that polycarbonate was very promising, though the most\nimportant factor was layering a soft material between the panel's surface and a\nhard material. In the cleaning system comparisons, the wiper-based system was\nmore efficient than the rail-based system in terms of cost, cleaning speed, and\ntotal power consumption.", "AI": {"tldr": "\u8bbe\u8ba1\u5e76\u6d4b\u8bd5\u4e86\u592a\u9633\u80fd\u677f\u7684\u6e05\u6d01\u673a\u5236\u548c\u4fdd\u62a4\u6750\u6599\uff0c\u53d1\u73b0\u96e8\u5237\u7cfb\u7edf\u6bd4\u8f68\u9053\u7cfb\u7edf\u66f4\u9ad8\u6548\uff0c\u805a\u78b3\u9178\u916f\u4f5c\u4e3a\u4fdd\u62a4\u6750\u6599\u5f88\u6709\u524d\u666f\uff0c\u4f46\u6700\u91cd\u8981\u7684\u662f\u5728\u9762\u677f\u8868\u9762\u548c\u786c\u8d28\u6750\u6599\u4e4b\u95f4\u94fa\u8bbe\u4e00\u5c42\u8f6f\u6750\u6599\u3002", "motivation": "\u89e3\u51b3\u592a\u9633\u80fd\u677f\u56e0\u7070\u5c18\u8986\u76d6\u6216\u592a\u7a7a\u788e\u7247\u649e\u51fb\u800c\u5bfc\u81f4\u7684\u64cd\u4f5c\u53d7\u9650\u6216\u7ec8\u6b62\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5e76\u6bd4\u8f83\u4e86\u96e8\u5237\u7cfb\u7edf\u548c\u8f68\u9053\u7cfb\u7edf\u4e24\u79cd\u6e05\u6d01\u673a\u5236\uff1b\u901a\u8fc7\u78b0\u649e\u6d4b\u8bd5\u8bc4\u4f30\u4e0d\u540c\u4fdd\u62a4\u6750\u6599\u7684\u6548\u679c\u3002", "result": "\u96e8\u5237\u7cfb\u7edf\u5728\u6210\u672c\u3001\u6e05\u6d01\u901f\u5ea6\u548c\u603b\u529f\u8017\u65b9\u9762\u4f18\u4e8e\u8f68\u9053\u7cfb\u7edf\uff1b\u805a\u78b3\u9178\u916f\u4f5c\u4e3a\u4fdd\u62a4\u6750\u6599\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9762\u677f\u8868\u9762\u4e0e\u786c\u8d28\u6750\u6599\u95f4\u589e\u52a0\u4e00\u5c42\u8f6f\u6750\u6599\u6700\u4e3a\u5173\u952e\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6e05\u6d01\u673a\u5236\u548c\u4fdd\u62a4\u6750\u6599\u65b9\u6848\u80fd\u591f\u6709\u6548\u63d0\u5347\u592a\u9633\u80fd\u677f\u5728\u6076\u52a3\u73af\u5883\u4e0b\u7684\u5de5\u4f5c\u6027\u80fd\u4e0e\u5bff\u547d\u3002"}}
{"id": "2511.04976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04976", "abs": "https://arxiv.org/abs/2511.04976", "authors": ["Xin Nie", "Zhiyuan Cheng", "Yuan Zhang", "Chao Ji", "Jiajia Wu", "Yuhan Zhang", "Jia Pan"], "title": "iFlyBot-VLM Technical Report", "comment": null, "summary": "We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) used\nto improve the domain of Embodied Intelligence. The central objective of\niFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional\nenvironmental perception and low-level robotic motion control. To this end, the\nmodel abstracts complex visual and spatial information into a body-agnostic and\ntransferable Operational Language, thereby enabling seamless perception-action\nclosed-loop coordination across diverse robotic platforms. The architecture of\niFlyBot-VLM is systematically designed to realize four key functional\ncapabilities essential for embodied intelligence: 1) Spatial Understanding and\nMetric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and\nControl Parameter Generation; 4) Task Planning and Skill Sequencing. We\nenvision iFlyBot-VLM as a scalable and generalizable foundation model for\nembodied AI, facilitating the progression from specialized task-oriented\nsystems toward generalist, cognitively capable agents. We conducted evaluations\non 10 current mainstream embodied intelligence-related VLM benchmark datasets,\nsuch as Blink and Where2Place, and achieved optimal performance while\npreserving the model's general capabilities. We will publicly release both the\ntraining data and model weights to foster further research and development in\nthe field of Embodied Intelligence.", "AI": {"tldr": "iFlyBot-VLM, a Vision-Language Model, aims to improve Embodied Intelligence by bridging the gap between high-dimensional perception and low-level robotic control. It abstracts information into an Operational Language, enabling coordination across various robotic platforms. The model is designed for four key capabilities: Spatial Understanding, Interactive Target Grounding, Action Abstraction, and Task Planning. It performed well on 10 benchmark datasets and will be publicly released to support further research.", "motivation": "The motivation behind iFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional environmental perception and low-level robotic motion control, thus improving the domain of Embodied Intelligence. This advancement is expected to facilitate the progression from specialized task-oriented systems towards more generalist, cognitively capable agents.", "method": "iFlyBot-VLM is designed with a systematic architecture that enables it to perform four key functions: 1) Spatial Understanding and Metric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and Control Parameter Generation; 4) Task Planning and Skill Sequencing. The model abstracts complex visual and spatial information into a body-agnostic and transferable Operational Language, which allows for seamless perception-action closed-loop coordination across different robotic platforms.", "result": "iFlyBot-VLM achieved optimal performance when evaluated on 10 mainstream embodied intelligence-related VLM benchmark datasets, including Blink and Where2Place. The model maintained its general capabilities throughout these tests, demonstrating its effectiveness and adaptability in various scenarios.", "conclusion": "iFlyBot-VLM represents a scalable and generalizable foundation model for embodied AI, with the potential to significantly advance the field of Embodied Intelligence. By facilitating the transition from specialized task-oriented systems to generalist, cognitively capable agents, iFlyBot-VLM opens up new possibilities for research and development. The public release of the training data and model weights will further promote progress in this area."}}
{"id": "2511.04992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04992", "abs": "https://arxiv.org/abs/2511.04992", "authors": ["Bibekananda Patra", "Sandipan Bandyopadhyay"], "title": "A semi-analytical approach for computing the largest singularity-free spheres of a class of 6-6 Stewart-Gough platforms for specified orientation workspaces", "comment": null, "summary": "This article presents a method for computing the largest singularity-free\nsphere (SFS) of a 6-6 Stewart-Gough platform manipulator (SGPM) over a\nspecified orientation workspace. For a fixed orientation of the moving\nplatform, the SFS is computed analytically. This process is repeated over a set\nof samples generated within the orientation workspace, and the smallest among\nthem is designated as the desired SFS for the given orientation workspace.\nNumerical experiments are performed on four distinct architectures of the SGPM\nto understand their relative performances w.r.t. SFS volumes over the same\norientation workspace. This study demonstrates the potential utility of the\nproposed computational method both in analysis and design of SGPMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b976-6 Stewart-Gough\u5e73\u53f0\u64cd\u4f5c\u5668\u5728\u6307\u5b9a\u59ff\u6001\u5de5\u4f5c\u7a7a\u95f4\u5185\u6700\u5927\u65e0\u5947\u70b9\u7403\u4f53\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u5206\u6790\u548c\u8bbe\u8ba1SGPM\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u662f\u4e3a\u4e86\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u7279\u5b9a\u7684\u59ff\u6001\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u786e\u5b9aStewart-Gough\u5e73\u53f0\u64cd\u4f5c\u5668\uff08SGPM\uff09\u7684\u6700\u5927\u65e0\u5947\u70b9\u7403\u4f53\uff08SFS\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u5e2e\u52a9\u5206\u6790\u548c\u8bbe\u8ba1SGPM\u3002", "method": "\u5bf9\u4e8e\u79fb\u52a8\u5e73\u53f0\u7684\u56fa\u5b9a\u59ff\u6001\uff0c\u901a\u8fc7\u89e3\u6790\u65b9\u6cd5\u8ba1\u7b97SFS\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u4f1a\u5728\u59ff\u6001\u5de5\u4f5c\u7a7a\u95f4\u7684\u4e00\u7ec4\u6837\u672c\u4e0a\u91cd\u590d\u8fdb\u884c\uff0c\u5176\u4e2d\u6700\u5c0f\u7684\u4e00\u4e2a\u88ab\u6307\u5b9a\u4e3a\u7ed9\u5b9a\u59ff\u6001\u5de5\u4f5c\u7a7a\u95f4\u6240\u8981\u6c42\u7684SFS\u3002", "result": "\u5bf9\u56db\u79cd\u4e0d\u540c\u7684SGPM\u67b6\u6784\u8fdb\u884c\u4e86\u6570\u503c\u5b9e\u9a8c\uff0c\u4ee5\u4e86\u89e3\u5b83\u4eec\u5728\u540c\u4e00\u59ff\u6001\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u76f8\u5bf9\u4e8eSFS\u4f53\u79ef\u7684\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8ba1\u7b97\u65b9\u6cd5\u5728SGPM\u7684\u5206\u6790\u548c\u8bbe\u8ba1\u65b9\u9762\u5177\u6709\u6f5c\u5728\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.05007", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05007", "abs": "https://arxiv.org/abs/2511.05007", "authors": ["Baiye Cheng", "Tianhai Liang", "Suning Huang", "Maanping Shao", "Feihong Zhang", "Botian Xu", "Zhengrong Xue", "Huazhe Xu"], "title": "MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery", "comment": null, "summary": "Diffusion policies have emerged as a powerful framework for robotic\nvisuomotor control, yet they often lack the robustness to recover from subtask\nfailures in long-horizon, multi-stage tasks and their learned representations\nof observations are often difficult to interpret. In this work, we propose the\nMixture of Experts-Enhanced Diffusion Policy (MoE-DP), where the core idea is\nto insert a Mixture of Experts (MoE) layer between the visual encoder and the\ndiffusion model. This layer decomposes the policy's knowledge into a set of\nspecialized experts, which are dynamically activated to handle different phases\nof a task. We demonstrate through extensive experiments that MoE-DP exhibits a\nstrong capability to recover from disturbances, significantly outperforming\nstandard baselines in robustness. On a suite of 6 long-horizon simulation\ntasks, this leads to a 36% average relative improvement in success rate under\ndisturbed conditions. This enhanced robustness is further validated in the real\nworld, where MoE-DP also shows significant performance gains. We further show\nthat MoE-DP learns an interpretable skill decomposition, where distinct experts\ncorrespond to semantic task primitives (e.g., approaching, grasping). This\nlearned structure can be leveraged for inference-time control, allowing for the\nrearrangement of subtasks without any re-training.Our video and code are\navailable at the https://moe-dp-website.github.io/MoE-DP-Website/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoE-DP\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u89c6\u89c9\u7f16\u7801\u5668\u548c\u6269\u6563\u6a21\u578b\u4e4b\u95f4\u63d2\u5165\u4e00\u4e2a\u4e13\u5bb6\u6df7\u5408\u5c42\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u6267\u884c\u957f\u65f6\u591a\u9636\u6bb5\u4efb\u52a1\u65f6\u4ece\u5b50\u4efb\u52a1\u5931\u8d25\u4e2d\u6062\u590d\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u53d7\u5230\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\u6bd4\u6807\u51c6\u57fa\u7ebf\u6709\u663e\u8457\u7684\u9c81\u68d2\u6027\u6539\u8fdb\uff0c\u5e76\u4e14\u5b66\u5230\u4e86\u53ef\u89e3\u91ca\u7684\u4efb\u52a1\u6280\u80fd\u5206\u89e3\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u7b56\u7565\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u5728\u5904\u7406\u957f\u65f6\u95f4\u3001\u591a\u9636\u6bb5\u4efb\u52a1\u65f6\u7f3a\u4e4f\u4ece\u5b50\u4efb\u52a1\u5931\u8d25\u4e2d\u6062\u590d\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u5176\u5b66\u4e60\u5230\u7684\u89c2\u5bdf\u8868\u793a\u96be\u4ee5\u89e3\u91ca\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86MoE-DP\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5728\u89c6\u89c9\u7f16\u7801\u5668\u4e0e\u6269\u6563\u6a21\u578b\u4e4b\u95f4\u5f15\u5165\u4e00\u4e2aMixture of Experts (MoE) \u5c42\uff0c\u5c06\u7b56\u7565\u77e5\u8bc6\u5206\u89e3\u4e3a\u4e00\u7ec4\u4e13\u95e8\u5316\u7684\u4e13\u5bb6\uff0c\u8fd9\u4e9b\u4e13\u5bb6\u53ef\u4ee5\u6839\u636e\u4efb\u52a1\u7684\u4e0d\u540c\u9636\u6bb5\u52a8\u6001\u6fc0\u6d3b\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u7cfb\u7edf\u5bf9\u5e72\u6270\u60c5\u51b5\u4e0b\u7684\u6062\u590d\u80fd\u529b\u3002", "result": "\u5728\u516d\u4e2a\u957f\u65f6\u95f4\u6a21\u62df\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u53d7\u5e72\u6270\u6761\u4ef6\u4e0b\uff0cMoE-DP\u76f8\u6bd4\u6807\u51c6\u57fa\u7ebf\u5e73\u5747\u76f8\u5bf9\u63d0\u9ad8\u4e8636%\u7684\u6210\u529f\u7387\uff1b\u6b64\u5916\uff0c\u5b83\u8fd8\u5c55\u793a\u4e86\u53ef\u89e3\u91ca\u6027\u7684\u6280\u80fd\u5206\u89e3\uff0c\u4e0d\u540c\u7684\u4e13\u5bb6\u5bf9\u5e94\u7740\u8bed\u4e49\u4efb\u52a1\u539f\u8bed\uff08\u4f8b\u5982\u63a5\u8fd1\u3001\u6293\u53d6\uff09\u3002", "conclusion": "MoE-DP\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u6267\u884c\u590d\u6742\u4efb\u52a1\u65f6\u9762\u5bf9\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u800c\u4e14\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6613\u4e8e\u7406\u89e3\u548c\u8c03\u6574\u7684\u5b66\u4e60\u7ed3\u6784\uff0c\u8fd9\u4f7f\u5f97\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8c03\u6574\u5b50\u4efb\u52a1\u987a\u5e8f\u3002"}}
{"id": "2511.05033", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05033", "abs": "https://arxiv.org/abs/2511.05033", "authors": ["Jennifer K. Leestma", "Siddharth R. Nathella", "Christoph P. O. Nuesslein", "Snehil Mathur", "Gregory S. Sawicki", "Aaron J. Young"], "title": "Epically Powerful: An open-source software and mechatronics infrastructure for wearable robotic systems", "comment": "11 pages, 5 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Epically Powerful is an open-source robotics infrastructure that streamlines\nthe underlying framework of wearable robotic systems - managing communication\nprotocols, clocking, actuator commands, visualization, sensor data acquisition,\ndata logging, and more - while also providing comprehensive guides for hardware\nselection, system assembly, and controller implementation. Epically Powerful\ncontains a code base enabling simplified user implementation via Python that\nseamlessly interfaces with various commercial state-of-the-art quasi-direct\ndrive (QDD) actuators, single-board computers, and common sensors, provides\nexample controllers, and enables real-time visualization. To further support\ndevice development, the package also includes a recommended parts list and\ncompatibility guide and detailed documentation on hardware and software\nimplementation. The goal of Epically Powerful is to lower the barrier to\ndeveloping and deploying custom wearable robotic systems without a\npre-specified form factor, enabling researchers to go from raw hardware to\nmodular, robust devices quickly and effectively. Though originally designed\nwith wearable robotics in mind, Epically Powerful is broadly applicable to\nother robotic domains that utilize QDD actuators, single-board computers, and\nsensors for closed-loop control.", "AI": {"tldr": "Epically Powerful\u662f\u4e00\u4e2a\u5f00\u6e90\u673a\u5668\u4eba\u57fa\u7840\u8bbe\u65bd\uff0c\u65e8\u5728\u7b80\u5316\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5e95\u5c42\u6846\u67b6\u7ba1\u7406\uff0c\u5e76\u63d0\u4f9b\u4ece\u786c\u4ef6\u9009\u62e9\u5230\u63a7\u5236\u5668\u5b9e\u73b0\u7684\u5168\u9762\u6307\u5357\u3002\u5b83\u652f\u6301Python\u7f16\u7a0b\u63a5\u53e3\u3001\u4e0e\u591a\u79cd\u5546\u7528\u6267\u884c\u5668\u548c\u4f20\u611f\u5668\u65e0\u7f1d\u5bf9\u63a5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u65f6\u53ef\u89c6\u5316\u529f\u80fd\uff0c\u4ee5\u52a0\u901f\u5b9a\u5236\u5316\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8bbe\u8ba1\u4e0e\u53d1\u5c55\u8fc7\u7a0b\u3002", "motivation": "\u964d\u4f4e\u5f00\u53d1\u548c\u90e8\u7f72\u81ea\u5b9a\u4e49\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u95e8\u69db\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5feb\u901f\u6709\u6548\u5730\u5c06\u539f\u59cb\u786c\u4ef6\u8f6c\u5316\u4e3a\u6a21\u5757\u5316\u4e14\u575a\u56fa\u8010\u7528\u7684\u8bbe\u5907\uff0c\u540c\u65f6\u8be5\u5e73\u53f0\u5bf9\u5176\u4ed6\u4f7f\u7528\u7c7b\u4f3c\u7ec4\u4ef6\u8fdb\u884c\u95ed\u73af\u63a7\u5236\u7684\u673a\u5668\u4eba\u9886\u57df\u4e5f\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u4ee3\u7801\u5e93\u6765\u7b80\u5316\u7528\u6237\u7aef\u7684\u5b9e\u65bd\uff0c\u8fd9\u4e2a\u4ee3\u7801\u5e93\u53ef\u4ee5\u901a\u8fc7Python\u8f7b\u677e\u5730\u4e0e\u5404\u79cd\u5546\u4e1a\u4e0a\u6700\u5148\u8fdb\u7684\u51c6\u76f4\u9a71\uff08QDD\uff09\u6267\u884c\u5668\u3001\u5355\u677f\u8ba1\u7b97\u673a\u4ee5\u53ca\u5e38\u89c1\u4f20\u611f\u5668\u63a5\u53e3\uff1b\u63d0\u4f9b\u793a\u4f8b\u63a7\u5236\u5668\uff1b\u5e76\u542f\u7528\u5b9e\u65f6\u53ef\u89c6\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u5305\u542b\u63a8\u8350\u96f6\u4ef6\u5217\u8868\u548c\u517c\u5bb9\u6027\u6307\u5357\u4ee5\u53ca\u8be6\u7ec6\u7684\u8f6f\u786c\u4ef6\u5b9e\u73b0\u6587\u6863\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aEpically Powerful\u7684\u5f00\u653e\u6e90\u7801\u673a\u5668\u4eba\u57fa\u7840\u8bbe\u65bd\uff0c\u5b83\u4e0d\u4ec5\u5904\u7406\u4e86\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u57fa\u7840\u67b6\u6784\u5c42\u9762\u95ee\u9898\uff0c\u5982\u901a\u4fe1\u534f\u8bae\u7ba1\u7406\u548c\u6570\u636e\u8bb0\u5f55\u7b49\uff0c\u800c\u4e14\u8fd8\u4fc3\u8fdb\u4e86\u4ece\u786c\u4ef6\u9009\u62e9\u5230\u6700\u7ec8\u63a7\u5236\u7cfb\u7edf\u5b9e\u73b0\u6574\u4e2a\u6d41\u7a0b\u4e2d\u7684\u7b80\u4fbf\u6027\u548c\u6548\u7387\u3002", "conclusion": "Epically Powerful\u6210\u529f\u5730\u4e3a\u60f3\u8981\u5f00\u53d1\u81ea\u5df1\u7684\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u7684\u7814\u7a76\u8005\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u800c\u7075\u6d3b\u7684\u8d77\u70b9\uff0c\u540c\u65f6\u4e5f\u4e3a\u5176\u4ed6\u4f9d\u8d56\u4e8eQDD\u6267\u884c\u5668\u3001\u5355\u677f\u8ba1\u7b97\u673a\u53ca\u4f20\u611f\u5668\u6280\u672f\u7684\u673a\u5668\u4eba\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.05052", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.05052", "abs": "https://arxiv.org/abs/2511.05052", "authors": ["Zihao Li", "Yiming Zhu", "Zhe Zhong", "Qinyuan Ren", "Yijiang Huang"], "title": "TAPOM: Task-Space Topology-Guided Motion Planning for Manipulating Elongated Object in Cluttered Environments", "comment": null, "summary": "Robotic manipulation in complex, constrained spaces is vital for widespread\napplications but challenging, particularly when navigating narrow passages with\nelongated objects. Existing planning methods often fail in these low-clearance\nscenarios due to the sampling difficulties or the local minima. This work\nproposes Topology-Aware Planning for Object Manipulation (TAPOM), which\nexplicitly incorporates task-space topological analysis to enable efficient\nplanning. TAPOM uses a high-level analysis to identify critical pathways and\ngenerate guiding keyframes, which are utilized in a low-level planner to find\nfeasible configuration space trajectories. Experimental validation demonstrates\nsignificantly high success rates and improved efficiency over state-of-the-art\nmethods on low-clearance manipulation tasks. This approach offers broad\nimplications for enhancing manipulation capabilities of robots in complex\nreal-world environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7269\u4f53\u64cd\u4f5c\u89c4\u5212\u65b9\u6cd5TAPOM\uff0c\u901a\u8fc7\u4efb\u52a1\u7a7a\u95f4\u7684\u62d3\u6251\u5206\u6790\u6765\u8bc6\u522b\u5173\u952e\u8def\u5f84\u5e76\u751f\u6210\u5f15\u5bfc\u5173\u952e\u5e27\uff0c\u4ece\u800c\u5728\u72ed\u7a84\u901a\u9053\u4e2d\u66f4\u9ad8\u6548\u5730\u8fdb\u884c\u673a\u5668\u4eba\u64cd\u4f5c\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u51c0\u7a7a\u64cd\u4f5c\u4efb\u52a1\u4e0a\u6bd4\u73b0\u6709\u6280\u672f\u5177\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u5728\u590d\u6742\u7684\u53d7\u9650\u7a7a\u95f4\u4e2d\u6267\u884c\u673a\u5668\u4eba\u64cd\u4f5c\u5bf9\u4e8e\u8bb8\u591a\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5c24\u5176\u5728\u7528\u7ec6\u957f\u7269\u4f53\u7a7f\u8fc7\u72ed\u7a84\u901a\u9053\u65f6\u6781\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u89c4\u5212\u65b9\u6cd5\u7531\u4e8e\u91c7\u6837\u56f0\u96be\u6216\u5c40\u90e8\u6781\u5c0f\u503c\u95ee\u9898\uff0c\u5728\u8fd9\u4e9b\u4f4e\u51c0\u7a7a\u573a\u666f\u4e2d\u5f80\u5f80\u5931\u8d25\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u540d\u4e3aTAPOM\u7684\u65b9\u6cd5\uff0c\u5b83\u663e\u5f0f\u5730\u7ed3\u5408\u4e86\u4efb\u52a1\u7a7a\u95f4\u7684\u62d3\u6251\u5206\u6790\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u89c4\u5212\u3002TAPOM\u91c7\u7528\u9ad8\u5c42\u6b21\u5206\u6790\u6765\u8bc6\u522b\u5173\u952e\u8def\u5f84\u5e76\u751f\u6210\u5f15\u5bfc\u5173\u952e\u5e27\uff0c\u8fd9\u4e9b\u5173\u952e\u5e27\u88ab\u7528\u4e8e\u4f4e\u5c42\u6b21\u89c4\u5212\u5668\u4ee5\u627e\u5230\u53ef\u884c\u7684\u914d\u7f6e\u7a7a\u95f4\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0cTAPOM\u5728\u4f4e\u51c0\u7a7a\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u6539\u8fdb\u7684\u6548\u7387\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u63d0\u9ad8\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u80fd\u529b\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u610f\u4e49\u3002"}}
{"id": "2511.05129", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05129", "abs": "https://arxiv.org/abs/2511.05129", "authors": ["Bin Fan", "Jianjian Jiang", "Zhuohao Li", "Yixiang He", "Xiaoming Wu", "Yihan Yang", "Shengbang Liu", "Weishi Zheng"], "title": "Decomposed Object Manipulation via Dual-Actor Policy", "comment": "9 pages, 7 figures, 5 tables", "summary": "Object manipulation, which focuses on learning to perform tasks on similar\nparts across different types of objects, can be divided into an approaching\nstage and a manipulation stage. However, previous works often ignore this\ncharacteristic of the task and rely on a single policy to directly learn the\nwhole process of object manipulation. To address this problem, we propose a\nnovel Dual-Actor Policy, termed DAP, which explicitly considers different\nstages and leverages heterogeneous visual priors to enhance each stage.\nSpecifically, we introduce an affordance-based actor to locate the functional\npart in the manipulation task, thereby improving the approaching process.\nFollowing this, we propose a motion flow-based actor to capture the movement of\nthe component, facilitating the manipulation process. Finally, we introduce a\ndecision maker to determine the current stage of DAP and select the\ncorresponding actor. Moreover, existing object manipulation datasets contain\nfew objects and lack the visual priors needed to support training. To address\nthis, we construct a simulated dataset, the Dual-Prior Object Manipulation\nDataset, which combines the two visual priors and includes seven tasks,\nincluding two challenging long-term, multi-stage tasks. Experimental results on\nour dataset, the RoboTwin benchmark and real-world scenarios illustrate that\nour method consistently outperforms the SOTA method by 5.55%, 14.7% and 10.4%\non average respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Dual-Actor Policy\uff08DAP\uff09\uff0c\u5b83\u901a\u8fc7\u5229\u7528\u4e0d\u540c\u7684\u89c6\u89c9\u5148\u9a8c\u6765\u6539\u8fdb\u5bf9\u8c61\u64cd\u4f5c\u4efb\u52a1\u7684\u4e0d\u540c\u9636\u6bb5\uff0c\u5305\u62ec\u63a5\u8fd1\u9636\u6bb5\u548c\u64cd\u4f5c\u9636\u6bb5\uff0c\u5e76\u4e14\u521b\u5efa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u8fd9\u4e24\u79cd\u89c6\u89c9\u5148\u9a8c\u7684\u6a21\u62df\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u4ee5\u5f80\u7684\u5bf9\u8c61\u64cd\u4f5c\u7814\u7a76\u5f80\u5f80\u5ffd\u89c6\u4e86\u4efb\u52a1\u4e2d\u7684\u4e0d\u540c\u9636\u6bb5\u7279\u6027\uff0c\u4f9d\u8d56\u5355\u4e00\u7b56\u7565\u76f4\u63a5\u5b66\u4e60\u6574\u4e2a\u64cd\u4f5c\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u6570\u636e\u96c6\u7f3a\u4e4f\u652f\u6301\u8bad\u7ec3\u6240\u9700\u7684\u89c6\u89c9\u5148\u9a8c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Dual-Actor Policy (DAP) \u548c\u4e00\u4e2a\u65b0\u7684\u6a21\u62df\u6570\u636e\u96c6\u3002", "method": "\u5f00\u53d1\u4e86Dual-Actor Policy (DAP)\uff0c\u5b83\u5305\u542b\u57fa\u4e8e\u529f\u80fd\u6027\u7684\u6267\u884c\u8005\u7528\u4e8e\u5b9a\u4f4d\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u529f\u80fd\u6027\u90e8\u5206\u4ee5\u6539\u5584\u63a5\u8fd1\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8fd0\u52a8\u6d41\u7684\u6267\u884c\u8005\u6355\u6349\u7ec4\u4ef6\u79fb\u52a8\u4ee5\u4fc3\u8fdb\u64cd\u4f5c\u8fc7\u7a0b\u3002\u53e6\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u51b3\u7b56\u5236\u5b9a\u8005\u6765\u5224\u65ad\u5f53\u524d\u9636\u6bb5\u5e76\u9009\u62e9\u76f8\u5e94\u7684\u6267\u884c\u8005\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aDual-Prior Object Manipulation Dataset\u7684\u65b0\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u7ed3\u5408\u4e86\u4e24\u79cd\u89c6\u89c9\u5148\u9a8c\uff0c\u5e76\u5305\u542b\u4e86\u4e03\u4e2a\u4efb\u52a1\uff0c\u5176\u4e2d\u4e24\u4e2a\u662f\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u671f\u591a\u9636\u6bb5\u4efb\u52a1\u3002", "result": "\u5728\u81ea\u5efa\u7684\u6570\u636e\u96c6\u3001RoboTwin\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb(SOTA)\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5206\u522b\u5e73\u5747\u63d0\u9ad8\u4e865.55%\u300114.7%\u548c10.4%\u7684\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684Dual-Actor Policy (DAP) \u901a\u8fc7\u660e\u786e\u8003\u8651\u5bf9\u8c61\u64cd\u4f5c\u4e2d\u7684\u4e0d\u540c\u9636\u6bb5\uff0c\u5e76\u5229\u7528\u5f02\u6784\u89c6\u89c9\u5148\u9a8c\u589e\u5f3a\u4e86\u6bcf\u4e2a\u9636\u6bb5\u7684\u8868\u73b0\u3002\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e5f\u4fc3\u8fdb\u4e86\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.05185", "categories": ["cs.RO", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.05185", "abs": "https://arxiv.org/abs/2511.05185", "authors": ["Adri\u00e1n Campazas-Vega", "Claudia \u00c1lvarez-Aparicio", "David Sobr\u00edn-Hidalgo", "Laura Inyesto-Alonso", "Francisco Javier Rodr\u00edguez-Lera", "Vicente Matell\u00e1n-Olivera", "\u00c1ngel Manuel Guerrero-Higueras"], "title": "Procedimiento de auditor\u00eda de ciberseguridad para sistemas aut\u00f3nomos: metodolog\u00eda, amenazas y mitigaciones", "comment": "32 pages, in Spanish language, 7 tables, 12 Figures. White paper\n  under the TESCAC project", "summary": "The deployment of autonomous systems has experienced remarkable growth in\nrecent years, driven by their integration into sectors such as industry,\nmedicine, logistics, and domestic environments. This expansion is accompanied\nby a series of security issues that entail significant risks due to the\ncritical nature of autonomous systems, especially those operating in\nhuman-interaction environments. Furthermore, technological advancement and the\nhigh operational and architectural complexity of autonomous systems have\nresulted in an increased attack surface. This article presents a specific\nsecurity auditing procedure for autonomous systems, based on a layer-structured\nmethodology, a threat taxonomy adapted to the robotic context, and a set of\nconcrete mitigation measures. The validity of the proposed approach is\ndemonstrated through four practical case studies applied to representative\nrobotic platforms: the Vision 60 military quadruped from Ghost Robotics, the A1\nrobot from Unitree Robotics, the UR3 collaborative arm from Universal Robots,\nand the Pepper social robot from Aldebaran Robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u4e3b\u7cfb\u7edf\u7684\u7279\u5b9a\u5b89\u5168\u5ba1\u8ba1\u7a0b\u5e8f\uff0c\u8be5\u7a0b\u5e8f\u57fa\u4e8e\u5206\u5c42\u7ed3\u6784\u7684\u65b9\u6cd5\u8bba\u3001\u9002\u5e94\u4e8e\u673a\u5668\u4eba\u73af\u5883\u7684\u5a01\u80c1\u5206\u7c7b\u4ee5\u53ca\u4e00\u7cfb\u5217\u5177\u4f53\u7684\u7f13\u89e3\u63aa\u65bd\u3002\u901a\u8fc7\u56db\u4e2a\u5b9e\u9645\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u7cfb\u7edf\u5728\u5de5\u4e1a\u3001\u533b\u5b66\u3001\u7269\u6d41\u548c\u5bb6\u5ead\u73af\u5883\u7b49\u9886\u57df\u7684\u96c6\u6210\u65e5\u76ca\u589e\u591a\uff0c\u5176\u9762\u4e34\u7684\u5b89\u5168\u95ee\u9898\u4e5f\u6108\u52a0\u7a81\u51fa\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u5728\u4eba\u7c7b\u4ea4\u4e92\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u7cfb\u7edf\u3002\u6280\u672f\u8fdb\u6b65\u548c\u81ea\u4e3b\u7cfb\u7edf\u7684\u9ad8\u64cd\u4f5c\u53ca\u67b6\u6784\u590d\u6742\u6027\u5bfc\u81f4\u4e86\u653b\u51fb\u9762\u7684\u6269\u5927\u3002", "method": "\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4e13\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u8bbe\u8ba1\u7684\u5b89\u5168\u5ba1\u8ba1\u7a0b\u5e8f\uff0c\u8be5\u7a0b\u5e8f\u4f9d\u8d56\u4e8e\u4e00\u79cd\u5206\u5c42\u7ed3\u6784\u7684\u65b9\u6cd5\u8bba\uff0c\u5e76\u4e14\u5305\u542b\u4e86\u7279\u522b\u8c03\u6574\u4ee5\u9002\u7528\u4e8e\u673a\u5668\u4eba\u80cc\u666f\u4e0b\u7684\u5a01\u80c1\u5206\u7c7b\u6cd5\u4ee5\u53ca\u4e00\u7ec4\u5177\u4f53\u7684\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u901a\u8fc7\u5e94\u7528\u4e8e\u56db\u4e2a\u4ee3\u8868\u6027\u673a\u5668\u4eba\u5e73\u53f0\u7684\u5b9e\u9645\u6848\u4f8b\u7814\u7a76\u5f97\u5230\u4e86\u9a8c\u8bc1\uff1aGhost Robotics\u516c\u53f8\u7684Vision 60\u519b\u7528\u56db\u8db3\u673a\u5668\u4eba\u3001Unitree Robotics\u516c\u53f8\u7684A1\u673a\u5668\u4eba\u3001Universal Robots\u516c\u53f8\u7684UR3\u534f\u4f5c\u81c2\u4ee5\u53caAldebaran Robotics\u516c\u53f8\u7684Pepper\u793e\u4ea4\u673a\u5668\u4eba\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u5957\u5168\u9762\u7684\u5b89\u5168\u5ba1\u8ba1\u6846\u67b6\u6765\u8bc4\u4f30\u5e76\u589e\u5f3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2511.05199", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05199", "abs": "https://arxiv.org/abs/2511.05199", "authors": ["Yichen Zhu", "Feifei Feng"], "title": "Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation", "comment": "Accepted by IROS 2025", "summary": "Robots operating in complex and uncertain environments face considerable\nchallenges. Advanced robotic systems often rely on extensive datasets to learn\nmanipulation tasks. In contrast, when humans are faced with unfamiliar tasks,\nsuch as assembling a chair, a common approach is to learn by watching video\ndemonstrations. In this paper, we propose a novel method for learning robot\npolicies by Retrieving-from-Video (RfV), using analogies from human\ndemonstrations to address manipulation tasks. Our system constructs a video\nbank comprising recordings of humans performing diverse daily tasks. To enrich\nthe knowledge from these videos, we extract mid-level information, such as\nobject affordance masks and hand motion trajectories, which serve as additional\ninputs to enhance the robot model's learning and generalization capabilities.\nWe further feature a dual-component system: a video retriever that taps into an\nexternal video bank to fetch task-relevant video based on task specification,\nand a policy generator that integrates this retrieved knowledge into the\nlearning cycle. This approach enables robots to craft adaptive responses to\nvarious scenarios and generalize to tasks beyond those in the training data.\nThrough rigorous testing in multiple simulated and real-world settings, our\nsystem demonstrates a marked improvement in performance over conventional\nrobotic systems, showcasing a significant breakthrough in the field of\nrobotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4ece\u89c6\u9891\u4e2d\u68c0\u7d22\uff08RfV\uff09\u5b66\u4e60\u673a\u5668\u4eba\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u4eba\u7c7b\u6f14\u793a\u7684\u7c7b\u6bd4\u6765\u89e3\u51b3\u64cd\u4f5c\u4efb\u52a1\u3002\u7cfb\u7edf\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4eba\u7c7b\u6267\u884c\u591a\u6837\u5316\u65e5\u5e38\u4efb\u52a1\u7684\u89c6\u9891\u5e93\uff0c\u5e76\u4ece\u4e2d\u63d0\u53d6\u4e2d\u7ea7\u4fe1\u606f\u4f5c\u4e3a\u989d\u5916\u8f93\u5165\u4ee5\u589e\u5f3a\u673a\u5668\u4eba\u6a21\u578b\u7684\u5b66\u4e60\u548c\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u7cfb\u7edf\u5305\u62ec\u4e00\u4e2a\u6839\u636e\u4efb\u52a1\u89c4\u683c\u4ece\u5916\u90e8\u89c6\u9891\u5e93\u83b7\u53d6\u76f8\u5173\u89c6\u9891\u7684\u89c6\u9891\u68c0\u7d22\u5668\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5c06\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\u6574\u5408\u8fdb\u5b66\u4e60\u5468\u671f\u7684\u7b56\u7565\u751f\u6210\u5668\u3002\u901a\u8fc7\u5728\u591a\u4e2a\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u4e25\u683c\u6d4b\u8bd5\uff0c\u8be5\u7cfb\u7edf\u76f8\u8f83\u4e8e\u4f20\u7edf\u673a\u5668\u4eba\u7cfb\u7edf\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u9488\u5bf9\u590d\u6742\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u64cd\u4f5c\u7684\u673a\u5668\u4eba\u9762\u4e34\u7684\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u901a\u8fc7\u89c2\u770b\u89c6\u9891\u6f14\u793a\u6765\u5b66\u4e60\u5e76\u5b8c\u6210\u65b0\u4efb\u52a1\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u6a21\u4eff\u4eba\u7c7b\u5728\u9762\u5bf9\u4e0d\u719f\u6089\u4efb\u52a1\u65f6\u7684\u5b66\u4e60\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRetrieving-from-Video (RfV) \u7684\u65b9\u6cd5\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u5efa\u7acb\u4e00\u4e2a\u89c6\u9891\u5e93\u3001\u4ece\u89c6\u9891\u4e2d\u62bd\u53d6\u7269\u4f53\u53ef\u7528\u6027\u63a9\u7801\u548c\u624b\u90e8\u8fd0\u52a8\u8f68\u8ff9\u7b49\u4e2d\u7ea7\u4fe1\u606f\u7684\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u8bbe\u8ba1\u4e86\u4e24\u90e8\u5206\u7cfb\u7edf\uff1a\u4e00\u4e2a\u57fa\u4e8e\u4efb\u52a1\u8bf4\u660e\u4ece\u5916\u90e8\u89c6\u9891\u5e93\u4e2d\u68c0\u7d22\u4efb\u52a1\u76f8\u5173\u89c6\u9891\u7684\u89c6\u9891\u68c0\u7d22\u5668\uff0c\u53e6\u4e00\u4e2a\u662f\u5c06\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\u878d\u5165\u5b66\u4e60\u5faa\u73af\u7684\u7b56\u7565\u751f\u6210\u5668\u3002", "result": "\u901a\u8fc7\u5728\u591a\u79cd\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e0b\u7684\u5e7f\u6cdb\u6d4b\u8bd5\uff0c\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u5c55\u73b0\u51fa\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\uff0c\u8868\u660e\u4e86\u5176\u5bf9\u4e8e\u5904\u7406\u8d85\u51fa\u8bad\u7ec3\u6570\u636e\u8303\u56f4\u7684\u4efb\u52a1\u5177\u6709\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u4ecb\u7ecd\u7684\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u5982\u4f55\u901a\u8fc7\u89c2\u5bdf\u89c6\u9891\u6f14\u793a\u6765\u5b66\u4e60\u65b0\u7684\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u9014\u5f84\uff0c\u4ee3\u8868\u4e86\u673a\u5668\u4eba\u5b66\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u7a81\u7834\u3002"}}
{"id": "2511.05203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05203", "abs": "https://arxiv.org/abs/2511.05203", "authors": ["Linus Nwankwo", "Bj\u00f6rn Ellensohn", "Christian Rauch", "Elmar Rueckert"], "title": "Beyond Master and Apprentice: Grounding Foundation Models for Symbiotic Interactive Learning in a Shared Latent Space", "comment": null, "summary": "Today's autonomous agents can understand free-form natural language\ninstructions and execute long-horizon tasks in a manner akin to human-level\nreasoning. These capabilities are mostly driven by large-scale pre-trained\nfoundation models (FMs). However, the approaches with which these models are\ngrounded for human-robot interaction (HRI) perpetuate a master-apprentice\nmodel, where the apprentice (embodied agent) passively receives and executes\nthe master's (human's) commands without reciprocal learning. This reactive\ninteraction approach does not capture the co-adaptive dynamics inherent in\neveryday multi-turn human-human interactions. To address this, we propose a\nSymbiotic Interactive Learning (SIL) approach that enables both the master and\nthe apprentice to co-adapt through mutual, bidirectional interactions. We\nformalised SIL as a co-adaptation process within a shared latent task space,\nwhere the agent and human maintain joint belief states that evolve based on\ninteraction history. This enables the agent to move beyond reactive execution\nto proactive clarification, adaptive suggestions, and shared plan refinement.\nTo realise these novel behaviours, we leveraged pre-trained FMs for spatial\nperception and reasoning, alongside a lightweight latent encoder that grounds\nthe models' outputs into task-specific representations. Furthermore, to ensure\nstability as the tasks evolve, we augment SIL with a memory architecture that\nprevents the forgetting of learned task-space representations. We validate SIL\non both simulated and real-world embodied tasks, including instruction\nfollowing, information retrieval, query-oriented reasoning, and interactive\ndialogues. Demos and resources are public\nat:~\\href{https://linusnep.github.io/SIL/}{https://linusnep.github.io/SIL/}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5171\u751f\u4ea4\u4e92\u5b66\u4e60\uff08SIL\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u5411\u4e92\u52a8\u4fc3\u8fdb\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u5171\u540c\u9002\u5e94\u3002\u6b64\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u7a7a\u95f4\u611f\u77e5\u548c\u63a8\u7406\uff0c\u5e76\u7ed3\u5408\u8f7b\u91cf\u7ea7\u6f5c\u5728\u7f16\u7801\u5668\u5c06\u6a21\u578b\u8f93\u51fa\u8f6c\u5316\u4e3a\u7279\u5b9a\u4efb\u52a1\u7684\u8868\u793a\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u4efb\u52a1\u6f14\u53d8\u8fc7\u7a0b\u4e2d\u7684\u7a33\u5b9a\u6027\uff0c\u8fd8\u5f15\u5165\u4e86\u8bb0\u5fc6\u67b6\u6784\u6765\u9632\u6b62\u5df2\u5b66\u4e60\u7684\u4efb\u52a1\u7a7a\u95f4\u8868\u793a\u88ab\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u65b9\u5f0f\u4e3b\u8981\u9075\u5faa\u4e00\u79cd\u4e3b\u4ece\u6a21\u5f0f\uff0c\u5728\u8fd9\u79cd\u6a21\u5f0f\u4e0b\uff0c\u673a\u5668\u4eba\u4f5c\u4e3a\u5b66\u5f92\u88ab\u52a8\u63a5\u6536\u5e76\u6267\u884c\u4eba\u7c7b\uff08\u4e3b\u4eba\uff09\u7684\u6307\u4ee4\uff0c\u800c\u6ca1\u6709\u76f8\u4e92\u5b66\u4e60\u7684\u8fc7\u7a0b\u3002\u8fd9\u79cd\u65b9\u5f0f\u7f3a\u4e4f\u65e5\u5e38\u4eba\u9645\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u56fa\u6709\u7684\u5171\u9002\u5e94\u52a8\u6001\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u5171\u751f\u4ea4\u4e92\u5b66\u4e60\uff08SIL\uff09\u7684\u65b9\u6cd5\uff0c\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u5171\u4eab\u6f5c\u5728\u4efb\u52a1\u7a7a\u95f4\u5185\u7684\u5171\u540c\u9002\u5e94\u8fc7\u7a0b\u3002\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u673a\u5668\u4eba\u548c\u4eba\u4fdd\u6301\u57fa\u4e8e\u4ea4\u4e92\u5386\u53f2\u53d1\u5c55\u7684\u8054\u5408\u4fe1\u5ff5\u72b6\u6001\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e86\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u7528\u4e8e\u7a7a\u95f4\u611f\u77e5\u4e0e\u63a8\u7406\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6f5c\u53d8\u91cf\u7f16\u7801\u5668\u5c06\u6a21\u578b\u8f93\u51fa\u8f6c\u6362\u6210\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u8868\u73b0\u5f62\u5f0f\u3002\u540c\u65f6\uff0c\u4e3a\u4e86\u7ef4\u6301\u968f\u65f6\u95f4\u53d1\u5c55\u4efb\u52a1\u7684\u7a33\u5b9a\u6027\uff0c\u7814\u7a76\u56e2\u961f\u8fd8\u589e\u52a0\u4e86\u8bb0\u5fc6\u7ed3\u6784\u4ee5\u907f\u514d\u5b66\u4e60\u5230\u7684\u4efb\u52a1\u7a7a\u95f4\u8868\u73b0\u88ab\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cSIL\u4e0d\u4ec5\u53ef\u4ee5\u5728\u6a21\u62df\u73af\u5883\u4e2d\u6709\u6548\u8fd0\u884c\uff0c\u800c\u4e14\u5728\u771f\u5b9e\u4e16\u754c\u7684\u8eab\u4f53\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u6307\u4ee4\u8ddf\u968f\u3001\u4fe1\u606f\u68c0\u7d22\u3001\u9762\u5411\u67e5\u8be2\u7684\u63a8\u7406\u4ee5\u53ca\u4e92\u52a8\u5bf9\u8bdd\u7b49\u65b9\u9762\u7684\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u5171\u751f\u4ea4\u4e92\u5b66\u4e60\u6846\u67b6\uff0c\u672c\u7814\u7a76\u6210\u529f\u5730\u4fc3\u8fdb\u4e86\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u66f4\u81ea\u7136\u3001\u66f4\u5177\u9002\u5e94\u6027\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u54cd\u5e94\u5f0f\u6267\u884c\u6a21\u5f0f\uff0c\u5b9e\u73b0\u4e86\u4e3b\u52a8\u6f84\u6e05\u3001\u9002\u5e94\u6027\u5efa\u8bae\u53ca\u5171\u4eab\u8ba1\u5212\u4f18\u5316\u7b49\u65b0\u884c\u4e3a\u3002"}}
{"id": "2511.05275", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05275", "abs": "https://arxiv.org/abs/2511.05275", "authors": ["Hokyun Im", "Euijin Jeong", "Jianlong Fu", "Andrey Kolobov", "Youngwoon Lee"], "title": "TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models", "comment": "Project webpage : https://jellyho.github.io/TwinVLA/", "summary": "Vision-language-action models (VLAs) trained on large-scale robotic datasets\nhave demonstrated strong performance on manipulation tasks, including bimanual\ntasks. However, because most public datasets focus on single-arm\ndemonstrations, adapting VLAs for bimanual tasks typically requires substantial\nadditional bimanual data and fine-tuning. To address this challenge, we\nintroduce TwinVLA, a modular framework that composes two copies of a pretrained\nsingle-arm VLA into a coordinated bimanual VLA. Unlike monolithic\ncross-embodiment models trained on mixtures of single-arm and bimanual data,\nTwinVLA improves both data efficiency and performance by composing pretrained\nsingle-arm policies. Across diverse bimanual tasks in real-world and simulation\nsettings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model\nwithout requiring any bimanual pretraining. Furthermore, it narrows the gap to\nstate-of-the-art model, $\\pi_0$ which rely on extensive proprietary bimanual\ndata and compute cost. These results establish our modular composition approach\nas a data-efficient and scalable path toward high-performance bimanual\nmanipulation, leveraging public single-arm data.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aTwinVLA\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5b83\u5c06\u4e24\u4e2a\u9884\u8bad\u7ec3\u7684\u5355\u81c2\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7ec4\u5408\u6210\u4e00\u4e2a\u534f\u8c03\u7684\u53cc\u81c2\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\uff0c\u5728\u591a\u79cd\u53cc\u81c2\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u540c\u89c4\u6a21\u7684\u6574\u4f53\u6a21\u578b\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u4efb\u4f55\u53cc\u81c2\u9884\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLAs\uff09\u5728\u5904\u7406\u53cc\u81c2\u4efb\u52a1\u65f6\u9700\u8981\u5927\u91cf\u7684\u989d\u5916\u53cc\u81c2\u6570\u636e\u53ca\u5fae\u8c03\uff0c\u56e0\u4e3a\u5927\u591a\u6570\u516c\u5f00\u7684\u6570\u636e\u96c6\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u81c2\u6f14\u793a\u4e0a\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u6311\u6218\uff0c\u63d0\u51fa\u4e86TwinVLA\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5f15\u5165TwinVLA\u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e24\u4e2a\u9884\u8bad\u7ec3\u7684\u5355\u81c2VLAs\u6765\u6784\u5efa\u4e00\u4e2a\u534f\u540c\u5de5\u4f5c\u7684\u53cc\u81c2VLA\u3002\u8fd9\u79cd\u65b9\u5f0f\u4e0e\u90a3\u4e9b\u4f7f\u7528\u6df7\u5408\u5355\u81c2\u548c\u53cc\u81c2\u6570\u636e\u8bad\u7ec3\u800c\u6210\u7684\u6574\u4f53\u8de8\u5b9e\u4f53\u6a21\u578b\u4e0d\u540c\uff0c\u65e8\u5728\u63d0\u5347\u6570\u636e\u6548\u7387\u548c\u8868\u73b0\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u65e0\u8bba\u662f\u5728\u73b0\u5b9e\u4e16\u754c\u8fd8\u662f\u6a21\u62df\u73af\u5883\u4e2d\u6267\u884c\u591a\u6837\u7684\u53cc\u81c2\u4efb\u52a1\u65f6\uff0cTwinVLA\u90fd\u4f18\u4e8e\u540c\u6837\u5927\u5c0f\u7684\u6574\u4f53RDT-1B\u6a21\u578b\uff0c\u800c\u4e14\u65e0\u9700\u8fdb\u884c\u4efb\u4f55\u53cc\u81c2\u9884\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u5b83\u4e5f\u7f29\u5c0f\u4e86\u4e0e\u4f9d\u8d56\u5927\u91cf\u4e13\u6709\u53cc\u81c2\u6570\u636e\u548c\u8ba1\u7b97\u6210\u672c\u7684\u6700\u5148\u8fdb\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8fd9\u4e9b\u6210\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u6a21\u5757\u5316\u7ec4\u5408\u65b9\u6cd5\u662f\u4e00\u6761\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u80fd\u591f\u5229\u7528\u516c\u5171\u5355\u81c2\u6570\u636e\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u53cc\u81c2\u64cd\u4f5c\u3002"}}
{"id": "2511.05307", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.05307", "abs": "https://arxiv.org/abs/2511.05307", "authors": ["Akua K. Dickson", "Juan C. Pacheco Garcia", "Andrew P. Sabelhaus"], "title": "Force-Safe Environment Maps and Real-Time Detection for Soft Robot Manipulators", "comment": null, "summary": "Soft robot manipulators have the potential for deployment in delicate\nenvironments to perform complex manipulation tasks. However, existing obstacle\ndetection and avoidance methods do not consider limits on the forces that\nmanipulators may exert upon contact with delicate obstacles. This work\nintroduces a framework that maps force safety criteria from task space (i.e.\npositions along the robot's body) to configuration space (i.e. the robot's\njoint angles) and enables real-time force safety detection. We incorporate\nlimits on allowable environmental contact forces for given task-space\nobstacles, and map them into configuration space (C-space) through the\nmanipulator's forward kinematics. This formulation ensures that configurations\nclassified as safe are provably below the maximum force thresholds, thereby\nallowing us to determine force-safe configurations of the soft robot\nmanipulator in real-time. We validate our approach in simulation and hardware\nexperiments on a two-segment pneumatic soft robot manipulator. Results\ndemonstrate that the proposed method accurately detects force safety during\ninteractions with deformable obstacles, thereby laying the foundation for\nreal-time safe planning of soft manipulators in delicate, cluttered\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4efb\u52a1\u7a7a\u95f4\u4e2d\u7684\u529b\u5b89\u5168\u6807\u51c6\u6620\u5c04\u5230\u914d\u7f6e\u7a7a\u95f4\u7684\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u529b\u5b89\u5168\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u8f6f\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u969c\u788d\u7269\u68c0\u6d4b\u548c\u907f\u969c\u65b9\u6cd5\u6ca1\u6709\u8003\u8651\u5230\u64cd\u4f5c\u5668\u5728\u63a5\u89e6\u8106\u5f31\u969c\u788d\u7269\u65f6\u53ef\u80fd\u65bd\u52a0\u7684\u529b\u91cf\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5c06\u5141\u8bb8\u7684\u73af\u5883\u63a5\u89e6\u529b\u9650\u5236\u7eb3\u5165\u7ed9\u5b9a\u7684\u4efb\u52a1\u7a7a\u95f4\u969c\u788d\u7269\u4e2d\uff0c\u5e76\u901a\u8fc7\u64cd\u4f5c\u5668\u7684\u6b63\u5411\u8fd0\u52a8\u5b66\u5c06\u5176\u6620\u5c04\u5230\u914d\u7f6e\u7a7a\u95f4\uff08C-\u7a7a\u95f4\uff09\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0e\u53ef\u53d8\u5f62\u969c\u788d\u7269\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u80fd\u591f\u51c6\u786e\u5730\u68c0\u6d4b\u529b\u7684\u5b89\u5168\u6027\uff0c\u4e3a\u5728\u7cbe\u7ec6\u3001\u6742\u4e71\u73af\u5883\u4e2d\u8f6f\u64cd\u7eb5\u5668\u7684\u5b9e\u65f6\u5b89\u5168\u89c4\u5212\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8fd9\u79cd\u65b0\u7684\u529b\u5b89\u5168\u68c0\u6d4b\u6846\u67b6\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u5b89\u5168\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2511.05379", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.05379", "abs": "https://arxiv.org/abs/2511.05379", "authors": ["Eric Godden", "Jacquie Groenewegen", "Matthew K. X. J. Pan"], "title": "ETHOS: A Robotic Encountered-Type Haptic Display for Social Interaction in Virtual Reality", "comment": "8 pages", "summary": "We present ETHOS (Encountered-Type Haptics for On-demand Social Interaction),\na dynamic encountered-type haptic display (ETHD) that enables natural physical\ncontact in virtual reality (VR) during social interactions such as handovers,\nfist bumps, and high-fives. The system integrates a torque-controlled robotic\nmanipulator with interchangeable passive props (silicone hand replicas and a\nbaton), marker-based physical-virtual registration via a ChArUco board, and a\nsafety monitor that gates motion based on the user's head and hand pose. We\nintroduce two control strategies: (i) a static mode that presents a stationary\nprop aligned with its virtual counterpart, consistent with prior ETHD\nbaselines, and (ii) a dynamic mode that continuously updates prop position by\nexponentially blending an initial mid-point trajectory with real-time hand\ntracking, generating a unique contact point for each interaction. Bench tests\nshow static colocation accuracy of 5.09 +/- 0.94 mm, while user interactions\nachieved temporal alignment with an average contact latency of 28.53 +/- 31.21\nms across all interaction and control conditions. These results demonstrate the\nfeasibility of recreating socially meaningful haptics in VR. By incorporating\nessential safety and control mechanisms, ETHOS establishes a practical\nfoundation for high-fidelity, dynamic interpersonal interactions in virtual\nenvironments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aETHOS\u7684\u52a8\u6001\u89e6\u89c9\u663e\u793a\u7cfb\u7edf\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u529b\u77e9\u63a7\u5236\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u3001\u53ef\u4e92\u6362\u7684\u88ab\u52a8\u9053\u5177\u3001\u7269\u7406-\u865a\u62df\u6ce8\u518c\u548c\u5b89\u5168\u76d1\u63a7\u7b49\u6280\u672f\uff0c\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u5b9e\u73b0\u81ea\u7136\u7684\u8eab\u4f53\u63a5\u89e6\uff0c\u5982\u4ea4\u63a5\u7269\u54c1\u3001\u78b0\u62f3\u548c\u51fb\u638c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5730\u91cd\u73b0\u793e\u4ea4\u4e92\u52a8\u4e2d\u7684\u89e6\u611f\u4f53\u9a8c\u3002", "motivation": "\u4e3a\u4e86\u5728\u865a\u62df\u73b0\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u52a0\u81ea\u7136\u4e14\u5bcc\u6709\u610f\u4e49\u7684\u793e\u4f1a\u4e92\u52a8\uff0c\u7279\u522b\u662f\u6d89\u53ca\u8eab\u4f53\u63a5\u89e6\u7684\u573a\u666f\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u53d1\u4e86ETHOS\u7cfb\u7edf\u3002", "method": "ETHOS\u7cfb\u7edf\u6574\u5408\u4e86\u529b\u77e9\u63a7\u5236\u7684\u673a\u68b0\u81c2\u4e0e\u53ef\u66ff\u6362\u7684\u88ab\u52a8\u9053\u5177\uff08\u7845\u80f6\u624b\u6a21\u548c\u68d2\uff09\uff0c\u5e76\u901a\u8fc7ChArUco\u677f\u5b9e\u73b0\u4e86\u57fa\u4e8e\u6807\u8bb0\u7684\u7269\u7406-\u865a\u62df\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u5b89\u5168\u76d1\u63a7\u673a\u5236\uff0c\u6839\u636e\u7528\u6237\u7684\u5934\u90e8\u548c\u624b\u90e8\u59ff\u6001\u6765\u63a7\u5236\u673a\u68b0\u81c2\u7684\u52a8\u4f5c\u3002\u6587\u4e2d\u63d0\u51fa\u4e86\u4e24\u79cd\u63a7\u5236\u7b56\u7565\uff1a\u4e00\u79cd\u662f\u9759\u6001\u6a21\u5f0f\uff0c\u53e6\u4e00\u79cd\u5219\u662f\u52a8\u6001\u6a21\u5f0f\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u9759\u6001\u5171\u4f4d\u7cbe\u5ea6\u4e3a5.09\u00b10.94\u6beb\u7c73\uff1b\u7528\u6237\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\uff0c\u6240\u6709\u4ea4\u4e92\u53ca\u63a7\u5236\u6761\u4ef6\u4e0b\u7684\u5e73\u5747\u63a5\u89e6\u5ef6\u8fdf\u4e3a28.53\u00b131.21\u6beb\u79d2\u3002\u8fd9\u4e9b\u7ed3\u679c\u663e\u793a\u4e86\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u91cd\u73b0\u5177\u6709\u793e\u4f1a\u610f\u4e49\u89e6\u89c9\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5173\u952e\u7684\u5b89\u5168\u6027\u548c\u63a7\u5236\u673a\u5236\uff0cETHOS\u4e3a\u865a\u62df\u73af\u5883\u4e2d\u9ad8\u4fdd\u771f\u5ea6\u7684\u52a8\u6001\u4eba\u9645\u4e92\u52a8\u5960\u5b9a\u4e86\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2511.05397", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.05397", "abs": "https://arxiv.org/abs/2511.05397", "authors": ["Samarth Chopra", "Alex McMoil", "Ben Carnovale", "Evan Sokolson", "Rajkumar Kubendran", "Samuel Dickerson"], "title": "EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation", "comment": "Submitted to ICRA 2026", "summary": "While Vision-Language-Action (VLA) models map visual inputs and language\ninstructions directly to robot actions, they often rely on costly hardware and\nstruggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF\nmanipulator that can be assembled for under $300, capable of modest payloads\nand workspace. A single unified model jointly outputs discrete and continuous\nactions, and our adaptive-horizon ensemble monitors motion uncertainty to\ntrigger on-the-fly re-planning for safe, reliable operation. On LIBERO,\nEverydayVLA matches state-of-the-art success rates, and in real-world tests it\noutperforms prior methods by 49% in-distribution and 34.9% out-of-distribution.\nBy combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA\ndemocratizes access to a robotic foundation model and paves the way for\neconomical use in homes and research labs alike. Experiment videos and details:\nhttps://everydayvla.github.io/", "AI": {"tldr": "\u4ecb\u7ecd\u4e86EverydayVLA\uff0c\u4e00\u79cd\u4f4e\u6210\u672c\u76846\u81ea\u7531\u5ea6\u64cd\u4f5c\u5668\uff0c\u80fd\u591f\u6267\u884c\u7531\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u6307\u5bfc\u7684\u4efb\u52a1\uff0c\u5177\u6709\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\u7684\u6210\u529f\u7387\uff0c\u5e76\u5728\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u5c06\u89c6\u89c9\u8f93\u5165\u548c\u8bed\u8a00\u6307\u4ee4\u6620\u5c04\u5230\u673a\u5668\u4eba\u52a8\u4f5c\u4e0a\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u786c\u4ef6\uff0c\u5e76\u4e14\u96be\u4ee5\u5e94\u5bf9\u65b0\u9896\u6216\u6742\u4e71\u7684\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86EverydayVLA\uff0c\u8fd9\u662f\u4e00\u4e2a\u6210\u672c\u4f4e\u4e8e300\u7f8e\u5143\u76846\u81ea\u7531\u5ea6\u64cd\u4f5c\u5668\uff0c\u80fd\u591f\u5904\u7406\u9002\u5ea6\u7684\u6709\u6548\u8f7d\u8377\u548c\u5de5\u4f5c\u7a7a\u95f4\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u7edf\u4e00\u6a21\u578b\u540c\u65f6\u8f93\u51fa\u79bb\u6563\u548c\u8fde\u7eed\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u89c6\u754c\u96c6\u5408\u76d1\u63a7\u8fd0\u52a8\u4e0d\u786e\u5b9a\u6027\u4ee5\u89e6\u53d1\u5373\u65f6\u91cd\u65b0\u89c4\u5212\u6765\u4fdd\u8bc1\u5b89\u5168\u53ef\u9760\u7684\u64cd\u4f5c\u3002", "result": "\u5728LIBERO\u6570\u636e\u96c6\u4e0a\uff0cEverydayVLA\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6210\u529f\u7387\uff1b\u5728\u73b0\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0c\u5b83\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e8649%\u7684\u5206\u5e03\u5185\u8868\u73b0\u4ee5\u53ca34.9%\u7684\u5206\u5e03\u5916\u8868\u73b0\u3002", "conclusion": "\u7ed3\u5408\u4e86\u6700\u5148\u8fdb\u7684VLA\u6280\u672f\u548c\u6210\u672c\u6548\u76ca\u9ad8\u7684\u786c\u4ef6\uff0cEverydayVLA\u4f7f\u66f4\u591a\u4eba\u80fd\u591f\u63a5\u89e6\u5230\u673a\u5668\u4eba\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u4e3a\u5bb6\u5ead\u548c\u7814\u7a76\u5b9e\u9a8c\u5ba4\u4e2d\u7684\u7ecf\u6d4e\u4f7f\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.05402", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.05402", "abs": "https://arxiv.org/abs/2511.05402", "authors": ["Muhammad Saud Ul Hassan", "Derek Vasquez", "Hamza Asif", "Christian Hubicki"], "title": "Stable and Robust SLIP Model Control via Energy Conservation-Based Feedback Cancellation for Quadrupedal Applications", "comment": null, "summary": "In this paper, we present an energy-conservation based control architecture\nfor stable dynamic motion in quadruped robots. We model the robot as a\nSpring-loaded Inverted Pendulum (SLIP), a model well-suited to represent the\nbouncing motion characteristic of running gaits observed in various biological\nquadrupeds and bio-inspired robotic systems. The model permits leg-orientation\ncontrol during flight and leg-length control during stance, a design choice\ninspired by natural quadruped behaviors and prevalent in robotic quadruped\nsystems. Our control algorithm uses the reduced-order SLIP dynamics of the\nquadruped to track a stable parabolic spline during stance, which is calculated\nusing the principle of energy conservation. Through simulations based on the\ndesign specifications of an actual quadruped robot, Ghost Robotics Minitaur, we\ndemonstrate that our control algorithm generates stable bouncing gaits.\nAdditionally, we illustrate the robustness of our controller by showcasing its\nability to maintain stable bouncing even when faced with up to a 10% error in\nsensor measurements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u5b88\u6052\u7684\u63a7\u5236\u67b6\u6784\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u7a33\u5b9a\u52a8\u6001\u8fd0\u52a8\u3002\u901a\u8fc7\u5c06\u673a\u5668\u4eba\u5efa\u6a21\u4e3a\u5f39\u7c27\u52a0\u8f7d\u5012\u7acb\u6446\uff08SLIP\uff09\uff0c\u5e76\u5229\u7528\u8be5\u6a21\u578b\u5728\u7a7a\u4e2d\u63a7\u5236\u817f\u7684\u65b9\u5411\u3001\u5728\u652f\u6491\u9636\u6bb5\u63a7\u5236\u817f\u957f\uff0c\u7ed3\u5408\u80fd\u91cf\u5b88\u6052\u539f\u5219\u8ba1\u7b97\u51fa\u7a33\u5b9a\u7684\u629b\u7269\u7ebf\u6837\u6761\u8def\u5f84\u8fdb\u884c\u8ddf\u8e2a\u3002\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\u8be5\u7b97\u6cd5\u80fd\u591f\u751f\u6210\u7a33\u5b9a\u7684\u8df3\u8dc3\u6b65\u6001\uff0c\u5e76\u4e14\u5728\u4f20\u611f\u5668\u6d4b\u91cf\u8bef\u5dee\u9ad8\u8fbe10%\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u3002", "motivation": "\u53d7\u5230\u81ea\u7136\u56db\u8db3\u52a8\u7269\u884c\u4e3a\u4ee5\u53ca\u751f\u7269\u542f\u53d1\u5f0f\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u542f\u53d1\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9002\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u5982\u771f\u5b9e\u56db\u8db3\u52a8\u7269\u5954\u8dd1\u65f6\u6240\u89c2\u5bdf\u5230\u7684\u90a3\u79cd\u5f39\u8df3\u8fd0\u52a8\u7279\u5f81\u3002", "method": "\u91c7\u7528Spring-loaded Inverted Pendulum (SLIP)\u6a21\u578b\u6765\u8868\u793a\u56db\u8db3\u673a\u5668\u4eba\uff1b\u5141\u8bb8\u5728\u98de\u884c\u8fc7\u7a0b\u4e2d\u63a7\u5236\u817f\u90e8\u65b9\u5411\uff0c\u5728\u652f\u6491\u9636\u6bb5\u63a7\u5236\u817f\u90e8\u957f\u5ea6\uff1b\u6839\u636e\u80fd\u91cf\u5b88\u6052\u539f\u7406\u8ba1\u7b97\u51fa\u4e00\u6761\u7a33\u5b9a\u7684\u629b\u7269\u7ebf\u6837\u6761\u8def\u5f84\u4f9b\u673a\u5668\u4eba\u8ddf\u8e2a\u3002", "result": "\u901a\u8fc7\u57fa\u4e8eGhost Robotics Minitaur\u8bbe\u8ba1\u89c4\u683c\u7684\u4eff\u771f\u6d4b\u8bd5\u8bc1\u660e\u4e86\u6240\u63d0\u63a7\u5236\u7b97\u6cd5\u80fd\u591f\u4ea7\u751f\u7a33\u5b9a\u7684\u8df3\u8dc3\u6b65\u6001\uff1b\u540c\u65f6\u5c55\u793a\u4e86\u63a7\u5236\u5668\u9762\u5bf9\u9ad8\u8fbe10%\u7684\u4f20\u611f\u5668\u6d4b\u91cf\u8bef\u5dee\u65f6\u4f9d\u65e7\u4fdd\u6301\u7a33\u5b9a\u8df3\u8dc3\u7684\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u5730\u8ba9\u56db\u8db3\u673a\u5668\u4eba\u6a21\u4eff\u4e86\u81ea\u7136\u754c\u7684\u56db\u8db3\u52a8\u7269\u7684\u8dd1\u6b65\u6b65\u6001\uff0c\u5e76\u4e14\u5c55\u73b0\u51fa\u4e86\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.05426", "categories": ["cs.RO", "J.2"], "pdf": "https://arxiv.org/pdf/2511.05426", "abs": "https://arxiv.org/abs/2511.05426", "authors": ["Luca Girardi", "Gabriel Maquignaz", "Stefano Mintchev"], "title": "Bioinspired Soft Quadrotors Jointly Unlock Agility, Squeezability, and Collision Resilience", "comment": "26 pages, 12 figures, 2 tables, 9 videos (not yet disclosed, awaiting\n  peer review)", "summary": "Natural flyers use soft wings to seamlessly enable a wide range of flight\nbehaviours, including agile manoeuvres, squeezing through narrow passageways,\nand withstanding collisions. In contrast, conventional quadrotor designs rely\non rigid frames that support agile flight but inherently limit collision\nresilience and squeezability, thereby constraining flight capabilities in\ncluttered environments. Inspired by the anisotropic stiffness and distributed\nmass-energy structures observed in biological organisms, we introduce\nFlexiQuad, a soft-frame quadrotor design approach that limits this trade-off.\nWe demonstrate a 405-gram FlexiQuad prototype, three orders of magnitude more\ncompliant than conventional quadrotors, yet capable of acrobatic manoeuvres\nwith peak speeds above 80 km/h and linear and angular accelerations exceeding 3\ng and 300 rad/s$^2$, respectively. Analysis demonstrates it can replicate\naccelerations of rigid counterparts up to a thrust-to-weight ratio of 8.\nSimultaneously, FlexiQuad exhibits fourfold higher collision resilience,\nsurviving frontal impacts at 5 m/s without damage and reducing destabilising\nforces in glancing collisions by a factor of 39. Its frame can fully compress,\nenabling flight through gaps as narrow as 70% of its nominal width. Our\nanalysis identifies an optimal structural softness range, from 0.006 to 0.77\nN/mm, comparable to that of natural flyers' wings, whereby agility,\nsqueezability, and collision resilience are jointly achieved for FlexiQuad\nmodels from 20 to 3000 grams. FlexiQuad expands hovering drone capabilities in\ncomplex environments, enabling robust physical interactions without\ncompromising flight performance.", "AI": {"tldr": "Inspired by the flexible and resilient wings of natural flyers, the FlexiQuad is a novel soft-frame quadrotor that offers high-speed maneuverability, the ability to withstand collisions, and can squeeze through tight spaces, outperforming traditional rigid-frame drones in complex environments.", "motivation": "The motivation behind this paper is to overcome the limitations of conventional quadrotor designs, which, due to their rigid frames, are not well-suited for flying in cluttered environments where collision resilience and the ability to pass through narrow spaces are crucial. The aim is to create a design that can offer both agile flight and enhanced flexibility and durability.", "method": "The researchers developed FlexiQuad, a soft-frame quadrotor design that incorporates anisotropic stiffness and distributed mass-energy structures inspired by biological organisms. They created a prototype weighing 405 grams, which is much more compliant than conventional designs, and demonstrated its ability to perform acrobatic maneuvers with high speeds and accelerations while also being highly resilient to collisions and able to squeeze through narrow gaps.", "result": "The FlexiQuad prototype proved to be capable of performing at high speeds and accelerations, similar to those of rigid frame quadrotors, while also demonstrating superior collision resilience and the ability to compress and fit through gaps as small as 70% of its nominal width. The optimal structural softness for achieving these characteristics was found to be in the range of 0.006 to 0.77 N/mm, which is comparable to the wings of natural flyers.", "conclusion": "FlexiQuad expands the capabilities of hovering drones in complex environments, allowing for robust physical interactions without compromising on flight performance. It achieves a balance between agility, squeezability, and collision resilience, making it suitable for a wide range of drone sizes from 20 to 3000 grams."}}
