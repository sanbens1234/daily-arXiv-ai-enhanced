{"id": "2510.20884", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20884", "abs": "https://arxiv.org/abs/2510.20884", "authors": ["Pranamya Kulkarni", "Puranjay Datta", "Burak Var\u0131c\u0131", "Emre Acart\u00fcrk", "Karthikeyan Shanmugam", "Ali Tajer"], "title": "ROPES: Robotic Pose Estimation via Score-Based Causal Representation Learning", "comment": "A preliminary version of this paper appeared at NeurIPS 2025 Workshop\n  on Embodied World Models for Decision Making", "summary": "Causal representation learning (CRL) has emerged as a powerful unsupervised\nframework that (i) disentangles the latent generative factors underlying\nhigh-dimensional data, and (ii) learns the cause-and-effect interactions among\nthe disentangled variables. Despite extensive recent advances in\nidentifiability and some practical progress, a substantial gap remains between\ntheory and real-world practice. This paper takes a step toward closing that gap\nby bringing CRL to robotics, a domain that has motivated CRL. Specifically,\nthis paper addresses the well-defined robot pose estimation -- the recovery of\nposition and orientation from raw images -- by introducing Robotic Pose\nEstimation via Score-Based CRL (ROPES). Being an unsupervised framework, ROPES\nembodies the essence of interventional CRL by identifying those generative\nfactors that are actuated: images are generated by intrinsic and extrinsic\nlatent factors (e.g., joint angles, arm/limb geometry, lighting, background,\nand camera configuration) and the objective is to disentangle and recover the\ncontrollable latent variables, i.e., those that can be directly manipulated\n(intervened upon) through actuation. Interventional CRL theory shows that\nvariables that undergo variations via interventions can be identified. In\nrobotics, such interventions arise naturally by commanding actuators of various\njoints and recording images under varied controls. Empirical evaluations in\nsemi-synthetic manipulator experiments demonstrate that ROPES successfully\ndisentangles latent generative factors with high fidelity with respect to the\nground truth. Crucially, this is achieved by leveraging only distributional\nchanges, without using any labeled data. The paper also includes a comparison\nwith a baseline based on a recently proposed semi-supervised framework. This\npaper concludes by positioning robot pose estimation as a near-practical\ntestbed for CRL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc4\u5206\u7684\u56e0\u679c\u8868\u793a\u5b66\u4e60(ROPES)\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u59ff\u6001\u4f30\u8ba1\u3002\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u5f0f\uff0c\u6210\u529f\u5730\u4ece\u539f\u59cb\u56fe\u50cf\u4e2d\u89e3\u8026\u5e76\u6062\u590d\u4e86\u53ef\u63a7\u7684\u6f5c\u5728\u53d8\u91cf\uff0c\u5982\u5173\u8282\u89d2\u5ea6\u7b49\uff0c\u800c\u65e0\u9700\u4f7f\u7528\u4efb\u4f55\u6807\u8bb0\u6570\u636e\u3002\u6b64\u5916\uff0c\u8fd8\u4e0e\u4e00\u4e2a\u534a\u76d1\u7763\u6846\u67b6\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5e76\u8ba4\u4e3a\u673a\u5668\u4eba\u59ff\u6001\u4f30\u8ba1\u53ef\u4ee5\u4f5c\u4e3a\u56e0\u679c\u8868\u793a\u5b66\u4e60\u7684\u4e00\u4e2a\u63a5\u8fd1\u5b9e\u7528\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "motivation": "\u5c3d\u7ba1\u56e0\u679c\u8868\u793a\u5b66\u4e60(CRL)\u5728\u7406\u8bba\u4e0a\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5c06CRL\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u9886\u57df\u6765\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u89e3\u51b3\u673a\u5668\u4eba\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\u2014\u2014\u5373\u4ece\u539f\u59cb\u56fe\u50cf\u4e2d\u6062\u590d\u4f4d\u7f6e\u548c\u65b9\u5411\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3aROPES\uff08Robotic Pose Estimation via Score-Based CRL\uff09\u7684\u65b9\u6cd5\uff0c\u5b83\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u80fd\u591f\u8bc6\u522b\u90a3\u4e9b\u88ab\u9a71\u52a8\u7684\u751f\u6210\u56e0\u7d20\u3002\u8fd9\u4e9b\u56e0\u7d20\u5305\u62ec\u5185\u5728\u548c\u5916\u5728\u7684\u6f5c\u5728\u56e0\u7d20\uff08\u4f8b\u5982\uff0c\u5173\u8282\u89d2\u5ea6\u3001\u80a2\u4f53\u51e0\u4f55\u5f62\u72b6\u3001\u7167\u660e\u3001\u80cc\u666f\u53ca\u76f8\u673a\u914d\u7f6e\uff09\u3002\u76ee\u6807\u662f\u89e3\u8026\u5e76\u6062\u590d\u53ef\u4ee5\u901a\u8fc7\u6267\u884c\u5668\u76f4\u63a5\u63a7\u5236\u7684\u6f5c\u5728\u53d8\u91cf\u3002", "result": "\u534a\u5408\u6210\u64cd\u4f5c\u5668\u5b9e\u9a8c\u4e2d\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cROPES\u80fd\u591f\u4ee5\u9ad8\u4fdd\u771f\u5ea6\u76f8\u5bf9\u4e8e\u771f\u5b9e\u503c\u89e3\u8026\u6f5c\u5728\u751f\u6210\u56e0\u7d20\u3002\u91cd\u8981\u7684\u662f\uff0c\u8fd9\u4ec5\u901a\u8fc7\u5229\u7528\u5206\u5e03\u53d8\u5316\u5b9e\u73b0\uff0c\u800c\u6ca1\u6709\u4f7f\u7528\u4efb\u4f55\u6807\u7b7e\u6570\u636e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u8ba4\u4e3a\uff0c\u673a\u5668\u4eba\u59ff\u6001\u4f30\u8ba1\u53ef\u4f5c\u4e3a\u56e0\u679c\u8868\u793a\u5b66\u4e60\u7684\u4e00\u4e2a\u51e0\u4e4e\u5b9e\u9645\u7684\u5e94\u7528\u573a\u666f\uff0c\u540c\u65f6\u5c55\u793a\u4e86ROPES\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.20916", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.20916", "abs": "https://arxiv.org/abs/2510.20916", "authors": ["Sydney M. Katz", "Robert J. Moss", "Dylan M. Asmar", "Wesley A. Olson", "James K. Kuchar", "Mykel J. Kochenderfer"], "title": "Aircraft Collision Avoidance Systems: Technological Challenges and Solutions on the Path to Regulatory Acceptance", "comment": "32 pages, 9 figures", "summary": "Aircraft collision avoidance systems is critical to modern aviation. These\nsystems are designed to predict potential collisions between aircraft and\nrecommend appropriate avoidance actions. Creating effective collision avoidance\nsystems requires solutions to a variety of technical challenges related to\nsurveillance, decision making, and validation. These challenges have sparked\nsignificant research and development efforts over the past several decades that\nhave resulted in a variety of proposed solutions. This article provides an\noverview of these challenges and solutions with an emphasis on those that have\nbeen put through a rigorous validation process and accepted by regulatory\nbodies. The challenges posed by the collision avoidance problem are often\npresent in other domains, and aircraft collision avoidance systems can serve as\ncase studies that provide valuable insights for a wide range of safety-critical\nsystems.", "AI": {"tldr": "\u672c\u6587\u6982\u8ff0\u4e86\u822a\u7a7a\u5668\u9632\u649e\u7cfb\u7edf\u9762\u4e34\u7684\u6280\u672f\u6311\u6218\u53ca\u5176\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u5f3a\u8c03\u4e86\u90a3\u4e9b\u7ecf\u8fc7\u4e25\u683c\u9a8c\u8bc1\u8fc7\u7a0b\u5e76\u88ab\u76d1\u7ba1\u673a\u6784\u63a5\u53d7\u7684\u65b9\u6848\u3002", "motivation": "\u822a\u7a7a\u5668\u9632\u649e\u7cfb\u7edf\u5bf9\u4e8e\u73b0\u4ee3\u822a\u7a7a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5f00\u53d1\u9762\u4e34\u7740\u4e0e\u76d1\u89c6\u3001\u51b3\u7b56\u5236\u5b9a\u53ca\u9a8c\u8bc1\u76f8\u5173\u7684\u591a\u79cd\u6280\u672f\u96be\u9898\u3002\u8fd9\u4e9b\u6311\u6218\u4e0d\u4ec5\u9650\u4e8e\u822a\u7a7a\u9886\u57df\uff0c\u4e5f\u4e3a\u5176\u4ed6\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002", "method": "\u901a\u8fc7\u56de\u987e\u8fc7\u53bb\u6570\u5341\u5e74\u95f4\u5728\u822a\u7a7a\u5668\u9632\u649e\u7cfb\u7edf\u65b9\u9762\u5f00\u5c55\u7684\u7814\u7a76\u4e0e\u53d1\u5c55\u5de5\u4f5c\uff0c\u603b\u7ed3\u51fa\u4e00\u7cfb\u5217\u5df2\u88ab\u63d0\u51fa\u5e76\u6d4b\u8bd5\u8fc7\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6587\u7ae0\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u51e0\u79cd\u7ecf\u8fc7\u5145\u5206\u9a8c\u8bc1\u4e14\u83b7\u5f97\u76d1\u7ba1\u673a\u6784\u8ba4\u53ef\u7684\u6709\u6548\u9632\u649e\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u822a\u7a7a\u5668\u9632\u649e\u7cfb\u7edf\u7684\u53d1\u5c55\u4e3a\u89e3\u51b3\u8be5\u9886\u57df\u53ca\u5176\u4ed6\u7c7b\u4f3c\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u9047\u5230\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u6848\u4f8b\u3002"}}
{"id": "2510.20965", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20965", "abs": "https://arxiv.org/abs/2510.20965", "authors": ["Jesse Haworth", "Juo-Tung Chen", "Nigel Nelson", "Ji Woong Kim", "Masoud Moghani", "Chelsea Finn", "Axel Krieger"], "title": "SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing", "comment": "10 pages, 5 figures, 4 tables, NeurIPS 2025", "summary": "Robotic suturing is a prototypical long-horizon dexterous manipulation task,\nrequiring coordinated needle grasping, precise tissue penetration, and secure\nknot tying. Despite numerous efforts toward end-to-end autonomy, a fully\nautonomous suturing pipeline has yet to be demonstrated on physical hardware.\nWe introduce SutureBot: an autonomous suturing benchmark on the da Vinci\nResearch Kit (dVRK), spanning needle pickup, tissue insertion, and knot tying.\nTo ensure repeatability, we release a high-fidelity dataset comprising 1,890\nsuturing demonstrations. Furthermore, we propose a goal-conditioned framework\nthat explicitly optimizes insertion-point precision, improving targeting\naccuracy by 59\\%-74\\% over a task-only baseline. To establish this task as a\nbenchmark for dexterous imitation learning, we evaluate state-of-the-art\nvision-language-action (VLA) models, including $\\pi_0$, GR00T N1, OpenVLA-OFT,\nand multitask ACT, each augmented with a high-level task-prediction policy.\nAutonomous suturing is a key milestone toward achieving robotic autonomy in\nsurgery. These contributions support reproducible evaluation and development of\nprecision-focused, long-horizon dexterous manipulation policies necessary for\nend-to-end suturing. Dataset is available at:\nhttps://huggingface.co/datasets/jchen396/suturebot", "AI": {"tldr": "\u4ecb\u7ecd\u4e86SutureBot\uff0c\u4e00\u4e2a\u5728da Vinci\u7814\u7a76\u5957\u4ef6\u4e0a\u5b9e\u73b0\u7684\u81ea\u4e3b\u7f1d\u5408\u57fa\u51c6\u7cfb\u7edf\uff0c\u5305\u62ec\u9488\u62fe\u53d6\u3001\u7ec4\u7ec7\u63d2\u5165\u548c\u6253\u7ed3\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\u4ee5\u63d0\u9ad8\u76ee\u6807\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6709\u8bb8\u591a\u52aa\u529b\u81f4\u529b\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u5316\uff0c\u4f46\u5728\u7269\u7406\u786c\u4ef6\u4e0a\u5c1a\u672a\u5c55\u793a\u51fa\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u7f1d\u5408\u6d41\u7a0b\u3002\u4e3a\u4e86\u4fc3\u8fdb\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\uff0c\u7814\u7a76\u8005\u4eec\u5e0c\u671b\u5efa\u7acb\u4e00\u4e2a\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u57fa\u51c6\u4efb\u52a1\uff0c\u652f\u6301\u7cbe\u786e\u5ea6\u9ad8\u7684\u957f\u671f\u7075\u5de7\u64cd\u4f5c\u7b56\u7565\u7684\u7814\u53d1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u76ee\u6807\u6761\u4ef6\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u660e\u786e\u4f18\u5316\u4e86\u63d2\u5165\u70b9\u7cbe\u5ea6\uff1b\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u6bcf\u79cd\u6a21\u578b\u90fd\u589e\u52a0\u4e86\u9ad8\u7ea7\u4efb\u52a1\u9884\u6d4b\u7b56\u7565\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u5e03\u4e86\u5305\u542b1890\u4e2a\u7f1d\u5408\u6f14\u793a\u7684\u9ad8\u4fdd\u771f\u5ea6\u6570\u636e\u96c6\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u76f8\u8f83\u4e8e\u4ec5\u57fa\u4e8e\u4efb\u52a1\u7684\u57fa\u7ebf\uff0c\u5728\u76ee\u6807\u51c6\u786e\u6027\u65b9\u9762\u63d0\u9ad8\u4e8659%-74%\u3002", "conclusion": "SutureBot\u4e3a\u7cbe\u5bc6\u5bfc\u5411\u7684\u957f\u671f\u7075\u5de7\u64cd\u63a7\u653f\u7b56\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u8bc4\u4f30\u548c\u53d1\u5c55\u57fa\u7840\uff0c\u662f\u671d\u5411\u624b\u672f\u4e2d\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u8fc8\u8fdb\u7684\u5173\u952e\u91cc\u7a0b\u7891\u3002"}}
{"id": "2510.20974", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20974", "abs": "https://arxiv.org/abs/2510.20974", "authors": ["Michael Bezick", "Vittorio Giammarino", "Ahmed H. Qureshi"], "title": "Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization", "comment": null, "summary": "Reinforcement Learning (RL) from raw visual input has achieved impressive\nsuccesses in recent years, yet it remains fragile to out-of-distribution\nvariations such as changes in lighting, color, and viewpoint. Point Cloud\nReinforcement Learning (PC-RL) offers a promising alternative by mitigating\nappearance-based brittleness, but its sensitivity to camera pose mismatches\ncontinues to undermine reliability in realistic settings. To address this\nchallenge, we propose PCA Point Cloud (PPC), a canonicalization framework\nspecifically tailored for downstream robotic control. PPC maps point clouds\nunder arbitrary rigid-body transformations to a unique canonical pose, aligning\nobservations to a consistent frame, thereby substantially decreasing\nviewpoint-induced inconsistencies. In our experiments, we show that PPC\nimproves robustness to unseen camera poses across challenging robotic tasks,\nproviding a principled alternative to domain randomization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPCA\u70b9\u4e91(PPC)\u7684\u89c4\u8303\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u70b9\u4e91\u6620\u5c04\u5230\u4e00\u4e2a\u72ec\u7279\u7684\u6807\u51c6\u59ff\u6001\u4e0b\uff0c\u4ece\u800c\u51cf\u5c11\u89c6\u89d2\u53d8\u5316\u5bfc\u81f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5bf9\u672a\u89c1\u8fc7\u7684\u76f8\u673a\u59ff\u6001\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4ece\u539f\u59cb\u89c6\u89c9\u8f93\u5165\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\uff0c\u4f46\u662f\u5b83\u5bf9\u4e8e\u5206\u5e03\u5916\u7684\u53d8\u5316\uff08\u5982\u5149\u7167\u3001\u989c\u8272\u548c\u89c6\u89d2\u7684\u53d8\u5316\uff09\u4ecd\u7136\u5f88\u8106\u5f31\u3002\u867d\u7136\u70b9\u4e91\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u5bf9\u76f8\u673a\u59ff\u6001\u4e0d\u5339\u914d\u7684\u654f\u611f\u6027\u7ee7\u7eed\u524a\u5f31\u4e86\u5728\u73b0\u5b9e\u8bbe\u7f6e\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86PCA\u70b9\u4e91(PPC)\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u95e8\u4e3a\u4e0b\u6e38\u673a\u5668\u4eba\u63a7\u5236\u8bbe\u8ba1\u7684\u89c4\u8303\u5316\u6846\u67b6\u3002\u901a\u8fc7PPC\uff0c\u53ef\u4ee5\u5728\u4efb\u610f\u521a\u4f53\u53d8\u6362\u4e0b\u5c06\u70b9\u4e91\u6620\u5c04\u81f3\u552f\u4e00\u786e\u5b9a\u7684\u6807\u51c6\u59ff\u6001\uff0c\u4f7f\u89c2\u5bdf\u7ed3\u679c\u5bf9\u9f50\u5230\u4e00\u81f4\u7684\u5750\u6807\u7cfb\u4e2d\uff0c\u4ece\u800c\u5927\u5e45\u51cf\u5c11\u4e86\u56e0\u89c6\u89d2\u4e0d\u540c\u800c\u5f15\u8d77\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPPC\u80fd\u591f\u63d0\u9ad8\u5bf9\u5177\u6709\u6311\u6218\u6027\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u672a\u89c1\u76f8\u673a\u59ff\u6001\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u9886\u57df\u968f\u673a\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "conclusion": "PCA\u70b9\u4e91(PPC)\u6846\u67b6\u4e3a\u89e3\u51b3\u7531\u4e8e\u76f8\u673a\u59ff\u6001\u5dee\u5f02\u5bfc\u81f4\u7684\u70b9\u4e91\u5f3a\u5316\u5b66\u4e60\u7a33\u5b9a\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u624b\u6bb5\uff0c\u5e76\u4e14\u5728\u591a\u79cd\u673a\u5668\u4eba\u5e94\u7528\u573a\u666f\u4e0b\u5c55\u793a\u51fa\u4e86\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.21026", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21026", "abs": "https://arxiv.org/abs/2510.21026", "authors": ["Sai Haneesh Allu", "Jishnu Jaykumar P", "Ninad Khargonkar", "Tyler Summers", "Jian Yao", "Yu Xiang"], "title": "HRT1: One-Shot Human-to-Robot Trajectory Transfer for Mobile Manipulation", "comment": "14 pages, 11 figures and 3 tables. Project page is available at\n  \\url{https://irvlutd.github.io/HRT1/}", "summary": "We introduce a novel system for human-to-robot trajectory transfer that\nenables robots to manipulate objects by learning from human demonstration\nvideos. The system consists of four modules. The first module is a data\ncollection module that is designed to collect human demonstration videos from\nthe point of view of a robot using an AR headset. The second module is a video\nunderstanding module that detects objects and extracts 3D human-hand\ntrajectories from demonstration videos. The third module transfers a human-hand\ntrajectory into a reference trajectory of a robot end-effector in 3D space. The\nlast module utilizes a trajectory optimization algorithm to solve a trajectory\nin the robot configuration space that can follow the end-effector trajectory\ntransferred from the human demonstration. Consequently, these modules enable a\nrobot to watch a human demonstration video once and then repeat the same mobile\nmanipulation task in different environments, even when objects are placed\ndifferently from the demonstrations. Experiments of different manipulation\ntasks are conducted on a mobile manipulator to verify the effectiveness of our\nsystem", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b66\u4e60\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u64cd\u4f5c\u7269\u4f53\u3002\u8be5\u7cfb\u7edf\u5305\u62ec\u56db\u4e2a\u6a21\u5757\uff1a\u6570\u636e\u6536\u96c6\u3001\u89c6\u9891\u7406\u89e3\u3001\u8f68\u8ff9\u8f6c\u79fb\u4ee5\u53ca\u8f68\u8ff9\u4f18\u5316\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u89c2\u770b\u4eba\u7c7b\u7684\u6f14\u793a\u89c6\u9891\u6765\u5b66\u4f1a\u5982\u4f55\u64cd\u4f5c\u7269\u4f53\uff0c\u5e76\u80fd\u591f\u5728\u4e0d\u540c\u7684\u73af\u5883\u4e2d\u91cd\u590d\u6267\u884c\u76f8\u540c\u7684\u4efb\u52a1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7531\u56db\u4e2a\u6a21\u5757\u7ec4\u6210\u7684\u7cfb\u7edf\uff1a1. \u91c7\u7528AR\u5934\u663e\u4ece\u673a\u5668\u4eba\u89c6\u89d2\u6536\u96c6\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\uff1b2. \u4ece\u89c6\u9891\u4e2d\u68c0\u6d4b\u7269\u4f53\u5e76\u63d0\u53d6\u4e09\u7ef4\u4eba\u624b\u8f68\u8ff9\uff1b3. \u5c06\u4eba\u624b\u8f68\u8ff9\u8f6c\u6362\u4e3a\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u53c2\u8003\u8f68\u8ff9\uff1b4. \u5229\u7528\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\u5728\u673a\u5668\u4eba\u914d\u7f6e\u7a7a\u95f4\u4e2d\u751f\u6210\u53ef\u8ddf\u968f\u8f6c\u6362\u540e\u8f68\u8ff9\u7684\u65b0\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u7cfb\u7edf\u80fd\u591f\u8ba9\u673a\u5668\u4eba\u4ec5\u9700\u89c2\u770b\u4e00\u6b21\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u5c31\u80fd\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u5b8c\u6210\u76f8\u540c\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u5373\u4f7f\u7269\u4f53\u6446\u653e\u4f4d\u7f6e\u4e0e\u6f14\u793a\u4e2d\u6709\u6240\u4e0d\u540c\u4e5f\u80fd\u6210\u529f\u5b8c\u6210\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u4eba\u7c7b\u5230\u673a\u5668\u4eba\u8f68\u8ff9\u8f6c\u79fb\u7684\u7cfb\u7edf\u5bf9\u4e8e\u589e\u5f3a\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u64cd\u4f5c\u80fd\u529b\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9002\u5e94\u65b0\u73af\u5883\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2510.21046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21046", "abs": "https://arxiv.org/abs/2510.21046", "authors": ["Zlatan Ajanovi\u0107", "Ravi Prakash", "Leandro de Souza Rosa", "Jens Kober"], "title": "Sequentially Teaching Sequential Tasks $(ST)^2$: Teaching Robots Long-horizon Manipulation Skills", "comment": null, "summary": "Learning from demonstration is effective for teaching robots complex skills\nwith high sample efficiency. However, teaching long-horizon tasks with multiple\nskills is difficult, as deviations accumulate, distributional shift increases,\nand human teachers become fatigued, raising the chance of failure. In this\nwork, we study user responses to two teaching frameworks: (i) a traditional\nmonolithic approach, where users demonstrate the entire trajectory of a\nlong-horizon task; and (ii) a sequential approach, where the task is segmented\nby the user and demonstrations are provided step by step. To support this\nstudy, we introduce $(ST)^2$, a sequential method for learning long-horizon\nmanipulation tasks that allows users to control the teaching flow by defining\nkey points, enabling incremental and structured demonstrations. We conducted a\nuser study on a restocking task with 16 participants in a realistic retail\nenvironment to evaluate both user preference and method effectiveness. Our\nobjective and subjective results show that both methods achieve similar\ntrajectory quality and success rates. Some participants preferred the\nsequential approach for its iterative control, while others favored the\nmonolithic approach for its simplicity.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7528\u6237\u5bf9\u4e24\u79cd\u6559\u5b66\u6846\u67b6\u7684\u53cd\u5e94\uff1a\u6574\u4f53\u65b9\u6cd5\u548c\u9010\u6b65\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e8f\u5217\u5316\u65b9\u6cd5(ST)\u00b2\u6765\u5b66\u4e60\u957f\u5468\u671f\u64cd\u4f5c\u4efb\u52a1\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u5b9a\u4e49\u5173\u952e\u70b9\u63a7\u5236\u6559\u5b66\u6d41\u7a0b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e24\u79cd\u65b9\u6cd5\u5728\u8f68\u8ff9\u8d28\u91cf\u548c\u6210\u529f\u7387\u4e0a\u76f8\u4f3c\uff0c\u4f46\u7528\u6237\u6839\u636e\u4e2a\u4eba\u504f\u597d\u6709\u6240\u9009\u62e9\u3002", "motivation": "\u89e3\u51b3\u957f\u65f6\u95f4\u591a\u6280\u80fd\u4efb\u52a1\u6559\u5b66\u4e2d\u9047\u5230\u7684\u95ee\u9898\uff0c\u5982\u504f\u5dee\u7d2f\u79ef\u3001\u5206\u5e03\u504f\u79fb\u589e\u52a0\u53ca\u4eba\u7c7b\u6559\u5e08\u75b2\u52b3\u7b49\uff0c\u4ece\u800c\u63d0\u9ad8\u6559\u5b66\u6548\u7387\u4e0e\u6210\u529f\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a(ST)\u00b2\u7684\u5e8f\u5217\u5316\u65b9\u6cd5\u7528\u4e8e\u5b66\u4e60\u957f\u5468\u671f\u64cd\u4f5c\u4efb\u52a1\uff0c\u8be5\u65b9\u6cd5\u8ba9\u7528\u6237\u80fd\u591f\u901a\u8fc7\u5b9a\u4e49\u5173\u952e\u70b9\u6765\u63a7\u5236\u6559\u5b66\u6d41\u7a0b\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u4e00\u9879\u6d89\u53ca16\u540d\u53c2\u4e0e\u8005\u7684\u771f\u5b9e\u96f6\u552e\u73af\u5883\u4e0b\u7684\u7528\u6237\u7814\u7a76\uff0c\u4ee5\u8bc4\u4f30\u7528\u6237\u504f\u597d\u548c\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5ba2\u89c2\u548c\u4e3b\u89c2\u7684\u7ed3\u679c\u663e\u793a\uff0c\u4e24\u79cd\u65b9\u6cd5\u5728\u8f68\u8ff9\u8d28\u91cf\u548c\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u76f8\u8fd1\u3002\u90e8\u5206\u53c2\u4e0e\u8005\u66f4\u559c\u6b22\u9010\u6b65\u65b9\u6cd5\u63d0\u4f9b\u7684\u8fed\u4ee3\u63a7\u5236\uff0c\u800c\u53e6\u4e00\u4e9b\u4eba\u5219\u503e\u5411\u4e8e\u6574\u4f53\u65b9\u6cd5\u7684\u7b80\u5355\u6027\u3002", "conclusion": "\u867d\u7136\u4e24\u79cd\u6559\u5b66\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u76f8\u5f53\uff0c\u4f46(ST)\u00b2\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u4e14\u53ef\u589e\u91cf\u5f0f\u6f14\u793a\u7684\u65b9\u6cd5\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u66f4\u591a\u7075\u6d3b\u6027\u3002\u4e0d\u540c\u7528\u6237\u57fa\u4e8e\u4e2a\u4eba\u559c\u597d\u5bf9\u4e24\u79cd\u65b9\u6cd5\u6709\u4e0d\u540c\u7684\u504f\u597d\u3002"}}
{"id": "2510.21074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21074", "abs": "https://arxiv.org/abs/2510.21074", "authors": ["Mitchell E. C. Sabbadini", "Andrew H. Liu", "Joseph Ruan", "Tyler S. Wilson", "Zachary Kingston", "Jonathan D. Gammell"], "title": "Revisiting Replanning from Scratch: Real-Time Incremental Planning with Fast Almost-Surely Asymptotically Optimal Planners", "comment": "Submitted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2026, 8 pages, 5 figures, 1 table. A video of this work can be found\n  at https://www.youtube.com/watch?v=XaZrFy8wGZs", "summary": "Robots operating in changing environments either predict obstacle changes\nand/or plan quickly enough to react to them. Predictive approaches require a\nstrong prior about the position and motion of obstacles. Reactive approaches\nrequire no assumptions about their environment but must replan quickly and find\nhigh-quality paths to navigate effectively.\n  Reactive approaches often reuse information between queries to reduce\nplanning cost. These techniques are conceptually sound but updating dense\nplanning graphs when information changes can be computationally prohibitive. It\ncan also require significant effort to detect the changes in some applications.\n  This paper revisits the long-held assumption that reactive replanning\nrequires updating existing plans. It shows that the incremental planning\nproblem can alternatively be solved more efficiently as a series of independent\nproblems using fast almost-surely asymptotically optimal (ASAO) planning\nalgorithms. These ASAO algorithms quickly find an initial solution and converge\ntowards an optimal solution which allows them to find consistent global plans\nin the presence of changing obstacles without requiring explicit plan reuse.\nThis is demonstrated with simulated experiments where Effort Informed Trees\n(EIT*) finds shorter median solution paths than the tested reactive planning\nalgorithms and is further validated using Asymptotically Optimal RRT-Connect\n(AORRTC) on a real-world planning problem on a robot arm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u5feb\u901f\u51e0\u4e4e\u80af\u5b9a\u6e10\u8fd1\u6700\u4f18(ASAO)\u89c4\u5212\u7b97\u6cd5\u6765\u89e3\u51b3\u589e\u91cf\u89c4\u5212\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u66f4\u65b0\u73b0\u6709\u7684\u8ba1\u5212\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u5b58\u5728\u53d8\u5316\u969c\u788d\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u627e\u5230\u4e00\u81f4\u7684\u5168\u5c40\u89c4\u5212\uff0c\u800c\u4e0d\u9700\u8981\u663e\u5f0f\u7684\u8ba1\u5212\u91cd\u7528\u3002", "motivation": "\u4f20\u7edf\u7684\u53cd\u5e94\u5f0f\u91cd\u89c4\u5212\u65b9\u6cd5\u9700\u8981\u66f4\u65b0\u5bc6\u96c6\u7684\u89c4\u5212\u56fe\uff0c\u8fd9\u5728\u8ba1\u7b97\u4e0a\u53ef\u80fd\u662f\u7981\u6b62\u7684\uff0c\u5e76\u4e14\u68c0\u6d4b\u67d0\u4e9b\u5e94\u7528\u4e2d\u7684\u53d8\u5316\u4e5f\u9700\u8981\u5927\u91cf\u7684\u52aa\u529b\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u91cd\u65b0\u5ba1\u89c6\u4e86\u53cd\u5e94\u5f0f\u91cd\u89c4\u5212\u9700\u8981\u66f4\u65b0\u73b0\u6709\u8ba1\u5212\u8fd9\u4e00\u957f\u671f\u6301\u6709\u7684\u5047\u8bbe\u3002", "method": "\u91c7\u7528\u5feb\u901f\u51e0\u4e4e\u80af\u5b9a\u6e10\u8fd1\u6700\u4f18(ASAO)\u89c4\u5212\u7b97\u6cd5\uff0c\u5982Effort Informed Trees (EIT*) \u548c Asymptotically Optimal RRT-Connect (AORRTC)\uff0c\u5c06\u589e\u91cf\u89c4\u5212\u95ee\u9898\u4f5c\u4e3a\u4e00\u7cfb\u5217\u72ec\u7acb\u7684\u95ee\u9898\u6765\u66f4\u6709\u6548\u5730\u89e3\u51b3\u3002\u8fd9\u4e9b\u7b97\u6cd5\u80fd\u591f\u5feb\u901f\u627e\u5230\u521d\u59cb\u89e3\u51b3\u65b9\u6848\u5e76\u8d8b\u5411\u4e8e\u6700\u4f18\u89e3\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0cEffort Informed Trees (EIT*) \u627e\u5230\u7684\u4e2d\u4f4d\u6570\u89e3\u8def\u5f84\u6bd4\u6d4b\u8bd5\u4e2d\u7684\u53cd\u5e94\u5f0f\u89c4\u5212\u7b97\u6cd5\u66f4\u77ed\u3002\u6b64\u5916\uff0cAsymptotically Optimal RRT-Connect (AORRTC) \u5728\u4e00\u4e2a\u5b9e\u9645\u673a\u5668\u4eba\u624b\u81c2\u4e0a\u7684\u89c4\u5212\u95ee\u9898\u4e2d\u4e5f\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4f7f\u7528ASAO\u7b97\u6cd5\uff0c\u65e0\u9700\u663e\u5f0f\u7684\u8ba1\u5212\u91cd\u7528\u4e5f\u53ef\u4ee5\u9ad8\u6548\u5730\u8fdb\u884c\u53cd\u5e94\u5f0f\u91cd\u89c4\u5212\uff0c\u5728\u9762\u5bf9\u73af\u5883\u53d8\u5316\u65f6\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8def\u5f84\u3002"}}
{"id": "2510.21164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21164", "abs": "https://arxiv.org/abs/2510.21164", "authors": ["Shamistan Karimov", "Elian Neppel", "Shreya Santra", "Kentaro Uno", "Kazuya Yoshida"], "title": "An Agnostic End-Effector Alignment Controller for Robust Assembly of Modular Space Robots", "comment": "6 pages, 12 figures. Accepted at iSparo 2025 | Video:\n  https://youtu.be/BW0YgSrvuDo", "summary": "Modular robots offer reconfigurability and fault tolerance essential for\nlunar missions, but require controllers that adapt safely to real-world\ndisturbances. We build on our previous hardware-agnostic actuator\nsynchronization in Motion Stack to develop a new controller enforcing adaptive\nvelocity bounds via a dynamic hypersphere clamp. Using only real-time\nend-effector and target pose measurements, the controller adjusts its\ntranslational and rotational speed limits to ensure smooth, stable alignment\nwithout abrupt motions. We implemented two variants, a discrete, step-based\nversion and a continuous, velocity-based version, and tested them on two\nMoonBot limbs in JAXA's lunar environment simulator. Field trials demonstrate\nthat the step-based variant produces highly predictable, low-wobble motions,\nwhile the continuous variant converges more quickly and maintains\nmillimeter-level positional accuracy, and both remain robust across limbs with\ndiffering mechanical imperfections and sensing noise (e.g., backlash and flex).\nThese results highlight the flexibility and robustness of our robot-agnostic\nframework for autonomous self-assembly and reconfiguration under harsh\nconditions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u63a7\u5236\u5668\uff0c\u8be5\u63a7\u5236\u5668\u901a\u8fc7\u52a8\u6001\u8d85\u7403\u4f53\u5939\u7d27\u6765\u5b9e\u65bd\u81ea\u9002\u5e94\u901f\u5ea6\u8fb9\u754c\uff0c\u4ee5\u786e\u4fdd\u6a21\u5757\u5316\u673a\u5668\u4eba\u5728\u6708\u7403\u4efb\u52a1\u4e2d\u80fd\u591f\u5e73\u7a33\u3001\u7a33\u5b9a\u5730\u5bf9\u9f50\u800c\u4e0d\u4f1a\u51fa\u73b0\u7a81\u7136\u7684\u52a8\u4f5c\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u5e72\u6270\u7684\u63a7\u5236\u5668\uff0c\u5bf9\u4e8e\u9700\u8981\u53ef\u91cd\u6784\u6027\u548c\u5bb9\u9519\u6027\u7684\u6708\u7403\u4efb\u52a1\u4e2d\u7684\u6a21\u5757\u5316\u673a\u5668\u4eba\u6765\u8bf4\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u5148\u524d\u786c\u4ef6\u65e0\u5173\u7684\u6267\u884c\u5668\u540c\u6b65\u6280\u672f\uff0c\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u8fd9\u79cd\u65b0\u7684\u63a7\u5236\u5668\uff0c\u5b83\u5229\u7528\u5b9e\u65f6\u672b\u7aef\u6267\u884c\u5668\u548c\u76ee\u6807\u59ff\u6001\u6d4b\u91cf\u503c\u8c03\u6574\u5176\u5e73\u79fb\u548c\u65cb\u8f6c\u901f\u5ea6\u9650\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u5b9e\u73b0\u4e86\u4e24\u79cd\u53d8\u4f53\uff1a\u4e00\u79cd\u662f\u79bb\u6563\u7684\u3001\u57fa\u4e8e\u6b65\u9aa4\u7684\u7248\u672c\uff1b\u53e6\u4e00\u79cd\u662f\u8fde\u7eed\u7684\u3001\u57fa\u4e8e\u901f\u5ea6\u7684\u7248\u672c\uff0c\u5e76\u5728\u65e5\u672c\u5b87\u5b99\u822a\u7a7a\u7814\u7a76\u5f00\u53d1\u673a\u6784\uff08JAXA\uff09\u7684\u6708\u7403\u73af\u5883\u6a21\u62df\u5668\u4e0a\u4f7f\u7528\u4e24\u4e2aMoonBot\u80a2\u4f53\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5b9e\u5730\u8bd5\u9a8c\u8bc1\u660e\uff0c\u57fa\u4e8e\u6b65\u9aa4\u7684\u7248\u672c\u4ea7\u751f\u4e86\u9ad8\u5ea6\u53ef\u9884\u6d4b\u4e14\u4f4e\u6446\u52a8\u7684\u52a8\u4f5c\uff0c\u800c\u8fde\u7eed\u7248\u672c\u5219\u66f4\u5feb\u6536\u655b\u5e76\u4fdd\u6301\u6beb\u7c73\u7ea7\u7684\u4f4d\u7f6e\u7cbe\u5ea6\u3002\u4e24\u8005\u90fd\u5bf9\u5177\u6709\u4e0d\u540c\u673a\u68b0\u7f3a\u9677\u548c\u611f\u77e5\u566a\u58f0\u7684\u80a2\u4f53\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u51fa\u4e86\u6211\u4eec\u673a\u5668\u4eba\u65e0\u5173\u6846\u67b6\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u8fdb\u884c\u81ea\u4e3b\u81ea\u7ec4\u88c5\u548c\u91cd\u65b0\u914d\u7f6e\u65f6\u7684\u7075\u6d3b\u6027\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.21215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21215", "abs": "https://arxiv.org/abs/2510.21215", "authors": ["Shuoshuo Ding", "Tiedong Zhang", "Dapeng Jiang", "Ming Lei"], "title": "Underwater Visual-Inertial-Acoustic-Depth SLAM with DVL Preintegration for Degraded Environments", "comment": "10 pages, 10 figures", "summary": "Visual degradation caused by limited visibility, insufficient lighting, and\nfeature scarcity in underwater environments presents significant challenges to\nvisual-inertial simultaneous localization and mapping (SLAM) systems. To\naddress these challenges, this paper proposes a graph-based\nvisual-inertial-acoustic-depth SLAM system that integrates a stereo camera, an\ninertial measurement unit (IMU), the Doppler velocity log (DVL), and a pressure\nsensor. The key innovation lies in the tight integration of four distinct\nsensor modalities to ensure reliable operation, even under degraded visual\nconditions. To mitigate DVL drift and improve measurement efficiency, we\npropose a novel velocity-bias-based DVL preintegration strategy. At the\nfrontend, hybrid tracking strategies and acoustic-inertial-depth joint\noptimization enhance system stability. Additionally, multi-source hybrid\nresiduals are incorporated into a graph optimization framework. Extensive\nquantitative and qualitative analyses of the proposed system are conducted in\nboth simulated and real-world underwater scenarios. The results demonstrate\nthat our approach outperforms current state-of-the-art stereo visual-inertial\nSLAM systems in both stability and localization accuracy, exhibiting\nexceptional robustness, particularly in visually challenging environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u89c6\u89c9-\u60ef\u6027-\u58f0\u5b66-\u6df1\u5ea6SLAM\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u7acb\u4f53\u76f8\u673a\u3001IMU\u3001\u591a\u666e\u52d2\u901f\u5ea6\u8bb0\u5f55\u4eea\uff08DVL\uff09\u548c\u538b\u529b\u4f20\u611f\u5668\uff0c\u4ee5\u89e3\u51b3\u6c34\u4e0b\u73af\u5883\u4e2d\u7531\u4e8e\u80fd\u89c1\u5ea6\u4f4e\u3001\u5149\u7167\u4e0d\u8db3\u53ca\u7279\u5f81\u7a00\u5c11\u5bfc\u81f4\u7684\u89c6\u89c9\u9000\u5316\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5165\u65b0\u7684DVL\u9884\u79ef\u5206\u7b56\u7565\u4ee5\u53ca\u524d\u7aef\u6df7\u5408\u8ddf\u8e2a\u7b56\u7565\u548c\u58f0\u5b66-\u60ef\u6027-\u6df1\u5ea6\u8054\u5408\u4f18\u5316\uff0c\u8be5\u7cfb\u7edf\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6c34\u4e0b\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u5b9a\u4f4d\u51c6\u786e\u6027\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u9000\u5316\u95ee\u9898\u5bf9\u89c6\u89c9-\u60ef\u6027\u540c\u65f6\u5b9a\u4f4d\u4e0e\u5efa\u56fe\uff08SLAM\uff09\u7cfb\u7edf\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u89c6\u89c9\u6761\u4ef6\u4e0d\u4f73\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u53ef\u9760\u5de5\u4f5c\u7684\u65b0\u578bSLAM\u7cfb\u7edf\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6574\u5408\u4e86\u7acb\u4f53\u76f8\u673a\u3001IMU\u3001DVL\u53ca\u538b\u529b\u4f20\u611f\u5668\u7684\u57fa\u4e8e\u56fe\u6a21\u578b\u7684\u89c6\u89c9-\u60ef\u6027-\u58f0\u5b66-\u6df1\u5ea6SLAM\u7cfb\u7edf\u3002\u7279\u522b\u5730\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u901f\u5ea6\u504f\u5dee\u7684DVL\u9884\u79ef\u5206\u65b9\u6cd5\u6765\u51cf\u5c11DVL\u6f02\u79fb\uff0c\u5e76\u91c7\u7528\u4e86\u6df7\u5408\u8ffd\u8e2a\u7b56\u7565\u52a0\u4e0a\u58f0\u5b66-\u60ef\u6027-\u6df1\u5ea6\u8054\u5408\u4f18\u5316\u6765\u63d0\u9ad8\u524d\u7aef\u5904\u7406\u80fd\u529b\u3002\u6574\u4e2a\u7cfb\u7edf\u8fd8\u5229\u7528\u4e86\u591a\u6e90\u6df7\u5408\u6b8b\u5dee\u8fdb\u884c\u56fe\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u5728\u6a21\u62df\u548c\u5b9e\u9645\u6c34\u4e0b\u573a\u666f\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9a\u91cf\u4e0e\u5b9a\u6027\u5206\u6790\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u7a33\u5b9a\u6027\u4e0e\u5b9a\u4f4d\u7cbe\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7acb\u4f53\u89c6\u89c9-\u60ef\u6027SLAM\u7cfb\u7edf\uff0c\u5c24\u5176\u662f\u5728\u89c6\u89c9\u6761\u4ef6\u6781\u5177\u6311\u6218\u6027\u7684\u73af\u5883\u4e0b\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u51fa\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u6c34\u4e0b\u590d\u6742\u89c6\u89c9\u6761\u4ef6\u7684\u65b0\u578b\u89c6\u89c9-\u60ef\u6027-\u58f0\u5b66-\u6df1\u5ea6SLAM\u7cfb\u7edf\uff0c\u4e3a\u672a\u6765\u6c34\u4e0b\u5bfc\u822a\u6280\u672f\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2510.21357", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21357", "abs": "https://arxiv.org/abs/2510.21357", "authors": ["Daniel Schleich", "Jan Quenzel", "Sven Behnke"], "title": "Remote Autonomy for Multiple Small Lowcost UAVs in GNSS-denied Search and Rescue Operations", "comment": "Accepted final version. IEEE International Symposium on Safety,\n  Security, and Rescue Robotics (SSRR), Galway, Ireland, 2025", "summary": "In recent years, consumer-grade UAVs have been widely adopted by first\nresponders. In general, they are operated manually, which requires trained\npilots, especially in unknown GNSS-denied environments and in the vicinity of\nstructures. Autonomous flight can facilitate the application of UAVs and reduce\noperator strain. However, autonomous systems usually require special\nprogramming interfaces, custom sensor setups, and strong onboard computers,\nwhich limits a broader deployment.\n  We present a system for autonomous flight using lightweight consumer-grade\nDJI drones. They are controlled by an Android app for state estimation and\nobstacle avoidance directly running on the UAV's remote control. Our ground\ncontrol station enables a single operator to configure and supervise multiple\nheterogeneous UAVs at once. Furthermore, it combines the observations of all\nUAVs into a joint 3D environment model for improved situational awareness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6d88\u8d39\u7ea7DJI\u65e0\u4eba\u673a\u7684\u81ea\u4e3b\u98de\u884c\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u9065\u63a7\u5668\u4e0a\u8fd0\u884c\u7684Android\u5e94\u7528\u7a0b\u5e8f\u5b9e\u73b0\u72b6\u6001\u4f30\u8ba1\u548c\u907f\u969c\uff0c\u5e76\u7ed3\u5408\u5730\u9762\u63a7\u5236\u7ad9\u5bf9\u591a\u67b6\u5f02\u6784\u65e0\u4eba\u673a\u8fdb\u884c\u914d\u7f6e\u3001\u76d1\u7763\u4ee5\u53ca\u73af\u5883\u4fe1\u606f\u6574\u5408\u3002", "motivation": "\u4e3a\u4e86\u51cf\u5c11\u64cd\u4f5c\u5458\u7684\u538b\u529b\u5e76\u4fc3\u8fdb\u65e0\u4eba\u673a\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u672a\u77e5\u7684\u65e0GNSS\u73af\u5883\u4e0b\u53ca\u7ed3\u6784\u9644\u8fd1\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5bf9\u7279\u6b8a\u7f16\u7a0b\u63a5\u53e3\u3001\u5b9a\u5236\u4f20\u611f\u5668\u8bbe\u7f6e\u548c\u5f3a\u5927\u673a\u8f7d\u8ba1\u7b97\u673a\u7684\u9700\u6c42\uff0c\u4ece\u800c\u6269\u5927\u65e0\u4eba\u673a\u7684\u90e8\u7f72\u8303\u56f4\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u6b3eAndroid\u5e94\u7528\u76f4\u63a5\u5728\u65e0\u4eba\u673a\u9065\u63a7\u5668\u4e0a\u8fd0\u884c\uff0c\u7528\u4e8e\u72b6\u6001\u4f30\u8ba1\u4e0e\u969c\u788d\u7269\u89c4\u907f\uff1b\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5730\u9762\u63a7\u5236\u7ad9\uff0c\u652f\u6301\u5355\u4e2a\u64cd\u4f5c\u5458\u540c\u65f6\u914d\u7f6e\u548c\u76d1\u63a7\u591a\u67b6\u4e0d\u540c\u7c7b\u578b\u7684\u65e0\u4eba\u673a\uff0c\u5e76\u5c06\u6240\u6709\u65e0\u4eba\u673a\u7684\u89c2\u6d4b\u6570\u636e\u5408\u5e76\u6210\u4e00\u4e2a\u8054\u54083D\u73af\u5883\u6a21\u578b\u4ee5\u63d0\u9ad8\u6001\u52bf\u611f\u77e5\u80fd\u529b\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u989d\u5916\u786c\u4ef6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6d88\u8d39\u7ea7\u65e0\u4eba\u673a\u7684\u81ea\u4e3b\u98de\u884c\uff0c\u5e76\u4e14\u80fd\u591f\u901a\u8fc7\u5355\u4e00\u5e73\u53f0\u7ba1\u7406\u591a\u67b6\u65e0\u4eba\u673a\uff0c\u6784\u5efa\u5171\u4eab\u7684\u4e09\u7ef4\u73af\u5883\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7b2c\u4e00\u54cd\u5e94\u8005\u63d0\u4f9b\u4e86\u66f4\u52a0\u4fbf\u6377\u6613\u7528\u7684\u81ea\u4e3b\u98de\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u64cd\u4f5c\u96be\u5ea6\u548c\u6280\u672f\u95e8\u69db\uff0c\u4f7f\u5f97\u666e\u901a\u6d88\u8d39\u8005\u7ea7\u522b\u7684\u65e0\u4eba\u673a\u4e5f\u80fd\u6267\u884c\u590d\u6742\u7684\u4efb\u52a1\u3002"}}
{"id": "2510.21369", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21369", "abs": "https://arxiv.org/abs/2510.21369", "authors": ["Vivian S. Medeiros", "Giovanni B. Dessy", "Thiago Boaventura", "Marcelo Becker", "Claudio Semini", "Victor Barasuol"], "title": "Load-bearing Assessment for Safe Locomotion of Quadruped Robots on Collapsing Terrain", "comment": null, "summary": "Collapsing terrains, often present in search and rescue missions or planetary\nexploration, pose significant challenges for quadruped robots. This paper\nintroduces a robust locomotion framework for safe navigation over unstable\nsurfaces by integrating terrain probing, load-bearing analysis, motion\nplanning, and control strategies. Unlike traditional methods that rely on\nspecialized sensors or external terrain mapping alone, our approach leverages\njoint measurements to assess terrain stability without hardware modifications.\nA Model Predictive Control (MPC) system optimizes robot motion, balancing\nstability and probing constraints, while a state machine coordinates terrain\nprobing actions, enabling the robot to detect collapsible regions and\ndynamically adjust its footholds. Experimental results on custom-made\ncollapsing platforms and rocky terrains demonstrate the framework's ability to\ntraverse collapsing terrain while maintaining stability and prioritizing\nsafety.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u8fd0\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5730\u5f62\u63a2\u6d4b\u3001\u627f\u91cd\u5206\u6790\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u7b56\u7565\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u5728\u4e0d\u7a33\u5b9a\u8868\u9762\u4e0a\u5b89\u5168\u5bfc\u822a\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u7a7f\u8d8a\u584c\u9677\u5730\u5f62\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u4f18\u5148\u8003\u8651\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u641c\u6551\u4efb\u52a1\u6216\u884c\u661f\u63a2\u7d22\u4e2d\u9047\u5230\u7684\u584c\u9677\u5730\u5f62\u7ed9\u56db\u8db3\u673a\u5668\u4eba\u7684\u79fb\u52a8\u5e26\u6765\u4e86\u5f88\u5927\u7684\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u56db\u8db3\u673a\u5668\u4eba\u5728\u8fd9\u4e9b\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u5730\u5f62\u63a2\u6d4b\u3001\u627f\u8f7d\u529b\u5206\u6790\u3001\u8fd0\u52a8\u89c4\u5212\u4ee5\u53ca\u63a7\u5236\u7b56\u7565\u7684\u7efc\u5408\u6846\u67b6\u3002\u7279\u522b\u5730\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5229\u7528\u5173\u8282\u6d4b\u91cf\u6765\u8bc4\u4f30\u5730\u5f62\u7a33\u5b9a\u6027\uff0c\u800c\u4e0d\u9700\u8981\u989d\u5916\u7684\u786c\u4ef6\u4fee\u6539\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7cfb\u7edf\u4ee5\u4f18\u5316\u673a\u5668\u4eba\u7684\u52a8\u4f5c\uff0c\u5728\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u8fdb\u884c\u63a2\u6d4b\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u70b9\uff1b\u5e76\u4e14\u4f7f\u7528\u72b6\u6001\u673a\u534f\u8c03\u5730\u5f62\u63a2\u6d4b\u884c\u4e3a\uff0c\u5141\u8bb8\u673a\u5668\u4eba\u68c0\u6d4b\u53ef\u584c\u9677\u533a\u57df\u5e76\u52a8\u6001\u8c03\u6574\u5176\u7acb\u8db3\u70b9\u3002", "result": "\u901a\u8fc7\u5bf9\u5b9a\u5236\u7684\u53ef\u584c\u9677\u5e73\u53f0\u53ca\u5ca9\u77f3\u5730\u5f62\u4e0a\u7684\u5b9e\u9a8c\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u8ba9\u56db\u8db3\u673a\u5668\u4eba\u6210\u529f\u7a7f\u8d8a\u584c\u9677\u5730\u5f62\uff0c\u5e76\u4e14\u5728\u6574\u4e2a\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u826f\u597d\u7684\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u9c81\u68d2\u8fd0\u52a8\u6846\u67b6\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u584c\u9677\u5730\u5f62\u6240\u5e26\u6765\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u5b83\u4eec\u5728\u6267\u884c\u641c\u7d22\u6551\u63f4\u4efb\u52a1\u6216\u884c\u661f\u63a2\u7d22\u65f6\u7684\u5b89\u5168\u6027\u4e0e\u6548\u7387\u3002"}}
{"id": "2510.21469", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21469", "abs": "https://arxiv.org/abs/2510.21469", "authors": ["Domenico Palmisano", "Giuseppe Palestra", "Berardina Nadja De Carolis"], "title": "Enhancing Social Robots through Resilient AI", "comment": "8 pages, Workshop on Adaptive Social Interaction based on user's\n  Mental mOdels and behaVior in HRI, The 17th International Conference on\n  Social Robotics, 10-12 September 2025, Naples (IT)", "summary": "As artificial intelligence continues to advance and becomes more integrated\ninto sensitive areas like healthcare, education, and everyday life, it's\ncrucial for these systems to be both resilient and robust. This paper shows how\nresilience is a fundamental characteristic of social robots, which, through it,\nensure trust in the robot itself-an essential element especially when operating\nin contexts with elderly people, who often have low trust in these systems.\nResilience is therefore the ability to operate under adverse or stressful\nconditions, even when degraded or weakened, while maintaining essential\noperational capabilities.", "AI": {"tldr": "\u672c\u6587\u5f3a\u8c03\u4e86\u793e\u4f1a\u673a\u5668\u4eba\u5728\u533b\u7597\u4fdd\u5065\u3001\u6559\u80b2\u548c\u65e5\u5e38\u751f\u6d3b\u7b49\u654f\u611f\u9886\u57df\u4e2d\u7684\u97e7\u6027\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u4e0e\u8001\u5e74\u4eba\u4e92\u52a8\u65f6\uff0c\u8fd9\u79cd\u97e7\u6027\u6709\u52a9\u4e8e\u5efa\u7acb\u5bf9\u673a\u5668\u4eba\u7684\u4fe1\u4efb\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\u53ca\u5176\u5728\u533b\u7597\u4fdd\u5065\u3001\u6559\u80b2\u7b49\u654f\u611f\u9886\u57df\u7684\u6df1\u5165\u5e94\u7528\uff0c\u786e\u4fdd\u8fd9\u4e9b\u7cfb\u7edf\u5177\u5907\u97e7\u6027\u548c\u9c81\u68d2\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5c24\u5176\u662f\u5728\u4e0e\u8001\u5e74\u4eba\u4e92\u52a8\u65f6\uff0c\u4ed6\u4eec\u5f80\u5f80\u5bf9\u8fd9\u7c7b\u7cfb\u7edf\u7f3a\u4e4f\u4fe1\u4efb\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5b9a\u4e49\u97e7\u6027\u2014\u2014\u5373\u5728\u4e0d\u5229\u6216\u538b\u529b\u6761\u4ef6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u57fa\u672c\u64cd\u4f5c\u80fd\u529b\u7684\u7279\u6027\u2014\u2014\u6765\u5c55\u793a\u5176\u5bf9\u4e8e\u793e\u4f1a\u673a\u5668\u4eba\u7684\u91cd\u8981\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u97e7\u6027\u662f\u793e\u4f1a\u673a\u5668\u4eba\u7684\u4e00\u79cd\u57fa\u672c\u7279\u5f81\uff0c\u5b83\u80fd\u591f\u4fdd\u8bc1\u673a\u5668\u4eba\u5373\u4f7f\u5728\u6027\u80fd\u4e0b\u964d\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u7ef4\u6301\u5173\u952e\u529f\u80fd\uff0c\u8fd9\u5bf9\u4e8e\u8d62\u5f97\u7528\u6237\u7279\u522b\u662f\u8001\u5e74\u7528\u6237\u7684\u4fe1\u4efb\u975e\u5e38\u91cd\u8981\u3002", "conclusion": "\u7efc\u4e0a\u6240\u8ff0\uff0c\u4e3a\u4e86\u63d0\u9ad8\u793e\u4f1a\u673a\u5668\u4eba\u5728\u5404\u79cd\u73af\u5883\u4e0b\u7684\u53ef\u9760\u6027\u548c\u88ab\u63a5\u53d7\u5ea6\uff0c\u589e\u5f3a\u5176\u97e7\u6027\u662f\u4e00\u4e2a\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2510.21571", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21571", "abs": "https://arxiv.org/abs/2510.21571", "authors": ["Qixiu Li", "Yu Deng", "Yaobo Liang", "Lin Luo", "Lei Zhou", "Chengtang Yao", "Lingqi Zeng", "Zhiyuan Feng", "Huizhi Liang", "Sicheng Xu", "Yizhong Zhang", "Xi Chen", "Hao Chen", "Lily Sun", "Dong Chen", "Jiaolong Yang", "Baining Guo"], "title": "Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos", "comment": "Project page: https://microsoft.github.io/VITRA/", "summary": "This paper presents a novel approach for pretraining robotic manipulation\nVision-Language-Action (VLA) models using a large corpus of unscripted\nreal-life video recordings of human hand activities. Treating human hand as\ndexterous robot end-effector, we show that \"in-the-wild\" egocentric human\nvideos without any annotations can be transformed into data formats fully\naligned with existing robotic V-L-A training data in terms of task granularity\nand labels. This is achieved by the development of a fully-automated holistic\nhuman activity analysis approach for arbitrary human hand videos. This approach\ncan generate atomic-level hand activity segments and their language\ndescriptions, each accompanied with framewise 3D hand motion and camera motion.\nWe process a large volume of egocentric videos and create a hand-VLA training\ndataset containing 1M episodes and 26M frames. This training data covers a wide\nrange of objects and concepts, dexterous manipulation tasks, and environment\nvariations in real life, vastly exceeding the coverage of existing robot data.\nWe design a dexterous hand VLA model architecture and pretrain the model on\nthis dataset. The model exhibits strong zero-shot capabilities on completely\nunseen real-world observations. Additionally, fine-tuning it on a small amount\nof real robot action data significantly improves task success rates and\ngeneralization to novel objects in real robotic experiments. We also\ndemonstrate the appealing scaling behavior of the model's task performance with\nrespect to pretraining data scale. We believe this work lays a solid foundation\nfor scalable VLA pretraining, advancing robots toward truly generalizable\nembodied intelligence.", "AI": {"tldr": "A novel method for pretraining robotic VLA models using unscripted human hand activity videos, resulting in a large dataset and a dexterous hand VLA model with strong zero-shot and fine-tuning performance.", "motivation": "The motivation behind this paper is to leverage unannotated, real-life human hand activity videos to create a large-scale training dataset for robotic manipulation Vision-Language-Action (VLA) models, in order to improve the generalization and dexterity of robots in real-world scenarios.", "method": "The authors developed a fully-automated holistic human activity analysis approach that can process unscripted real-life video recordings of human hand activities, generating atomic-level hand activity segments with language descriptions and 3D motion data. They created a large-scale training dataset from these videos and used it to pretrain a dexterous hand VLA model.", "result": "The pretrained model demonstrated robust zero-shot performance on unseen real-world observations and, when fine-tuned with a small amount of real robot action data, achieved higher task success rates and better generalization to new objects. The model also showed an appealing increase in task performance as the scale of pretraining data grew.", "conclusion": "This work establishes a strong foundation for scalable Vision-Language-Action (VLA) pretraining, which is key to advancing robots towards more generalizable embodied intelligence. The model shows zero-shot capabilities and improved task success rates with fine-tuning on real robot data, along with good scaling behavior as the amount of pretraining data increases."}}
{"id": "2510.21648", "categories": ["cs.RO", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.21648", "abs": "https://arxiv.org/abs/2510.21648", "authors": ["Inbazhagan Ravikumar", "Ram Sundhar", "Narendhiran Vijayakumar"], "title": "Design and Structural Validation of a Micro-UAV with On-Board Dynamic Route Planning", "comment": "8 pages, 4 figures, 4 tables", "summary": "Micro aerial vehicles are becoming increasingly important in search and\nrescue operations due to their agility, speed, and ability to access confined\nspaces or hazardous areas. However, designing lightweight aerial systems\npresents significant structural, aerodynamic, and computational challenges.\nThis work addresses two key limitations in many low-cost aerial systems under\ntwo kilograms: their lack of structural durability during flight through rough\nterrains and inability to replan paths dynamically when new victims or\nobstacles are detected. We present a fully customised drone built from scratch\nusing only commonly available components and materials, emphasising modularity,\nlow cost, and ease of assembly. The structural frame is reinforced with\nlightweight yet durable materials to withstand impact, while the onboard\ncontrol system is powered entirely by free, open-source software solutions. The\nproposed system demonstrates real-time perception and adaptive navigation\ncapabilities without relying on expensive hardware accelerators, offering an\naffordable and practical solution for real-world search and rescue missions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5b8c\u5168\u5b9a\u5236\u7684\u4f4e\u6210\u672c\u65e0\u4eba\u673a\uff0c\u5b83\u4f7f\u7528\u5e38\u89c1\u7684\u7ec4\u4ef6\u548c\u6750\u6599\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\uff0c\u5f3a\u8c03\u6a21\u5757\u5316\u3001\u4f4e\u6210\u672c\u548c\u6613\u4e8e\u7ec4\u88c5\u3002\u8be5\u65e0\u4eba\u673a\u7ed3\u6784\u6846\u67b6\u91c7\u7528\u8f7b\u4fbf\u4f46\u8010\u7528\u7684\u6750\u6599\u52a0\u56fa\uff0c\u80fd\u591f\u627f\u53d7\u51b2\u51fb\uff1b\u5176\u673a\u8f7d\u63a7\u5236\u7cfb\u7edf\u5b8c\u5168\u7531\u514d\u8d39\u5f00\u6e90\u8f6f\u4ef6\u89e3\u51b3\u65b9\u6848\u9a71\u52a8\uff0c\u5c55\u793a\u4e86\u5b9e\u65f6\u611f\u77e5\u548c\u81ea\u9002\u5e94\u5bfc\u822a\u80fd\u529b\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u641c\u6551\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ecf\u6d4e\u5b9e\u60e0\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5fae\u578b\u98de\u884c\u5668\u5728\u641c\u7d22\u548c\u6551\u63f4\u884c\u52a8\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u4eec\u5177\u6709\u654f\u6377\u6027\u3001\u901f\u5ea6\u4ee5\u53ca\u8fdb\u5165\u72ed\u5c0f\u7a7a\u95f4\u6216\u5371\u9669\u533a\u57df\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u7a7a\u4e2d\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u7684\u7ed3\u6784\u3001\u7a7a\u6c14\u52a8\u529b\u5b66\u548c\u8ba1\u7b97\u6311\u6218\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u8bb8\u591a\u4e24\u516c\u65a4\u4ee5\u4e0b\u7684\u4f4e\u6210\u672c\u7a7a\u4e2d\u7cfb\u7edf\u6765\u8bf4\uff0c\u5b83\u4eec\u5728\u7a7f\u8d8a\u5d0e\u5c96\u5730\u5f62\u65f6\u7f3a\u4e4f\u7ed3\u6784\u8010\u4e45\u6027\uff0c\u5e76\u4e14\u65e0\u6cd5\u5728\u68c0\u6d4b\u5230\u65b0\u53d7\u5bb3\u8005\u6216\u969c\u788d\u7269\u65f6\u52a8\u6001\u91cd\u65b0\u89c4\u5212\u8def\u5f84\u3002", "method": "\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u5b9a\u5236\u7684\u65e0\u4eba\u673a\uff0c\u4ece\u5934\u5f00\u59cb\u4f7f\u7528\u4ec5\u5e38\u89c1\u7684\u7ec4\u4ef6\u548c\u6750\u6599\u5efa\u9020\uff0c\u6ce8\u91cd\u6a21\u5757\u5316\u3001\u4f4e\u6210\u672c\u4e0e\u6613\u88c5\u914d\u6027\u3002\u7ed3\u6784\u6846\u67b6\u901a\u8fc7\u8f7b\u8d28\u5374\u575a\u56fa\u7684\u6750\u6599\u5f97\u5230\u52a0\u5f3a\u4ee5\u62b5\u6297\u51b2\u51fb\uff0c\u800c\u673a\u8f7d\u63a7\u5236\u7cfb\u7edf\u5219\u5b8c\u5168\u4f9d\u8d56\u4e8e\u81ea\u7531\u5f00\u653e\u6e90\u4ee3\u7801\u8f6f\u4ef6\u89e3\u51b3\u65b9\u6848\u6765\u8fd0\u4f5c\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5c55\u793a\u51fa\u65e0\u9700\u4f9d\u9760\u6602\u8d35\u786c\u4ef6\u52a0\u901f\u5668\u5373\u53ef\u5b9e\u73b0\u7684\u5b9e\u65f6\u611f\u77e5\u4e0e\u9002\u5e94\u6027\u5bfc\u822a\u529f\u80fd\uff0c\u4e3a\u5b9e\u9645\u4e2d\u7684\u641c\u7d22\u53ca\u6551\u63f4\u4efb\u52a1\u63d0\u4f9b\u4e86\u65e2\u8d1f\u62c5\u5f97\u8d77\u53c8\u5207\u5b9e\u53ef\u884c\u7684\u9009\u62e9\u3002", "conclusion": "\u901a\u8fc7\u672c\u7814\u7a76\u5f00\u53d1\u7684\u8fd9\u6b3e\u5168\u5b9a\u5236\u5316\u65e0\u4eba\u673a\uff0c\u4e0d\u4ec5\u89e3\u51b3\u4e86\u73b0\u6709\u4f4e\u6210\u672c\u65e0\u4eba\u673a\u5728\u7ed3\u6784\u8010\u4e45\u6027\u548c\u52a8\u6001\u8def\u5f84\u91cd\u89c4\u5212\u65b9\u9762\u7684\u5c40\u9650\uff0c\u8fd8\u8bc1\u660e\u4e86\u5229\u7528\u901a\u7528\u90e8\u4ef6\u7ed3\u5408\u5f00\u6e90\u8f6f\u4ef6\u53ef\u4ee5\u521b\u9020\u51fa\u9002\u5408\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u641c\u7d22\u548c\u6551\u63f4\u9886\u57df\u3002"}}
