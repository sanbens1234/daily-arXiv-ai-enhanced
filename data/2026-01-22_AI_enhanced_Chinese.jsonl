{"id": "2601.14352", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14352", "abs": "https://arxiv.org/abs/2601.14352", "authors": ["Huajie Tan", "Enshen Zhou", "Zhiyu Li", "Yijie Xu", "Yuheng Ji", "Xiansheng Chen", "Cheng Chi", "Pengwei Wang", "Huizhu Jia", "Yulong Ao", "Mingyu Cao", "Sixiang Chen", "Zhe Li", "Mengzhen Liu", "Zixiao Wang", "Shanyu Rong", "Yaoxu Lyu", "Zhongxia Zhao", "Peterson Co", "Yibo Li", "Yi Han", "Shaoxuan Xie", "Guocai Yao", "Songjing Wang", "Leiduo Zhang", "Xi Yang", "Yance Jiao", "Donghai Shi", "Kunchang Xie", "Shaokai Nie", "Chunlei Men", "Yonghua Lin", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "title": "RoboBrain 2.5: Depth in Sight, Time in Mind", "comment": "37 pages, 13 figures, Technical Report", "summary": "We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io", "AI": {"tldr": "RoboBrain 2.5\u662f\u4e00\u4e2a\u5148\u8fdb\u7684AI\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u7684\u65f6\u7a7a\u76d1\u7763\u8bad\u7ec3\uff0c\u589e\u5f3a\u4e86\u901a\u7528\u611f\u77e5\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\u3002\u5b83\u5f15\u5165\u4e86\u7cbe\u786e3D\u7a7a\u95f4\u63a8\u7406\u548c\u5bc6\u96c6\u65f6\u95f4\u4ef7\u503c\u4f30\u8ba1\u4e24\u5927\u5347\u7ea7\uff0c\u63a8\u52a8\u4e86\u66f4\u7269\u7406\u5316\u548c\u6267\u884c\u611f\u77e5\u7684\u5177\u8eab\u667a\u80fd\u53d1\u5c55\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7cbe\u7ec6\u7684\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u5f00\u53d1RoboBrain 2.5\u7684\u4e3b\u8981\u52a8\u673a\u662f\u63d0\u5347AI\u5728\u7406\u89e3\u7269\u7406\u4e16\u754c\u4ee5\u53ca\u6267\u884c\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u65f6\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5f15\u5165\u5bf93D\u7a7a\u95f4\u66f4\u52a0\u51c6\u786e\u7684\u7406\u89e3\u4e0e\u8bc4\u4f30\u884c\u52a8\u8fdb\u5c55\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u66f4\u4e3a\u7cbe\u51c6\u4e14\u5177\u5907\u81ea\u6211\u53cd\u9988\u673a\u5236\u7684\u673a\u5668\u4eba\u667a\u80fd\u3002", "method": "RoboBrain 2.5\u901a\u8fc7\u5bf9\u9ad8\u8d28\u91cf\u65f6\u7a7a\u6570\u636e\u8fdb\u884c\u5e7f\u6cdb\u8bad\u7ec3\u6765\u6539\u8fdb\u5176\u524d\u8eab\u7248\u672c\u3002\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u4ece2D\u50cf\u7d20\u76f8\u5bf9\u5b9a\u4f4d\u5230\u6df1\u5ea6\u611f\u77e5\u5750\u6807\u9884\u6d4b\u53ca\u7edd\u5bf9\u5ea6\u91cf\u7ea6\u675f\u7406\u89e3\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e3D\u7a7a\u95f4\u63a8\u7406\uff1b\u540c\u65f6\uff0c\u8fd8\u53d1\u5c55\u4e86\u80fd\u591f\u63d0\u4f9b\u5bc6\u96c6\u6b65\u9aa4\u7ea7\u8fdb\u5ea6\u9884\u6d4b\u548c\u72b6\u6001\u7406\u89e3\u7684\u5bc6\u96c6\u65f6\u95f4\u4ef7\u503c\u4f30\u8ba1\u6280\u672f\u3002", "result": "\u8fd9\u4e9b\u6539\u8fdb\u4f7f\u5f97RoboBrain 2.5\u80fd\u591f\u751f\u6210\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u5b8c\u65743D\u64cd\u7eb5\u8f68\u8ff9\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u63d0\u4f9b\u7a33\u5b9a\u53cd\u9988\u4fe1\u53f7\u4ee5\u652f\u6301\u540e\u7eed\u5b66\u4e60\u8fc7\u7a0b\u3002", "conclusion": "RoboBrain 2.5\u5c55\u793a\u4e86\u5728\u589e\u5f3aAI\u7cfb\u7edf\u5bf9\u4e8e\u771f\u5b9e\u4e16\u754c\u73af\u5883\u7684\u7406\u89e3\u529b\u53ca\u5176\u6267\u884c\u590d\u6742\u7cbe\u5bc6\u64cd\u63a7\u4efb\u52a1\u65b9\u9762\u53d6\u5f97\u7684\u91cd\u5927\u8fdb\u6b65\u3002"}}
{"id": "2601.14445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14445", "abs": "https://arxiv.org/abs/2601.14445", "authors": ["Aiden Mazidi", "Majid Roshanfar", "Amir Sayadi", "Javad Dargahi", "Jake Barralet", "Liane S. Feldman", "Amir Hooshiar"], "title": "Robust Haptic Rendering Using a Nonlinear Impedance Matching Approach (NIMA) for Robotic Laparoscopic Surgery", "comment": null, "summary": "Background: The integration of haptic feedback into robot-assisted minimally invasive surgery (RAMIS) has long been limited by challenges in accurately rendering forces and ensuring system safety. The need for robust, high-fidelity haptic systems is critical for enhancing the precision and reliability of teleoperated surgical tools. Methods: In this study, we present a Nonlinear Impedance Matching Approach (NIMA) designed to improve force rendering by accurately modelling complex tool-tissue interactions. Based on our previously validated Impedance Matching Approach (IMA), our novel NIMA method includes nonlinear dynamics to capture and render tool-tissue forces effectively. Results: NIMA improves force feedback accuracy with a mean absolute error (MAE) of 0.01 (SD 0.02) N, achieving a 95% reduction in MAE compared to IMA. Furthermore, NIMA effectively eliminates haptic \"kickback\" by ensuring no force is applied by the haptic device to the user's hand when they release the handle, enhancing both patient safety and user comfort. Conclusion: NIMA's ability to account for nonlinearities in tool-tissue interactions provides an improvement in force fidelity, responsiveness, and precision across various surgical conditions. Our findings promote the advancement of haptic feedback systems for robotic surgery, offering a realistic and reliable interface for robot-assisted surgical procedures.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u7ebf\u6027\u963b\u6297\u5339\u914d\u65b9\u6cd5\uff08NIMA\uff09\uff0c\u7528\u4e8e\u6539\u5584\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u624b\u672f\u4e2d\u7684\u529b\u53cd\u9988\u7cbe\u5ea6\u3002NIMA\u901a\u8fc7\u51c6\u786e\u5efa\u6a21\u5de5\u5177-\u7ec4\u7ec7\u4ea4\u4e92\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u529b\u53cd\u9988\u7684\u51c6\u786e\u6027\uff0c\u5e76\u89e3\u51b3\u4e86\u89e6\u89c9\u201c\u56de\u5f39\u201d\u95ee\u9898\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u60a3\u8005\u5b89\u5168\u6027\u548c\u7528\u6237\u8212\u9002\u5ea6\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u624b\u672f\u4e2d\uff0c\u51c6\u786e\u518d\u73b0\u529b\u53cd\u9988\u5e76\u4fdd\u8bc1\u7cfb\u7edf\u5b89\u5168\u6027\u4e00\u76f4\u9762\u4e34\u6311\u6218\u3002\u5f00\u53d1\u4e00\u4e2a\u9c81\u68d2\u4e14\u9ad8\u4fdd\u771f\u7684\u89e6\u89c9\u7cfb\u7edf\u5bf9\u4e8e\u63d0\u9ad8\u8fdc\u7a0b\u64cd\u4f5c\u5916\u79d1\u5de5\u5177\u7684\u7cbe\u786e\u5ea6\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u975e\u7ebf\u6027\u963b\u6297\u5339\u914d\u65b9\u6cd5\uff08NIMA\uff09\uff0c\u5b83\u57fa\u4e8e\u5148\u524d\u9a8c\u8bc1\u8fc7\u7684\u963b\u6297\u5339\u914d\u65b9\u6cd5\uff08IMA\uff09\u8fdb\u884c\u6539\u8fdb\uff0c\u901a\u8fc7\u5f15\u5165\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u6765\u66f4\u6709\u6548\u5730\u6355\u6349\u548c\u518d\u73b0\u5de5\u5177\u4e0e\u7ec4\u7ec7\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u529b\u3002", "result": "NIMA\u5c06\u529b\u53cd\u9988\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u81f30.01\u725b\u987f\uff08\u6807\u51c6\u5dee0.02\u725b\u987f\uff09\uff0c\u76f8\u6bd4IMA\u51cf\u5c11\u4e8695%\u7684\u8bef\u5dee\u3002\u6b64\u5916\uff0cNIMA\u8fd8\u6d88\u9664\u4e86\u7528\u6237\u91ca\u653e\u624b\u67c4\u65f6\u7531\u89e6\u89c9\u88c5\u7f6e\u65bd\u52a0\u7ed9\u7528\u6237\u7684\u529b\uff0c\u5373\u89e3\u51b3\u4e86\u89e6\u89c9\u2018\u56de\u5f39\u2019\u73b0\u8c61\uff0c\u8fd9\u4e0d\u4ec5\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u4e5f\u589e\u52a0\u4e86\u5b89\u5168\u6027\u3002", "conclusion": "NIMA\u80fd\u591f\u66f4\u597d\u5730\u8003\u8651\u5de5\u5177-\u7ec4\u7ec7\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u7684\u975e\u7ebf\u6027\u56e0\u7d20\uff0c\u4ece\u800c\u5728\u591a\u79cd\u624b\u672f\u6761\u4ef6\u4e0b\u63d0\u4f9b\u66f4\u4f73\u7684\u529b\u91cf\u4fdd\u771f\u5ea6\u3001\u54cd\u5e94\u901f\u5ea6\u53ca\u7cbe\u786e\u5ea6\u3002\u8fd9\u4e9b\u53d1\u73b0\u4fc3\u8fdb\u4e86\u673a\u5668\u4eba\u624b\u672f\u4e2d\u89e6\u89c9\u53cd\u9988\u7cfb\u7edf\u7684\u8fdb\u6b65\uff0c\u4e3a\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u63d0\u4f9b\u4e86\u66f4\u52a0\u73b0\u5b9e\u53ef\u9760\u7684\u754c\u9762\u3002"}}
{"id": "2601.14492", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14492", "abs": "https://arxiv.org/abs/2601.14492", "authors": ["Malak Mansour", "Ali Abouzeid", "Zezhou Sun", "Qinbo Sun", "Dezhen Song", "Abdalla Swikir"], "title": "UNCLE-Grasp: Uncertainty-Aware Grasping of Leaf-Occluded Strawberries", "comment": null, "summary": "Robotic strawberry harvesting is challenging under partial occlusion, where leaves induce significant geometric uncertainty and make grasp decisions based on a single deterministic shape estimate unreliable. From a single partial observation, multiple incompatible 3D completions may be plausible, causing grasps that appear feasible on one completion to fail on another. We propose an uncertainty-aware grasping pipeline for partially occluded strawberries that explicitly models completion uncertainty arising from both occlusion and learned shape reconstruction. Our approach uses point cloud completion with Monte Carlo dropout to sample multiple shape hypotheses, generates candidate grasps for each completion, and evaluates grasp feasibility using physically grounded force-closure-based metrics. Rather than selecting a grasp based on a single estimate, we aggregate feasibility across completions and apply a conservative lower confidence bound (LCB) criterion to decide whether a grasp should be attempted or safely abstained. We evaluate the proposed method in simulation and on a physical robot across increasing levels of synthetic and real leaf occlusion. Results show that uncertainty-aware decision making enables reliable abstention from high-risk grasp attempts under severe occlusion while maintaining robust grasp execution when geometric confidence is sufficient, outperforming deterministic baselines in both simulated and physical robot experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u90e8\u5206\u906e\u6321\u8349\u8393\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6293\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u70b9\u4e91\u8865\u5168\u548c\u8499\u7279\u5361\u6d1bdropout\u6765\u91c7\u6837\u591a\u4e2a\u5f62\u72b6\u5047\u8bbe\uff0c\u5e76\u4f7f\u7528\u7269\u7406\u57fa\u7840\u7684\u529b\u95ed\u5408\u5ea6\u91cf\u8bc4\u4f30\u6293\u53d6\u53ef\u884c\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u5b9e\u9645\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8e\u786e\u5b9a\u6027\u57fa\u7ebf\u3002", "motivation": "\u5728\u90e8\u5206\u906e\u6321\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8e\u5355\u4e00\u786e\u5b9a\u6027\u5f62\u72b6\u4f30\u8ba1\u7684\u6293\u53d6\u51b3\u7b56\u4e0d\u53ef\u9760\uff0c\u56e0\u4e3a\u4ece\u5355\u6b21\u5c40\u90e8\u89c2\u5bdf\u53ef\u80fd\u63a8\u5bfc\u51fa\u591a\u79cd\u4e0d\u517c\u5bb9\u76843D\u8865\u5168\u65b9\u6848\uff0c\u5bfc\u81f4\u67d0\u4e9b\u770b\u4f3c\u53ef\u884c\u7684\u6293\u53d6\u52a8\u4f5c\u5b9e\u9645\u4e0a\u4f1a\u5931\u8d25\u3002", "method": "\u91c7\u7528\u70b9\u4e91\u8865\u5168\u7ed3\u5408\u8499\u7279\u5361\u6d1bdropout\u6280\u672f\u751f\u6210\u591a\u4e2a\u5f62\u72b6\u5047\u8bbe\uff0c\u4e3a\u6bcf\u4e2a\u5047\u8bbe\u751f\u6210\u5019\u9009\u6293\u53d6\u59ff\u6001\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u7269\u7406\u7684\u529b\u95ed\u5408\u6307\u6807\u8bc4\u4f30\u6293\u53d6\u53ef\u884c\u6027\u3002\u6700\u7ec8\uff0c\u901a\u8fc7\u7efc\u5408\u5404\u8865\u5168\u65b9\u6848\u4e0b\u7684\u53ef\u884c\u6027\u5e76\u5e94\u7528\u4fdd\u5b88\u7684\u4e0b\u7f6e\u4fe1\u754c\uff08LCB\uff09\u51c6\u5219\u51b3\u5b9a\u662f\u5426\u6267\u884c\u6293\u53d6\u6216\u5b89\u5168\u653e\u5f03\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u4eff\u771f\u73af\u5883\u53ca\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e25\u91cd\u906e\u6321\u6761\u4ef6\u4e0b\uff0c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u80fd\u591f\u53ef\u9760\u5730\u907f\u514d\u9ad8\u98ce\u9669\u6293\u53d6\u5c1d\u8bd5\uff0c\u540c\u65f6\u5728\u51e0\u4f55\u4fe1\u5fc3\u5145\u8db3\u65f6\u4fdd\u6301\u7a33\u5065\u7684\u6293\u53d6\u6267\u884c\u6548\u679c\uff0c\u603b\u4f53\u8868\u73b0\u4f18\u4e8e\u786e\u5b9a\u6027\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u6709\u6548\u5904\u7406\u90e8\u5206\u906e\u6321\u573a\u666f\u4e0b\u8349\u8393\u91c7\u6458\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u5f62\u72b6\u91cd\u5efa\u4e0d\u786e\u5b9a\u6027\u4e0e\u6293\u53d6\u7b56\u7565\u9009\u62e9\u63d0\u9ad8\u4e86\u6293\u53d6\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.14550", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14550", "abs": "https://arxiv.org/abs/2601.14550", "authors": ["Tailai Cheng", "Kejia Chen", "Lingyun Chen", "Liding Zhang", "Yue Zhang", "Yao Ling", "Mahdi Hamad", "Zhenshan Bing", "Fan Wu", "Karan Sharma", "Alois Knoll"], "title": "TacUMI: A Multi-Modal Universal Manipulation Interface for Contact-Rich Tasks", "comment": null, "summary": "Task decomposition is critical for understanding and learning complex long-horizon manipulation tasks. Especially for tasks involving rich physical interactions, relying solely on visual observations and robot proprioceptive information often fails to reveal the underlying event transitions. This raises the requirement for efficient collection of high-quality multi-modal data as well as robust segmentation method to decompose demonstrations into meaningful modules. Building on the idea of the handheld demonstration device Universal Manipulation Interface (UMI), we introduce TacUMI, a multi-modal data collection system that integrates additionally ViTac sensors, force-torque sensor, and pose tracker into a compact, robot-compatible gripper design, which enables synchronized acquisition of all these modalities during human demonstrations. We then propose a multi-modal segmentation framework that leverages temporal models to detect semantically meaningful event boundaries in sequential manipulations. Evaluation on a challenging cable mounting task shows more than 90 percent segmentation accuracy and highlights a remarkable improvement with more modalities, which validates that TacUMI establishes a practical foundation for both scalable collection and segmentation of multi-modal demonstrations in contact-rich tasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6570\u636e\u6536\u96c6\u7cfb\u7edfTacUMI\u53ca\u5176\u914d\u5957\u7684\u591a\u6a21\u6001\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u590d\u6742\u957f\u5468\u671f\u64cd\u4f5c\u4efb\u52a1\u7684\u7406\u89e3\u548c\u5b66\u4e60\u3002\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u4f20\u611f\u5668\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u540c\u6b65\u83b7\u53d6\u4eba\u7c7b\u6f14\u793a\u4e2d\u7684\u591a\u79cd\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u6a21\u578b\u8bc6\u522b\u64cd\u4f5c\u5e8f\u5217\u4e2d\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u4e8b\u4ef6\u8fb9\u754c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u7535\u7f06\u5b89\u88c5\u4efb\u52a1\u4e0a\uff0c\u5176\u5206\u5272\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u4e14\u968f\u7740\u6a21\u6001\u6570\u91cf\u7684\u589e\u52a0\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u9488\u5bf9\u4ec5\u4f9d\u8d56\u89c6\u89c9\u89c2\u5bdf\u4e0e\u673a\u5668\u4eba\u672c\u4f53\u611f\u89c9\u4fe1\u606f\u96be\u4ee5\u63ed\u793a\u590d\u6742\u957f\u5468\u671f\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6f5c\u5728\u4e8b\u4ef6\u8f6c\u6362\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u9700\u8981\u9ad8\u6548\u6536\u96c6\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u4ee5\u53ca\u5f00\u53d1\u7a33\u5065\u7684\u5206\u5272\u65b9\u6cd5\u6765\u5c06\u6f14\u793a\u5206\u89e3\u4e3a\u6709\u610f\u4e49\u7684\u6a21\u5757\u3002", "method": "\u5f00\u53d1\u4e86\u540d\u4e3aTacUMI\u7684\u591a\u6a21\u6001\u6570\u636e\u6536\u96c6\u7cfb\u7edf\uff0c\u5b83\u96c6\u6210\u4e86ViTac\u4f20\u611f\u5668\u3001\u529b\u77e9\u4f20\u611f\u5668\u53ca\u59ff\u6001\u8ffd\u8e2a\u5668\u4e8e\u4e00\u4e2a\u7d27\u51d1\u4e14\u517c\u5bb9\u673a\u5668\u4eba\u7684\u5939\u722a\u8bbe\u8ba1\u4e2d\uff1b\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u5206\u5272\u6846\u67b6\uff0c\u5229\u7528\u65f6\u95f4\u6a21\u578b\u5728\u8fde\u7eed\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u68c0\u6d4b\u8bed\u4e49\u4e0a\u91cd\u8981\u7684\u4e8b\u4ef6\u8fb9\u754c\u3002", "result": "\u5bf9\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u7535\u7f06\u5b89\u88c5\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u5206\u5272\u51c6\u786e\u7387\u8d85\u8fc7\u4e8690%\u3002\u540c\u65f6\uff0c\u968f\u7740\u4f7f\u7528\u6a21\u6001\u6570\u91cf\u7684\u589e\u52a0\uff0c\u7cfb\u7edf\u7684\u6027\u80fd\u5f97\u5230\u4e86\u663e\u8457\u6539\u5584\u3002", "conclusion": "TacUMI\u4e0d\u4ec5\u4e3a\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u7684\u591a\u6a21\u6001\u6f14\u793a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u6536\u96c6\u624b\u6bb5\uff0c\u8fd8\u4e3a\u5176\u5206\u5272\u5960\u5b9a\u4e86\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2601.14617", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.14617", "abs": "https://arxiv.org/abs/2601.14617", "authors": ["Yunfeng Lin", "Li Xu", "Yong Yu", "Jiangmiao Pang", "Weinan Zhang"], "title": "UniCon: A Unified System for Efficient Robot Learning Transfers", "comment": "in submission, under review", "summary": "Deploying learning-based controllers across heterogeneous robots is challenging due to platform differences, inconsistent interfaces, and inefficient middleware. To address these issues, we present UniCon, a lightweight framework that standardizes states, control flow, and instrumentation across platforms. It decomposes workflows into execution graphs with reusable components, separating system states from control logic to enable plug-and-play deployment across various robot morphologies. Unlike traditional middleware, it prioritizes efficiency through batched, vectorized data flow, minimizing communication overhead and improving inference latency. This modular, data-oriented approach enables seamless sim-to-real transfer with minimal re-engineering. We demonstrate that UniCon reduces code redundancy when transferring workflows and achieves higher inference efficiency compared to ROS-based systems. Deployed on over 12 robot models from 7 manufacturers, it has been successfully integrated into ongoing research projects, proving its effectiveness in real-world scenarios.", "AI": {"tldr": "UniCon, a lightweight and efficient framework, standardizes the deployment of learning-based controllers across different robot platforms, reducing code redundancy and improving inference efficiency.", "motivation": "The motivation behind this paper is to tackle the challenges faced when deploying learning-based controllers on various robots, such as platform differences, inconsistent interfaces, and inefficient middleware. These issues can lead to increased development time, higher costs, and reduced performance of robotic systems.", "method": "The authors introduce UniCon, a novel framework designed to standardize states, control flow, and instrumentation across diverse robot platforms. It employs a modular, data-oriented design that separates system states from control logic, allowing for more flexible and efficient deployment. A key feature of UniCon is its use of batched, vectorized data flow, which minimizes communication overhead and enhances inference latency compared to traditional middleware solutions like ROS.", "result": "The implementation of UniCon has shown significant improvements in terms of reducing code redundancy during workflow transfers between different robot models and achieving better inference efficiency than ROS-based systems. The framework has been successfully deployed on over 12 distinct robot models from 7 different manufacturers and has been integrated into multiple ongoing research projects, demonstrating its practical value and adaptability in real-world applications.", "conclusion": "In conclusion, UniCon provides a promising solution for the seamless and efficient deployment of learning-based controllers across heterogeneous robot platforms. By addressing critical challenges related to platform compatibility and middleware inefficiency, it enables researchers and developers to focus more on advancing control algorithms rather than dealing with integration issues, thus accelerating progress in robotics."}}
{"id": "2601.14622", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14622", "abs": "https://arxiv.org/abs/2601.14622", "authors": ["Ling Xiao", "Toshihiko Yamasaki"], "title": "Probing Prompt Design for Socially Compliant Robot Navigation with Vision Language Models", "comment": null, "summary": "Language models are increasingly used for social robot navigation, yet existing benchmarks largely overlook principled prompt design for socially compliant behavior. This limitation is particularly relevant in practice, as many systems rely on small vision language models (VLMs) for efficiency. Compared to large language models, small VLMs exhibit weaker decision-making capabilities, making effective prompt design critical for accurate navigation. Inspired by cognitive theories of human learning and motivation, we study prompt design along two dimensions: system guidance (action-focused, reasoning-oriented, and perception-reasoning prompts) and motivational framing, where models compete against humans, other AI systems, or their past selves. Experiments on two socially compliant navigation datasets reveal three key findings. First, for non-finetuned GPT-4o, competition against humans achieves the best performance, while competition against other AI systems performs worst. For finetuned models, competition against the model's past self yields the strongest results, followed by competition against humans, with performance further influenced by coupling effects among prompt design, model choice, and dataset characteristics. Second, inappropriate system prompt design can significantly degrade performance, even compared to direct finetuning. Third, while direct finetuning substantially improves semantic-level metrics such as perception, prediction, and reasoning, it yields limited gains in action accuracy. In contrast, our system prompts produce a disproportionately larger improvement in action accuracy, indicating that the proposed prompt design primarily acts as a decision-level constraint rather than a representational enhancement.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5bf9\u4e8e\u672a\u5fae\u8c03\u7684GPT-4o\uff0c\u5728\u4e0e\u4eba\u7c7b\u7ade\u4e89\u65f6\u8868\u73b0\u6700\u4f73\uff1b\u800c\u5bf9\u4e8e\u5fae\u8c03\u8fc7\u7684\u6a21\u578b\uff0c\u4e0e\u8fc7\u53bb\u7684\u81ea\u5df1\u7ade\u4e89\u4ea7\u751f\u6700\u597d\u7684\u7ed3\u679c\u3002\u7cfb\u7edf\u63d0\u793a\u8bbe\u8ba1\u5bf9\u793e\u4f1a\u5408\u89c4\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9002\u5f53\u7684\u63d0\u793a\u8bbe\u8ba1\u53ef\u4ee5\u6bd4\u76f4\u63a5\u5fae\u8c03\u66f4\u6709\u6548\u5730\u63d0\u9ad8\u884c\u52a8\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u793e\u4ea4\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u4f7f\u7528\u7684\u8bed\u8a00\u6a21\u578b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5ffd\u7565\u4e86\u4e3a\u4e86\u5b9e\u73b0\u793e\u4f1a\u5408\u89c4\u884c\u4e3a\u800c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u3002\u7279\u522b\u662f\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u76f8\u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u8f83\u5f31\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u4f7f\u5f97\u6709\u6548\u7684\u63d0\u793a\u8bbe\u8ba1\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u4e24\u4e2a\u7ef4\u5ea6\u63a2\u7d22\u4e86\u63d0\u793a\u8bbe\u8ba1\uff1a\u7cfb\u7edf\u6307\u5bfc\uff08\u884c\u52a8\u5bfc\u5411\u3001\u63a8\u7406\u5bfc\u5411\u548c\u611f\u77e5\u63a8\u7406\u63d0\u793a\uff09\u548c\u52a8\u673a\u6846\u67b6\u8bbe\u5b9a\uff0c\u5176\u4e2d\u8ba9\u6a21\u578b\u4e0e\u4eba\u7c7b\u3001\u5176\u4ed6AI\u7cfb\u7edf\u6216\u5b83\u4eec\u7684\u8fc7\u53bb\u7248\u672c\u8fdb\u884c\u7ade\u4e89\u3002\u5b9e\u9a8c\u57fa\u4e8e\u4e24\u4e2a\u793e\u4f1a\u5408\u89c4\u5bfc\u822a\u6570\u636e\u96c6\u5c55\u5f00\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u672a\u7ecf\u5fae\u8c03\u7684GPT-4o\u800c\u8a00\uff0c\u4e0e\u4eba\u7c7b\u7684\u7ade\u4e89\u80fd\u591f\u83b7\u5f97\u6700\u4f73\u8868\u73b0\uff0c\u800c\u4e0e\u5176\u4ed6AI\u7cfb\u7edf\u7684\u7ade\u4e89\u5219\u8868\u73b0\u6700\u5dee\u3002\u5bf9\u4e8e\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u6765\u8bf4\uff0c\u4e0e\u5176\u81ea\u8eab\u5386\u53f2\u7248\u672c\u7684\u7ade\u4e89\u80fd\u4ea7\u751f\u6700\u5f3a\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u4e0d\u5f53\u7684\u7cfb\u7edf\u63d0\u793a\u8bbe\u8ba1\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\u3002\u867d\u7136\u76f4\u63a5\u5fae\u8c03\u5927\u5e45\u63d0\u9ad8\u4e86\u8bed\u4e49\u5c42\u9762\u5982\u611f\u77e5\u3001\u9884\u6d4b\u548c\u63a8\u7406\u7b49\u6307\u6807\uff0c\u4f46\u5bf9\u884c\u52a8\u51c6\u786e\u6027\u7684\u63d0\u5347\u6709\u9650\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7cfb\u7edf\u63d0\u793a\u80fd\u591f\u5728\u884c\u52a8\u51c6\u786e\u6027\u4e0a\u5e26\u6765\u4e0d\u6210\u6bd4\u4f8b\u7684\u66f4\u5927\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u6070\u5f53\u63d0\u793a\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4f1a\u5408\u89c4\u5bfc\u822a\u573a\u666f\u4e0b\u3002\u5408\u9002\u7684\u6fc0\u52b1\u6846\u67b6\u7ed3\u5408\u6709\u6548\u7684\u7cfb\u7edf\u63d0\u793a\u8bbe\u8ba1\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u673a\u5668\u4eba\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5176\u51b3\u7b56\u6c34\u5e73\u3002"}}
{"id": "2601.14628", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14628", "abs": "https://arxiv.org/abs/2601.14628", "authors": ["Weiyu Guo", "He Zhang", "Pengteng Li", "Tiefu Cai", "Ziyang Chen", "Yandong Guo", "Xiao He", "Yongkui Yang", "Ying Sun", "Hui Xiong"], "title": "A Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control", "comment": null, "summary": "Recent advances in embodied intelligence have leveraged massive scaling of data and model parameters to master natural-language command following and multi-task control. In contrast, biological systems demonstrate an innate ability to acquire skills rapidly from sparse experience. Crucially, current robotic policies struggle to replicate the dynamic stability, reflexive responsiveness, and temporal memory inherent in biological motion. Here we present Neuromorphic Vision-Language-Action (NeuroVLA), a framework that mimics the structural organization of the bio-nervous system between the cortex, cerebellum, and spinal cord. We adopt a system-level bio-inspired design: a high-level model plans goals, an adaptive cerebellum module stabilizes motion using high-frequency sensors feedback, and a bio-inspired spinal layer executes lightning-fast actions generation. NeuroVLA represents the first deployment of a neuromorphic VLA on physical robotics, achieving state-of-the-art performance. We observe the emergence of biological motor characteristics without additional data or special guidance: it stops the shaking in robotic arms, saves significant energy(only 0.4w on Neuromorphic Processor), shows temporal memory ability and triggers safety reflexes in less than 20 milliseconds.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6a21\u4eff\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u7ed3\u6784\u7684Neuromorphic Vision-Language-Action (NeuroVLA)\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u7684\u9996\u6b21\u90e8\u7f72\uff0c\u5e76\u5c55\u73b0\u4e86\u751f\u7269\u8fd0\u52a8\u7279\u6027\uff0c\u5982\u51cf\u5c11\u673a\u68b0\u81c2\u6296\u52a8\u3001\u8282\u7701\u80fd\u6e90\u3001\u8868\u73b0\u51fa\u65f6\u95f4\u8bb0\u5fc6\u80fd\u529b\u548c\u5feb\u901f\u89e6\u53d1\u5b89\u5168\u53cd\u5c04\u3002", "motivation": "\u5f53\u524d\u7684\u673a\u5668\u4eba\u7b56\u7565\u96be\u4ee5\u590d\u5236\u751f\u7269\u8fd0\u52a8\u4e2d\u7684\u52a8\u6001\u7a33\u5b9a\u6027\u3001\u53cd\u5c04\u54cd\u5e94\u6027\u548c\u65f6\u95f4\u8bb0\u5fc6\u80fd\u529b\u3002\u800c\u751f\u7269\u7cfb\u7edf\u5374\u80fd\u4ece\u7a00\u758f\u7684\u7ecf\u9a8c\u4e2d\u5feb\u901f\u5b66\u4e60\u6280\u80fd\u3002", "method": "\u63d0\u51fa\u4e86Neuromorphic Vision-Language-Action (NeuroVLA)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6a21\u62df\u4e86\u5927\u8111\u76ae\u5c42\u3001\u5c0f\u8111\u548c\u810a\u9ad3\u4e4b\u95f4\u7684\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u7ed3\u6784\u3002\u91c7\u7528\u4e86\u7cfb\u7edf\u7ea7\u7684\u751f\u7269\u542f\u53d1\u8bbe\u8ba1\uff1a\u9ad8\u7ea7\u6a21\u578b\u89c4\u5212\u76ee\u6807\uff0c\u81ea\u9002\u5e94\u5c0f\u8111\u6a21\u5757\u4f7f\u7528\u9ad8\u9891\u4f20\u611f\u5668\u53cd\u9988\u7a33\u5b9a\u52a8\u4f5c\uff0c\u751f\u7269\u542f\u53d1\u7684\u810a\u67f1\u5c42\u6267\u884c\u6781\u901f\u52a8\u4f5c\u751f\u6210\u3002", "result": "NeuroVLA\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u6700\u65b0\u7684\u6027\u80fd\u8868\u73b0\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u7279\u6b8a\u6307\u5bfc\u5373\u53ef\u51fa\u73b0\u751f\u7269\u8fd0\u52a8\u7279\u5f81\uff1a\u51cf\u5c11\u4e86\u673a\u68b0\u81c2\u6296\u52a8\u3001\u663e\u8457\u8282\u80fd\uff08\u4ec5\u97000.4\u74e6\uff09\u3001\u663e\u793a\u65f6\u95f4\u8bb0\u5fc6\u80fd\u529b\u5e76\u80fd\u572820\u6beb\u79d2\u5185\u89e6\u53d1\u5b89\u5168\u53cd\u5c04\u3002", "conclusion": "NeuroVLA\u4ee3\u8868\u4e86\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u795e\u7ecf\u5f62\u6001VLA\u7684\u9996\u6b21\u90e8\u7f72\uff0c\u5c55\u793a\u4e86\u4e0e\u751f\u7269\u8fd0\u52a8\u76f8\u4f3c\u7684\u91cd\u8981\u7279\u6027\uff0c\u4e3a\u672a\u6765\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2601.14634", "categories": ["cs.RO", "physics.bio-ph"], "pdf": "https://arxiv.org/pdf/2601.14634", "abs": "https://arxiv.org/abs/2601.14634", "authors": ["Satoru Hashimoto", "Yinlai Jiang", "Hiroshi Yokoi", "Shunta Togo"], "title": "Landing-Induced Viscoelastic Changes in an Anthropomimetic Foot Joint Structure are Modulated by Foot Structure and Posture", "comment": "27 pages, preprint", "summary": "Cadaveric studies have provided important insights into the mechanics of the human foot arch and plantar fascia. However, repeatedly probing posture-dependent viscoelastic responses immediately after landing impact is difficult in biological specimens, leaving the contribution of skeletal architecture to landing dynamics incompletely understood. In this study, we developed an anthropomimetic foot joint structure aimed at replicating the skeletal geometry of the human foot. Using a vertical drop apparatus that simulates landing and a viscoelastic system-identification model, we investigated how skeletal structure and posture modulate the apparent post-impact viscoelastic response. The results show that the multi-jointed anthropomimetic structure exhibited a higher damping ratio than simplified flat and rigid feet. Moreover, ankle dorsiflexion and toe extension systematically shifted the identified parameters, reducing the damping ratio under the tested conditions. Taken together, these findings indicate that an arch-like, multi-jointed skeletal architecture can enhance impact attenuation in an anthropomimetic mechanical foot, and that morphology and passive posture alone can tune the trade-off between attenuation and rebound. The observed posture-dependent trends are qualitatively consistent with reported differences in human landing strategies, suggesting that skeletal architecture may partly account for the modulation. Furthermore, these results highlight the engineering advantage of anatomically informed skeletal replication for achieving human-like apparent viscoelastic behavior through postural adjustment during landing.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u6a21\u4eff\u4eba\u7c7b\u8db3\u90e8\u9aa8\u9abc\u51e0\u4f55\u7ed3\u6784\u7684\u62df\u4eba\u5316\u8db3\u5173\u8282\u7ed3\u6784\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u8fd9\u79cd\u591a\u5173\u8282\u7684\u62f1\u5f62\u7ed3\u6784\u80fd\u591f\u6bd4\u7b80\u5355\u7684\u5e73\u8db3\u6216\u521a\u6027\u8db3\u63d0\u4f9b\u66f4\u9ad8\u7684\u963b\u5c3c\u6bd4\uff0c\u5e76\u4e14\u901a\u8fc7\u8c03\u6574\u59ff\u6001\u53ef\u4ee5\u8c03\u8282\u51b2\u51fb\u8870\u51cf\u4e0e\u53cd\u5f39\u4e4b\u95f4\u7684\u5e73\u8861\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u89e3\u5256\u5b66\u4e0a\u7684\u9aa8\u9abc\u590d\u5236\u4ee5\u53ca\u7740\u9646\u65f6\u7684\u59ff\u6001\u8c03\u6574\uff0c\u5728\u5de5\u7a0b\u4e0a\u5177\u6709\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8868\u89c2\u7c98\u5f39\u6027\u884c\u4e3a\u7684\u4f18\u52bf\u3002", "motivation": "\u5c3d\u7ba1\u5c38\u4f53\u7814\u7a76\u4e3a\u7406\u89e3\u4eba\u811a\u5f13\u548c\u8db3\u5e95\u7b4b\u819c\u7684\u529b\u5b66\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u4f46\u7acb\u5373\u5728\u751f\u7269\u6837\u672c\u4e2d\u91cd\u590d\u63a2\u7d22\u7740\u9646\u51b2\u51fb\u540e\u7684\u59ff\u52bf\u4f9d\u8d56\u6027\u7c98\u5f39\u6027\u53cd\u5e94\u662f\u56f0\u96be\u7684\uff0c\u8fd9\u5bfc\u81f4\u9aa8\u9abc\u7ed3\u6784\u5bf9\u843d\u5730\u52a8\u529b\u5b66\u7684\u8d21\u732e\u5c1a\u672a\u88ab\u5b8c\u5168\u7406\u89e3\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u65e8\u5728\u590d\u5236\u4eba\u811a\u9aa8\u9abc\u51e0\u4f55\u7ed3\u6784\u7684\u62df\u4eba\u5316\u8db3\u5173\u8282\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u6a21\u62df\u7740\u9646\u7684\u5782\u76f4\u4e0b\u843d\u88c5\u7f6e\u548c\u7c98\u5f39\u6027\u7cfb\u7edf\u8bc6\u522b\u6a21\u578b\u6765\u7814\u7a76\u9aa8\u9abc\u7ed3\u6784\u548c\u59ff\u6001\u5982\u4f55\u8c03\u8282\u7740\u9646\u540e\u660e\u663e\u7684\u7c98\u5f39\u6027\u54cd\u5e94\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u591a\u5173\u8282\u7684\u62df\u4eba\u5316\u7ed3\u6784\u663e\u793a\u51fa\u6bd4\u7b80\u5316\u540e\u7684\u5e73\u8db3\u548c\u521a\u6027\u8db3\u66f4\u9ad8\u7684\u963b\u5c3c\u6bd4\u3002\u6b64\u5916\uff0c\u8e1d\u5173\u8282\u80cc\u5c48\u548c\u8dbe\u4f38\u5c55\u7cfb\u7edf\u5730\u6539\u53d8\u4e86\u6240\u786e\u5b9a\u7684\u53c2\u6570\uff0c\u5728\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u964d\u4f4e\u4e86\u963b\u5c3c\u6bd4\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u4e00\u79cd\u62f1\u5f62\u3001\u591a\u5173\u8282\u7684\u9aa8\u9abc\u7ed3\u6784\u53ef\u4ee5\u5728\u62df\u4eba\u5316\u7684\u673a\u68b0\u8db3\u4e2d\u589e\u5f3a\u51b2\u51fb\u8870\u51cf\uff0c\u5e76\u4e14\u5f62\u6001\u548c\u88ab\u52a8\u59ff\u6001\u672c\u8eab\u5c31\u80fd\u8c03\u8282\u8870\u51cf\u4e0e\u53cd\u5f39\u4e4b\u95f4\u7684\u6743\u8861\u3002\u89c2\u5bdf\u5230\u7684\u59ff\u6001\u4f9d\u8d56\u8d8b\u52bf\u4e0e\u62a5\u544a\u7684\u4eba\u7c7b\u7740\u9646\u7b56\u7565\u5dee\u5f02\u5b9a\u6027\u4e00\u81f4\uff0c\u6697\u793a\u4e86\u9aa8\u9abc\u7ed3\u6784\u53ef\u80fd\u90e8\u5206\u89e3\u91ca\u4e86\u8fd9\u79cd\u8c03\u8282\u3002"}}
{"id": "2601.14681", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14681", "abs": "https://arxiv.org/abs/2601.14681", "authors": ["Shuhao Liao", "Xuxin Lv", "Jeric Lew", "Shizhe Zhang", "Jingsong Liang", "Peizhuo Li", "Yuhong Cao", "Wenjun Wu", "Guillaume Sartoretti"], "title": "FARE: Fast-Slow Agentic Robotic Exploration", "comment": null, "summary": "This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale $200m\\times130m$ building environment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFARE\u7684\u81ea\u4e3b\u63a2\u7d22\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u8fdb\u884c\u5168\u5c40\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u7b56\u7565\u4ee5\u5b9e\u73b0\u5c40\u90e8\u51b3\u7b56\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u3002\u901a\u8fc7\u5c06\u8bed\u4e49\u63a8\u7406\u4e0e\u51e0\u4f55\u51b3\u7b56\u5206\u79bb\uff0cFARE\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u5728\u5927\u89c4\u6a21\u5efa\u7b51\u73af\u5883\u7684\u5b9e\u9645\u90e8\u7f72\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6574\u5408\u9ad8\u7ea7\u522b\u7684\u8bed\u4e49\u63a8\u7406\u4ee5\u53ca\u5feb\u901f\u7684\u5c40\u90e8\u63a7\u5236\u6765\u4f18\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5c42\u81ea\u4e3b\u63a2\u7d22\u6846\u67b6FARE\uff0c\u5b83\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6267\u884c\u5168\u5c40\u5c42\u9762\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u540c\u65f6\u4f9d\u9760\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b56\u7565\u5904\u7406\u57fa\u4e8e\u5c40\u90e8\u89c2\u5bdf\u7684\u5373\u65f6\u51b3\u7b56\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u5feb\u6162\u601d\u7ef4\u6a21\u5f0f\uff0c\u5176\u4e2d\u6162\u601d\u7ef4\u90e8\u5206\u8d1f\u8d23\u89e3\u6790\u73af\u5883\u63cf\u8ff0\u5e76\u751f\u6210\u63a2\u7d22\u7b56\u7565\uff1b\u800c\u5feb\u601d\u7ef4\u90e8\u5206\u5219\u6839\u636e\u6b64\u7b56\u7565\u53ca\u5f53\u524d\u611f\u77e5\u4fe1\u606f\u4f5c\u51fa\u53cd\u5e94\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u5757\u6027\u7684\u526a\u679d\u673a\u5236\u4ee5\u51cf\u5c11\u5197\u4f59\u7ed3\u6784\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6a21\u62df\u73af\u5883\u4e2d\uff0cFARE\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u5728\u63a2\u7d22\u6548\u7387\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002\u5e76\u4e14\uff0c\u5f53\u5e94\u7528\u4e8e\u5b9e\u9645\u786c\u4ef6\u5e76\u5728\u590d\u6742\u7684\u5927\u578b\u5efa\u7b51\u73af\u5883\u4e2d\u6d4b\u8bd5\u65f6\uff0cFARE\u4e5f\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "FARE\u6846\u67b6\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u5168\u5c40\u8bed\u4e49\u63a8\u7406\u4e0e\u5c40\u90e8\u5373\u65f6\u54cd\u5e94\uff0c\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u81ea\u4e3b\u63a2\u7d22\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2601.14809", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14809", "abs": "https://arxiv.org/abs/2601.14809", "authors": ["Muhammad Adel Yusuf", "Ali Nasir", "Zeeshan Hameed Khan"], "title": "Stochastic Decision-Making Framework for Human-Robot Collaboration in Industrial Applications", "comment": "Under Review by IEEE Transactions on Human Machine Systems", "summary": "Collaborative robots, or cobots, are increasingly integrated into various industrial and service settings to work efficiently and safely alongside humans. However, for effective human-robot collaboration, robots must reason based on human factors such as motivation level and aggression level. This paper proposes an approach for decision-making in human-robot collaborative (HRC) environments utilizing stochastic modeling. By leveraging probabilistic models and control strategies, the proposed method aims to anticipate human actions and emotions, enabling cobots to adapt their behavior accordingly. So far, most of the research has been done to detect the intentions of human co-workers. This paper discusses the theoretical framework, implementation strategies, simulation results, and potential applications of the bilateral collaboration approach for safety and efficiency in collaborative robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4eba\u673a\u534f\u4f5c\u73af\u5883\u4e2d\u5229\u7528\u968f\u673a\u5efa\u6a21\u8fdb\u884c\u51b3\u7b56\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u6982\u7387\u6a21\u578b\u548c\u63a7\u5236\u7b56\u7565\u9884\u6d4b\u4eba\u7c7b\u7684\u884c\u4e3a\u548c\u60c5\u7eea\uff0c\u4f7f\u534f\u4f5c\u673a\u5668\u4eba\u80fd\u591f\u76f8\u5e94\u5730\u8c03\u6574\u5176\u884c\u4e3a\uff0c\u4ece\u800c\u63d0\u9ad8\u534f\u4f5c\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\uff0c\u673a\u5668\u4eba\u9700\u8981\u57fa\u4e8e\u4eba\u7c7b\u56e0\u7d20\uff08\u5982\u52a8\u673a\u6c34\u5e73\u548c\u653b\u51fb\u6027\u6c34\u5e73\uff09\u8fdb\u884c\u63a8\u7406\u3002\u5f53\u524d\u5927\u591a\u6570\u7814\u7a76\u96c6\u4e2d\u5728\u68c0\u6d4b\u4eba\u7c7b\u540c\u4e8b\u7684\u610f\u56fe\u4e0a\uff0c\u800c\u672c\u6587\u5219\u66f4\u8fdb\u4e00\u6b65\uff0c\u5173\u6ce8\u4e8e\u7406\u89e3\u5e76\u9884\u6d4b\u4eba\u7c7b\u7684\u60c5\u7eea\u548c\u52a8\u4f5c\uff0c\u4ee5\u4fbf\u534f\u4f5c\u673a\u5668\u4eba\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u8fd9\u4e9b\u53d8\u5316\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u968f\u673a\u5efa\u6a21\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u51b3\u7b56\u8fc7\u7a0b\u3002\u901a\u8fc7\u4f7f\u7528\u6982\u7387\u6a21\u578b\u548c\u63a7\u5236\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u8bd5\u56fe\u9884\u6d4b\u4eba\u7c7b\u4f19\u4f34\u7684\u52a8\u4f5c\u53ca\u60c5\u611f\u72b6\u6001\uff0c\u8fdb\u800c\u8ba9\u534f\u4f5c\u673a\u5668\u4eba\u80fd\u591f\u6839\u636e\u8fd9\u4e9b\u4fe1\u606f\u8c03\u6574\u81ea\u8eab\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u7406\u8bba\u6846\u67b6\u3001\u5b9e\u65bd\u7b56\u7565\u3001\u6a21\u62df\u7ed3\u679c\u4ee5\u53ca\u53cc\u8fb9\u534f\u4f5c\u65b9\u6cd5\u5728\u534f\u4f5c\u673a\u5668\u4eba\u5b89\u5168\u4e0e\u6548\u7387\u65b9\u9762\u7684\u6f5c\u5728\u5e94\u7528\u3002\u867d\u7136\u5177\u4f53\u6570\u636e\u672a\u88ab\u63d0\u53ca\uff0c\u4f46\u6a21\u62df\u7ed3\u679c\u663e\u793a\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u91c7\u7528\u968f\u673a\u5efa\u6a21\u6280\u672f\u53ef\u4ee5\u5e2e\u52a9\u534f\u4f5c\u673a\u5668\u4eba\u66f4\u597d\u5730\u7406\u89e3\u548c\u9002\u5e94\u4eba\u7c7b\u5408\u4f5c\u4f19\u4f34\u7684\u884c\u4e3a\u4e0e\u60c5\u611f\uff0c\u8fd9\u5bf9\u4e8e\u63d0\u5347\u4eba\u673a\u534f\u540c\u5de5\u4f5c\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.14837", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14837", "abs": "https://arxiv.org/abs/2601.14837", "authors": ["B. Calm\u00e9", "N. J. Greenidge", "A. Metcalf", "A. Bacchetti", "G. Loza", "D. Kpeglo", "P. Lloyd", "V. Pensabene", "J. H. Chandler", "P. Valdastri"], "title": "Moving Beyond Compliance in Soft-Robotic Catheters Through Modularity for Precision Therapies", "comment": "31 pages, 6 figures, 7 supplementary figures", "summary": "Soft robotic instruments could navigate delicate, tortuous anatomy more safely than rigid tools, but clinical adoption is limited by insufficient tip functionalization and real-time feedback at the tissue interface. Few sensing and therapeutic modules are compact, robust, and adaptable enough to measure, and respond to, subtle physiological cues during intraluminal procedures. We present a 1.47 mm diameter modular soft robotic catheter that integrates sensing, actuation, and therapy while retaining the compliance needed for safe endoluminal navigation. Validated across multiple in vivo settings, we emphasize its utility in endoscopic retrograde cholangiopancreatography (ERCP), a highly technical procedure and a key access route to the pancreas, an organ that is fragile, difficult to instrument, and central to diseases such as pancreatic cancer. Our architecture supports up to four independently controlled functional units, allowing customizable combinations of anchoring, manipulation, sensing, and targeted drug delivery. In a live porcine model, we demonstrate semi-autonomous deployment into the pancreatic duct and 7.5 cm of endoscopic navigation within it, a region currently inaccessible with standard catheters. A closed-loop autonomous/shared-control system that combines a learned model, magnetic actuation, onboard shape sensing, and visual marker tracking further improves cannulation accuracy. Together, these results establish a scalable platform for multifunctional soft robotic catheters and a new paradigm for complex endoluminal interventions, with potential to reduce radiation exposure, shorten training, and accelerate clinical translation of soft robotic technologies.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u76f4\u5f84\u4e3a1.47\u6beb\u7c73\u7684\u6a21\u5757\u5316\u8f6f\u4f53\u673a\u5668\u4eba\u5bfc\u7ba1\uff0c\u8be5\u5bfc\u7ba1\u96c6\u6210\u4e86\u611f\u5e94\u3001\u9a71\u52a8\u548c\u6cbb\u7597\u529f\u80fd\uff0c\u5e76\u4fdd\u6301\u4e86\u5b89\u5168\u5185\u8154\u5bfc\u822a\u6240\u9700\u7684\u67d4\u987a\u6027\u3002\u5b83\u5728\u591a\u79cd\u4f53\u5185\u73af\u5883\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u7279\u522b\u9002\u7528\u4e8eERCP\u7a0b\u5e8f\uff0c\u5c55\u793a\u4e86\u5728\u6d3b\u732a\u6a21\u578b\u4e2d\u81ea\u4e3b\u90e8\u7f72\u5230\u80f0\u7ba1\u5e76\u8fdb\u884c\u957f\u8fbe7.5\u5398\u7c73\u7684\u5185\u955c\u5bfc\u822a\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u95ed\u73af\u81ea\u4e3b/\u5171\u4eab\u63a7\u5236\u7cfb\u7edf\u4ee5\u63d0\u9ad8\u63d2\u7ba1\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u5de5\u5177\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u53d7\u5230\u9650\u5236\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u8db3\u591f\u7684\u5c16\u7aef\u529f\u80fd\u5316\u4ee5\u53ca\u5728\u7ec4\u7ec7\u754c\u9762\u7684\u5b9e\u65f6\u53cd\u9988\u3002\u5f88\u5c11\u6709\u4f20\u611f\u5668\u548c\u6cbb\u7597\u6a21\u5757\u65e2\u5c0f\u5de7\u53c8\u8db3\u591f\u575a\u56fa\u4e14\u9002\u5e94\u6027\u5f3a\uff0c\u53ef\u4ee5\u5728\u5185\u8154\u624b\u672f\u8fc7\u7a0b\u4e2d\u6d4b\u91cf\u5e76\u54cd\u5e94\u5fae\u5999\u7684\u751f\u7406\u4fe1\u53f7\u3002", "method": "\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u4e86\u4e00\u79cd\u96c6\u6210\u4f20\u611f\u3001\u9a71\u52a8\u548c\u6cbb\u7597\u529f\u80fd\u76841.47\u6beb\u7c73\u76f4\u5f84\u6a21\u5757\u5316\u8f6f\u4f53\u673a\u5668\u4eba\u5bfc\u7ba1\uff0c\u80fd\u591f\u652f\u6301\u591a\u8fbe\u56db\u4e2a\u72ec\u7acb\u63a7\u5236\u7684\u529f\u80fd\u5355\u5143\u3002\u901a\u8fc7\u7ed3\u5408\u5b66\u4e60\u6a21\u578b\u3001\u78c1\u529b\u9a71\u52a8\u3001\u673a\u8f7d\u5f62\u72b6\u611f\u5e94\u548c\u89c6\u89c9\u6807\u8bb0\u8ddf\u8e2a\uff0c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u95ed\u73af\u4fdd\u5b88/\u5171\u4eab\u63a7\u5236\u7cfb\u7edf\u6765\u63d0\u9ad8\u63d2\u7ba1\u7cbe\u5ea6\u3002", "result": "\u5728\u6d3b\u4f53\u732a\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u8bbe\u5907\u5c55\u793a\u4e86\u534a\u81ea\u4e3b\u8fdb\u5165\u80f0\u7ba1\u5e76\u5728\u5176\u4e2d\u8fdb\u884c\u957f\u8fbe7.5\u5398\u7c73\u5185\u955c\u5bfc\u822a\u7684\u80fd\u529b\uff0c\u8fd9\u662f\u5f53\u524d\u6807\u51c6\u5bfc\u7ba1\u65e0\u6cd5\u5230\u8fbe\u7684\u533a\u57df\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u7684\u95ed\u73af\u7cfb\u7edf\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u63d2\u7ba1\u51c6\u786e\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u591a\u529f\u80fd\u8f6f\u4f53\u673a\u5668\u4eba\u5bfc\u7ba1\u5e73\u53f0\uff0c\u4e3a\u590d\u6742\u7684\u5185\u8154\u5e72\u9884\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u8303\u5f0f\uff0c\u5177\u6709\u51cf\u5c11\u8f90\u5c04\u66b4\u9732\u3001\u7f29\u77ed\u57f9\u8bad\u65f6\u95f4\u53ca\u52a0\u901f\u8f6f\u4f53\u673a\u5668\u4eba\u6280\u672f\u4e34\u5e8a\u8f6c\u5316\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.14871", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14871", "abs": "https://arxiv.org/abs/2601.14871", "authors": ["Zejian Cui", "Ferdinando Rodriguez y Baena"], "title": "On-the-fly hand-eye calibration for the da Vinci surgical robot", "comment": "16 pages, 13 figures", "summary": "In Robot-Assisted Minimally Invasive Surgery (RMIS), accurate tool localization is crucial to ensure patient safety and successful task execution. However, this remains challenging for cable-driven robots, such as the da Vinci robot, because erroneous encoder readings lead to pose estimation errors. In this study, we propose a calibration framework to produce accurate tool localization results through computing the hand-eye transformation matrix on-the-fly. The framework consists of two interrelated algorithms: the feature association block and the hand-eye calibration block, which provide robust correspondences for key points detected on monocular images without pre-training, and offer the versatility to accommodate various surgical scenarios by adopting an array of filter approaches, respectively. To validate its efficacy, we test the framework extensively on publicly available video datasets that feature multiple surgical instruments conducting tasks in both in vitro and ex vivo scenarios, under varying illumination conditions and with different levels of key point measurement accuracy. The results show a significant reduction in tool localization errors under the proposed calibration framework, with accuracies comparable to other state-of-the-art methods while being more time-efficient.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6821\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u624b\u672f\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u51c6\u786e\u7684\u5de5\u5177\u5b9a\u4f4d\u3002\u901a\u8fc7\u4e24\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u7b97\u6cd5\uff1a\u7279\u5f81\u5173\u8054\u5757\u548c\u624b\u773c\u6821\u51c6\u5757\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4e0d\u540c\u624b\u672f\u573a\u666f\u4e0b\u63d0\u4f9b\u9c81\u68d2\u7684\u5173\u952e\u70b9\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u9884\u8bad\u7ec3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0d\u540c\u7684\u5149\u7167\u6761\u4ef6\u548c\u5173\u952e\u70b9\u6d4b\u91cf\u7cbe\u5ea6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u5de5\u5177\u5b9a\u4f4d\u8bef\u5dee\uff0c\u540c\u65f6\u6bd4\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u624b\u672f\u4e2d\uff0c\u7cbe\u786e\u7684\u5de5\u5177\u5b9a\u4f4d\u5bf9\u4e8e\u786e\u4fdd\u60a3\u8005\u5b89\u5168\u548c\u4efb\u52a1\u6210\u529f\u6267\u884c\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u7535\u7f06\u9a71\u52a8\u7684\u673a\u5668\u4eba\uff08\u5982\u8fbe\u82ac\u5947\u673a\u5668\u4eba\uff09\u6765\u8bf4\uff0c\u7531\u4e8e\u7f16\u7801\u5668\u8bfb\u6570\u9519\u8bef\u5bfc\u81f4\u59ff\u6001\u4f30\u8ba1\u8bef\u5dee\uff0c\u8fd9\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6821\u51c6\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u65f6\u8ba1\u7b97\u624b\u773c\u53d8\u6362\u77e9\u9635\u4ee5\u4ea7\u751f\u51c6\u786e\u7684\u5de5\u5177\u5b9a\u4f4d\u7ed3\u679c\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u7b97\u6cd5\uff1a\u7279\u5f81\u5173\u8054\u6a21\u5757\u548c\u624b\u773c\u6821\u51c6\u6a21\u5757\u3002\u524d\u8005\u65e0\u9700\u9884\u5148\u8bad\u7ec3\u5373\u53ef\u4e3a\u5355\u76ee\u56fe\u50cf\u4e0a\u68c0\u6d4b\u5230\u7684\u5173\u952e\u70b9\u63d0\u4f9b\u7a33\u5065\u7684\u5bf9\u5e94\u5173\u7cfb\uff1b\u540e\u8005\u5219\u901a\u8fc7\u91c7\u7528\u4e00\u7cfb\u5217\u8fc7\u6ee4\u65b9\u6cd5\u6765\u9002\u5e94\u5404\u79cd\u5916\u79d1\u624b\u672f\u573a\u666f\u3002", "result": "\u901a\u8fc7\u5bf9\u516c\u5f00\u53ef\u7528\u89c6\u9891\u6570\u636e\u96c6\u8fdb\u884c\u5e7f\u6cdb\u6d4b\u8bd5\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u591a\u79cd\u5916\u79d1\u5668\u68b0\u5728\u4f53\u5916\u548c\u4f53\u5185\u6761\u4ef6\u4e0b\u6267\u884c\u4efb\u52a1\u7684\u60c5\u51b5\uff0c\u7ed3\u679c\u663e\u793a\u6240\u63d0\u51fa\u7684\u6821\u51c6\u6846\u67b6\u4e0b\u7684\u5de5\u5177\u5b9a\u4f4d\u8bef\u5dee\u663e\u8457\u51cf\u5c11\u3002\u4e0e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u5177\u6709\u53ef\u6bd4\u8f83\u7684\u51c6\u786e\u6027\uff0c\u800c\u4e14\u66f4\u52a0\u65f6\u95f4\u9ad8\u6548\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u6821\u51c6\u6846\u67b6\u80fd\u6709\u6548\u63d0\u9ad8\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u624b\u672f\u4e2d\u7684\u5de5\u5177\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5c24\u5176\u9002\u5408\u4e8e\u9762\u5bf9\u53d8\u5316\u7684\u7167\u660e\u6761\u4ef6\u53ca\u4e0d\u540c\u6c34\u5e73\u7684\u5173\u952e\u70b9\u6d4b\u91cf\u7cbe\u5ea6\u65f6\u3002"}}
{"id": "2601.14874", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14874", "abs": "https://arxiv.org/abs/2601.14874", "authors": ["Yara Mahmoud", "Yasheerah Yaqoot", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou"], "title": "HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation", "comment": "This paper has been accepted for publication at LBR of HRI 2026 conference", "summary": "Humanoid robots must adapt their contact behavior to diverse objects and tasks, yet most controllers rely on fixed, hand-tuned impedance gains and gripper settings. This paper introduces HumanoidVLM, a vision-language driven retrieval framework that enables the Unitree G1 humanoid to select task-appropriate Cartesian impedance parameters and gripper configurations directly from an egocentric RGB image. The system couples a vision-language model for semantic task inference with a FAISS-based Retrieval-Augmented Generation (RAG) module that retrieves experimentally validated stiffness-damping pairs and object-specific grasp angles from two custom databases, and executes them through a task-space impedance controller for compliant manipulation. We evaluate HumanoidVLM on 14 visual scenarios and achieve a retrieval accuracy of 93%. Real-world experiments show stable interaction dynamics, with z-axis tracking errors typically within 1-3.5 cm and virtual forces consistent with task-dependent impedance settings. These results demonstrate the feasibility of linking semantic perception with retrieval-based control as an interpretable path toward adaptive humanoid manipulation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u89c6\u89c9-\u8bed\u8a00\u9a71\u52a8\u7684\u68c0\u7d22\u6846\u67b6HumanoidVLM\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u6839\u636e\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u56fe\u50cf\u81ea\u9002\u5e94\u9009\u62e9\u9002\u5408\u4efb\u52a1\u7684\u963b\u6297\u53c2\u6570\u548c\u5939\u722a\u914d\u7f6e\u3002\u901a\u8fc7\u4e0e\u8bed\u4e49\u4efb\u52a1\u63a8\u7406\u6a21\u578b\u53ca\u57fa\u4e8eFAISS\u7684\u68c0\u7d22\u6a21\u5757\u7ed3\u5408\uff0c\u8be5\u7cfb\u7edf\u4ece\u5b9a\u5236\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u7ecf\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u521a\u5ea6-\u963b\u5c3c\u5bf9\u548c\u5bf9\u8c61\u7279\u5b9a\u6293\u63e1\u89d2\u5ea6\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u7a7a\u95f4\u963b\u6297\u63a7\u5236\u5668\u6267\u884c\uff0c\u5b9e\u73b0\u987a\u4ece\u6027\u64cd\u4f5c\u3002", "motivation": "\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u9762\u5bf9\u4e0d\u540c\u7269\u4f53\u548c\u4efb\u52a1\u65f6\u9700\u8981\u8c03\u6574\u5176\u63a5\u89e6\u884c\u4e3a\uff0c\u4f46\u73b0\u6709\u63a7\u5236\u5668\u901a\u5e38\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u624b\u52a8\u8c03\u6821\u963b\u6297\u589e\u76ca\u548c\u5939\u722a\u8bbe\u7f6e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u6765\u589e\u5f3a\u673a\u5668\u4eba\u7684\u9002\u5e94\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u540d\u4e3aHumanoidVLM\uff0c\u5b83\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u4efb\u52a1\u63a8\u65ad\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8eFAISS\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6a21\u5757\uff0c\u4ece\u4e24\u4e2a\u5b9a\u5236\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u51fa\u5b9e\u9a8c\u9a8c\u8bc1\u8fc7\u7684\u521a\u5ea6-\u963b\u5c3c\u914d\u5bf9\u4ee5\u53ca\u9488\u5bf9\u7279\u5b9a\u5bf9\u8c61\u7684\u6700\u4f73\u6293\u63e1\u89d2\u5ea6\u3002\u968f\u540e\uff0c\u8fd9\u4e9b\u53c2\u6570\u901a\u8fc7\u4efb\u52a1\u7a7a\u95f4\u963b\u6297\u63a7\u5236\u5668\u88ab\u5e94\u7528\u5230\u5b9e\u9645\u64cd\u4f5c\u4e2d\u3002", "result": "\u572814\u4e2a\u89c6\u89c9\u573a\u666f\u4e0a\u8bc4\u4f30\u4e86HumanoidVLM\uff0c\u8fbe\u5230\u4e8693%\u7684\u68c0\u7d22\u51c6\u786e\u7387\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0c\u4ea4\u4e92\u52a8\u6001\u7a33\u5b9a\uff0cz\u8f74\u8ddf\u8e2a\u8bef\u5dee\u901a\u5e38\u4fdd\u6301\u57281-3.5\u5398\u7c73\u8303\u56f4\u5185\uff0c\u865a\u62df\u529b\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u963b\u6297\u8bbe\u7f6e\u4e00\u81f4\u3002", "conclusion": "\u7ed3\u679c\u8bc1\u660e\u4e86\u5c06\u8bed\u4e49\u611f\u77e5\u4e0e\u57fa\u4e8e\u68c0\u7d22\u7684\u63a7\u5236\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5728\u671d\u7740\u66f4\u52a0\u7075\u6d3b\u3001\u53ef\u89e3\u91ca\u7684\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u63a7\u65b9\u5411\u4e0a\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2601.14921", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14921", "abs": "https://arxiv.org/abs/2601.14921", "authors": ["Sarat Ahmad", "Maryam Hafeez", "Syed Ali Raza Zaidi"], "title": "Vision-Language Models on the Edge for Real-Time Robotic Perception", "comment": null, "summary": "Vision-Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5\\%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728Open RAN\u548c\u591a\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97(MEC)\u57fa\u7840\u8bbe\u65bd\u4e0a\u90e8\u7f72\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLMs)\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u611f\u77e5\u4e0e\u4ea4\u4e92\u7684\u5b9e\u65f6\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u4e8e\u4e91\u7aef\u90e8\u7f72\uff0c\u8fb9\u7f18\u90e8\u7f72\u80fd\u591f\u4fdd\u6301\u63a5\u8fd1\u4e91\u7aef\u7684\u51c6\u786e\u6027\u540c\u65f6\u51cf\u5c115%\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\uff1b\u6b64\u5916\uff0c\u5bf9\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4f18\u5316\u7684\u5c0f\u578b\u6a21\u578bQwen2-VL-2B-Instruct\uff0c\u867d\u7136\u5176\u54cd\u5e94\u65f6\u95f4\u7f29\u77ed\u8d85\u8fc7\u4e00\u534a\uff0c\u4f46\u727a\u7272\u4e86\u4e00\u5b9a\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u4e0e\u4ea4\u4e92\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u5230\u5ef6\u8fdf\u3001\u6709\u9650\u7684\u677f\u8f7d\u8d44\u6e90\u4ee5\u53ca\u4e91\u5378\u8f7d\u5e26\u6765\u7684\u9690\u79c1\u98ce\u9669\u7b49\u56e0\u7d20\u9650\u5236\u3002\u901a\u8fc7\u5c06\u8ba1\u7b97\u63a8\u5411\u6570\u636e\u6e90\u9644\u8fd1\uff0c6G\u6280\u672f\u7279\u522b\u662f\u5f00\u653e\u65e0\u7ebf\u63a5\u5165\u7f51(ORAN)\u4e0e\u591a\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97(MEC)\u63d0\u4f9b\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u7684\u53ef\u80fd\u6027\u3002", "method": "\u91c7\u7528Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4f5c\u4e3a\u7269\u7406\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eWebRTC\u7684\u6570\u636e\u6d41\u4f20\u8f93\u7ba1\u9053\uff0c\u5c06\u591a\u6a21\u6001\u6570\u636e\u53d1\u9001\u81f3\u8fb9\u7f18\u8282\u70b9\uff0c\u5e76\u5bf9\u6bd4\u5206\u6790\u4e86\u4f4d\u4e8e\u8fb9\u7f18\u670d\u52a1\u5668\u4e0a\u7684LLaMA-3.2-11B-Vision-Instruct\u6a21\u578b\u4e0e\u4e91\u7aef\u7248\u672c\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u3002\u540c\u65f6\uff0c\u8fd8\u8bc4\u4f30\u4e86\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4f18\u5316\u8fc7\u7684\u7d27\u51d1\u578b\u6a21\u578bQwen2-VL-2B-Instruct\u7684\u8868\u73b0\u3002", "result": "\u8fb9\u7f18\u90e8\u7f72\u65b9\u6848\u80fd\u591f\u5728\u4fdd\u8bc1\u63a5\u8fd1\u4e91\u7aef\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e5%\u3002\u800c\u5bf9\u4e8e\u5c0f\u578b\u5316\u6a21\u578bQwen2-VL-2B-Instruct\u6765\u8bf4\uff0c\u5b83\u5b9e\u73b0\u4e86\u4e9a\u79d2\u7ea7\u54cd\u5e94\u901f\u5ea6\uff0c\u4f7f\u5f97\u5ef6\u8fdf\u51cf\u5c11\u4e86\u8d85\u8fc7\u4e00\u534a\uff0c\u4e0d\u8fc7\u8fd9\u662f\u4ee5\u727a\u7272\u90e8\u5206\u51c6\u786e\u6027\u4e3a\u4ee3\u4ef7\u6362\u6765\u7684\u3002", "conclusion": "\u5229\u7528ORAN/MEC\u67b6\u6784\u90e8\u7f72VLMs\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u73b0\u6709\u6311\u6218\uff0c\u5305\u62ec\u5ef6\u8fdf\u6539\u5584\u53ca\u9690\u79c1\u4fdd\u62a4\u589e\u5f3a\u7b49\u3002\u5c3d\u7ba1\u5b58\u5728\u51c6\u786e\u6027\u4e0e\u54cd\u5e94\u65f6\u95f4\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4f46\u5bf9\u4e8e\u7279\u5b9a\u5e94\u7528\u573a\u666f\u800c\u8a00\uff0c\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u89c4\u6a21\u548c\u90e8\u7f72\u65b9\u5f0f\u662f\u53ef\u884c\u4e14\u6709\u76ca\u7684\u3002"}}
{"id": "2601.14945", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14945", "abs": "https://arxiv.org/abs/2601.14945", "authors": ["Yuteng Sun", "Haoran Wang", "Ruofei Bai", "Zhengguo Li", "Jun Li", "Meng Yee", "Chuah", "Wei Yun Yau"], "title": "TIDAL: Temporally Interleaved Diffusion and Action Loop for High-Frequency VLA Control", "comment": null, "summary": "Large-scale Vision-Language-Action (VLA) models offer semantic generalization but suffer from high inference latency, limiting them to low-frequency batch-and-execute paradigm. This frequency mismatch creates an execution blind spot, causing failures in dynamic environments where targets move during the open-loop execution window. We propose TIDAL (Temporally Interleaved Diffusion and Action Loop), a hierarchical framework that decouples semantic reasoning from high-frequency actuation. TIDAL operates as a backbone-agnostic module for diffusion-based VLAs, using a dual-frequency architecture to redistribute the computational budget. Specifically, a low-frequency macro-intent loop caches semantic embeddings, while a high-frequency micro-control loop interleaves single-step flow integration with execution. This design enables approximately 9 Hz control updates on edge hardware (vs. approximately 2.4 Hz baselines) without increasing marginal overhead. To handle the resulting latency shift, we introduce a temporally misaligned training strategy where the policy learns predictive compensation using stale semantic intent alongside real-time proprioception. Additionally, we address the insensitivity of static vision encoders to velocity by incorporating a differential motion predictor. TIDAL is architectural, making it orthogonal to system-level optimizations. Experiments show a 2x performance gain over open-loop baselines in dynamic interception tasks. Despite a marginal regression in static success rates, our approach yields a 4x increase in feedback frequency and extends the effective horizon of semantic embeddings beyond the native action chunk size. Under non-paused inference protocols, TIDAL remains robust where standard baselines fail due to latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86TIDAL\uff0c\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u53cc\u9891\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u8bed\u4e49\u63a8\u7406\u4e0e\u9ad8\u9891\u52a8\u4f5c\u6267\u884c\u89e3\u8026\u6765\u89e3\u51b3\u5927\u5ef6\u8fdf\u95ee\u9898\u3002\u5728\u52a8\u6001\u62e6\u622a\u4efb\u52a1\u4e2d\u6bd4\u5f00\u73af\u57fa\u7ebf\u6027\u80fd\u63d0\u9ad82\u500d\uff0c\u5e76\u4e14\u53cd\u9988\u9891\u7387\u63d0\u9ad8\u4e864\u500d\u3002", "motivation": "\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u867d\u7136\u63d0\u4f9b\u4e86\u8bed\u4e49\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u9ad8\u63a8\u7406\u5ef6\u8fdf\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5e94\u7528\u573a\u666f\uff0c\u5c24\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4f1a\u5bfc\u81f4\u6267\u884c\u5931\u8d25\u3002", "method": "TIDAL\u91c7\u7528\u4e86\u53cc\u9891\u67b6\u6784\uff0c\u4f4e\u9891\u5b8f\u610f\u56fe\u5faa\u73af\u7f13\u5b58\u8bed\u4e49\u5d4c\u5165\uff0c\u800c\u9ad8\u9891\u5fae\u63a7\u5236\u5faa\u73af\u5219\u7ed3\u5408\u5355\u6b65\u6d41\u96c6\u6210\u4e0e\u6267\u884c\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65f6\u95f4\u4e0d\u5bf9\u9f50\u7684\u8bad\u7ec3\u7b56\u7565\u4ee5\u53ca\u4e00\u4e2a\u5dee\u5206\u8fd0\u52a8\u9884\u6d4b\u5668\u4ee5\u5904\u7406\u5ef6\u8fdf\u79fb\u4f4d\u548c\u901f\u5ea6\u4e0d\u654f\u611f\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u52a8\u6001\u62e6\u622a\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u5f00\u73af\u57fa\u7ebf\u65b9\u6cd5\uff0cTIDAL\u8868\u73b0\u51fa\u7ea62\u500d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u53cd\u9988\u9891\u7387\u63d0\u9ad8\u4e864\u500d\u3002\u5373\u4f7f\u662f\u5728\u975e\u6682\u505c\u63a8\u7406\u534f\u8bae\u4e0b\uff0cTIDAL\u4e5f\u80fd\u4fdd\u6301\u7a33\u5065\u8868\u73b0\u3002", "conclusion": "TIDAL\u6846\u67b6\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u8ba1\u7b97\u9884\u7b97\u6709\u6548\u5730\u89e3\u51b3\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e0b\u7684\u6267\u884c\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002"}}
{"id": "2601.14973", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14973", "abs": "https://arxiv.org/abs/2601.14973", "authors": ["Faryal Batool", "Iana Zhura", "Valerii Serpiva", "Roohan Ahmed Khan", "Ivan Valuev", "Issatay Tokmurziyev", "Dzmitry Tsetserukou"], "title": "HumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV", "comment": "This paper has been accepted at HRI, Late Breaking Report, 2026", "summary": "Reliable human--robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents HumanDiffusion, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11--based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans. We evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of 80% across accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHumanDiffusion\u7684\u8f7b\u91cf\u7ea7\u56fe\u50cf\u6761\u4ef6\u6269\u6563\u89c4\u5212\u5668\uff0c\u5b83\u80fd\u76f4\u63a5\u4eceRGB\u56fe\u50cf\u751f\u6210\u4eba\u611f\u77e5\u7684\u5bfc\u822a\u8f68\u8ff9\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8eYOLO-11\u7684\u4eba\u4f53\u68c0\u6d4b\u4e0e\u6269\u6563\u9a71\u52a8\u7684\u8f68\u8ff9\u751f\u6210\u6280\u672f\u3002\u8be5\u7cfb\u7edf\u5728\u6a21\u62df\u548c\u5b9e\u9645\u5ba4\u5185\u707e\u96be\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u663e\u793a\u4e86\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4eba\u7c7b-\u673a\u5668\u4eba\u534f\u4f5c\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u53ef\u9760\u7684\u7d27\u6025\u60c5\u51b5\u4e0b\u7684\u4eba\u5458-\u673a\u5668\u4eba\u534f\u4f5c\u9700\u8981\u80fd\u591f\u68c0\u6d4b\u4eba\u7c7b\u3001\u63a8\u65ad\u5bfc\u822a\u76ee\u6807\u5e76\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b89\u5168\u64cd\u4f5c\u7684\u81ea\u4e3b\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8eYOLO-11\u7684\u4eba\u4f53\u68c0\u6d4b\u4e0e\u6269\u6563\u9a71\u52a8\u7684\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u5f00\u53d1\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u56fe\u50cf\u6761\u4ef6\u6269\u6563\u89c4\u5212\u5668\uff08HumanDiffusion\uff09\uff0c\u80fd\u591f\u76f4\u63a5\u4eceRGB\u56fe\u50cf\u751f\u6210\u4eba\u611f\u77e5\u7684\u5bfc\u822a\u8f68\u8ff9\u3002", "result": "\u5728300\u4e2a\u6837\u672c\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6a21\u578b\u8fbe\u5230\u4e86\u50cf\u7d20\u7a7a\u95f4\u8f68\u8ff9\u91cd\u5efa\u5747\u65b9\u8bef\u5dee\u4e3a0.02\u7684\u6210\u7ee9\uff1b\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u90e8\u5206\u906e\u6321\u6761\u4ef6\u4e0b\u6267\u884c\u4e8b\u6545\u54cd\u5e94\u548c\u641c\u7d22\u5b9a\u4f4d\u4efb\u52a1\u7684\u6574\u4f53\u4efb\u52a1\u6210\u529f\u7387\u8fbe\u5230\u4e8680%\u3002", "conclusion": "\u57fa\u4e8e\u4eba\u7684\u6761\u4ef6\u6269\u6563\u89c4\u5212\u4e3a\u4eba\u673a\u4ea4\u4e92\u65e0\u4eba\u673a\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u65f6\u95f4\u7d27\u8feb\u7684\u63f4\u52a9\u8bbe\u7f6e\u4e0b\u3002"}}
{"id": "2601.14998", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14998", "abs": "https://arxiv.org/abs/2601.14998", "authors": ["Adip Ranjan Das", "Maria Koskinopoulou"], "title": "Graph-Based Adaptive Planning for Coordinated Dual-Arm Robotic Disassembly of Electronic Devices (eGRAP)", "comment": "7 Pages, 8 Figures, 5 Tables", "summary": "E-waste is growing rapidly while recycling rates remain low. We propose an electronic-device Graph-based Adaptive Planning (eGRAP) that integrates vision, dynamic planning, and dual-arm execution for autonomous disassembly. A camera-equipped arm identifies parts and estimates their poses, and a directed graph encodes which parts must be removed first. A scheduler uses topological ordering of this graph to select valid next steps and assign them to two robot arms, allowing independent tasks to run in parallel. One arm carries a screwdriver (with an eye-in-hand depth camera) and the other holds or handles components. We demonstrate eGRAP on 3.5in hard drives: as parts are unscrewed and removed, the system updates its graph and plan online. Experiments show consistent full disassembly of each HDD, with high success rates and efficient cycle times, illustrating the method's ability to adaptively coordinate dual-arm tasks in real time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u81ea\u9002\u5e94\u89c4\u5212\u65b9\u6cd5eGRAP\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u3001\u52a8\u6001\u89c4\u5212\u548c\u53cc\u81c2\u6267\u884c\u6765\u5b9e\u73b0\u7535\u5b50\u8bbe\u5907\u7684\u81ea\u4e3b\u62c6\u89e3\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u65f6\u9002\u5e94\u6027\u5730\u534f\u8c03\u53cc\u81c2\u4efb\u52a1\uff0c\u6210\u529f\u4e14\u9ad8\u6548\u5730\u5b8c\u6210\u4e863.5\u82f1\u5bf8\u786c\u76d8\u9a71\u52a8\u5668\u7684\u5b8c\u5168\u62c6\u89e3\u3002", "motivation": "\u9274\u4e8e\u7535\u5b50\u5e9f\u7269\u7684\u589e\u957f\u901f\u5ea6\u8fdc\u8d85\u5176\u56de\u6536\u7387\u7684\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u63d0\u9ad8\u7535\u5b50\u5e9f\u5f03\u7269\u5904\u7406\u6548\u7387\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e86\u7535\u5b50\u8bbe\u5907\u56fe\u57fa\u81ea\u9002\u5e94\u89c4\u5212\uff08eGRAP\uff09\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u89c6\u89c9\u8bc6\u522b\u3001\u52a8\u6001\u89c4\u5212\u4ee5\u53ca\u53cc\u81c2\u673a\u5668\u4eba\u64cd\u4f5c\u4ee5\u5b8c\u6210\u81ea\u52a8\u62c6\u5378\u4efb\u52a1\u3002\u5176\u4e2d\u4e00\u4e2a\u673a\u68b0\u81c2\u914d\u5907\u87ba\u4e1d\u5200\u53ca\u773c\u5728\u624b\u6df1\u5ea6\u76f8\u673a\u7528\u4e8e\u62c6\u5378\u5de5\u4f5c\uff0c\u53e6\u4e00\u81c2\u5219\u8d1f\u8d23\u6301\u63e1\u6216\u5904\u7406\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5bf9\u4e8e3.5\u82f1\u5bf8\u786c\u76d8\u7684\u62c6\u89e3\uff0ceGRAP\u7cfb\u7edf\u80fd\u591f\u5728\u7ebf\u66f4\u65b0\u5176\u56fe\u7ed3\u6784\u4e0e\u8ba1\u5212\uff0c\u5e76\u4fdd\u6301\u8f83\u9ad8\u7684\u6210\u529f\u7387\u548c\u8f83\u77ed\u7684\u64cd\u4f5c\u5468\u671f\u65f6\u95f4\u3002", "conclusion": "eGRAP\u5c55\u793a\u4e86\u5176\u5b9e\u65f6\u8c03\u6574\u53cc\u81c2\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u5feb\u901f\u589e\u957f\u7684\u7535\u5b50\u5e9f\u7269\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.15006", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15006", "abs": "https://arxiv.org/abs/2601.15006", "authors": ["Fumiya Ohnishi", "Masaki Takahashi"], "title": "DWPP: Dynamic Window Pure Pursuit Considering Velocity and Acceleration Constraints", "comment": "28 pages, 12 figures", "summary": "Pure pursuit and its variants are widely used for mobile robot path tracking owing to their simplicity and computational efficiency. However, many conventional approaches do not explicitly account for velocity and acceleration constraints, resulting in discrepancies between commanded and actual velocities that result in overshoot and degraded tracking performance. To address this problem, this paper proposes dynamic window pure pursuit (DWPP), which fundamentally reformulates the command velocity computation process to explicitly incorporate velocity and acceleration constraints. Specifically, DWPP formulates command velocity computation in the velocity space (the $v$-$\u03c9$ plane) and selects the command velocity as the point within the dynamic window that is closest to the line $\u03c9= \u03bav$. Experimental results demonstrate that DWPP avoids constraint-violating commands and achieves superior path-tracking accuracy compared with conventional pure pursuit methods. The proposed method has been integrated into the official Nav2 repository and is publicly available (https://github.com/ros-navigation/navigation2).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u52a8\u6001\u7a97\u53e3\u7eaf\u8ffd\u8e2a\uff08DWPP\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u901f\u5ea6\u7a7a\u95f4\u4e2d\u91cd\u65b0\u5b9a\u4e49\u4e86\u6307\u4ee4\u901f\u5ea6\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u4ee5\u660e\u786e\u8003\u8651\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u7684\u7ea6\u675f\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDWPP\u76f8\u6bd4\u4f20\u7edf\u7eaf\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u5728\u8def\u5f84\u8ddf\u8e2a\u7cbe\u5ea6\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u5e76\u4e14\u907f\u514d\u4e86\u8fdd\u53cd\u7ea6\u675f\u7684\u547d\u4ee4\u3002", "motivation": "\u4f20\u7edf\u7684\u7eaf\u8ffd\u8e2a\u53ca\u5176\u53d8\u4f53\u65b9\u6cd5\u7531\u4e8e\u7b80\u5355\u6027\u548c\u8ba1\u7b97\u6548\u7387\u800c\u88ab\u5e7f\u6cdb\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u7684\u8def\u5f84\u8ddf\u8e2a\u3002\u7136\u800c\uff0c\u8bb8\u591a\u4f20\u7edf\u65b9\u6cd5\u6ca1\u6709\u660e\u786e\u8003\u8651\u5230\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u7684\u9650\u5236\uff0c\u5bfc\u81f4\u6307\u4ee4\u901f\u5ea6\u4e0e\u5b9e\u9645\u901f\u5ea6\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u5f15\u8d77\u8d85\u8c03\u548c\u8ddf\u8e2a\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u52a8\u6001\u7a97\u53e3\u7eaf\u8ffd\u8e2a\uff08DWPP\uff09\uff0c\u901a\u8fc7\u5728\u901f\u5ea6\u7a7a\u95f4\uff08v-\u03c9\u5e73\u9762\uff09\u5185\u91cd\u6784\u6307\u4ee4\u901f\u5ea6\u8ba1\u7b97\u6d41\u7a0b\uff0c\u76f4\u63a5\u5c06\u901f\u5ea6\u4e0e\u52a0\u901f\u5ea6\u7ea6\u675f\u7eb3\u5165\u8003\u91cf\u3002\u5177\u4f53\u6765\u8bf4\uff0cDWPP\u9009\u62e9\u52a8\u6001\u7a97\u53e3\u5185\u6700\u63a5\u8fd1\u76f4\u7ebf\u03c9=\u03bav\u7684\u70b9\u4f5c\u4e3a\u6307\u4ee4\u901f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0cDWPP\u80fd\u591f\u907f\u514d\u4ea7\u751f\u8fdd\u53cd\u7ea6\u675f\u6761\u4ef6\u7684\u6307\u4ee4\uff0c\u5e76\u4e14\u76f8\u5bf9\u4e8e\u4f20\u7edf\u7684\u7eaf\u8ffd\u8e2a\u65b9\u6cd5\u800c\u8a00\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u8def\u5f84\u8ddf\u8e2a\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u7a97\u53e3\u7684\u6982\u5ff5\u6765\u4f18\u5316\u7eaf\u8ffd\u8e2a\u7b97\u6cd5\uff0cDWPP\u6210\u529f\u89e3\u51b3\u4e86\u56e0\u672a\u5145\u5206\u8003\u8651\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u7ea6\u675f\u800c\u5bfc\u81f4\u7684\u8def\u5f84\u8ddf\u8e2a\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u8def\u5f84\u8ddf\u8e2a\u7684\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2601.15018", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15018", "abs": "https://arxiv.org/abs/2601.15018", "authors": ["Leon Tolksdorf", "Arturo Tejada", "Jonas Bauernfeind", "Christian Birkner", "Nathan van de Wouw"], "title": "Risk Estimation for Automated Driving", "comment": "10 pages, 5 figures", "summary": "Safety is a central requirement for automated vehicles. As such, the assessment of risk in automated driving is key in supporting both motion planning technologies and safety evaluation. In automated driving, risk is characterized by two aspects. The first aspect is the uncertainty on the state estimates of other road participants by an automated vehicle. The second aspect is the severity of a collision event with said traffic participants. Here, the uncertainty aspect typically causes the risk to be non-zero for near-collision events. This makes risk particularly useful for automated vehicle motion planning. Namely, constraining or minimizing risk naturally navigates the automated vehicle around traffic participants while keeping a safety distance based on the level of uncertainty and the potential severity of the impending collision. Existing approaches to calculate the risk either resort to empirical modeling or severe approximations, and, hence, lack generalizability and accuracy. In this paper, we combine recent advances in collision probability estimation with the concept of collision severity to develop a general method for accurate risk estimation. The proposed method allows us to assign individual severity functions for different collision constellations, such as, e.g., frontal or side collisions. Furthermore, we show that the proposed approach is computationally efficient, which is beneficial, e.g., in real-time motion planning applications. The programming code for an exemplary implementation of Gaussian uncertainties is also provided.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u78b0\u649e\u6982\u7387\u4f30\u8ba1\u548c\u78b0\u649e\u4e25\u91cd\u6027\u6982\u5ff5\u7684\u4e00\u822c\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u51c6\u786e\u7684\u98ce\u9669\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u4e0d\u540c\u7c7b\u578b\u7684\u78b0\u649e\u5206\u914d\u72ec\u7acb\u7684\u4e25\u91cd\u6027\u51fd\u6570\uff0c\u5e76\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u98ce\u9669\u8ba1\u7b97\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e8e\u7ecf\u9a8c\u5efa\u6a21\uff0c\u8981\u4e48\u8fdb\u884c\u4e25\u91cd\u7684\u8fd1\u4f3c\u5904\u7406\uff0c\u56e0\u6b64\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u51c6\u786e\u6027\u3002\u4e3a\u4e86\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u6280\u672f\u548c\u5b89\u5168\u8bc4\u4f30\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u52a0\u51c6\u786e\u548c\u901a\u7528\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6574\u5408\u78b0\u649e\u6982\u7387\u4f30\u8ba1\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u4e0e\u78b0\u649e\u4e25\u91cd\u6027\u7684\u6982\u5ff5\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u9488\u5bf9\u4e0d\u540c\u7684\u78b0\u649e\u60c5\u51b5\uff08\u5982\u6b63\u9762\u6216\u4fa7\u9762\u78b0\u649e\uff09\u6307\u5b9a\u7279\u5b9a\u7684\u4e25\u91cd\u6027\u51fd\u6570\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5bf9\u4e0d\u540c\u7c7b\u578b\u78b0\u649e\u4e8b\u4ef6\u7684\u7cbe\u786e\u98ce\u9669\u4f30\u8ba1\uff0c\u5e76\u4e14\u5728\u8ba1\u7b97\u4e0a\u8db3\u591f\u9ad8\u6548\uff0c\u53ef\u5e94\u7528\u4e8e\u5b9e\u65f6\u573a\u666f\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u57fa\u4e8e\u9ad8\u65af\u4e0d\u786e\u5b9a\u6027\u7684\u793a\u4f8b\u4ee3\u7801\u5b9e\u73b0\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u7684\u65b9\u6cd5\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u3001\u66f4\u901a\u7528\u7684\u98ce\u9669\u8bc4\u4f30\u624b\u6bb5\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u53ca\u5176\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2601.15039", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15039", "abs": "https://arxiv.org/abs/2601.15039", "authors": ["Jiyao Zhang", "Zhiyuan Ma", "Tianhao Wu", "Zeyuan Chen", "Hao Dong"], "title": "CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes", "comment": null, "summary": "Dexterous grasping in cluttered environments presents substantial challenges due to the high degrees of freedom of dexterous hands, occlusion, and potential collisions arising from diverse object geometries and complex layouts. To address these challenges, we propose CADGrasp, a two-stage algorithm for general dexterous grasping using single-view point cloud inputs. In the first stage, we predict sparse IBS, a scene-decoupled, contact- and collision-aware representation, as the optimization target. Sparse IBS compactly encodes the geometric and contact relationships between the dexterous hand and the scene, enabling stable and collision-free dexterous grasp pose optimization. To enhance the prediction of this high-dimensional representation, we introduce an occupancy-diffusion model with voxel-level conditional guidance and force closure score filtering. In the second stage, we develop several energy functions and ranking strategies for optimization based on sparse IBS to generate high-quality dexterous grasp poses. Extensive experiments in both simulated and real-world settings validate the effectiveness of our approach, demonstrating its capability to mitigate collisions while maintaining a high grasp success rate across diverse objects and complex scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCADGrasp\u7684\u4e24\u9636\u6bb5\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u6742\u4e71\u73af\u5883\u4e2d\u7684\u7075\u5de7\u6293\u53d6\u95ee\u9898\u3002\u901a\u8fc7\u5355\u89c6\u89d2\u70b9\u4e91\u8f93\u5165\uff0c\u8be5\u7b97\u6cd5\u9996\u5148\u9884\u6d4b\u4e00\u4e2a\u7a00\u758fIBS\u4f5c\u4e3a\u4f18\u5316\u76ee\u6807\uff0c\u7136\u540e\u57fa\u4e8e\u6b64\u8fdb\u884c\u4f18\u5316\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7075\u5de7\u6293\u53d6\u59ff\u6001\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5b9e\u73b0\u7075\u5de7\u6293\u53d6\u9762\u4e34\u8bb8\u591a\u6311\u6218\uff0c\u5982\u624b\u90e8\u81ea\u7531\u5ea6\u9ad8\u3001\u906e\u6321\u4ee5\u53ca\u7531\u4e0d\u540c\u7269\u4f53\u5f62\u72b6\u548c\u590d\u6742\u5e03\u5c40\u5f15\u8d77\u7684\u6f5c\u5728\u78b0\u649e\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u6cd5\u3002", "method": "CADGrasp\u7b97\u6cd5\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\uff0c\u9884\u6d4b\u4e00\u79cd\u4e0e\u573a\u666f\u89e3\u8026\u4e14\u8003\u8651\u63a5\u89e6\u548c\u78b0\u649e\u7684\u8868\u793a\u2014\u2014\u7a00\u758fIBS\uff0c\u5e76\u5f15\u5165\u4e86\u4f53\u7d20\u7ea7\u6761\u4ef6\u5f15\u5bfc\u7684\u5360\u7528\u6269\u6563\u6a21\u578b\u53ca\u529b\u95ed\u5408\u5f97\u5206\u8fc7\u6ee4\u6765\u6539\u8fdb\u9ad8\u7ef4\u8868\u793a\u7684\u9884\u6d4b\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff0c\u57fa\u4e8e\u7a00\u758fIBS\u5f00\u53d1\u4e86\u51e0\u79cd\u80fd\u91cf\u51fd\u6570\u548c\u6392\u540d\u7b56\u7565\u6765\u8fdb\u884c\u4f18\u5316\uff0c\u4ece\u800c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7075\u5de7\u6293\u53d6\u59ff\u6001\u3002", "result": "\u901a\u8fc7\u5728\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11\u78b0\u649e\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u6293\u53d6\u6210\u529f\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684CADGrasp\u7b97\u6cd5\u80fd\u591f\u5728\u591a\u6837\u5316\u7269\u4f53\u548c\u590d\u6742\u573a\u666f\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684\u7075\u5de7\u6293\u53d6\u3002"}}
{"id": "2601.15056", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15056", "abs": "https://arxiv.org/abs/2601.15056", "authors": ["Maria T. Tagliaferri", "Inseung Kang"], "title": "Systematic Evaluation of Hip Exoskeleton Assistance Parameters for Enhancing Gait Stability During Ground Slip Perturbations", "comment": null, "summary": "Falls are the leading cause of injury related hospitalization and mortality among older adults. Consequently, mitigating age-related declines in gait stability and reducing fall risk during walking is a critical goal for assistive devices. Lower-limb exoskeletons have the potential to support users in maintaining stability during walking. However, most exoskeleton controllers are optimized to reduce the energetic cost of walking rather than to improve stability. While some studies report stability benefits with assistance, the effects of specific parameters, such as assistance magnitude and duration, remain unexplored. To address this gap, we systematically modulated the magnitude and duration of torque provided by a bilateral hip exoskeleton during slip perturbations in eight healthy adults, quantifying stability using whole-body angular momentum (WBAM). WBAM responses were governed by a significant interaction between assistance magnitude and duration, with duration determining whether exoskeleton assistance was stabilizing or destabilizing relative to not wearing the exoskeleton device. Compared to an existing energy-optimized controller, experimentally identified stability-optimal parameters reduced WBAM range by 25.7% on average. Notably, substantial inter-subject variability was observed in the parameter combinations that minimized WBAM during perturbations. We found that optimizing exoskeleton assistance for energetic outcomes alone is insufficient for improving reactive stability during gait perturbations. Stability-focused exoskeleton control should prioritize temporal assistance parameters and include user-specific personalization. This study represents an important step toward personalized, stability-focused exoskeleton control, with direct implications for improving stability and reducing fall risk in older adults.", "AI": {"tldr": "\u7814\u7a76\u4e86\u53cc\u4fa7\u9acb\u90e8\u5916\u9aa8\u9abc\u5728\u6ed1\u52a8\u5e72\u6270\u671f\u95f4\u5bf9\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4f18\u5316\u5916\u9aa8\u9abc\u8f85\u52a9\u4ee5\u63d0\u9ad8\u80fd\u91cf\u6548\u7387\u4e0d\u8db3\u4ee5\u6539\u5584\u6b65\u6001\u5e72\u6270\u65f6\u7684\u53cd\u5e94\u7a33\u5b9a\u6027\u3002\u4e3a\u63d0\u9ad8\u8001\u5e74\u4eba\u884c\u8d70\u7a33\u5b9a\u6027\u5e76\u51cf\u5c11\u8dcc\u5012\u98ce\u9669\uff0c\u5916\u9aa8\u9abc\u63a7\u5236\u5e94\u4f18\u5148\u8003\u8651\u65f6\u95f4\u8f85\u52a9\u53c2\u6570\uff0c\u5e76\u5305\u62ec\u7528\u6237\u7279\u5b9a\u7684\u4e2a\u6027\u5316\u8bbe\u7f6e\u3002", "motivation": "\u9274\u4e8e\u8dcc\u5012\u662f\u5bfc\u81f4\u8001\u5e74\u4eba\u53d7\u4f24\u4f4f\u9662\u548c\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8c03\u6574\u5916\u9aa8\u9abc\u63d0\u4f9b\u7684\u626d\u77e9\u5927\u5c0f\u4e0e\u65f6\u957f\u6765\u63a2\u7d22\u5176\u5982\u4f55\u5e2e\u52a9\u7ef4\u6301\u6b65\u884c\u8fc7\u7a0b\u4e2d\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6ed1\u52a8\u5e72\u6270\u60c5\u51b5\u4e0b\u7684\u6574\u4f53\u8eab\u4f53\u89d2\u52a8\u91cf\uff08WBAM\uff09\u53d8\u5316\u3002", "method": "\u7814\u7a76\u8005\u7cfb\u7edf\u5730\u6539\u53d8\u4e86\u53cc\u8fb9\u9acb\u5173\u8282\u5916\u9aa8\u9abc\u63d0\u4f9b\u7ed9\u516b\u540d\u5065\u5eb7\u6210\u5e74\u4eba\u7684\u626d\u77e9\u5e45\u5ea6\u4e0e\u6301\u7eed\u65f6\u95f4\uff0c\u5728\u6ed1\u52a8\u5e72\u6270\u4e0b\u6d4b\u91cf\u4e86\u4ed6\u4eec\u7684\u5168\u8eab\u89d2\u52a8\u91cf(WBAM)\u4f5c\u4e3a\u7a33\u5b9a\u6027\u7684\u6307\u6807\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c\u5916\u9aa8\u9abc\u8f85\u52a9\u7684\u6548\u679c\u53d6\u51b3\u4e8e\u626d\u77e9\u5927\u5c0f\u4e0e\u4f5c\u7528\u65f6\u95f4\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff1b\u76f8\u8f83\u4e8e\u4e0d\u7a7f\u6234\u8bbe\u5907\u7684\u60c5\u51b5\uff0c\u9002\u5f53\u8c03\u6574\u8fd9\u4e9b\u53c2\u6570\u53ef\u4ee5\u663e\u8457\u964d\u4f4eWBAM\u7684\u53d8\u5316\u8303\u56f4\u8fbe25.7%\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u4e2a\u4f53\u95f4\u5bf9\u4e8e\u6700\u80fd\u51cf\u5c11WBAM\u6ce2\u52a8\u7684\u6700\u4f73\u53c2\u6570\u7ec4\u5408\u5b58\u5728\u8f83\u5927\u5dee\u5f02\u3002", "conclusion": "\u5355\u72ec\u4f18\u5316\u5916\u9aa8\u9abc\u4ee5\u5b9e\u73b0\u80fd\u6e90\u6548\u7387\u5e76\u4e0d\u8db3\u4ee5\u589e\u5f3a\u6b65\u6001\u53d7\u5230\u5e72\u6270\u65f6\u7684\u54cd\u5e94\u7a33\u5b9a\u6027\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u652f\u6301\u8001\u5e74\u4eba\u7fa4\uff0c\u672a\u6765\u7684\u5916\u9aa8\u9abc\u63a7\u5236\u7cfb\u7edf\u8bbe\u8ba1\u5e94\u8be5\u66f4\u52a0\u6ce8\u91cd\u65f6\u95f4\u76f8\u5173\u53c2\u6570\u7684\u9009\u62e9\uff0c\u5e76\u7ed3\u5408\u6bcf\u4f4d\u7528\u6237\u7684\u4e2a\u4eba\u7279\u70b9\u8fdb\u884c\u5b9a\u5236\u5316\u8c03\u6574\u3002"}}
{"id": "2601.15069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15069", "abs": "https://arxiv.org/abs/2601.15069", "authors": ["Yanran Jiang", "Pavan Sikka", "Leimin Tian", "Dana Kuliic", "Cecile Paris"], "title": "Influence of Operator Expertise on Robot Supervision and Intervention", "comment": null, "summary": "With increasing levels of robot autonomy, robots are increasingly being supervised by users with varying levels of robotics expertise. As the diversity of the user population increases, it is important to understand how users with different expertise levels approach the supervision task and how this impacts performance of the human-robot team. This exploratory study investigates how operators with varying expertise levels perceive information and make intervention decisions when supervising a remote robot. We conducted a user study (N=27) where participants supervised a robot autonomously exploring four unknown tunnel environments in a simulator, and provided waypoints to intervene when they believed the robot had encountered difficulties. By analyzing the interaction data and questionnaire responses, we identify differing patterns in intervention timing and decision-making strategies across novice, intermediate, and expert users.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4e0d\u540c\u4e13\u4e1a\u6c34\u5e73\u7684\u7528\u6237\u5728\u76d1\u7763\u8fdc\u7a0b\u673a\u5668\u4eba\u65f6\u5982\u4f55\u611f\u77e5\u4fe1\u606f\u5e76\u505a\u51fa\u5e72\u9884\u51b3\u7b56\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u53d1\u73b0\u4e86\u65b0\u624b\u3001\u4e2d\u7ea7\u548c\u4e13\u5bb6\u7528\u6237\u5728\u5e72\u9884\u65f6\u673a\u548c\u51b3\u7b56\u7b56\u7565\u4e0a\u7684\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u7684\u63d0\u9ad8\uff0c\u76d1\u7763\u8fd9\u4e9b\u673a\u5668\u4eba\u7684\u7528\u6237\u7684\u4e13\u4e1a\u6c34\u5e73\u4e5f\u5404\u4e0d\u76f8\u540c\u3002\u4e3a\u4e86\u7406\u89e3\u8fd9\u79cd\u591a\u6837\u6027\u5bf9\u4eba\u673a\u56e2\u961f\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u9700\u8981\u7814\u7a76\u4e0d\u540c\u7ecf\u9a8c\u6c34\u5e73\u7684\u64cd\u4f5c\u8005\u5728\u76d1\u7763\u4efb\u52a1\u4e2d\u5982\u4f55\u5904\u7406\u4fe1\u606f\u4ee5\u53ca\u51b3\u5b9a\u4f55\u65f6\u5e72\u9884\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u4e2a\u5305\u542b27\u540d\u53c2\u4e0e\u8005\u7684\u7814\u7a76\u9879\u76ee\uff0c\u8ba9\u53c2\u4e0e\u8005\u5728\u4e00\u4e2a\u6a21\u62df\u5668\u91cc\u76d1\u7763\u4e00\u53f0\u81ea\u52a8\u63a2\u7d22\u56db\u4e2a\u672a\u77e5\u96a7\u9053\u73af\u5883\u7684\u673a\u5668\u4eba\uff0c\u5e76\u8981\u6c42\u4ed6\u4eec\u5728\u8ba4\u4e3a\u673a\u5668\u4eba\u9047\u5230\u56f0\u96be\u65f6\u63d0\u4f9b\u822a\u70b9\u8fdb\u884c\u5e72\u9884\u3002\u901a\u8fc7\u5206\u6790\u4e92\u52a8\u6570\u636e\u53ca\u95ee\u5377\u8c03\u67e5\u56de\u590d\u6765\u8bc6\u522b\u4e0d\u540c\u7ea7\u522b\u7528\u6237\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5e72\u9884\u65f6\u95f4\u7684\u9009\u62e9\u4e0e\u51b3\u7b56\u7b56\u7565\u4e0a\uff0c\u65b0\u624b\u3001\u4e2d\u7ea7\u7528\u6237\u548c\u4e13\u5bb6\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u4e86\u89e3\u4e0d\u540c\u7ecf\u9a8c\u6c34\u5e73\u64cd\u4f5c\u8005\u4e4b\u95f4\u7684\u8fd9\u4e9b\u5dee\u5f02\u5bf9\u4e8e\u6539\u8fdb\u4eba\u673a\u4ea4\u4e92\u754c\u9762\u8bbe\u8ba1\u3001\u57f9\u8bad\u6750\u6599\u4ee5\u53ca\u672a\u6765\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5f00\u53d1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.15164", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15164", "abs": "https://arxiv.org/abs/2601.15164", "authors": ["Yaru Liu", "Ao-bo Wang", "Nanyang Ye"], "title": "V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks", "comment": null, "summary": "Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently \"succeed\" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., \"get ready for work\") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out \"silent failures\" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.", "AI": {"tldr": "V-CAGE, a framework for generating high-quality, semantically aligned manipulation datasets, improves the physical and semantic fidelity of synthetic scenes, thereby enhancing the performance of downstream policies.", "motivation": "The motivation is to overcome the challenges in learning long-horizon embodied behaviors from synthetic data, such as physically implausible scenes, lack of task semantic satisfaction, and the need to translate high-level instructions into executable actions.", "method": "V-CAGE uses a context-aware instantiation mechanism for scene synthesis, ensuring geometric consistency, and a hierarchical instruction decomposition module that breaks down high-level goals into action primitives. It also incorporates a VLM-based verification loop to ensure semantic correctness by filtering out silent failures.", "result": "Experiments show that V-CAGE generates datasets with higher physical and semantic accuracy, leading to improved success rates and better generalization in downstream policies compared to non-verified baseline methods.", "conclusion": "V-CAGE effectively addresses the limitations of current synthetic data generation methods for embodied AI, providing a robust solution for creating semantically rich and physically plausible datasets."}}
