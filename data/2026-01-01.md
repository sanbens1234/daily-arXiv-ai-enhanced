<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 27]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Simultaneous Extrinsic Contact and In-Hand Pose Estimation via Distributed Tactile Sensing](https://arxiv.org/abs/2512.23856)
*Mark Van der Merwe,Kei Ota,Dmitry Berenson,Nima Fazeli,Devesh K. Jha*

Main category: cs.RO

TL;DR: 该论文提出了一种方法，通过将来自传感器的局部观测与接触的物理约束配对，以解决自主操纵中物体姿态和接触点估计的问题。这种方法在仅触觉信息可用的情况下，尤其优于现有的几何和接触信息估计流程。


<details>
  <summary>Details</summary>
Motivation: 进行抓握操作时，需要准确理解手中物体的姿态以及互动过程中产生的外在接触。然而，提供精确的姿态和接触点估计是具有挑战性的。触觉传感器可以提供局部几何形状和抓握力的信息，但其局部感知特性意味着仅从触觉来解析姿态和接触点往往是一个定义不明确的问题，因为多种配置可能都符合观察结果。视觉反馈可以帮助解决这些模糊性，但可能会受到噪声和遮挡的影响。

Method: 论文提出的方法是将来自传感的局部观察与接触的物理约束相结合。作者提出了一系列因素，确保与触觉观察的局部一致性，并强制执行物理合理性，即所估计的姿态和接触必须尊重准静态刚体交互的动力学和力约束。问题被形式化为一个因子图，允许高效估计。

Result: 实验表明，该方法在仅有触觉信息可用的情况下，尤其优于现有的基于几何和接触信息的估计流程。

Conclusion: 本研究提出的方法通过结合触觉数据和接触的物理约束，在估计物体姿态和接触方面展示了优越性能，特别是在缺乏视觉反馈的情境下。

Abstract: Prehensile autonomous manipulation, such as peg insertion, tool use, or assembly, require precise in-hand understanding of the object pose and the extrinsic contacts made during interactions. Providing accurate estimation of pose and contacts is challenging. Tactile sensors can provide local geometry at the sensor and force information about the grasp, but the locality of sensing means resolving poses and contacts from tactile alone is often an ill-posed problem, as multiple configurations can be consistent with the observations. Adding visual feedback can help resolve ambiguities, but can suffer from noise and occlusions. In this work, we propose a method that pairs local observations from sensing with the physical constraints of contact. We propose a set of factors that ensure local consistency with tactile observations as well as enforcing physical plausibility, namely, that the estimated pose and contacts must respect the kinematic and force constraints of quasi-static rigid body interactions. We formalize our problem as a factor graph, allowing for efficient estimation. In our experiments, we demonstrate that our method outperforms existing geometric and contact-informed estimation pipelines, especially when only tactile information is available. Video results can be found at https://tacgraph.github.io/.

</details>


### [2] [SHIELD: Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone Exploration](https://arxiv.org/abs/2512.23972)
*Liangtao Feng,Zhenchang Liu,Feng Zhang,Xuefeng Ren*

Main category: cs.RO

TL;DR: 本文提出了一种名为SHIELD的方法，旨在通过维持观测质量占据地图和采用混合边界方法来解决基于LiDAR的无人机探索中遇到的问题，并通过仿真和飞行实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 激光雷达尽管提供了广阔的视野优势，但在UAV探索应用中仍面临如点云观测质量差、传统边界方法计算负担重以及难以通过光线投射分类无点云区域等挑战。

Method: 提出了SHIELD方法，该方法保持了一个观测质量的占据地图，并在该地图上执行光线投射以处理探索过程中点云质量不一致的问题；同时采用了混合边界方法来解决计算负担和点云质量探索限制问题；此外还提出了向外球形投影光线投射策略，以确保开阔区域中的飞行安全与探索效率。

Result: 通过仿真和实际飞行实验验证了SHIELD方法的有效性。

Conclusion: SHIELD提供了一种有效提升基于LiDAR的无人机探索性能的方法，有助于克服现有技术面临的多种挑战。

Abstract: This paper introduces SHIELD, a Spherical-Projection Hybrid-Frontier Integration for Efficient LiDAR-based Drone exploration method. Although laser LiDAR offers the advantage of a wide field of view, its application in UAV exploration still faces several challenges. The observation quality of LiDAR point clouds is generally inferior to that of depth cameras. Traditional frontier methods based on known and unknown regions impose a heavy computational burden, especially when handling the wide field of view of LiDAR. In addition, regions without point cloud are also difficult to classify as free space through raycasting. To address these problems, the SHIELD is proposed. It maintains an observation-quality occupancy map and performs ray-casting on this map to address the issue of inconsistent point-cloud quality during exploration. A hybrid frontier method is used to tackle both the computational burden and the limitations of point-cloud quality exploration. In addition, an outward spherical-projection ray-casting strategy is proposed to jointly ensure flight safety and exploration efficiency in open areas. Simulations and flight experiments prove the effectiveness of SHIELD. This work will be open-sourced to contribute to the research community.

</details>


### [3] [Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training](https://arxiv.org/abs/2512.24125)
*Yi Liu,Sukai Wang,Dafeng Wei,Xiaowei Cai,Linqing Zhong,Jiange Yang,Guanghui Ren,Jinyu Zhang,Maoqing Yao,Chuankang Li,Xindong He,Liliang Chen,Jianlan Luo*

Main category: cs.RO

TL;DR: 本文提出了一种新的评估框架ERIQ来衡量机器人操作中的具身推理能力，并引入了FACT方法以改善从推理到精确执行的转换过程，从而在真实任务中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作（VLA）模型在开放世界环境中难以同时实现广泛的泛化能力和高精度的动作执行。大型视觉-语言模型虽然提高了语义泛化能力，但由于缺乏足够的具身推理导致行为脆弱；而强大的推理能力如果没有精确控制也显得不足。为了解决这一瓶颈问题，作者提出了一个新的评估基准和改进方法。

Method: 首先，通过开发一个名为Embodied Reasoning Intelligence Quotient (ERIQ)的大规模具身推理基准来对机器人的具身推理能力进行解耦和量化评估。接着，为了连接推理与精确执行之间的差距，提出了基于流匹配的动作标记器(FACT)，它能够将连续控制转化为离散序列，同时保持轨迹重建的高保真度。最后，GenieReasoner被设计用来在一个统一的空间内共同优化推理和行动。

Result: 研究结果表明，具身推理能力和端到端VLA泛化之间存在强烈的正相关关系。所提出的GenieReasoner在实际任务中表现优于连续动作基线以及先前的离散动作基线。

Conclusion: ERIQ和FACT提供了一个原则性的框架，用于诊断并克服推理与精度之间的权衡问题，推动了稳健且通用的机器人操作技术的发展。

Abstract: General-purpose robotic systems operating in open-world environments must achieve both broad generalization and high-precision action execution, a combination that remains challenging for existing Vision-Language-Action (VLA) models. While large Vision-Language Models (VLMs) improve semantic generalization, insufficient embodied reasoning leads to brittle behavior, and conversely, strong reasoning alone is inadequate without precise control. To provide a decoupled and quantitative assessment of this bottleneck, we introduce Embodied Reasoning Intelligence Quotient (ERIQ), a large-scale embodied reasoning benchmark in robotic manipulation, comprising 6K+ question-answer pairs across four reasoning dimensions. By decoupling reasoning from execution, ERIQ enables systematic evaluation and reveals a strong positive correlation between embodied reasoning capability and end-to-end VLA generalization. To bridge the gap from reasoning to precise execution, we propose FACT, a flow-matching-based action tokenizer that converts continuous control into discrete sequences while preserving high-fidelity trajectory reconstruction. The resulting GenieReasoner jointly optimizes reasoning and action in a unified space, outperforming both continuous-action and prior discrete-action baselines in real-world tasks. Together, ERIQ and FACT provide a principled framework for diagnosing and overcoming the reasoning-precision trade-off, advancing robust, general-purpose robotic manipulation.

</details>


### [4] [ROBOPOL: Social Robotics Meets Vehicular Communications for Cooperative Automated Driving](https://arxiv.org/abs/2512.24129)
*Manuel Bied,John Arockiasamy,Andy Comeca,Maximilian Schrapel,Victoria Yang,Alexey Rolich,Barbara Bruno,Maike Schwammberger,Dieter Fiems,Alexey Vinel*

Main category: cs.RO

TL;DR: 本文提出使用社交机器人作为自动驾驶车辆与弱势道路使用者之间的协调者，并概述了实现这一目标所需的四个关键促成因素：高级感知、车载通信、人机社交互动和形式化规范。此外，文章还报告了前三个促成因素的概念验证整合情况，设想了一个社交机器人在有合作自动驾驶电动自行车的场景中为行人提供建议的情景。


<details>
  <summary>Details</summary>
Motivation: 随着交通系统向完全自主化的方向发展，在所谓的混合交通中，自动驾驶车辆与人类参与者共用道路是不可避免的情况。即使道路上的所有车辆都是自动驾驶的，行人仍然需要过马路。基于此，作者提出了利用社交机器人作为自动驾驶车辆与弱势道路用户（如行人）之间的中介者的想法。

Method: 研究识别并提出了四个关键促成要素来支持社交机器人在交通中的作用：1. 高级感知能力，使机器人能够观察周围环境；2. 车辆间通讯技术，让联网汽车可以共享意图且机器人能发送引导指令；3. 人际-机器人社会交互机制，帮助机器人有效地与弱势道路用户及驾驶员沟通；4. 形式化定义方法，赋予机器人理解交通规则并据此规划路径的能力。此外，文中还介绍了前三项促成要素首次概念验证集成的工作进展。

Result: 通过将所提出的前三项关键促成要素进行初步整合，研究团队成功地开发出了一个原型系统，其中社交机器人能够在存在合作型自动驾驶电动自行车的情况下，向行人提供指导信息。这标志着朝向构建更加安全、有效的混合交通环境迈出了重要一步。

Conclusion: 本研究表明，通过结合先进的感知技术、车辆间通信以及改善的人机交互体验，社交机器人可以在促进自动驾驶车辆与行人等弱势道路使用者间的和谐共存方面发挥重要作用。未来的研究将继续探索如何进一步优化这些技术，并考虑引入正式规范以增强机器人的决策能力。

Abstract: On the way towards full autonomy, sharing roads between automated vehicles and human actors in so-called mixed traffic is unavoidable. Moreover, even if all vehicles on the road were autonomous, pedestrians would still be crossing the streets. We propose social robots as moderators between autonomous vehicles and vulnerable road users (VRU). To this end, we identify four enablers requiring integration: (1) advanced perception, allowing the robot to see the environment; (2) vehicular communications allowing connected vehicles to share intentions and the robot to send guiding commands; (3) social human-robot interaction allowing the robot to effectively communicate with VRUs and drivers; (4) formal specification allowing the robot to understand traffic and plan accordingly. This paper presents an overview of the key enablers and report on a first proof-of-concept integration of the first three enablers envisioning a social robot advising pedestrians in scenarios with a cooperative automated e-bike.

</details>


### [5] [GR-Dexter Technical Report](https://arxiv.org/abs/2512.24210)
*Ruoshi Wen,Guangzeng Chen,Zhongren Cui,Min Du,Yang Gou,Zhigang Han,Liqun Huang,Mingyu Lei,Yunfei Li,Zhuohang Li,Wenlei Liu,Yuxiao Liu,Xiao Ma,Hao Niu,Yutao Ouyang,Zeyu Ren,Haixin Shi,Wei Xu,Haoxiang Zhang,Jiajun Zhang,Xiao Zhang,Liwei Zheng,Weiheng Zhong,Yifei Zhou,Zhengming Zhu,Hang Li*

Main category: cs.RO

TL;DR: 提出了GR-Dexter，这是一个面向双臂灵巧手机器人的视觉-语言-动作综合框架，结合了21自由度机械手设计、直观的双臂遥操作系统以及一种新的训练方法，该方法利用了遥操作机器人轨迹和大规模视觉-语言及精心策划的跨实体数据集。在现实世界的长期日常操作和泛化取放任务中表现出色，并对未见过的对象和指令提高了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型大多局限于夹爪，对于具有高自由度（DoF）灵巧手的双臂机器人而言，将VLA策略扩展到此类机器人上仍然存在挑战，原因在于扩大的动作空间、频繁的手-物体遮挡以及收集真实机器人数据的成本。

Method: 开发了一个名为GR-Dexter的硬件-模型-数据综合框架，旨在支持基于VLA的双臂灵巧手机器人的通用操作。它包括一款紧凑型21-DoF机械手的设计、一个用于收集真实机器人数据的直观双臂遥控操作系统，以及一种结合了遥控操作机器人轨迹与大规模视觉-语言和精选跨实体数据集的训练方案。

Result: GR-Dexter在实际评估中展示了强大的域内性能，并且对于未见过的对象和指令展现了更好的鲁棒性。这些评估覆盖了长时间日常生活操作及泛化的拾取放置任务。

Conclusion: GR-Dexter代表了向通用灵巧手机器人操控迈进的一个实用步骤。

Abstract: Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.

</details>


### [6] [RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation](https://arxiv.org/abs/2512.24212)
*Ming-Ming Yu,Yi Chen,Börje F. Karlsson,Wenjun Wu*

Main category: cs.RO

TL;DR: 提出了RANGER，一个仅使用单目相机的零样本、开放词汇语义导航框架，它利用强大的3D基础模型消除了对深度和姿态信息的依赖，并展示了强大的上下文学习能力。通过观察新环境的短视频，系统可以在不进行架构修改或微调的情况下显著提高任务效率。实验表明，RANGER在导航成功率和探索效率方面表现出色，同时展现了优秀的ICL适应性，无需预先对环境进行3D建模。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在寻找复杂环境中目标时过度依赖模拟器提供的精确深度和姿态信息的问题，以及缺乏上下文学习能力导致难以快速适应新环境的问题。

Method: 开发了RANGER框架，该框架包括基于关键帧的3D重建、语义点云生成、视觉-语言模型驱动的探索价值评估、高级自适应航路点选择和低级动作执行等组件。

Result: 在HM3D基准测试和真实世界环境中的实验显示，RANGER在导航成功率、探索效率以及ICL适应性方面均取得了具有竞争力的表现。

Conclusion: RANGER为现实世界中的机器人应用提供了一种有效的方法来实现零样本对象目标导航，能够在没有精细调整的情况下搜索任意对象，并且能够很好地适应新环境。

Abstract: Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required.

</details>


### [7] [Heteroscedastic Bayesian Optimization-Based Dynamic PID Tuning for Accurate and Robust UAV Trajectory Tracking](https://arxiv.org/abs/2512.24249)
*Fuqiang Gu,Jiangshan Ai,Xu Lu,Xianlei Long,Yan Li,Tao Jiang,Chao Chen,Huidong Liu*

Main category: cs.RO

TL;DR: 本文提出了一种新的控制算法HBO-PID，结合了异方差贝叶斯优化框架与经典PID控制器，以提高四旋翼无人机在复杂环境下的轨迹跟踪精度和鲁棒性。实验表明，该方法比现有最先进方法的位置精度提高了24.7%至42.9%，角度精度提高了40.9%至78.4%。


<details>
  <summary>Details</summary>
Motivation: 由于四旋翼系统具有欠驱动、非线性和高度耦合的动力学特性，传统的轨迹跟踪控制算法性能有限。为了克服这些挑战，研究提出了HBO-PID控制算法，旨在实现更准确且鲁棒的轨迹跟踪。

Method: 通过将异方差贝叶斯优化（HBO）框架与PID控制器相结合，形成HBO-PID控制算法。此方法特别之处在于它能够显式建模输入依赖的噪声方差，从而更好地适应动态复杂的环境。此外，采用两阶段优化策略来加速找到最优控制器参数的过程。

Result: 仿真和实际场景测试显示，HBO-PID相比当前最先进的方法，在位置精度上提升了24.7%到42.9%，在角度精度方面则提升了40.9%至78.4%。

Conclusion: 研究表明，所提出的HBO-PID算法能够在保持高精度的同时增强轨迹跟踪任务中的鲁棒性，显著优于现有的最新技术。

Abstract: Unmanned Aerial Vehicles (UAVs) play an important role in various applications, where precise trajectory tracking is crucial. However, conventional control algorithms for trajectory tracking often exhibit limited performance due to the underactuated, nonlinear, and highly coupled dynamics of quadrotor systems. To address these challenges, we propose HBO-PID, a novel control algorithm that integrates the Heteroscedastic Bayesian Optimization (HBO) framework with the classical PID controller to achieve accurate and robust trajectory tracking. By explicitly modeling input-dependent noise variance, the proposed method can better adapt to dynamic and complex environments, and therefore improve the accuracy and robustness of trajectory tracking. To accelerate the convergence of optimization, we adopt a two-stage optimization strategy that allow us to more efficiently find the optimal controller parameters. Through experiments in both simulation and real-world scenarios, we demonstrate that the proposed method significantly outperforms state-of-the-art (SOTA) methods. Compared to SOTA methods, it improves the position accuracy by 24.7% to 42.9%, and the angular accuracy by 40.9% to 78.4%.

</details>


### [8] [Real-world Reinforcement Learning from Suboptimal Interventions](https://arxiv.org/abs/2512.24288)
*Yinuo Zhao,Huiqian Jin,Lechun Jiang,Xinyi Zhang,Kun Wu,Pei Ren,Zhiyuan Xu,Zhengping Che,Lei Sun,Dapeng Wu,Chi Harold Liu,Jian Tang*

Main category: cs.RO

TL;DR: 提出了一种针对现实世界机器人操作任务的状态级拉格朗日强化学习算法SiLRI，该算法能够有效利用可能次优和有噪声的人类干预来加速学习过程，同时不受其限制。实验结果显示，相比最新的RL方法HIL-SERL，SiLRI在达到90%成功率所需时间上至少减少了50%，并在长时序操作任务中实现了100%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有现实世界中的强化学习方法往往假设人类干预在整个状态空间内都是最优的，而忽略了即使是专家也无法始终提供所有状态下最优行动或完全避免错误的事实。直接混合干预数据与机器人收集的数据会继承RL样本效率低下的问题，而纯粹模仿干预数据最终可能会降低RL所能达到的最佳表现。因此，如何利用潜在次优且含噪声的人类干预来加速学习而不被其束缚成为了一个开放性问题。

Method: 提出了名为SiLRI（State-wise Lagrangian Reinforcement Learning with Interventions）的方法，将在线操作问题定义为一个约束条件下的RL优化问题，其中每个状态的约束边界由人类干预的不确定性决定。通过引入状态级拉格朗日乘子，并通过最小-最大优化共同优化策略与拉格朗日乘子以达到鞍点。此方法基于人作为副驾的远程操作系统实现。

Result: 实验证明，SiLRI能够有效地利用次优的人类干预，在不同操作任务中，相较于最先进RL方法HIL-SERL，达到90%成功率所需的时间至少缩短了50%，并且在其他RL方法难以成功的长时序操作任务上达到了100%成功率。

Conclusion: SiLRI是一种新的状态级拉格朗日强化学习算法，旨在解决现实世界机器人操作任务中遇到的问题。它通过更智能地结合人类干预与自主学习，显著提高了学习效率及最终性能。

Abstract: Real-world reinforcement learning (RL) offers a promising approach to training precise and dexterous robotic manipulation policies in an online manner, enabling robots to learn from their own experience while gradually reducing human labor. However, prior real-world RL methods often assume that human interventions are optimal across the entire state space, overlooking the fact that even expert operators cannot consistently provide optimal actions in all states or completely avoid mistakes. Indiscriminately mixing intervention data with robot-collected data inherits the sample inefficiency of RL, while purely imitating intervention data can ultimately degrade the final performance achievable by RL. The question of how to leverage potentially suboptimal and noisy human interventions to accelerate learning without being constrained by them thus remains open. To address this challenge, we propose SiLRI, a state-wise Lagrangian reinforcement learning algorithm for real-world robot manipulation tasks. Specifically, we formulate the online manipulation problem as a constrained RL optimization, where the constraint bound at each state is determined by the uncertainty of human interventions. We then introduce a state-wise Lagrange multiplier and solve the problem via a min-max optimization, jointly optimizing the policy and the Lagrange multiplier to reach a saddle point. Built upon a human-as-copilot teleoperation system, our algorithm is evaluated through real-world experiments on diverse manipulation tasks. Experimental results show that SiLRI effectively exploits human suboptimal interventions, reducing the time required to reach a 90% success rate by at least 50% compared with the state-of-the-art RL method HIL-SERL, and achieving a 100% success rate on long-horizon manipulation tasks where other RL methods struggle to succeed. Project website: https://silri-rl.github.io/.

</details>


### [9] [World In Your Hands: A Large-Scale and Open-source Ecosystem for Learning Human-centric Manipulation in the Wild](https://arxiv.org/abs/2512.24310)
*TARS Robotics,Yuhang Zheng,Jichao Peng,Weize Li,Yupeng Zheng,Xiang Li,Yujie Jin,Julong Wei,Guanhua Zhang,Ruiling Zheng,Ming Cao,Songen Gu,Zhenhong Zou,Kaige Li,Ke Wu,Mingmin Yang,Jiahao Liu,Pengfei Li,Hengjie Si,Feiyu Zhu,Wang Fu,Likun Wang,Ruiwen Yao,Jieru Zhao,Yilun Chen,Wenchao Din*

Main category: cs.RO

TL;DR: 本文介绍了一个大规模开源生态系统World In Your Hands (WiYH)，旨在解决灵巧手操作数据规模和多样性不足的问题。该系统包括用于准确动作捕捉的数据收集套件Oracle Suite、包含超过1000小时多模态操作数据的WiYH数据集以及支持从感知到行动任务的广泛注释和基准测试。实验表明，整合WiYH的人类中心数据能够显著提高灵巧手策略在桌面操作任务中的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前灵巧手操作数据集面临场景多样性有限、模态不匹配及基准测试不足等问题，限制了政策的泛化能力。为了解决这些问题并促进更广泛的泛化，提出了一个新的大规模开源生态系统。

Method: 开发了名为World In Your Hands (WiYH) 的生态系统，它由三部分组成：一个穿戴式数据收集套装（Oracle Suite），配备自动标注流程以实现精确的动作捕捉；WiYH数据集，覆盖数百种技能，在多样化的真实世界场景中提供了超过1,000小时的多模态操作数据；还有全面的注释和基准测试来支持从感知到行动的各种任务。

Result: 基于WiYH生态系统的实验显示，将WiYH的人类中心数据融入其中，可以显著增强灵巧手策略在桌面操作任务上的泛化性和鲁棒性。

Conclusion: World In Your Hands项目有望为社区带来关于人类中心数据收集与策略学习的新见解，并通过提供更大规模且多样化的数据集来改善灵巧手操作模型的表现。

Abstract: Large-scale pre-training is fundamental for generalization in language and vision models, but data for dexterous hand manipulation remains limited in scale and diversity, hindering policy generalization. Limited scenario diversity, misaligned modalities, and insufficient benchmarking constrain current human manipulation datasets. To address these gaps, we introduce World In Your Hands (WiYH), a large-scale open-source ecosystem for human-centric manipulation learning. WiYH includes (1) the Oracle Suite, a wearable data collection kit with an auto-labeling pipeline for accurate motion capture; (2) the WiYH Dataset, featuring over 1,000 hours of multi-modal manipulation data across hundreds of skills in diverse real-world scenarios; and (3) extensive annotations and benchmarks supporting tasks from perception to action. Furthermore, experiments based on the WiYH ecosystem show that integrating WiYH's human-centric data significantly enhances the generalization and robustness of dexterous hand policies in tabletop manipulation tasks. We believe that World In Your Hands will bring new insights into human-centric data collection and policy learning to the community.

</details>


### [10] [3D Path-Following Guidance via Nonlinear Model Predictive Control for Fixed-Wing Small UAS](https://arxiv.org/abs/2512.24326)
*Camron Alexander Hirst,Chris Reale,Eric Frew*

Main category: cs.RO

TL;DR: 本文介绍了两种基于非线性模型预测控制（MPC）的3D路径跟随制导算法的设计、实现和飞行测试结果，特别适用于固定翼小型无人飞机系统。结果显示了非线性MPC在三维路径跟随制导中的实际可行性和优越性能，地面速度可达36米/秒。


<details>
  <summary>Details</summary>
Motivation: 为了提高固定翼小型无人飞机系统的3D路径跟随性能，特别是处理高曲率路径时的精确度和稳定性。

Method: 首先对RAAVEN小型无人飞机进行了增控建模和系统识别以启用MPC。接着展示了两种MPC公式：一种在整个MPC视野内安排静态参考路径速率，鼓励恒定惯性速度；另一种受模型预测轮廓控制启发，在控制器操作期间动态优化参考路径速率，允许路径进展与路径距离之间的加权折衷。

Result: 这两种控制器都设计用于通用平滑的3D弧长参数化曲线，并且在多个高曲率测试路径上进行了飞行实验，与基准前视制导律进行了比较。结果显示，非线性MPC方法在高达36米/秒的地面速度下展现了出色的3D路径跟随性能。

Conclusion: 研究表明，基于非线性MPC的3D路径跟随制导算法不仅在理论上具有优势，而且在实际应用中也表现出色，特别是在需要高精度导航的小型无人飞机系统中。

Abstract: This paper presents the design, implementation, and flight test results of two novel 3D path-following guidance algorithms based on nonlinear model predictive control (MPC), with specific application to fixed-wing small uncrewed aircraft systems. To enable MPC, control-augmented modelling and system identification of the RAAVEN small uncrewed aircraft is presented. Two formulations of MPC are then showcased. The first schedules a static reference path rate over the MPC horizon, incentivizing a constant inertial speed. The second, with inspiration from model predictive contouring control, dynamically optimizes for the reference path rate over the controller horizon as the system operates. This allows for a weighted tradeoff between path progression and distance from path, two competing objectives in path-following guidance. Both controllers are formulated to operate over general smooth 3D arc-length parameterized curves. The MPC guidance algorithms are flown over several high-curvature test paths, with comparison to a baseline lookahead guidance law. The results showcase the real-world feasibility and superior performance of nonlinear MPC for 3D path-following guidance at ground speeds up to 36 meters per second.

</details>


### [11] [Geometric Multi-Session Map Merging with Learned Local Descriptors](https://arxiv.org/abs/2512.24384)
*Yanlong Ma,Nakul S. Joshi,Christa S. Robison,Philip R. Osteen,Brett T. Lopez*

Main category: cs.RO

TL;DR: 本文提出了一种基于学习的局部描述符框架GMLD，用于大规模多会话点云地图合并，通过关键点感知编码器和平面几何变换器提取特征，并在因子图优化阶段加入跨会话扫描匹配成本因子，实验结果表明该方法在回环检测和相对位姿估计上表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了支持大规模环境下的长期自主操作，需要有效地将不同会话中收集的地图进行合并，特别是在有重叠区域的情况下。

Method: GMLD框架采用了关键点感知编码器和平面几何变换器来提取具有区分性的特征，以完成回环检测和相对位姿估计任务。此外，在因子图优化过程中引入了跨会话的扫描匹配成本因子，以此提高全局一致性。

Result: 在公开数据集及自采多样化环境数据上的评估显示，GMLD能够实现精确且鲁棒的地图合并，同时保持较低的误差率；所学到的特征对于回环检测和相对位姿估计均表现出色。

Conclusion: GMLD为大型环境中多会话点云地图的有效合并提供了一个强大的解决方案，显著提高了地图合并的质量与可靠性。

Abstract: Multi-session map merging is crucial for extended autonomous operations in large-scale environments. In this paper, we present GMLD, a learning-based local descriptor framework for large-scale multi-session point cloud map merging that systematically aligns maps collected across different sessions with overlapping regions. The proposed framework employs a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for loop closure detection and relative pose estimation. To further improve global consistency, we include inter-session scan matching cost factors in the factor-graph optimization stage. We evaluate our framework on the public datasets, as well as self-collected data from diverse environments. The results show accurate and robust map merging with low error, and the learned features deliver strong performance in both loop closure detection and relative pose estimation.

</details>


### [12] [Fast and Realistic Automated Scenario Simulations and Reporting for an Autonomous Racing Stack](https://arxiv.org/abs/2512.24402)
*Giovanni Lambertini,Matteo Pini,Eugenio Mascaro,Francesco Moretti,Ayoub Raji,Marko Bertogna*

Main category: cs.RO

TL;DR: 本文介绍了一个为自动驾驶赛车堆栈ur.autopilot设计的自动化模拟和报告流程，该流程能够以比实时快三倍的速度执行软件堆栈和模拟，并且可以在本地或GitHub上进行持续集成/持续交付。


<details>
  <summary>Details</summary>
Motivation: 为了高效验证自动驾驶赛车中的关键模块（如高速超车操作或定位），需要一个可以快速执行并能根据不同初始条件初始化车辆的模拟环境。此外，还需要一种方式来模拟传感器延迟、扰动以及修改堆栈中任意节点输出，以测试系统的鲁棒性。

Method: 开发了一种基于高保真车辆模型并与功能性模拟单元(FMU)接口相连的模拟系统。该系统支持多种运行场景，允许用户在不同的起始条件下初始化车辆及其配置。同时，还实现了一个故障注入模块，可用来引入传感器延迟与扰动，并修改堆栈中任何节点的输出。

Result: 创建了一个高效的自动化模拟及报告流程，能够以接近实时三倍的速度运行模拟，并且集成了持续集成/持续交付功能。此流程对于验证自动驾驶赛车技术中的复杂任务至关重要。

Conclusion: 通过建立这样一个灵活且强大的自动化模拟与报告框架，研究人员能够在更短的时间内完成对自动驾驶赛车系统更全面的测试与验证工作。

Abstract: In this paper, we describe the automated simulation and reporting pipeline implemented for our autonomous racing stack, ur.autopilot. The backbone of the simulation is based on a high-fidelity model of the vehicle interfaced as a Functional Mockup Unit (FMU). The pipeline can execute the software stack and the simulation up to three times faster than real-time, locally or on GitHub for Continuous Integration/- Continuous Delivery (CI/CD). As the most important input of the pipeline, there is a set of running scenarios. Each scenario allows the initialization of the ego vehicle in different initial conditions (position and speed), as well as the initialization of any other configuration of the stack. This functionality is essential to validate efficiently critical modules, like the one responsible for high-speed overtaking maneuvers or localization, which are among the most challenging aspects of autonomous racing. Moreover, we describe how we implemented a fault injection module, capable of introducing sensor delays and perturbations as well as modifying outputs of any node of the stack. Finally, we describe the design of our automated reporting process, aimed at maximizing the effectiveness of the simulation analysis.

</details>


### [13] [Subsecond 3D Mesh Generation for Robot Manipulation](https://arxiv.org/abs/2512.24428)
*Qian Wang,Omar Abdellall,Tony Gao,Xiatao Sun,Daniel Rakita*

Main category: cs.RO

TL;DR: 本文介绍了一个端到端系统，能够在不到一秒的时间内从单个RGB-D图像生成高质量且上下文相关的3D网格，解决了3D网格生成速度慢和缺乏上下文关联的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管自动3D网格生成方法近年来取得了显著进展，但仍然存在两个关键挑战：一是生成高保真度网格的速度过慢，不适合实时使用；二是需要将网格正确地从场景中分割出来，并以正确的比例和姿态注册。

Method: 研究者们提出了一种集成开放词汇对象分割、加速扩散式网格生成以及鲁棒点云配准的管道，每个步骤都针对速度和准确性进行了优化。

Result: 该系统在实际操作任务中的有效性得到了证明，表明它可以作为机器人感知和规划的一种实用、按需表示方法。

Conclusion: 这项工作为实现快速准确的3D网格生成提供了解决方案，有助于推动机器人技术的发展。

Abstract: 3D meshes are a fundamental representation widely used in computer science and engineering. In robotics, they are particularly valuable because they capture objects in a form that aligns directly with how robots interact with the physical world, enabling core capabilities such as predicting stable grasps, detecting collisions, and simulating dynamics. Although automatic 3D mesh generation methods have shown promising progress in recent years, potentially offering a path toward real-time robot perception, two critical challenges remain. First, generating high-fidelity meshes is prohibitively slow for real-time use, often requiring tens of seconds per object. Second, mesh generation by itself is insufficient. In robotics, a mesh must be contextually grounded, i.e., correctly segmented from the scene and registered with the proper scale and pose. Additionally, unless these contextual grounding steps remain efficient, they simply introduce new bottlenecks. In this work, we introduce an end-to-end system that addresses these challenges, producing a high-quality, contextually grounded 3D mesh from a single RGB-D image in under one second. Our pipeline integrates open-vocabulary object segmentation, accelerated diffusion-based mesh generation, and robust point cloud registration, each optimized for both speed and accuracy. We demonstrate its effectiveness in a real-world manipulation task, showing that it enables meshes to be used as a practical, on-demand representation for robotics perception and planning.

</details>


### [14] [Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models](https://arxiv.org/abs/2512.24470)
*Kim Alexander Christensen,Andreas Gudahl Tufte,Alexey Gusev,Rohan Sinha,Milan Ganai,Ole Andreas Alsos,Marco Pavoned,Martin Steinert*

Main category: cs.RO

TL;DR: 该研究提出了一种基于视觉-语言模型（VLMs）的语义观察系统，用于在海事自主船遇到超出设计运行域的情况时，提供一个短时、可由人类接管的应急操作策略。这种系统能够识别环境中的异常情况，并选择保持安全距离的动作，符合IMO MASS Code草案的要求。


<details>
  <summary>Details</summary>
Motivation: 国际海事组织（IMO）对自主和远程监管的海事船只提出了新的要求，包括检测偏离其设计运行域的情况并进入预设的应急模式通知操作员，允许立即的人工干预，并且未经批准不得更改航行计划。为了满足这些需求，特别是在警报到人工接管之间的过渡期间，需要一种能在短时间内被人类覆盖的应急机动方案。传统海事自主系统在应对依赖于情境意义的正确行动上存在困难。

Method: 研究人员提出使用视觉-语言模型（VLMs）来为这类超出常规分布的情形提供语义意识，并构建了一个快速慢速异常处理管道，其中包括一个短期、可以由人类随时接管的应急机动选项。所提出的“语义瞭望”系统是一个仅依赖摄像头的候选约束视觉-语言模型，它能够在持续的人类监督下从水面上的有效轨迹中选择一个谨慎的行为（或停留）。

Result: 通过40个港口场景测试了每次调用下的场景理解与延迟、与人类共识的一致性（采用三者多数投票原则）、火灾危险场景下的短期风险缓解效果以及一次从警报到应急机动再到操作员接管的实际水上试验。结果显示，响应时间低于10秒的模型仍能保持较慢但最先进的模型的大部分感知能力；相比仅基于几何信息的基础方法，所提应急机动选择器表现更优并在火灾场景中增加了安全距离。

Conclusion: 实验结果支持将视觉-语言模型作为符合IMO MASS Code草案要求的语义应急机动选择器使用，且在实际可接受的延迟范围内有效。这为进一步研究领域适应性的混合自主系统提供了动力，该系统结合基础模型语义与多传感器鸟瞰视角感知及短期重规划功能。

Abstract: The draft IMO MASS Code requires autonomous and remotely supervised maritime vessels to detect departures from their operational design domain, enter a predefined fallback that notifies the operator, permit immediate human override, and avoid changing the voyage plan without approval. Meeting these obligations in the alert-to-takeover gap calls for a short-horizon, human-overridable fallback maneuver. Classical maritime autonomy stacks struggle when the correct action depends on meaning (e.g., diver-down flag means people in the water, fire close by means hazard). We argue (i) that vision-language models (VLMs) provide semantic awareness for such out-of-distribution situations, and (ii) that a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback maneuver makes this practical in the handover window. We introduce Semantic Lookout, a camera-only, candidate-constrained vision-language model (VLM) fallback maneuver selector that selects one cautious action (or station-keeping) from water-valid, world-anchored trajectories under continuous human authority. On 40 harbor scenes we measure per-call scene understanding and latency, alignment with human consensus (model majority-of-three voting), short-horizon risk-relief on fire hazard scenes, and an on-water alert->fallback maneuver->operator handover. Sub-10 s models retain most of the awareness of slower state-of-the-art models. The fallback maneuver selector outperforms geometry-only baselines and increases standoff distance on fire scenes. A field run verifies end-to-end operation. These results support VLMs as semantic fallback maneuver selectors compatible with the draft IMO MASS Code, within practical latency budgets, and motivate future work on domain-adapted, hybrid autonomy that pairs foundation-model semantics with multi-sensor bird's-eye-view perception and short-horizon replanning.

</details>


### [15] [DISF: Disentangled Iterative Surface Fitting for Contact-stable Grasp Planning with Grasp Pose Alignment to the Object Center of Mass](https://arxiv.org/abs/2512.24550)
*Tomoya Yamanokuchi,Alberto Bacchin,Emilio Olivastri,Ryotaro Arifuku,Takamitsu Matsubara,Emanuele Menegatti*

Main category: cs.RO

TL;DR: 提出了一种新的表面拟合算法，该算法在保持几何兼容性的同时集成了接触稳定性，通过三个连续步骤优化抓取姿态，并在仿真和实际机器人实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于表面拟合的抓取规划算法主要关注夹具与物体表面之间的几何对齐，而忽略了接触点分布的稳定性，导致由于接触配置不足而产生不稳定的抓取。

Method: 受人类抓取行为启发，将抓取姿态优化拆分为三个连续步骤：1) 旋转优化以对齐接触法线；2) 平移细化以改善夹具框架原点与物体质心（CoM）之间的对齐；3) 调整夹具开口以优化接触点分布。

Result: 在包含15个对象的模拟环境中，以及在UR3e机器人的实际抓取实验中，证明了DISF方法能够减少质心错位并保持几何兼容性，从而相比基线方法实现了更高的抓取成功率。

Conclusion: 提出的DISF算法不仅提高了抓取操作中的几何对齐度，还增强了接触点分布的稳定性，最终提高了抓取的成功率。

Abstract: In this work, we address the limitation of surface fitting-based grasp planning algorithm, which primarily focuses on geometric alignment between the gripper and object surface while overlooking the stability of contact point distribution, often resulting in unstable grasps due to inadequate contact configurations. To overcome this limitation, we propose a novel surface fitting algorithm that integrates contact stability while preserving geometric compatibility. Inspired by human grasping behavior, our method disentangles the grasp pose optimization into three sequential steps: (1) rotation optimization to align contact normals, (2) translation refinement to improve the alignment between the gripper frame origin and the object Center of Mass (CoM), and (3) gripper aperture adjustment to optimize contact point distribution. We validate our approach in simulation across 15 objects under both Known-shape (with clean CAD-derived dataset) and Observed-shape (with YCB object dataset) settings, including cross-platform grasp execution on three robot--gripper platforms. We further validate the method in real-world grasp experiments on a UR3e robot. Overall, DISF reduces CoM misalignment while maintaining geometric compatibility, translating into higher grasp success in both simulation and real-world execution compared to baselines. Additional videos and supplementary results are available on our project page: https://tomoya-yamanokuchi.github.io/disf-ras-project-page/

</details>


### [16] [Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding](https://arxiv.org/abs/2512.24638)
*Qingda Hu,Ziheng Qiu,Zijun Xu,Kaizhao Zhang,Xizhou Bu,Zuolei Sun,Bo Zhang,Jieru Zhao,Zhongxue Gan,Wenchao Ding*

Main category: cs.RO

TL;DR: 本文提出了一种新的视觉运动策略PAM，它配备有自适应工作记忆，能够有效处理机器人操作中的状态模糊问题。通过最小的额外训练成本，PAM支持长达300帧的历史窗口同时保持高速推理。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作中，相同观察可能对应多种行为轨迹，这导致了状态模糊性的问题。视觉运动策略需要从历史数据中准确提取信息以确定当前任务阶段。然而，简单地扩展历史窗口不仅计算成本高还可能导致过拟合。受到人类连续推理和工作记忆重编码机制的启发，作者提出了PAM来解决这些问题。

Method: PAM采用了分层框架特征提取器生成两种不同的表示形式，分别用于动作原语和时间解歧。为了紧凑表示，使用了具有范围特定查询的上下文路由器来产生跨多个历史长度的紧凑上下文特征，并引入了一个辅助目标——重建历史信息，以确保上下文路由器作为一个有效的瓶颈。整个方法采用两阶段方式训练，几乎不增加额外的训练成本。

Result: PAM被设计用来处理多种状态模糊情景，在大约10秒的历史窗口下，仍能支持稳定的训练过程并且维持高于20Hz的推理速度。

Conclusion: 实验结果表明，PAM能够在多种场景下有效应对状态模糊问题，同时保持高效的训练稳定性和推理速度。

Abstract: State ambiguity is common in robotic manipulation. Identical observations may correspond to multiple valid behavior trajectories. The visuomotor policy must correctly extract the appropriate types and levels of information from the history to identify the current task phase. However, naively extending the history window is computationally expensive and may cause severe overfitting. Inspired by the continuous nature of human reasoning and the recoding of working memory, we introduce PAM, a novel visuomotor Policy equipped with Adaptive working Memory. With minimal additional training cost in a two-stage manner, PAM supports a 300-frame history window while maintaining high inference speed. Specifically, a hierarchical frame feature extractor yields two distinct representations for motion primitives and temporal disambiguation. For compact representation, a context router with range-specific queries is employed to produce compact context features across multiple history lengths. And an auxiliary objective of reconstructing historical information is introduced to ensure that the context router acts as an effective bottleneck. We meticulously design 7 tasks and verify that PAM can handle multiple scenarios of state ambiguity simultaneously. With a history window of approximately 10 seconds, PAM still supports stable training and maintains inference speeds above 20Hz. Project website: https://tinda24.github.io/pam/

</details>


### [17] [RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence](https://arxiv.org/abs/2512.24653)
*Chengkai Hou,Kun Wu,Jiaming Liu,Zhengping Che,Di Wu,Fei Liao,Guangrun Li,Jingyang He,Qiuxuan Feng,Zhao Jin,Chenyang Gu,Zhuoyang Liu,Nuowei Han,Xiangju Mi,Yaoxu Lv,Yankai Fu,Gaole Dai,Langzhe Gu,Tao Li,Yuheng Zhang,Yixue Zhang,Xinhua Wang,Shichao Fan,Meng Li,Zhen Zhao,Ning Liu,Zhiyuan Xu,Pei Ren,Junjie Ji,Haonan Liu,Kuan Cheng,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: RoboMIND 2.0是一个包含超过31万条双臂操作轨迹的数据集，旨在提高机器人在复杂任务和未结构化环境中的泛化能力。此外，还提出了MIND-2系统，该系统通过离线强化学习优化，结合了高层语义规划器与低层视觉-语言-动作执行器，以实现更精确的动作控制。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的模仿学习方法受限于大规模、多样化的真实世界演示数据的稀缺性，导致现有模型在长时间范围的双臂任务和移动操纵方面的能力有限。为了解决这个问题，并支持接触丰富和空间扩展的任务研究。

Method: 构建了一个名为RoboMIND 2.0的大规模真实世界数据集，包含了来自六个不同机器人实体的超过31万条双臂操作轨迹以及739个复杂任务。此外，还开发了高保真的数字孪生环境，并提供额外的20,000条模拟轨迹数据集来促进从模拟到真实的迁移。基于此数据集，提出了一种通过离线强化学习优化的分层双系统框架——MIND-2系统。

Result: RoboMIND 2.0 数据集和 MIND-2 系统的引入有望显著提升机器人在处理长期视野下的双臂任务及在非结构化环境中进行移动操作时的表现。特别是，MIND-2 系统能够将抽象自然语言指令分解成具体的子目标，并生成精确的身体感知运动动作。

Conclusion: 通过发布RoboMIND 2.0数据集及其配套的模拟数据集，加上MIND-2系统的提出，本研究为解决机器人在复杂任务中面临的挑战提供了新的解决方案，增强了机器人技术在更多样化场景下的应用潜力。

Abstract: While data-driven imitation learning has revolutionized robotic manipulation, current approaches remain constrained by the scarcity of large-scale, diverse real-world demonstrations. Consequently, the ability of existing models to generalize across long-horizon bimanual tasks and mobile manipulation in unstructured environments remains limited. To bridge this gap, we present RoboMIND 2.0, a comprehensive real-world dataset comprising over 310K dual-arm manipulation trajectories collected across six distinct robot embodiments and 739 complex tasks. Crucially, to support research in contact-rich and spatially extended tasks, the dataset incorporates 12K tactile-enhanced episodes and 20K mobile manipulation trajectories. Complementing this physical data, we construct high-fidelity digital twins of our real-world environments, releasing an additional 20K-trajectory simulated dataset to facilitate robust sim-to-real transfer. To fully exploit the potential of RoboMIND 2.0, we propose MIND-2 system, a hierarchical dual-system frame-work optimized via offline reinforcement learning. MIND-2 integrates a high-level semantic planner (MIND-2-VLM) to decompose abstract natural language instructions into grounded subgoals, coupled with a low-level Vision-Language-Action executor (MIND-2-VLA), which generates precise, proprioception-aware motor actions.

</details>


### [18] [Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand: Toward Dexterous Manipulation for Payload Constrained Humanoids](https://arxiv.org/abs/2512.24657)
*Sungjae Min,Hyungjoo Kim,David Hyunchul Shim*

Main category: cs.RO

TL;DR: 本文介绍了一种轻量级的人形机器人手，它通过Bowden线缆驱动并结合滚动接触关节优化和拮抗式线缆驱动技术，实现了在保持人类形态尺寸的同时提供强大的抓握力、快速的动作速度以及多自由度。


<details>
  <summary>Details</summary>
Motivation: 为了实现人形机器人的高灵巧性，需要开发出能够在人类尺寸限制内同时提供高抓握力、快速动作速度、多自由度及轻量化结构的机器人手。然而，满足这些要求往往会导致使用更重的执行器和更大体积的传动系统，从而显著限制了机械臂的有效载荷能力。

Method: 研究者提出了一种利用Bowden线缆驱动的轻量级仿生手设计，该设计独特地结合了滚动接触关节优化与拮抗式线缆驱动方式，使得每个关节仅需一个电机控制且几乎不受线缆长度变化的影响。此外，将执行器模块移至躯干部分的设计进一步减少了远端质量，同时保持了仿生比例和灵活性。

Result: 所提出的这种手部组装体（不包括远程执行器和Bowden套管）的远端质量仅为236克，并能够可靠地完成灵巧任务，指尖力量超过18牛顿，并能举起自重一百倍以上的负载。另外，通过Cutkosky分类抓取测试和受扰动下的运动一致性验证了其鲁棒性。

Conclusion: 本研究展示了一种创新性的轻量级人形机器人手设计方案，不仅解决了传统设计中重量与性能之间的矛盾，还展示了出色的负载能力和鲁棒性，为未来人形机器人向更高水平灵巧性发展提供了新的可能性。

Abstract: Humanoid robots toward human-level dexterity require robotic hands capable of simultaneously providing high grasping force, rapid actuation speeds, multiple degrees of freedom, and lightweight structures within human-like size constraints. Meeting these conflicting requirements remains challenging, as satisfying this combination typically necessitates heavier actuators and bulkier transmission systems, significantly restricting the payload capacity of robot arms. In this letter, we present a lightweight anthropomorphic hand actuated by Bowden cables, which uniquely combines rolling-contact joint optimization with antagonistic cable actuation, enabling single-motor-per-joint control with negligible cable-length deviation. By relocating the actuator module to the torso, the design substantially reduces distal mass while maintaining anthropomorphic scale and dexterity. Additionally, this antagonistic cable actuation eliminates the need for synchronization between motors. Using the proposed methods, the hand assembly with a distal mass of 236g (excluding remote actuators and Bowden sheaths) demonstrated reliable execution of dexterous tasks, exceeding 18N fingertip force and lifting payloads over one hundred times its own mass. Furthermore, robustness was validated through Cutkosky taxonomy grasps and trajectory consistency under perturbed actuator-hand transformations.

</details>


### [19] [VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots](https://arxiv.org/abs/2512.24673)
*Yongsheng Zhao,Lei Zhao,Baoping Cheng,Gongxin Yao,Xuanzhang Wen,Han Gao*

Main category: cs.RO

TL;DR: 本文提出了一种新的框架VLA-RAIL，通过异步进行模型推理和机器人运动控制，解决了现有方法在机器人动作执行过程中存在的抖动、停滞或暂停问题。该框架包括轨迹平滑器和块融合器两大核心贡献，实验结果表明VLA-RAIL显著减少了运动抖动，提高了执行速度和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作（VLA）模型在机器人技术领域取得了显著突破，但现有的方法在连续执行动作序列时存在抖动、停滞甚至暂停的问题，这不仅限制了可达到的执行速度，也降低了任务完成的整体成功率。

Method: 提出了VLA-RAIL框架，该框架包含两部分：一是使用多项式拟合来过滤单个动作片段轨迹中的噪声和抖动的轨迹平滑器；二是确保当前执行轨迹与新到达的动作片段之间平稳对接，并保持位置、速度及加速度连续性的块融合器。

Result: 通过动态仿真任务基准测试以及多个真实世界操作任务验证了VLA-RAIL的有效性。实验结果显示，VLA-RAIL能够显著减少运动抖动，提高执行速度，并改善任务成功率。

Conclusion: VLA-RAIL为解决VLA模型中动作执行过程中的抖动、停滞等问题提供了一个有效的解决方案，对于推动VLA模型的大规模部署具有重要意义。

Abstract: Vision-Language-Action (VLA) models have achieved remarkable breakthroughs in robotics, with the action chunk playing a dominant role in these advances. Given the real-time and continuous nature of robotic motion control, the strategies for fusing a queue of successive action chunks have a profound impact on the overall performance of VLA models. Existing methods suffer from jitter, stalling, or even pauses in robotic action execution, which not only limits the achievable execution speed but also reduces the overall success rate of task completion. This paper introduces VLA-RAIL (A Real-Time Asynchronous Inference Linker), a novel framework designed to address these issues by conducting model inference and robot motion control asynchronously and guaranteeing smooth, continuous, and high-speed action execution. The core contributions of the paper are two fold: a Trajectory Smoother that effectively filters out the noise and jitter in the trajectory of one action chunk using polynomial fitting and a Chunk Fuser that seamlessly align the current executing trajectory and the newly arrived chunk, ensuring position, velocity, and acceleration continuity between two successive action chunks. We validate the effectiveness of VLA-RAIL on a benchmark of dynamic simulation tasks and several real-world manipulation tasks. Experimental results demonstrate that VLA-RAIL significantly reduces motion jitter, enhances execution speed, and improves task success rates, which will become a key infrastructure for the large-scale deployment of VLA models.

</details>


### [20] [ReSPIRe: Informative and Reusable Belief Tree Search for Robot Probabilistic Search and Tracking in Unknown Environments](https://arxiv.org/abs/2512.24680)
*Kangjie Zhou,Zhaoyang Li,Han Gao,Yao Su,Hangxin Liu,Junzhi Yu,Chang Liu*

Main category: cs.RO

TL;DR: 本文提出了一种名为ReSPIRe的信息轨迹规划方法，用于在未知杂乱环境中进行目标搜索与跟踪（SAT），该方法能在非常不准确的先验目标信息和有限的感知视野下工作。通过使用基于sigma点的新颖近似方法快速准确地估计非高斯信念分布下的互信息奖励，并引入分层粒子结构来处理不确定性，同时开发了可重用信念树搜索方法以提高在线轨迹规划效率。仿真和实际实验表明，ReSPIRe在保持出色的计算效率的同时，在MI近似误差、搜索效率和跟踪性能稳定性方面优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 针对在未知且复杂的环境下，当面对相当不准确的先验目标信息以及有限的传感器视野时，如何有效地执行目标搜索与追踪任务的问题。

Method: 开发了一种基于sigma点的新型逼近方法，能够快速而准确地评估非高斯信念分布条件下的互信息收益；提出了ReSPIRe中的层次化粒子架构，不仅提取关键粒子引导全局路线，还自适应调整粒子数目以提高规划效率；基于此层次结构，进一步发展了可复用信念树搜索策略，构建策略树支持不确定性条件下的在线路径规划。

Result: 广泛的模拟和真实世界测试显示，相较于其他代表性基线方法，ReSPIRe展现出更低的MI近似错误率、更高的搜寻效率及更稳定的追踪表现，同时保持着优秀的计算效率。

Conclusion: ReSPIRe提供了一种高效解决未知复杂环境内目标搜索与追踪问题的新途径，其通过减少MI估计误差、提升搜索效率及增强跟踪稳定性证明了自身相对于现有解决方案的优势。

Abstract: Target search and tracking (SAT) is a fundamental problem for various robotic applications such as search and rescue and environmental exploration. This paper proposes an informative trajectory planning approach, namely ReSPIRe, for SAT in unknown cluttered environments under considerably inaccurate prior target information and limited sensing field of view. We first develop a novel sigma point-based approximation approach to fast and accurately estimate mutual information reward under non-Gaussian belief distributions, utilizing informative sampling in state and observation spaces to mitigate the computational intractability of integral calculation. To tackle significant uncertainty associated with inadequate prior target information, we propose the hierarchical particle structure in ReSPIRe, which not only extracts critical particles for global route guidance, but also adjusts the particle number adaptively for planning efficiency. Building upon the hierarchical structure, we develop the reusable belief tree search approach to build a policy tree for online trajectory planning under uncertainty, which reuses rollout evaluation to improve planning efficiency. Extensive simulations and real-world experiments demonstrate that ReSPIRe outperforms representative benchmark methods with smaller MI approximation error, higher search efficiency, and more stable tracking performance, while maintaining outstanding computational efficiency.

</details>


### [21] [CREPES-X: Hierarchical Bearing-Distance-Inertial Direct Cooperative Relative Pose Estimation System](https://arxiv.org/abs/2512.24688)
*Zhehan Li,Zheng Wang,Jiadong Lu,Qi Liu,Zhiren Xun,Yue Wang,Fei Gao,Chao Xu,Yanjun Cao*

Main category: cs.RO

TL;DR: 本文提出了一种名为CREPES-X的层次化相对定位框架，该框架通过结合红外LED、红外摄像头、超宽带模块和IMU等紧凑硬件设计以及两阶段估计器来提高多机器人系统在复杂环境下的相对定位速度、准确性和鲁棒性。实验结果表明CREPES-X能够有效应对高达90%的角度异常值，并且在实际数据集上实现了0.073米和1.817度的RMSE。


<details>
  <summary>Details</summary>
Motivation: 现有的多机器人系统中的相对定位方法要么依赖于共享的环境特征或惯性假设，要么在复杂的环境中容易受到非视距退化和异常值的影响。对于数十个机器人的互测融合（如方位角、距离和惯性测量）来说，实现鲁棒且高效的处理仍然是一个挑战。

Method: 提出了CREPES-X，这是一种不需要任何全局信息的层次化相对定位框架。它首先采用一种紧凑型硬件设计，包括红外LED、红外相机、超宽带模块及IMU封装在一个边长不超过6厘米的立方体内。接着，CREPES-X实施了一个两阶段层次化估计器以满足不同的要求，考虑了速度、精度和鲁棒性。第一阶段是一个单帧相对估计器，通过闭式解和鲁棒的角度异常值剔除提供即时的相对姿态。第二阶段则是一个多帧相对估计器，通过利用IMU预积分并结合松散和紧密耦合优化来探索基于机器人中心的相对运动学，从而提供精确而鲁棒的相对状态。

Result: 广泛的模拟与现实世界实验验证了CREPES-X的有效性，显示出其在面对高达90%角度异常值时仍具有鲁棒性，并且在真实世界的数据集中实现了0.073米的位置误差和1.817度的方向误差。

Conclusion: CREPES-X作为一种新型的相对定位解决方案，在无需依赖全局信息的前提下，为多机器人系统提供了更快速、更准确同时也更加鲁棒的相对定位能力。

Abstract: Relative localization is critical for cooperation in autonomous multi-robot systems. Existing approaches either rely on shared environmental features or inertial assumptions or suffer from non-line-of-sight degradation and outliers in complex environments. Robust and efficient fusion of inter-robot measurements such as bearings, distances, and inertials for tens of robots remains challenging. We present CREPES-X (Cooperative RElative Pose Estimation System with multiple eXtended features), a hierarchical relative localization framework that enhances speed, accuracy, and robustness under challenging conditions, without requiring any global information. CREPES-X starts with a compact hardware design: InfraRed (IR) LEDs, an IR camera, an ultra-wideband module, and an IMU housed in a cube no larger than 6cm on each side. Then CREPES-X implements a two-stage hierarchical estimator to meet different requirements, considering speed, accuracy, and robustness. First, we propose a single-frame relative estimator that provides instant relative poses for multi-robot setups through a closed-form solution and robust bearing outlier rejection. Then a multi-frame relative estimator is designed to offer accurate and robust relative states by exploring IMU pre-integration via robocentric relative kinematics with loosely- and tightly-coupled optimization. Extensive simulations and real-world experiments validate the effectiveness of CREPES-X, showing robustness to up to 90% bearing outliers, proving resilience in challenging conditions, and achieving RMSE of 0.073m and 1.817° in real-world datasets.

</details>


### [22] [Dynamic Policy Learning for Legged Robot with Simplified Model Pretraining and Model Homotopy Transfer](https://arxiv.org/abs/2512.24698)
*Dongyun Kang,Min-Gyu Kim,Tae-Gyu Song,Hajun Kim,Sehoon Ha,Hae-Won Park*

Main category: cs.RO

TL;DR: 本文提出了一种基于连续学习框架的方法，结合简化的模型预训练和模型同伦转移来有效生成并优化复杂的动态行为。该方法通过从单刚体模型到全身模型逐步转移策略，减少了性能损失，并在一系列动态任务中表现出更快的收敛速度和更好的稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在各种腿部机器人运动任务中取得了显著成功，但要产生高度动态的行为通常需要大量的奖励调整或高质量的演示。利用降阶模型可以帮助缓解这些问题，但在将策略转移到全身动力学环境时，模型差异构成了重大挑战。因此，作者们旨在开发一种能够高效生成并细化复杂动态行为的新方法。

Method: 首先使用单刚体模型对策略进行预训练，在简化环境中捕捉核心运动模式；然后采用连续策略逐渐将该策略转移到全身环境，最小化性能损失。为了定义连续路径，通过逐渐重新分配躯干和腿之间的质量和惯性，从单刚体模型到全身模型引入了模型同伦。

Result: 所提出的方法不仅实现了更快的收敛速度，而且与基线方法相比，在转移过程中表现出更优的稳定性。此外，该框架在包括翻转和借助墙壁完成的动作等一系列动态任务上得到了验证，并成功部署于真实的四足机器人上。

Conclusion: 这项工作介绍了一种新的学习框架，它能有效地生成并改善复杂动态行为，同时在实际应用中展示出良好的稳定性和效率。

Abstract: Generating dynamic motions for legged robots remains a challenging problem. While reinforcement learning has achieved notable success in various legged locomotion tasks, producing highly dynamic behaviors often requires extensive reward tuning or high-quality demonstrations. Leveraging reduced-order models can help mitigate these challenges. However, the model discrepancy poses a significant challenge when transferring policies to full-body dynamics environments. In this work, we introduce a continuation-based learning framework that combines simplified model pretraining and model homotopy transfer to efficiently generate and refine complex dynamic behaviors. First, we pretrain the policy using a single rigid body model to capture core motion patterns in a simplified environment. Next, we employ a continuation strategy to progressively transfer the policy to the full-body environment, minimizing performance loss. To define the continuation path, we introduce a model homotopy from the single rigid body model to the full-body model by gradually redistributing mass and inertia between the trunk and legs. The proposed method not only achieves faster convergence but also demonstrates superior stability during the transfer process compared to baseline methods. Our framework is validated on a range of dynamic tasks, including flips and wall-assisted maneuvers, and is successfully deployed on a real quadrupedal robot.

</details>


### [23] [LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving](https://arxiv.org/abs/2512.24712)
*Qian Cheng,Weitao Zhou,Cheng Jing,Nanshan Deng,Junze Wen,Zhaoyang Liu,Kun Jiang,Diange Yang*

Main category: cs.RO

TL;DR: 本文提出了一种名为LSRE的框架，该框架能够在自动驾驶中实现实时语义风险评估，而无需对每一帧进行大规模视觉-语言模型（VLM）查询。通过将语言定义的安全语义编码到轻量级潜在分类器中，LSRE在保持低计算延迟的同时提供了与大型VLM基线相当的语义风险检测准确性，并且能够更早地预测危险情况。此外，它还能泛化至罕见但语义相似的测试案例中。


<details>
  <summary>Details</summary>
Motivation: 现实世界的自动驾驶不仅需要遵守法律规定的交通规则，还需遵循许多复杂的、难以明确编码的人类社会规则。尽管大型视觉-语言模型可以解释这些语义约束，但其推理成本使其不适合实时部署。因此，需要一种方法来高效地处理这些语义约束。

Method: 提出了一个名为LSRE（Latent Semantic Rule Encoding）的框架，该框架能将稀疏采样的VLM判断转化为循环世界模型潜在空间中的决策边界。通过这种方式，LSRE利用轻量级的潜在分类器实现了基于语言定义安全语义的实时语义风险评估，避免了每帧都需调用VLM的需求。

Result: 在CARLA平台上针对六个语义失败场景进行的实验表明，LSRE达到了与大型VLM基线相媲美的语义风险检测准确率，同时提供了明显提前的风险预判能力，并保持了较低的计算延迟。此外，LSRE还展示了对于少见但语义相似测试案例的良好泛化性能。

Conclusion: 研究表明，通过语言引导的潜在分类提供了一种有效且可部署的方法来进行自动驾驶中的语义安全监控。LSRE框架证明了即使是在不频繁调用昂贵的VLM的情况下，也能实现高性能的语义理解及风险评估。

Abstract: Real-world autonomous driving must adhere to complex human social rules that extend beyond legally codified traffic regulations. Many of these semantic constraints, such as yielding to emergency vehicles, complying with traffic officers' gestures, or stopping for school buses, are intuitive for humans yet difficult to encode explicitly. Although large vision-language models (VLMs) can interpret such semantics, their inference cost makes them impractical for real-time deployment.This work proposes LSRE, a Latent Semantic Rule Encoding framework that converts sparsely sampled VLM judgments into decision boundaries within the latent space of a recurrent world model. By encoding language-defined safety semantics into a lightweight latent classifier, LSRE enables real-time semantic risk assessment at 10 Hz without per-frame VLM queries. Experiments on six semantic-failure scenarios in CARLA demonstrate that LSRE attains semantic risk detection accuracy comparable to a large VLM baseline, while providing substantially earlier hazard anticipation and maintaining low computational latency. LSRE further generalizes to rarely seen semantic-similar test cases, indicating that language-guided latent classification offers an effective and deployable mechanism for semantic safety monitoring in autonomous driving.

</details>


### [24] [Control of Microrobots with Reinforcement Learning under On-Device Compute Constraints](https://arxiv.org/abs/2512.24740)
*Yichen Liu,Kesava Viswanadha,Zhongyu Li,Nelson Lojo,Kristofer S. J. Pister*

Main category: cs.RO

TL;DR: 本文研究了通过边缘机器学习方法来实现微小四足机器人在受限计算、内存和功耗条件下的鲁棒运动。使用强化学习训练了一个紧凑的多层感知器策略，并通过量化技术提高资源受限硬件上的推理更新率，同时提出了基于设备功耗预算选择最优步态模式的方法。


<details>
  <summary>Details</summary>
Motivation: 为了使微型机器人能够在计算、内存和功率受限的情况下执行稳健的地形移动任务，本文探索了一种边缘机器学习方法以实现低延迟控制。

Method: 采用强化学习（RL）训练了一个紧凑的32位浮点数多层感知器(MLP)策略，并在大规模并行GPU模拟中进行训练；利用域随机化增强鲁棒性；研究了整数量化（Int8）技术以提高资源受限硬件上的推理更新速率；提出了一种资源感知的步态调度观点，根据设备功耗预算选择能够最大化预期RL奖励的步态模式。

Result: 成功地将MLP策略部署到了一个超小型系统级芯片上，并观察到域随机化训练可以改善现实世界中非均匀地形上大型机器人行为的稳定性。不过，作者并未声称其实现了零样本迁移。

Conclusion: 本研究表明，通过边缘机器学习与适当的硬件优化策略相结合，可以在计算资源极为有限的情况下有效提升微型机器人的运动性能。

Abstract: An important function of autonomous microrobots is the ability to perform robust movement over terrain. This paper explores an edge ML approach to microrobot locomotion, allowing for on-device, lower latency control under compute, memory, and power constraints. This paper explores the locomotion of a sub-centimeter quadrupedal microrobot via reinforcement learning (RL) and deploys the resulting controller on an ultra-small system-on-chip (SoC), SC$μ$M-3C, featuring an ARM Cortex-M0 microcontroller running at 5 MHz. We train a compact FP32 multilayer perceptron (MLP) policy with two hidden layers ($[128, 64]$) in a massively parallel GPU simulation and enhance robustness by utilizing domain randomization over simulation parameters. We then study integer (Int8) quantization (per-tensor and per-feature) to allow for higher inference update rates on our resource-limited hardware, and we connect hardware power budgets to achievable update frequency via a cycles-per-update model for inference on our Cortex-M0. We propose a resource-aware gait scheduling viewpoint: given a device power budget, we can select the gait mode (trot/intermediate/gallop) that maximizes expected RL reward at a corresponding feasible update frequency. Finally, we deploy our MLP policy on a real-world large-scale robot on uneven terrain, qualitatively noting that domain-randomized training can improve out-of-distribution stability. We do not claim real-world large-robot empirical zero-shot transfer in this work.

</details>


### [25] [Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow](https://arxiv.org/abs/2512.24766)
*Karthik Dharmarajan,Wenlong Huang,Jiajun Wu,Li Fei-Fei,Ruohan Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为Dream2Flow的框架，通过3D物体流作为中间表示来连接视频生成和机器人控制。该方法能够从生成的视频中重建3D物体运动，并将操作形式化为物体轨迹跟踪，从而克服了具身化差距，使得预先训练好的视频模型可以零样本指导机器人对多种类别的物体进行操作。


<details>
  <summary>Details</summary>
Motivation: 虽然生成式视频建模在零样本推理关于开放世界操控中的合理物理交互方面表现出色，但将这种由人类主导的动作转换成机器人系统所需的低级动作仍然是一个挑战。作者注意到，给定初始图像和任务指令后，这些模型擅长合成合理的物体运动。因此，研究旨在通过一种新的方法来填补这一空白，使视频生成技术能够更直接地应用于机器人控制领域。

Method: 引入Dream2Flow框架，它使用3D物体流作为中介来桥接视频生成与机器人控制之间的差距。该方法首先从生成的视频中恢复出3D物体运动，然后将操纵过程定义为追踪物体轨迹的任务。通过这种方式，Dream2Flow能够在不依赖于特定执行器的情况下描述状态变化，进而利用轨迹优化或强化学习将重建得到的3D物体流动转化为可执行的低级命令。

Result: 仿真及真实世界实验表明，3D物体流作为一种通用且可扩展的接口，在适应视频生成模型以执行开放世界的机器人操作方面表现良好。这证明了Dream2Flow框架的有效性及其在处理不同类型物体（包括刚性、铰接式、可变形以及颗粒状）时的能力。

Conclusion: Dream2Flow框架成功地利用了3D物体流作为桥梁，将基于视频生成的人工智能技术与实际机器人控制系统相结合，实现了对于多种类型物体的无样本转移操作能力。这种方法不仅展示了视频生成模型在开放世界机器人操作中的潜力，还提供了一个通用且可扩展的方法来解决视频到动作转化的问题。

Abstract: Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.

</details>


### [26] [ArtiSG: Functional 3D Scene Graph Construction via Human-demonstrated Articulated Objects Manipulation](https://arxiv.org/abs/2512.24845)
*Qiuyi Gu,Yuze Sheng,Jincheng Yu,Jiahao Tang,Xiaolong Shan,Zhaoyang Shen,Tinghao Yi,Xiaodan Liang,Xinlei Chen,Yu Wang*

Main category: cs.RO

TL;DR: ArtiSG, a new framework, enhances 3D scene graphs with functional information by encoding human demonstrations into robotic memory, improving the recall of functional elements and the precision of articulation estimation for better manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景图虽然为机器人导航和规划提供了语义理解，但往往缺乏物理操作所需的功能信息，特别是关于关节式物体的信息。从静态观察中推断关节机制的方法容易受到视觉模糊的影响，而从状态变化中估计参数的方法通常依赖于固定摄像机和无障碍视角等受限设置。此外，通用物体检测器经常遗漏小把手等细粒度功能元素。

Method: 提出了一种名为ArtiSG的框架，通过将人类演示编码到结构化的机器人记忆中来构建功能性的3D场景图。该方法利用了一个强大的关节数据收集管道，使用便携式设置来准确估计即使在相机自我运动下的6自由度关节轨迹和轴线。我们将这些运动学先验整合进一个层次化且开放词汇表的图中，并利用交互数据来发现视觉感知所忽略的不显眼功能元素。

Result: 广泛的现实世界实验表明，ArtiSG在功能性元素召回率和关节估计精度方面显著优于基线。此外，我们还展示了所构建的图作为可靠的功能性记忆，能够有效引导机器人在包含多种关节物体的真实环境中执行语言导向的操作任务。

Conclusion: ArtiSG通过将人类演示编码入机器人的结构化记忆中，成功地增强了3D场景图的功能信息，这不仅提高了对细微功能元素的识别能力及关节估计的准确性，也为机器人执行基于自然语言指令的操作任务提供了有力支持。

Abstract: 3D scene graphs have empowered robots with semantic understanding for navigation and planning, yet they often lack the functional information required for physical manipulation, particularly regarding articulated objects. Existing approaches for inferring articulation mechanisms from static observations are prone to visual ambiguity, while methods that estimate parameters from state changes typically rely on constrained settings such as fixed cameras and unobstructed views. Furthermore, fine-grained functional elements like small handles are frequently missed by general object detectors. To bridge this gap, we present ArtiSG, a framework that constructs functional 3D scene graphs by encoding human demonstrations into structured robotic memory. Our approach leverages a robust articulation data collection pipeline utilizing a portable setup to accurately estimate 6-DoF articulation trajectories and axes even under camera ego-motion. We integrate these kinematic priors into a hierarchical and open-vocabulary graph while utilizing interaction data to discover inconspicuous functional elements missed by visual perception. Extensive real-world experiments demonstrate that ArtiSG significantly outperforms baselines in functional element recall and articulation estimation precision. Moreover, we show that the constructed graph serves as a reliable functional memory that effectively guides robots to perform language-directed manipulation tasks in real-world environments containing diverse articulated objects.

</details>


### [27] [Coordinated Humanoid Manipulation with Choice Policies](https://arxiv.org/abs/2512.25072)
*Haozhi Qi,Yen-Jen Wang,Toru Lin,Brent Yi,Yi Ma,Koushil Sreenath,Jitendra Malik*

Main category: cs.RO

TL;DR: 该论文提出了一种结合模块化遥操作界面和可扩展学习框架的系统，用于解决人形机器人在以人为中心的环境中实现全身协调的问题。通过引入选择策略（Choice Policy），一种模仿学习方法，能够生成多个候选动作并学习对它们进行评分。实验验证了该方法在洗碗机装载和全身擦白板任务中的有效性，显示其性能优于扩散策略和标准行为克隆。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决人形机器人在执行涉及头部、手部及腿部协同工作的复杂任务时所面临的挑战，尤其是在非结构化的人类中心环境中实现有效的全身协调控制。

Method: 采用的方法是开发了一个将模块化的遥控操作系统与一个可扩展的学习框架相结合的新系统。该系统首先通过将人形机器人的控制分解为直观的子模块来收集高质量的演示数据；接着引入了一种名为‘选择策略’的新颖模仿学习方法，它能够产生多种候选动作，并从中学习最佳的选择。

Result: 实验结果表明，在实际应用如装填洗碗机以及全身参与的白板清洁等任务中，所提出的‘选择策略’明显优于现有的扩散政策和常规的行为克隆技术。此外，研究还发现手眼协调对于长时间任务的成功至关重要。

Conclusion: 这项工作为人形机器人在非结构化环境中实现协调操控提供了一条实用且可扩展的数据收集与学习路径，强调了手眼协调的重要性，并展示了所提方法的有效性。

Abstract: Humanoid robots hold great promise for operating in human-centric environments, yet achieving robust whole-body coordination across the head, hands, and legs remains a major challenge. We present a system that combines a modular teleoperation interface with a scalable learning framework to address this problem. Our teleoperation design decomposes humanoid control into intuitive submodules, which include hand-eye coordination, grasp primitives, arm end-effector tracking, and locomotion. This modularity allows us to collect high-quality demonstrations efficiently. Building on this, we introduce Choice Policy, an imitation learning approach that generates multiple candidate actions and learns to score them. This architecture enables both fast inference and effective modeling of multimodal behaviors. We validate our approach on two real-world tasks: dishwasher loading and whole-body loco-manipulation for whiteboard wiping. Experiments show that Choice Policy significantly outperforms diffusion policies and standard behavior cloning. Furthermore, our results indicate that hand-eye coordination is critical for success in long-horizon tasks. Our work demonstrates a practical path toward scalable data collection and learning for coordinated humanoid manipulation in unstructured environments.

</details>
