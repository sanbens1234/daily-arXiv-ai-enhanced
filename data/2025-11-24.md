<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 25]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [A*-based Temporal Logic Path Planning with User Preferences on Relaxed Task Satisfaction](https://arxiv.org/abs/2511.16844)
*Disha Kamale,Xi Yu,Cristian-Ioan Vasile*

Main category: cs.RO

TL;DR: 本研究提出了一种基于A*的规划框架，用于在大型机器人环境中处理时间逻辑任务。通过将用户对任务放松的偏好整合到规划过程中，即使无法完全遵守任务要求，也能实现尽可能高的任务满意度。此外，还提出了一种简单高效的启发式方法，以减少规划时间和搜索内存的需求。


<details>
  <summary>Details</summary>
Motivation: 当面临大规模机器人环境中的时间逻辑任务时，如果完全满足所有任务要求变得不可行，则需要一种方法来最大化任务满意度同时考虑用户的偏好。这表明了开发能够有效解决此类问题的规划算法的重要性。

Method: 研究者采用自动机表示法来表达时间逻辑目标和用户偏好，并在此基础上构建了一个基于A*搜索算法的规划框架。为了提高效率，他们还设计了一种新的启发式策略，使得该方案能够在较短时间内完成大范围环境下的路径规划。

Result: 实验结果表明，所提出的启发式方法不仅显著减少了所需的时间和内存资源，而且还能保证生成接近最优解的高层轨迹。通过对多个案例的研究，进一步验证了该方法的有效性及其在实际应用中的潜力。

Conclusion: 这项工作为在复杂且规模庞大的机器人系统中执行时间逻辑任务提供了一个有效的解决方案。通过结合用户偏好并利用先进的搜索技术，它能够快速找到满足给定条件的最佳行动方案。

Abstract: In this work, we consider the problem of planning for temporal logic tasks in large robot environments. When full task compliance is unattainable, we aim to achieve the best possible task satisfaction by integrating user preferences for relaxation into the planning process. Utilizing the automata-based representations for temporal logic goals and user preferences, we propose an A*-based planning framework. This approach effectively tackles large-scale problems while generating near-optimal high-level trajectories. To facilitate this, we propose a simple, efficient heuristic that allows for planning over large robot environments in a fraction of time and search memory as compared to uninformed search algorithms. We present extensive case studies to demonstrate the scalability, runtime analysis as well as empirical bounds on the suboptimality of the proposed heuristic.

</details>


### [2] [Single-Pixel Tactile Skin via Compressive Sampling](https://arxiv.org/abs/2511.16898)
*Ariel Slepyan,Laura Xing,Rudy Zhang,Nitish Thakor*

Main category: cs.RO

TL;DR: 本文介绍了一种单像素触觉皮肤(SPTS)技术，该技术通过压缩采样从整个传感器阵列经由单一输出通道重建丰富的触觉信息。它简化了布线需求，极大地减少了数据采集要求，并且支持自适应重建，能够以高达3500 FPS的速度实现物体分类和捕捉瞬态动态。


<details>
  <summary>Details</summary>
Motivation: 开发大面积、高速电子皮肤对于机器人学、假肢以及人机交互领域是一个巨大的挑战，当前面临的主要问题是布线复杂性和数据瓶颈。为解决这些问题，作者提出了单像素触觉皮肤（SPTS）的概念。

Method: 研究者们提出了一种名为单像素触觉皮肤（SPTS）的新方法，它采用压缩采样技术来重构来自整个传感器阵列的丰富触觉信息。每个感测元件配备了一个小型微控制器，它们共同参与一个全局总和计算，实现了硬件层面的分布式压缩感知。此外，该设计具有灵活性及可串接性，将布线简化至少数输入线与一条输出线。

Result: 实验结果表明，SPTS系统能够在有效3500帧/秒的速度下完成物体分类，并且能够捕捉到如8毫秒内的弹射物撞击这样的瞬态动力学现象。同时，SPTS还支持自适应重建功能，使得感知精度可以随着测量时间而扩展。这意味着可以通过最少7%的数据量快速定位接触点，随后逐步细化成高保真图像。

Conclusion: 这项工作为面向机器人和人机界面的大规模触觉智能提供了一条高效路径，解决了现有技术中存在的布线复杂度高和数据传输瓶颈问题。

Abstract: Development of large-area, high-speed electronic skins is a grand challenge for robotics, prosthetics, and human-machine interfaces, but is fundamentally limited by wiring complexity and data bottlenecks. Here, we introduce Single-Pixel Tactile Skin (SPTS), a paradigm that uses compressive sampling to reconstruct rich tactile information from an entire sensor array via a single output channel. This is achieved through a direct circuit-level implementation where each sensing element, equipped with a miniature microcontroller, contributes a dynamically weighted analog signal to a global sum, performing distributed compressed sensing in hardware. Our flexible, daisy-chainable design simplifies wiring to a few input lines and one output, and significantly reduces measurement requirements compared to raster scanning methods. We demonstrate the system's performance by achieving object classification at an effective 3500 FPS and by capturing transient dynamics, resolving an 8 ms projectile impact into 23 frames. A key feature is the support for adaptive reconstruction, where sensing fidelity scales with measurement time. This allows for rapid contact localization using as little as 7% of total data, followed by progressive refinement to a high-fidelity image - a capability critical for responsive robotic systems. This work offers an efficient pathway towards large-scale tactile intelligence for robotics and human-machine interfaces.

</details>


### [3] [Multi-UAV Swarm Obstacle Avoidance Based on Potential Field Optimization](https://arxiv.org/abs/2511.16911)
*Yendo Hu,Yiliang Wu,Weican Chen*

Main category: cs.RO

TL;DR: 提出了一种结合改进的多机器人编队避障算法(MRF IAPF)和增强型人工势场(APF)的新型混合算法，旨在优化多无人机场景下的路径规划问题。该算法通过集成障碍排斥力、无人机间相互作用力及目标吸引力，并引入了细化的单无人机路径优化机制，包括碰撞风险评估与辅助子目标策略，在静态未知障碍物环境中表现出了更优的路径长度优化效果和航向稳定性。


<details>
  <summary>Details</summary>
Motivation: 在多无人机场景中，传统的人工势场(APF)方法由于不合理的避障路径规划，常导致飞行路径冗余、频繁的突然航向改变以及在避障过程中容易发生无人机间的碰撞。为解决这些问题，提出了新的混合算法。

Method: 该研究提出的新算法结合了改进的多机器人编队避障(MRF IAPF)算法与针对单无人机路径规划优化后的增强型APF。其核心思想包括整合来自MRF IAPF的三种交互力——障碍排斥力、无人机间相互作用力、目标吸引力；同时加入了一个精炼的单无人机路径优化机制，该机制包含碰撞风险评估和辅助子目标策略。当无人机面临高碰撞威胁时，会生成临时航点以指导避障，确保最终能够准确到达实际目标位置。

Result: 仿真结果显示，相比于基于传统APF的编队算法，所提算法在路径长度优化和航向稳定性方面取得了显著改善，能够有效避开障碍物并快速恢复编队配置。

Conclusion: 新提出的混合算法不仅解决了多无人机系统中存在的路径冗余、频繁航向调整及潜在碰撞问题，而且在静态未知障碍物环境下展现了良好的适应性和有效性。

Abstract: In multi UAV scenarios,the traditional Artificial Potential Field (APF) method often leads to redundant flight paths and frequent abrupt heading changes due to unreasonable obstacle avoidance path planning,and is highly prone to inter UAV collisions during the obstacle avoidance process.To address these issues,this study proposes a novel hybrid algorithm that combines the improved Multi-Robot Formation Obstacle Avoidance (MRF IAPF) algorithm with an enhanced APF optimized for single UAV path planning.Its core ideas are as follows:first,integrating three types of interaction forces from MRF IAPF obstacle repulsion force,inter UAV interaction force,and target attraction force;second,incorporating a refined single UAV path optimization mechanism,including collision risk assessment and an auxiliary sub goal strategy.When a UAV faces a high collision threat,temporary waypoints are generated to guide obstacle avoidance,ensuring eventual precise arrival at the actual target.Simulation results demonstrate that compared with traditional APF based formation algorithms,the proposed algorithm achieves significant improvements in path length optimization and heading stability,can effectively avoid obstacles and quickly restore the formation configuration,thus verifying its applicability and effectiveness in static environments with unknown obstacles.

</details>


### [4] [MobileOcc: A Human-Aware Semantic Occupancy Dataset for Mobile Robots](https://arxiv.org/abs/2511.16949)
*Junseo Kim,Guido Dumont,Xinyu Gao,Gang Chen,Holger Caesar,Javier Alonso-Mora*

Main category: cs.RO

TL;DR: 本文提出了MobileOcc，一个针对在拥挤的人类环境中操作的移动机器人设计的语义占用数据集。该数据集通过结合静态物体占用注释和专门用于人类占用建模的新网格优化框架构建而成。基于此数据集，作者为占用预测和行人速度预测两个任务设立了基准，并展示了其方法在不同数据集上的稳健性能。


<details>
  <summary>Details</summary>
Motivation: 在行人密集环境中运行的移动机器人需要精确的3D语义占用感知能力，但相较于自动驾驶领域，在这方面的工作较少。为了填补这一空白，研究者开发了适用于此类环境下的移动机器人语义占用数据集。

Method: 通过创建包含静态物体占用标记及一种专为人类占用建模而设计的新颖网格优化架构的数据集MobileOcc。此过程包括从2D图像重建可变形的人体几何结构，并使用相关联的LiDAR点云数据进行细化与优化。

Result: 利用MobileOcc数据集，研究团队为占用预测和行人速度预测两项任务设定了基准，并采用单目、立体视觉以及全景式占用等不同方法进行了评估。此外，还进一步验证了其标注方法在3D人体姿态估计数据集上的有效性。结果显示，所提出的方法能够跨不同数据集表现出稳定性能。

Conclusion: 本研究成功地构建了一个专注于行人丰富环境下移动机器人的语义占用数据集MobileOcc，并且通过实验证明了它对于提高占用预测精度及行人行为理解的有效性。

Abstract: Dense 3D semantic occupancy perception is critical for mobile robots operating in pedestrian-rich environments, yet it remains underexplored compared to its application in autonomous driving. To address this gap, we present MobileOcc, a semantic occupancy dataset for mobile robots operating in crowded human environments. Our dataset is built using an annotation pipeline that incorporates static object occupancy annotations and a novel mesh optimization framework explicitly designed for human occupancy modeling. It reconstructs deformable human geometry from 2D images and subsequently refines and optimizes it using associated LiDAR point data. Using MobileOcc, we establish benchmarks for two tasks, i) Occupancy prediction and ii) Pedestrian velocity prediction, using different methods including monocular, stereo, and panoptic occupancy, with metrics and baseline implementations for reproducible comparison. Beyond occupancy prediction, we further assess our annotation method on 3D human pose estimation datasets. Results demonstrate that our method exhibits robust performance across different datasets.

</details>


### [5] [Stable Offline Hand-Eye Calibration for any Robot with Just One Mark](https://arxiv.org/abs/2511.17001)
*Sicheng Xie,Lingchen Meng,Zhiying Du,Shuyuan Tu,Haidong Cao,Jiaqi Leng,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: CalibAll是一种仅需一个标记的简单而有效的方法，通过无训练、稳定和准确的相机外参估计，在不同机器人和数据集上实现了粗到细的标定流程。实验结果表明，该方法在三个机器人平台上表现出色，优于现有最先进方法，并且能够产生深度图、链路级掩码和末端执行器2D轨迹等有用的辅助注释，进一步支持下游任务。


<details>
  <summary>Details</summary>
Motivation: 模仿学习通过从摄像机空间观察映射到机器人空间动作，在各种机器人任务中取得了显著的成功。最近的研究表明，使用机器人到摄像机的转换信息（即，摄像机外参）对学习过程有利并能产生更好的结果。然而，摄像机外参往往不可用，而且估计方法通常会受到局部极小值和泛化能力差的问题。因此，提出了一种新的方法来解决这个问题。

Method: 提出了CalibAll方法，它只需要在末端执行器上标注一个单一标记，并利用视觉基础模型(VFM)出现的对应能力自动定位跨机器人的相应标记。结合这个标记与点跟踪及3D EEF轨迹，通过时间透视n点(PnP)获得粗略的摄像机外参。然后，通过基于渲染的优化进一步细化这一估计，使渲染和真实遮罩对齐，从而得到精确稳定的摄像机外参。

Result: 实验结果表明，所提出的方法优于最先进的方法，在三个机器人平台上显示出强大的鲁棒性和普遍有效性。此外，还产生了诸如深度图、链路级掩码和末端执行器2D轨迹等有用的辅助注释，可以进一步支持下游任务。

Conclusion: CalibAll提供了一个无需训练、稳定且精确的解决方案用于相机外参估计，适用于多种机器人平台和数据集，能够改善模仿学习的效果，并为后续任务提供了有价值的辅助信息。

Abstract: Imitation learning has achieved remarkable success in a variety of robotic tasks by learning a mapping function from camera-space observations to robot-space actions. Recent work indicates that the use of robot-to-camera transformation information ({\ie}, camera extrinsics) benefits the learning process and produces better results. However, camera extrinsics are oftentimes unavailable and estimation methods usually suffer from local minima and poor generalizations. In this paper, we present CalibAll, a simple yet effective method that \textbf{requires only a single mark} and performs training-free, stable, and accurate camera extrinsic estimation across diverse robots and datasets through a coarse-to-fine calibration pipeline. In particular, we annotate a single mark on an end-effector (EEF), and leverage the correspondence ability emerged from vision foundation models (VFM) to automatically localize the corresponding mark across robots in diverse datasets. Using this mark, together with point tracking and the 3D EEF trajectory, we obtain a coarse camera extrinsic via temporal Perspective-n-Point (PnP). This estimate is further refined through a rendering-based optimization that aligns rendered and ground-true masks, yielding accurate and stable camera extrinsic. Experimental results demonstrate that our method outperforms state-of-the-art approaches, showing strong robustness and general effectiveness across three robot platforms. It also produces useful auxiliary annotations such as depth maps, link-wise masks, and end-effector 2D trajectories, which can further support downstream tasks.

</details>


### [6] [MfNeuPAN: Proactive End-to-End Navigation in Dynamic Environments via Direct Multi-Frame Point Constraints](https://arxiv.org/abs/2511.17013)
*Yiwen Ying,Hanjing Ye,Senzi Luo,Luyao Liu,Yu Zhan,Li He,Hong Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种新的框架，利用多帧点约束（包括当前和未来帧）来实现主动端到端导航，通过预测移动障碍物的未来路径，使机器人能够提前避开潜在危险，从而提高了在未知动态环境中的导航鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 基于模型和基于学习的方法在高度动态场景中往往表现不佳，传统方法假设环境是静态的，无法适应实时变化；而基于学习的方法依赖于单帧观察进行运动约束估计，限制了其适应性。因此，需要一种能够在复杂和动态环境中提高机器人避障能力的新方法。

Method: 论文提出了一种新框架，该框架利用多帧点约束（包括由专用模块预测的当前及未来帧）来进行导航。通过加入一个可以基于多帧观察预测移动障碍物未来路径的预测模块，使得机器人能够预先识别并避开可能遇到的风险。

Result: 仿真和实际测试验证了所提方法的有效性，在未知动态环境下提升了导航的鲁棒性和效率。

Conclusion: 提出的框架通过采用多帧信息预测移动障碍物的行为，并据此做出导航决策，有效解决了现有方法在处理高动态环境下的局限性问题，为机器人提供了一种更加高效、安全的避障解决方案。

Abstract: Obstacle avoidance in complex and dynamic environments is a critical challenge for real-time robot navigation. Model-based and learning-based methods often fail in highly dynamic scenarios because traditional methods assume a static environment and cannot adapt to real-time changes, while learning-based methods rely on single-frame observations for motion constraint estimation, limiting their adaptability. To overcome these limitations, this paper proposes a novel framework that leverages multi-frame point constraints, including current and future frames predicted by a dedicated module, to enable proactive end-to-end navigation. By incorporating a prediction module that forecasts the future path of moving obstacles based on multi-frame observations, our method allows the robot to proactively anticipate and avoid potential dangers. This proactive planning capability significantly enhances navigation robustness and efficiency in unknown dynamic environments. Simulations and real-world experiments validate the effectiveness of our approach.

</details>


### [7] [H-GAR: A Hierarchical Interaction Framework via Goal-Driven Observation-Action Refinement for Robotic Manipulation](https://arxiv.org/abs/2511.17079)
*Yijie Zhu,Rui Shao,Ziyang Liu,Jie He,Jizhihui Liu,Jiuru Wang,Zitong Yu*

Main category: cs.RO

TL;DR: 提出了一种名为H-GAR的层级交互框架，通过目标导向的观察-动作细化来改进机器人操作任务中的预测模型。该框架首先生成目标观察和粗略动作草图，并通过两个协同模块（目标条件观察合成器GOS和交互感知动作精炼器IAAR）来细化动作并合成中间观察，以确保动作与目标的一致性以及时间上的连贯性。实验表明H-GAR在模拟和真实世界机器人操作任务中均达到领先水平。


<details>
  <summary>Details</summary>
Motivation: 现有的视频和动作预测模型通常将观察和动作生成处理为单一且目标无关的方式，导致语义不匹配的预测结果和不连贯的行为。为了提高机器人操控过程中未来观测与动作之间的一致性和连贯性，提出了新的方法。

Method: H-GAR是一个层级式的交互框架，它通过目标驱动的观察-动作细化来工作。首先，系统产生一个目标观察及一个粗略的动作草图，指明了朝向目标的大致路径。接着，通过两个相互配合的模块：(1) 目标条件观察合成器(GOS)，基于粗粒度的动作和预测的目标观察合成中间观察；(2) 交互意识动作精炼器(IAAR)，利用来自中间观察的反馈以及历史动作记忆库对粗略动作进行精细化，使之成为细粒度、目标一致的动作。

Result: H-GAR能够更准确地执行操作任务，在一系列仿真和实际机器人操作任务中展示了其卓越性能。

Conclusion: H-GAR通过结合目标定位与显式动作-观察互动，以粗到细的方式提升了机器人操纵任务中的预测准确性，实现了最先进的表现。

Abstract: Unified video and action prediction models hold great potential for robotic manipulation, as future observations offer contextual cues for planning, while actions reveal how interactions shape the environment. However, most existing approaches treat observation and action generation in a monolithic and goal-agnostic manner, often leading to semantically misaligned predictions and incoherent behaviors. To this end, we propose H-GAR, a Hierarchical interaction framework via Goal-driven observation-Action Refinement.To anchor prediction to the task objective, H-GAR first produces a goal observation and a coarse action sketch that outline a high-level route toward the goal. To enable explicit interaction between observation and action under the guidance of the goal observation for more coherent decision-making, we devise two synergistic modules. (1) Goal-Conditioned Observation Synthesizer (GOS) synthesizes intermediate observations based on the coarse-grained actions and the predicted goal observation. (2) Interaction-Aware Action Refiner (IAAR) refines coarse actions into fine-grained, goal-consistent actions by leveraging feedback from the intermediate observations and a Historical Action Memory Bank that encodes prior actions to ensure temporal consistency. By integrating goal grounding with explicit action-observation interaction in a coarse-to-fine manner, H-GAR enables more accurate manipulation. Extensive experiments on both simulation and real-world robotic manipulation tasks demonstrate that H-GAR achieves state-of-the-art performance.

</details>


### [8] [Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation](https://arxiv.org/abs/2511.17097)
*Shuo Wang,Yucheng Wang,Guoxin Lian,Yongcai Wang,Maiyue Chen,Kaihui Wang,Bo Zhang,Zhizhong Su,Yutian Zhou,Wanting Li,Deying Li,Zhaoxin Fan*

Main category: cs.RO

TL;DR: Progress-Think提出了一种新的方法，通过从视觉观察预测指令样式的进度来进行语义进度推理，从而提高视觉-语言导航的准确性。该方法采用三阶段框架进行训练，并在R2R-CE和RxR-CE数据集上展示了最新的成功案例与效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型主要集中在直接动作预测上，而早期进展方法则预测数值成就；两者都忽略了观测序列与指令序列之间单调共进的特性。基于此发现，本研究旨在通过引入能够根据视觉观察预测指令式进度的语义进度推理来改进导航准确性。

Method: Progress-Think采用了一个三阶段框架：首先，自对齐进度预训练（Self-Aligned Progress Pretraining）通过一种新颖的可微分对齐方式，在视觉历史和指令前缀之间启动一个推理模块；接着，进度引导策略预训练（Progress-Guided Policy Pretraining）将学到的进度状态注入导航上下文中，指导策略朝向一致的动作发展；最后，进度-策略协同微调（Progress-Policy Co-Finetuning）使用定制化的进度感知强化目标同时优化两个模块。

Result: 实验表明，在R2R-CE和RxR-CE数据集上，该方法达到了最先进的成功率与效率，证明了语义进度能够提供更加一致的导航进展表示。

Conclusion: 通过引入语义进度推理机制，Progress-Think能够有效地提升视觉-语言导航任务中对于长期行为一致性以及多步骤指令理解的能力。

Abstract: Vision-Language Navigation requires agents to act coherently over long horizons by understanding not only local visual context but also how far they have advanced within a multi-step instruction. However, recent Vision-Language-Action models focus on direct action prediction and earlier progress methods predict numeric achievements; both overlook the monotonic co-progression property of the observation and instruction sequences. Building on this insight, Progress-Think introduces semantic progress reasoning, predicting instruction-style progress from visual observations to enable more accurate navigation. To achieve this without expensive annotations, we propose a three-stage framework. In the initial stage, Self-Aligned Progress Pretraining bootstraps a reasoning module via a novel differentiable alignment between visual history and instruction prefixes. Then, Progress-Guided Policy Pretraining injects learned progress states into the navigation context, guiding the policy toward consistent actions. Finally, Progress-Policy Co-Finetuning jointly optimizes both modules with tailored progress-aware reinforcement objectives. Experiments on R2R-CE and RxR-CE show state-of-the-art success and efficiency, demonstrating that semantic progress yields a more consistent representation of navigation advancement.

</details>


### [9] [Reflection-Based Relative Localization for Cooperative UAV Teams Using Active Markers](https://arxiv.org/abs/2511.17166)
*Tim Lakemann,Daniel Bonilla Licea,Viktor Walter,Martin Saska*

Main category: cs.RO

TL;DR: 本研究提出了一种新的方法，利用通常不希望出现的主动标记反射来实现多机器人团队中的机载相对定位。该方法无需预先了解机器人大小或预定义标记配置，且独立于表面属性，特别适用于在未知环境中合作的异构微型飞行群。通过室内外实验验证了该方法的有效性，表明其在没有事先了解队员大小的情况下也能可靠运行，并且比现有技术具有更长的有效范围（超过30米）和更高的精度。


<details>
  <summary>Details</summary>
Motivation: 为了解决多机器人系统中由于环境中的主动标记反射导致的定位模糊问题，同时考虑到对于不同表面上特别是动态水面不确定性的处理，以及在未知环境下异构微空中集群的合作需求。

Method: 提出了一种新的机载相对定位方法，这种方法利用环境中的主动标记反射，不需要预先知道机器人的尺寸或预设的标记配置，并且能够独立于物体表面特性工作。

Result: 通过室内和室外实验验证了所提出的基于反射的定位系统可以在不了解队伍成员大小的前提下稳定运作，并且相比现有方法提供了更大的有效作用距离（超过30米）及更高的精确度。

Conclusion: 这项新方法为多机器人团队在未知环境下的相对定位提供了一个有效的解决方案，特别是在处理非平坦表面如动态水面时表现优异。

Abstract: Reflections of active markers in the environment are a common source of ambiguity in onboard visual relative localization. This work presents a novel approach for onboard relative localization in multi-robot teams that exploits these typically unwanted reflections of active markers in the environment. It operates without prior knowledge of robot size or predefined marker configurations and remains independent of surface properties, an essential feature for heterogeneous micro-aerial swarms cooperating in unknown environments. It explicitly accounts for uncertainties caused by non-flat surfaces, with a particular focus on dynamic water surfaces, which are especially relevant for marine deployments. We validated the approach in both indoor and outdoor experiments, demonstrating that the proposed reflection-based localization system operates reliably without prior knowledge of team member size and achieves greater effective range (above 30 m) and accuracy than state-of-the-art methods. The video and source code of this work will be made publicly available after publication.

</details>


### [10] [Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models](https://arxiv.org/abs/2511.17178)
*Kento Kawaharazuka,Yoshiki Obinata,Naoaki Kanazawa,Haoyu Jia,Kei Okada*

Main category: cs.RO

TL;DR: 该研究提出了一种利用大型语言模型(LLMs)来提高基于黑盒优化的机器人本体设计效率的方法，通过与基于黑盒优化的采样过程并行地使用LLMs进行采样，并向其提供问题设置和广泛反馈，从而实现更高效的设计解决方案探索。


<details>
  <summary>Details</summary>
Motivation: 尽管数值优化速度快，但不适合处理复杂结构或离散值的情况，因此经常采用黑盒优化。然而，黑盒优化存在采样效率低的问题，需要大量的采样迭代才能获得良好的解。

Method: 研究者们提出了一种方法，通过利用大型语言模型（LLMs）来增强基于黑盒优化的机器人身体设计效率。在基于黑盒优化的采样过程中，同时使用LLMs进行采样，且为LLMs提供了问题设定和广泛的反馈信息。

Result: 研究表明，这种方法能够使得设计解决方案的探索更加高效。

Conclusion: 通过结合大型语言模型与传统黑盒优化技术，可以有效提高机器人设计过程中对复杂结构及离散值情形下的优化效率，但同时也讨论了此方法的特点和局限性。

Abstract: Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations.

</details>


### [11] [A ROS2 Interface for Universal Robots Collaborative Manipulators Based on ur_rtde](https://arxiv.org/abs/2511.17237)
*Alessio Saccuti,Riccardo Monica,Jacopo Aleotti*

Main category: cs.RO

TL;DR: 本文介绍了一个基于ur_rtde C++库的新ROS2驱动程序，适用于UR机器人操作器。该驱动程序支持Universal Robots URScripts的高级命令，并通过插件系统允许添加自定义命令，已实现包括基于路径点运动执行在内的多种命令，且作为开源发布。


<details>
  <summary>Details</summary>
Motivation: 开发一个灵活、可适应多种应用需求的ROS2驱动程序，以支持UR机器人操作器的功能扩展与定制化。

Method: 基于ur_rtde C++库构建新的ROS2驱动；利用插件机制提供自定义命令的支持；实现了若干功能，特别是沿路径点轨迹移动的能力。

Result: 成功开发出了一款面向UR机器人系列的操作器驱动程序，它不仅支持标准的URScript命令集，还能够通过插件方式轻松扩展新功能。

Conclusion: 本研究提出的ROS2驱动为UR机器人提供了更加强大和灵活的控制能力，有助于推动相关领域的进一步发展。

Abstract: In this paper a novel ROS2 driver for UR robot manipulators is presented, based on the ur_rtde C++ library. The proposed driver aims to be a flexible solution, adaptable to a wide range of applications. The driver exposes the high-level commands of Universal Robots URScripts, and custom commands can be added using a plugin system. Several commands have been implemented, including motion execution along a waypoint-based path. The driver is published as open source.

</details>


### [12] [Simulation of Active Soft Nets for Capture of Space Debris](https://arxiv.org/abs/2511.17266)
*Leone Costi,Dario Izzo*

Main category: cs.RO

TL;DR: 本文提出了一种基于开源物理引擎MuJoCo的模拟器，用于设计和控制软机器人网以自主清除太空垃圾。研究了不同的机械模型和控制策略来实现对目标碎片的捕捉，结果表明更柔顺的网在尝试捕捉Envisat时表现更好，并且与滑模控制器结合使用时，在所有测试案例中都能成功捕获。


<details>
  <summary>Details</summary>
Motivation: 为了解决太空垃圾问题，特别是针对不再使用的大型卫星如Envisat，开发一个能够有效模拟软机器人网动态行为及控制策略的仿真环境。

Method: 开发了一个基于MuJoCo的模拟器，该模拟器可以模拟网的动力学、网与碎片之间的接触、网的自接触、轨道力学以及能够控制网上四个角落卫星推进器的控制器。通过对比不同机械模型和控制策略来评估捕获效率。

Result: 结果显示，更柔顺的网在尝试捕捉Envisat时表现得更好；当与滑模控制器配合使用时，软网能够在所有测试案例中实现成功捕获，并显示出更高的接触面积和更多的接触点数量。

Conclusion: 本研究表明，采用柔顺度较高的网并结合特定类型的控制器（例如滑模控制器）可以显著提高从相对静态配置开始捕捉空间碎片（如Envisat）的成功率。

Abstract: In this work, we propose a simulator, based on the open-source physics engine MuJoCo, for the design and control of soft robotic nets for the autonomous removal of space debris. The proposed simulator includes net dynamics, contact between the net and the debris, self-contact of the net, orbital mechanics, and a controller that can actuate thrusters on the four satellites at the corners of the net. It showcases the case of capturing Envisat, a large ESA satellite that remains in orbit as space debris following the end of its mission. This work investigates different mechanical models, which can be used to simulate the net dynamics, simulating various degrees of compliance, and different control strategies to achieve the capture of the debris, depending on the relative position of the net and the target. Unlike previous works on this topic, we do not assume that the net has been previously ballistically thrown toward the target, and we start from a relatively static configuration. The results show that a more compliant net achieves higher performance when attempting the capture of Envisat. Moreover, when paired with a sliding mode controller, soft nets are able to achieve successful capture in 100% of the tested cases, whilst also showcasing a higher effective area at contact and a higher number of contact points between net and Envisat.

</details>


### [13] [Leveraging CVAE for Joint Configuration Estimation of Multifingered Grippers from Point Cloud Data](https://arxiv.org/abs/2511.17276)
*Julien Merand,Boris Meden,Mathieu Grossard*

Main category: cs.RO

TL;DR: 本文提出了一种基于条件变分自编码器（CVAE）的方法，直接从多指夹持器的点云数据中确定关节配置，该方法在MultiDex抓取数据集上得到了验证，展示了其在抓取规划中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的逆运动学技术虽然可以提供基于指尖姿态的数学精确解，但在面对复杂的机械手结构时往往需要额外的决策过程或数值逼近算法来解决中间关节的位置问题。为了解决这些挑战，并提高关节配置估计的效率和准确性，提出了新的方法。

Method: 通过使用条件变分自编码器(CVAE)，将多指夹持器关键结构元素的点云数据作为输入，重建相应的关节配置。

Result: 所提出的方法在Allegro Hand上进行了测试，运行时间仅为0.05毫秒，并且达到了与现有最先进技术相媲美的精度。

Conclusion: 基于机器学习的方法能够有效地克服传统IK技术面临的挑战，对于更复杂的手部模型而言，此方法在关节配置估计方面具有显著优势。

Abstract: This paper presents an efficient approach for determining the joint configuration of a multifingered gripper solely from the point cloud data of its poly-articulated chain, as generated by visual sensors, simulations or even generative neural networks. Well-known inverse kinematics (IK) techniques can provide mathematically exact solutions (when they exist) for joint configuration determination based solely on the fingertip pose, but often require post-hoc decision-making by considering the positions of all intermediate phalanges in the gripper's fingers, or rely on algorithms to numerically approximate solutions for more complex kinematics. In contrast, our method leverages machine learning to implicitly overcome these challenges. This is achieved through a Conditional Variational Auto-Encoder (CVAE), which takes point cloud data of key structural elements as input and reconstructs the corresponding joint configurations. We validate our approach on the MultiDex grasping dataset using the Allegro Hand, operating within 0.05 milliseconds and achieving accuracy comparable to state-of-the-art methods. This highlights the effectiveness of our pipeline for joint configuration estimation within the broader context of AI-driven techniques for grasp planning.

</details>


### [14] [MonoSpheres: Large-Scale Monocular SLAM-Based UAV Exploration through Perception-Coupled Mapping and Planning](https://arxiv.org/abs/2511.17299)
*Tomáš Musil,Matěj Petrlík,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出了一种新的基于单目视觉的探索方法，能够在仅有单一单目相机而无密集测距传感器的情况下，安全地覆盖大规模非结构化室内外3D环境。通过在映射和规划中明确考虑稀疏单目SLAM前端的特性，并开源实现以支持未来研究。


<details>
  <summary>Details</summary>
Motivation: 为了解决移动机器人在只配备一个单目相机且没有密集距离传感器的情况下对未知环境进行自主探索的问题，特别是针对大型非结构化室内和室外3D环境。

Method: 提出的方法包括：1) 映射模块解决了稀疏深度数据、自由空间间隙以及大深度不确定性问题，通过对纹理稀疏区域过度采样自由空间并跟踪障碍物位置不确定性来处理；2) 规划模块通过快速重新规划和感知意识航向控制来应对增加的自由空间不确定性；3) 当考虑到视差要求及无纹理表面的可能性时，即使使用稀疏单目深度数据也能实现前沿探索。

Result: 该方法在多样化的现实世界与模拟环境中得到了广泛评估，包括消融研究。据作者所知，这是首次实现在真实世界非结构化户外环境中进行3D单目探索的方法。

Conclusion: 所提出的方法证明了利用稀疏单目视觉信息进行有效探索的可能性，并为未来相关研究提供了开放源代码实现。

Abstract: Autonomous exploration of unknown environments is a key capability for mobile robots, but it is largely unsolved for robots equipped with only a single monocular camera and no dense range sensors. In this paper, we present a novel approach to monocular vision-based exploration that can safely cover large-scale unstructured indoor and outdoor 3D environments by explicitly accounting for the properties of a sparse monocular SLAM frontend in both mapping and planning. The mapping module solves the problems of sparse depth data, free-space gaps, and large depth uncertainty by oversampling free space in texture-sparse areas and keeping track of obstacle position uncertainty. The planning module handles the added free-space uncertainty through rapid replanning and perception-aware heading control. We further show that frontier-based exploration is possible with sparse monocular depth data when parallax requirements and the possibility of textureless surfaces are taken into account. We evaluate our approach extensively in diverse real-world and simulated environments, including ablation studies. To the best of the authors' knowledge, the proposed method is the first to achieve 3D monocular exploration in real-world unstructured outdoor environments. We open-source our implementation to support future research.

</details>


### [15] [FORWARD: Dataset of a forwarder operating in rough terrain](https://arxiv.org/abs/2511.17318)
*Mikael Lundbäck,Erik Wallin,Carola Häggström,Mattias Nyström,Andreas Grönlund,Mats Richardson,Petrus Jönsson,William Arnvik,Lucas Hedström,Arvid Fälldin,Martin Servin*

Main category: cs.RO

TL;DR: 本文介绍了一个名为FORWARD的高分辨率多模态数据集，该数据集记录了瑞典中部两个伐木场中一款配备多种传感器的Komatsu前装机在复杂地形中的作业情况。数据包括驾驶速度、燃油消耗、车辆位置等信息，并且对约18小时的常规木材提取工作进行了注释。此外还包含了实验场景说明，旨在促进森林机械交通性、感知和自主控制模型及算法的发展。


<details>
  <summary>Details</summary>
Motivation: 为了促进使用人工智能、仿真和物理测试平台开发森林机械的交通性、感知以及自主控制模型和算法的研究进展。

Method: 通过收集装备有RTK-GNSS、360度摄像头、操作员振动传感器、内部CAN总线信号记录和多个IMU等多种传感器的Komatsu前装机，在瑞典中部两个伐木场地进行实际作业时的数据。这些数据涵盖了从视频材料到生产日志文件等多方面的信息，并且对特定的工作场景进行了详细描述。

Result: 创建了一个包含约18小时常规木材提取工作的全面数据集，其中每个工作元素都基于360度视频材料进行了单独注释。此外，还提供了不同条件下（如是否安装钢轨、负载重量变化及目标行驶速度差异）重复驾驶相同路线的具体实验场景说明。

Conclusion: FORWARD数据集为研究人员提供了一个宝贵资源，可用于开发更高效、安全且环保的森林机械设备自动化解决方案。

Abstract: We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with a variety of sensors, including RTK-GNSS, 360-camera, operator vibration sensors, internal CAN-bus signal recording, and multiple IMUs. The data includes event time logs recorded in 5 Hz with e.g., driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas laser-scanned with very high-resolution, $\sim$1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weight, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.

</details>


### [16] [Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM](https://arxiv.org/abs/2511.17335)
*Chiori Hori,Yoshiki Masuyama,Siddarth Jain,Radu Corcodel,Devesh Jha,Diego Romeres,Jonathan Le Roux*

Main category: cs.RO

TL;DR: 本文提出了一种长上下文Q-former，用于在全视频中整合左右上下文依赖关系，并通过直接将文本嵌入提供给LLM解码器来减轻Q-former中文本信息的高度抽象性。实验表明确认生成的准确性是动作规划性能的关键因素，而长上下文Q-former通过结合VideoLLaMA3改善了确认和动作规划的效果。


<details>
  <summary>Details</summary>
Motivation: 为了实现人机协作完成共同目标，机器人需要理解人的行为以及与周围环境的互动。现有方法主要集中在基于单个片段的处理上，未能充分利用长时序上下文信息，这限制了机器人对连续任务步骤间依赖性的理解。

Method: 提出了一个长上下文Q-former框架，该框架能够在完整的视频内容中考虑前后背景信息；引入了一种文本条件化方法，直接将文本嵌入输入到大语言模型（LLM）解码器中，以降低由Q-former处理的信息抽象程度。

Result: 使用YouCook2数据集进行实验验证，结果显示确认生成的准确度对于动作规划的表现至关重要；同时证明了所提出的长上下文Q-former结合VideoLLaMA3后，在提高确认生成及动作规划方面具有显著效果。

Conclusion: 本文介绍的方法能够有效提升机器人在理解和执行复杂多步骤任务时的表现，特别是在需要考虑更广泛上下文信息的情况下。

Abstract: Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.

</details>


### [17] [METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model](https://arxiv.org/abs/2511.17366)
*Yankai Fu,Ning Chen,Junkai Zhao,Shaozhe Shan,Guocai Yao,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出了一种名为METIS的视觉-语言-动作(VLA)模型，用于灵巧操作，该模型在多源自我中心数据集上预训练，并通过EgoAtlas和动态感知动态相结合，实现了在真实世界任务中的高效部署和卓越表现。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够执行多样化任务的通用机器人是一个开放性挑战，特别是在灵巧操作方面。主要瓶颈在于获取大量带有动作标注的数据非常困难且成本高昂。人类数据可以为学习机器人动作提供丰富的先验知识，但以往的研究通常受限于场景有限以及人与机器人之间的视觉差异大。

Method: 提出了METIS，一种视觉-语言-行动（VLA）模型，专门针对灵巧操作设计，并在多源第一视角数据集上进行了预训练。首先创建了EgoAtlas，它将来自不同来源的大规模人类与机器人数据整合在一起，并统一在一个一致的动作空间下。此外还提取了运动感知动态，这是一种紧凑且离散化的运动表示形式，为VLA训练提供了高效而富有表现力的监督。

Result: 实验结果表明，METIS方法在六个现实世界任务中达到了最高的平均成功率，并且在面对分布外情况时表现出色的泛化能力和鲁棒性。

Conclusion: METIS代表了向通用灵巧操作模型迈进的重要一步。

Abstract: Building a generalist robot that can perceive, reason, and act across diverse tasks remains an open challenge, especially for dexterous manipulation. A major bottleneck lies in the scarcity of large-scale, action-annotated data for dexterous skills, as teleoperation is difficult and costly. Human data, with its vast scale and diverse manipulation behaviors, provides rich priors for learning robotic actions. While prior works have explored leveraging human demonstrations, they are often constrained by limited scenarios and a large visual gap between human and robots. To eliminate these limitations, we propose METIS, a vision-language-action (VLA) model for dexterous manipulation pretrained on multi-source egocentric datasets. We first construct EgoAtlas, which integrates large-scale human and robotic data from multiple sources, all unified under a consistent action space. We further extract motion-aware dynamics, a compact and discretized motion representation, which provides efficient and expressive supervision for VLA training. Built upon them, METIS integrates reasoning and acting into a unified framework, enabling effective deployment to downstream dexterous manipulation tasks. Our method demonstrates exceptional dexterous manipulation capabilities, achieving highest average success rate in six real-world tasks. Experimental results also highlight the superior generalization and robustness to out-of-distribution scenarios. These findings emphasize METIS as a promising step toward a generalist model for dexterous manipulation.

</details>


### [18] [Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data](https://arxiv.org/abs/2511.17373)
*Yixuan Pan,Ruoyi Qiao,Li Chen,Kashyap Chitta,Liang Pan,Haoguang Mai,Qingwen Bu,Hao Zhao,Cunyuan Zheng,Ping Luo,Hongyang Li*

Main category: cs.RO

TL;DR: 本文提出了AMS（Agility Meets Stability），这是一种统一了动态动作追踪和极限平衡保持的框架，能够在单一策略下实现敏捷技能执行与零样本极端平衡动作，为未来人形机器人应用提供了一种多功能控制范式。


<details>
  <summary>Details</summary>
Motivation: 目前的人形机器人控制器在处理敏捷动态技能或稳定性关键行为方面表现突出，但现有方法往往专注于一种能力而牺牲另一种。为了使人形机器人能够同时具备敏捷性和稳定性，研究人员开发了AMS框架。

Method: 通过利用异构数据源，包括提供丰富敏捷行为的人体运动捕捉数据集以及捕捉稳定配置的物理约束合成平衡运动，结合设计了一个混合奖励方案来调和敏捷性和稳定性之间的不同优化目标。此外，还采用了性能驱动采样和特定于运动的奖励塑造的自适应学习策略，以有效训练多样化的运动分布。

Result: 实验结果表明，单一策略不仅能够执行诸如跳舞、跑步等敏捷技能，还能完成零样本的极端平衡动作，如叶问蹲。这些成果均在模拟环境及真实的Unitree G1人形机器人上得到了验证。

Conclusion: AMS框架成功地将敏捷性与稳定性相结合，在单个策略中实现了广泛的运动技能，为人形机器人的未来发展提供了新的控制范例。

Abstract: Humanoid robots are envisioned to perform a wide range of tasks in human-centered environments, requiring controllers that combine agility with robust balance. Recent advances in locomotion and whole-body tracking have enabled impressive progress in either agile dynamic skills or stability-critical behaviors, but existing methods remain specialized, focusing on one capability while compromising the other. In this work, we introduce AMS (Agility Meets Stability), the first framework that unifies both dynamic motion tracking and extreme balance maintenance in a single policy. Our key insight is to leverage heterogeneous data sources: human motion capture datasets that provide rich, agile behaviors, and physically constrained synthetic balance motions that capture stability configurations. To reconcile the divergent optimization goals of agility and stability, we design a hybrid reward scheme that applies general tracking objectives across all data while injecting balance-specific priors only into synthetic motions. Further, an adaptive learning strategy with performance-driven sampling and motion-specific reward shaping enables efficient training across diverse motion distributions. We validate AMS extensively in simulation and on a real Unitree G1 humanoid. Experiments demonstrate that a single policy can execute agile skills such as dancing and running, while also performing zero-shot extreme balance motions like Ip Man's Squat, highlighting AMS as a versatile control paradigm for future humanoid applications.

</details>


### [19] [Vector Cost Behavioral Planning for Autonomous Robotic Systems with Contemporary Validation Strategies](https://arxiv.org/abs/2511.17375)
*Benjamin R. Toaz,Quentin Goss,John Thompson,Seta Boğosyan,Shaunak D. Bopardikar,Mustafa İlhan Akbaş,Metin Gökaşan*

Main category: cs.RO

TL;DR: 本文介绍了一种向量成本双矩阵博弈方法，用于多目标决策制定，特别是在自主机器人系统中优化多个目标的同时避免在被忽略的目标上出现最坏情况。文章还通过可解释的人工智能(XAI)软件和SEMBAS技术来分析高维决策数据以及参数空间中的性能模式。该研究结合了博弈论规划与智能系统验证的方面，形成了一套新颖且全面的仿真流程，并展示了向量成本方法相较于标量化方法的巨大改进。


<details>
  <summary>Details</summary>
Motivation: 为了使自主机器人系统能够同时优化多个目标并避免忽视某些目标而造成的最差情形，作者提出并扩展了向量成本双矩阵博弈的方法。此外，希望通过比较此方法与传统标量化方法在竞争性路径规划中的表现，证明其优越性。

Method: 采用向量成本双矩阵游戏作为主要手段解决多目标决策问题；利用XAI工具辅助分析复杂的决策过程；应用SEMBAS策略探索参数空间内不同性能模式；构建了一个整合了博弈论规划及智能系统验证的新颖仿真平台。

Result: 研究表明，在处理多目标决策任务时，向量成本方法比传统的加权求和标量化方法表现出显著优势。所提出的框架不仅提高了决策质量，而且提供了更好的可解释性和通用性。

Conclusion: 本研究成功地将向量成本双矩阵博弈应用于多目标决策制定领域，并通过与现有方法对比验证了其有效性。基于此开发的综合仿真流水线为机器人行为规划提供了一个强大且易于理解的工具。

Abstract: The vector cost bimatrix game is a method for multi-objective decision making that enables autonomous robotic systems to optimize for multiple goals at once while avoiding worst-case scenarios in neglected objectives. We expand this approach to arbitrary numbers of objectives and compare its performance to scalar weighted sum methods during competitive motion planning. Explainable Artificial Intelligence (XAI) software is used to aid in the analysis of high dimensional decision-making data. State-space Exploration of Multidimensional Boundaries using Adherence Strategies (SEMBAS) is applied to explore performance modes in the parameter space as a sensitivity study for the baseline and proposed frameworks. While some works have explored aspects of game theoretic planning and intelligent systems validation separately, we combine each of these into a novel and comprehensive simulation pipeline. This integration demonstrates a dramatic improvement of the vector cost method over scalarization and offers an interpretable and generalizable framework for robotic behavioral planning. Code available at https://github.com/toazbenj/race_simulation. The video companion to this work is available at https://tinyurl.com/vectorcostvideo.

</details>


### [20] [Feasibility of Embodied Dynamics Based Bayesian Learning for Continuous Pursuit Motion Control of Assistive Mobile Robots in the Built Environment](https://arxiv.org/abs/2511.17401)
*Xiaoshan Zhou,Carol C. Menassa,Vineet R. Kamat*

Main category: cs.RO

TL;DR: 本文提出并验证了一种受大脑启发的贝叶斯推理框架，用于解决基于EEG的脑-机接口中连续追踪运动控制的问题。该方法通过自动生成相关性确定进行特征选择和持续在线学习，与传统的自回归和EEGNet方法相比，在累积会话转移学习设置下将预测速度与真实速度之间的归一化均方误差降低了72%。


<details>
  <summary>Details</summary>
Motivation: 当前大多数基于脑-机接口（BCI）的移动控制系统仅限于离散命令，缺乏支持用户实时自由调整速度和方向的能力。这种自然的移动控制对于轮椅使用者在复杂公共空间中的导航、社交互动及灵活舒适地移动至关重要。因此，研究旨在填补BCI中连续追踪运动控制的空白。

Method: 提出一种受大脑启发的贝叶斯推断框架，解码加速度为基础的运动表示中的体现动态。该方法采用自动相关性确定来进行特征选择，并结合持续在线学习。使用一个包含四名受试者执行基于运动想象的目标跟随任务16小时EEG数据的公开数据集来验证方法的有效性。

Result: 与自回归和基于EEGNet的方法相比，在会话累积迁移学习环境中，所提方法使预测速度与实际速度之间的归一化均方误差减少了72%。

Conclusion: 本研究表明，基于体化认知理论的大脑内在运动控制动力学可以通过EEG解码得到实证支持。从实践角度来看，以生物运动相同的动力学原理为基础的EEG解码为实现更加稳定直观的BCI控制提供了有希望的方向。

Abstract: Non-invasive electroencephalography (EEG)-based brain-computer interfaces (BCIs) offer an intuitive means for individuals with severe motor impairments to independently operate assistive robotic wheelchairs and navigate built environments. Despite considerable progress in BCI research, most current motion control systems are limited to discrete commands, rather than supporting continuous pursuit, where users can freely adjust speed and direction in real time. Such natural mobility control is, however, essential for wheelchair users to navigate complex public spaces, such as transit stations, airports, hospitals, and indoor corridors, to interact socially with the dynamic populations with agility, and to move flexibly and comfortably as autonomous driving is refined to allow movement at will. In this study, we address the gap of continuous pursuit motion control in BCIs by proposing and validating a brain-inspired Bayesian inference framework, where embodied dynamics in acceleration-based motor representations are decoded. This approach contrasts with conventional kinematics-level decoding and deep learning-based methods. Using a public dataset with sixteen hours of EEG from four subjects performing motor imagery-based target-following, we demonstrate that our method, utilizing Automatic Relevance Determination for feature selection and continual online learning, reduces the normalized mean squared error between predicted and true velocities by 72% compared to autoregressive and EEGNet-based methods in a session-accumulative transfer learning setting. Theoretically, these findings empirically support embodied cognition theory and reveal the brain's intrinsic motor control dynamics in an embodied and predictive nature. Practically, grounding EEG decoding in the same dynamical principles that govern biological motion offers a promising path toward more stable and intuitive BCI control.

</details>


### [21] [SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding](https://arxiv.org/abs/2511.17411)
*Nikolay Nikolov,Giuliano Albanese,Sombit Dey,Aleksandar Yanev,Luc Van Gool,Jan-Nico Zaech,Danda Pani Paudel*

Main category: cs.RO

TL;DR: 本文提出了一种新的机器人基础模型SPEAR-1，该模型通过增强预训练的视觉-语言模型（VLM）以具备3D理解能力，并结合了基于语言指令的实体控制。相较于现有的最先进模型，SPEAR-1在使用20倍少的机器人演示数据的情况下仍能取得更好或相当的表现。


<details>
  <summary>Details</summary>
Motivation: 目前的机器人基础模型(RFMs)主要通过对互联网预训练的视觉-语言模型(VLMs)进行微调构建而成，但这些VLMs由于缺乏对3D空间推理的理解，在处理实际三维世界中的机器人控制任务时表现有限。直接利用大规模机器人数据来弥补这一差距成本高昂且难以扩展。因此，研究者提出了通过给易于收集的非机器人图像数据添加3D注释的方式，来提高预训练VLMs的3D理解能力。

Method: 研究团队开发了一个名为SPEAR-VLM的3D感知视觉-语言模型，它能够从单张2D图片中推断出物体在3D空间中的坐标位置。基于此，他们推出了SPEAR-1，一个集成了地面3D感知与语言指导实体控制功能于一体的机器人基础模型。SPEAR-1是在约45百万帧来自24个开放X-体现数据集的数据上训练而成。

Result: 实验结果显示，SPEAR-1不仅在性能上优于或达到了现有最先进的模型如$π_0$-FAST和$π_{0.5}$的标准，而且仅需使用后者所用机器人演示数据量的大约二十分之一。

Conclusion: 本研究证明了通过精心设计的训练策略增强VLMs的3D理解能力，可以显著提升RFMs在实体控制任务中的可靠性，超越单纯依赖机器人数据所能达到的效果。此外，研究者还公开了模型权重及3D标注数据集以便于进一步的研究工作。

Abstract: Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control. Yet their ability to generalize across new environments, tasks, and embodiments remains limited. We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs). However, these VLMs are trained on 2D image-language tasks and lack the 3D spatial reasoning inherently required for embodied control in the 3D world. Bridging this gap directly with large-scale robotic data is costly and difficult to scale. Instead, we propose to enrich easy-to-collect non-robotic image data with 3D annotations and enhance a pretrained VLM with 3D understanding capabilities. Following this strategy, we train SPEAR-VLM, a 3D-aware VLM that infers object coordinates in 3D space from a single 2D image. Building on SPEAR-VLM, we introduce our main contribution, $~\textbf{SPEAR-1}$: a robotic foundation model that integrates grounded 3D perception with language-instructed embodied control. Trained on $\sim$45M frames from 24 Open X-Embodiment datasets, SPEAR-1 outperforms or matches state-of-the-art models such as $π_0$-FAST and $π_{0.5}$, while it uses 20$\times$ fewer robot demonstrations. This carefully-engineered training strategy unlocks new VLM capabilities and as a consequence boosts the reliability of embodied control beyond what is achievable with only robotic data. We make our model weights and 3D-annotated datasets publicly available.

</details>


### [22] [RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation](https://arxiv.org/abs/2511.17441)
*Shihan Wu,Xuecheng Liu,Shaoxuan Xie,Pengwei Wang,Xinghang Li,Bowen Yang,Zhe Li,Kai Zhu,Hongyu Wu,Yiheng Liu,Zhaoye Long,Yue Wang,Chong Liu,Dihan Wang,Ziqiang Ni,Xiang Yang,You Liu,Ruoxuan Feng,Runtian Xu,Lei Zhang,Denghang Huang,Chenghao Jin,Anlan Yin,Xinlong Wang,Zhenguo Sun,Junkai Zhao,Mengfei Du,Mingyu Cao,Xiansheng Chen,Hongyang Cheng,Xiaojie Zhang,Yankai Fu,Ning Chen,Cheng Chi,Sixiang Chen,Huaihai Lyu,Xiaoshuai Hao,Yankai Fu,Yequan Wang,Bo Lei,Dong Liu,Xi Yang,Yance Jiao,Tengfei Pan,Yunyan Zhang,Songjing Wang,Ziqian Zhang,Xu Liu,Ji Zhang,Caowei Meng,Zhizheng Zhang,Jiyang Gao,Song Wang,Xiaokun Leng,Zhiqiang Xie,Zhenzhen Zhou,Peng Huang,Wu Yang,Yandong Guo,Yichao Zhu,Suibing Zheng,Hao Cheng,Xinmin Ding,Yang Yue,Huanqian Wang,Chi Chen,Jingrui Pang,YuXi Qian,Haoran Geng,Lianli Gao,Haiyuan Li,Bin Fang,Gao Huang,Yaodong Yang,Hao Dong,He Wang,Hang Zhao,Yadong Mu,Di Hu,Hao Zhao,Tiejun Huang,Shanghang Zhang,Yonghua Lin,Zhongyuan Wang,Guocai Yao*

Main category: cs.RO

TL;DR: 本文介绍了一个名为RoboCOIN的数据集，它包含超过180,000个双手机器人操作示例，来自15个不同的机器人平台，并且覆盖了多种环境和任务。此外还提出了一个处理框架CoRobot，该框架使用RTML进行质量评估、自动化注释生成和统一的多体现管理。实验表明，RoboCOIN在多体现双手学习方面具有可靠性和有效性。


<details>
  <summary>Details</summary>
Motivation: 由于不同机器人平台之间的硬件异质性，大规模且多样化的双手机器人数据集仍然稀缺。为了解决这个问题并实现类似人类的灵巧度，作者们开发了这个综合性的多体现双手机器人操作数据集。

Method: 创建了一个名为RoboCOIN的大规模双手机器人操作数据集，涵盖了16种场景下的421项任务；设计了一个能力金字塔模型来提供多层次的注解；开发了CoRobot框架，包括RTML用于质量评估、自动生成注释以及统一管理多个体现。

Result: 通过广泛的实验证明了RoboCOIN在多体现双手机器人学习中的可靠性与有效性，对各种模型架构和机器人平台都有显著的性能提升。

Conclusion: RoboCOIN作为一个全面的多体现双手机器人操作数据集，不仅填补了这一领域的空白，而且通过其创新的层次能力金字塔和CoRobot框架，为提高机器人的双手机动能力和跨平台研究提供了有力支持。

Abstract: Bimanual manipulation is essential for achieving human-like dexterity in robots, but the large-scale and diverse bimanual robot datasets remain scarce due to hardware heterogeneity across robotic platforms. To address the challenge, we present RoboCOIN, a comprehensive multi-embodiment bimanual manipulation dataset with over 180,000 demonstrations collected from 15 distinct robotic platforms. The dataset covers 16 scenarios, including residential, commercial, and working environments, with 421 tasks systematically organized by bimanual coordination patterns and object properties. Our key innovation is a hierarchical capability pyramid that provides multi-level annotations, spanning trajectory-level concepts, segment-level subtasks, and frame-level kinematics. We further develop CoRobot, a comprehensive processing framework featuring Robot Trajectory Markup Language (RTML) for quality assessment, automated annotation generation, and unified multi-embodiment management. Extensive experiments demonstrate the reliability and effectiveness of RoboCOIN in multi-embodiment bimanual learning, with significant performance improvements across various model architectures and robotic platforms. The complete dataset and framework are open-sourced and publicly available for further research purposes. Project website: https://FlagOpen.github.io/RoboCOIN/.

</details>


### [23] [MDG: Masked Denoising Generation for Multi-Agent Behavior Modeling in Traffic Environments](https://arxiv.org/abs/2511.17496)
*Zhiyu Huang,Zewei Zhou,Tianhui Cai,Yun Zhang,Jiaqi Ma*

Main category: cs.RO

TL;DR: 提出了一种名为Masked Denoising Generation (MDG)的新型生成框架，用于多智能体行为建模。该方法通过连续、针对每个智能体和时间步长的噪声掩码来实现局部去噪和可控轨迹生成，适用于开放环路预测、封闭环路仿真、运动规划等多种场景，并在实际驾驶数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散和自回归的方法在处理多智能体行为建模时存在迭代采样、顺序解码或任务特定设计等方面的限制，影响了效率与重用性。为了克服这些问题，研究提出了一个新的统一生成框架。

Method: 本研究引入了Masked Denoising Generation (MDG)，它将多智能体行为建模重新定义为独立加噪时空张量的重建过程。MDG利用连续性的、针对每个智能体及其时间步的噪声掩码，使得可以在在一个或少数几次前向传递中完成局部去噪及可控轨迹生成。

Result: MDG在Waymo Sim Agents和nuPlan Planning基准测试中的闭环性能达到了竞争水平，同时提供了高效、一致且可控的开环多智能体轨迹生成能力。

Conclusion: MDG作为一种简单而通用的多智能体行为建模范式，能够有效地应对从开放环路预测到封闭环路模拟等不同需求下的挑战。

Abstract: Modeling realistic and interactive multi-agent behavior is critical to autonomous driving and traffic simulation. However, existing diffusion and autoregressive approaches are limited by iterative sampling, sequential decoding, or task-specific designs, which hinder efficiency and reuse. We propose Masked Denoising Generation (MDG), a unified generative framework that reformulates multi-agent behavior modeling as the reconstruction of independently noised spatiotemporal tensors. Instead of relying on diffusion time steps or discrete tokenization, MDG applies continuous, per-agent and per-timestep noise masks that enable localized denoising and controllable trajectory generation in a single or few forward passes. This mask-driven formulation generalizes across open-loop prediction, closed-loop simulation, motion planning, and conditional generation within one model. Trained on large-scale real-world driving datasets, MDG achieves competitive closed-loop performance on the Waymo Sim Agents and nuPlan Planning benchmarks, while providing efficient, consistent, and controllable open-loop multi-agent trajectory generation. These results position MDG as a simple yet versatile paradigm for multi-agent behavior modeling.

</details>


### [24] [HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation](https://arxiv.org/abs/2511.17497)
*Yuezhan Tao,Dexter Ong,Fernando Cladera,Jason Hughes,Camillo J. Taylor,Pratik Chaudhari,Vijay Kumar*

Main category: cs.RO

TL;DR: 本文提出了一种名为HALO的系统，它使用单目相机、GPS和IMU实现实时高空度量-语义制图与探索。该系统能够在大规模户外环境中高效地完成多任务自然语言指定的任务，并在仿真及实际实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了解决远距离视觉实时密集3D重建以及大规模户外环境的地图绘制和探索问题，特别是同时保证场景几何与语义准确性的挑战。

Method: 通过整合单目视觉、GPS定位和IMU数据，开发了HALO系统来实现高空度量-语义地图创建与探索；利用所获得的信息规划信息路径以执行多任务自然语言指令。

Result: 在面积达78,000平方米的大规模环境中进行模拟测试时，HALO相比现有最先进方法能够减少探索时间并提高高达68%的竞争比；实地测试表明所有组件可以在机器人上运行，并支持在40米高度覆盖24,600平方米区域的有效自主任务执行。

Conclusion: HALO系统证明了其在大规模环境中的有效性，不仅提高了探索效率还增强了任务执行能力，为未来相关领域的研究提供了新的方向。

Abstract: We demonstrate real-time high-altitude aerial metric-semantic mapping and exploration using a monocular camera paired with a global positioning system (GPS) and an inertial measurement unit (IMU). Our system, named HALO, addresses two key challenges: (i) real-time dense 3D reconstruction using vision at large distances, and (ii) mapping and exploration of large-scale outdoor environments with accurate scene geometry and semantics. We demonstrate that HALO can plan informative paths that exploit this information to complete missions with multiple tasks specified in natural language. In simulation-based evaluation across large-scale environments of size up to 78,000 sq. m., HALO consistently completes tasks with less exploration time and achieves up to 68% higher competitive ratio in terms of the distance traveled compared to the state-of-the-art semantic exploration baseline. We use real-world experiments on a custom quadrotor platform to demonstrate that (i) all modules can run onboard the robot, and that (ii) in diverse environments HALO can support effective autonomous execution of missions covering up to 24,600 sq. m. area at an altitude of 40 m. Experiment videos and more details can be found on our project page: https://tyuezhan.github.io/halo/.

</details>


### [25] [RynnVLA-002: A Unified Vision-Language-Action and World Model](https://arxiv.org/abs/2511.17502)
*Jun Cen,Siteng Huang,Yuqian Yuan,Hangjie Yuan,Chaohui Yu,Yuming Jiang,Jiayan Guo,Kehan Li,Hao Luo,Fan Wang,Xin Li,Deli Zhao,Hao Chen*

Main category: cs.RO

TL;DR: 介绍了RynnVLA-002，一种统一的视觉-语言-动作（VLA）和世界模型。该模型通过联合学习环境动态和动作规划，在仿真和真实机器人任务中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够结合视觉理解与动作生成，并且可以预测未来图像状态的世界模型，以此来提高在不同任务中的表现。

Method: 创建了RynnVLA-002，它整合了视觉-语言-动作模型与世界模型的功能，允许从图像观察中生成后续动作并利用动作和视觉输入预测未来的图像状态。

Result: RynnVLA-002在LIBERO模拟基准测试中达到97.4%的成功率，并且在LeRobot的实际实验中，其集成的世界模型使整体成功率提高了50%。

Conclusion: RynnVLA-002展示了将VLA模型与世界模型相结合的好处，通过相互增强实现了优于单独模型的表现。

Abstract: We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.

</details>
