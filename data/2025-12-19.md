<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 18]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Large Video Planner Enables Generalizable Robot Control](https://arxiv.org/abs/2512.15840)
*Boyuan Chen,Tianyuan Zhang,Haoran Geng,Kiwhan Song,Caiyi Zhang,Peihao Li,William T. Freeman,Jitendra Malik,Pieter Abbeel,Russ Tedrake,Vincent Sitzmann,Yilun Du*

Main category: cs.RO

TL;DR: 本研究探索了一种新的范式，即使用大规模视频预训练作为构建机器人基础模型的主要方式。通过互联网规模的人类活动和任务演示视频数据集训练了一个开放视频模型，用于生成机器人规划。该模型能够为新场景和任务生成零样本视频计划，并从中提取可执行的机器人动作。实验结果表明了其在真实世界中的鲁棒性、强泛化能力和可行性。


<details>
  <summary>Details</summary>
Motivation: 现有的工作主要通过扩展多模态大语言模型（MLLMs）来添加动作输出，创建视觉-语言-动作（VLA）系统，以期将MLLM的大规模语言和图像预训练有效转移到动作输出模式上。本文探索了另一种方法，即利用大规模视频预训练作为构建机器人基础模型的主要手段，因为视频能捕捉物理世界的时空状态和动作序列，与机器人行为自然对齐。

Method: 研究人员收集了一个互联网规模的视频数据集，包含了人类活动和任务演示，并首次在基础模型规模上训练了一个开放式视频模型用于生成机器人规划。该模型可以为新场景和任务产生零样本视频计划，并通过后处理步骤提取出可由机器人执行的动作。

Result: 实验评估显示，该模型在第三方选定的任务中以及实际机器人实验里表现出了成功的物理执行能力。这证明了它具有强大的指令跟随能力、良好的泛化性能及现实世界应用的可行性。

Conclusion: 研究表明，基于大规模视频预训练的方法对于开发通用型机器人决策模型是有效的。所提出的方法不仅展现了出色的泛化能力，还展示了在真实环境中实施的可能性。团队公开了模型和数据集以促进开放且可重复的基于视频的机器人学习研究。

Abstract: General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.

</details>


### [2] [SORS: A Modular, High-Fidelity Simulator for Soft Robots](https://arxiv.org/abs/2512.15994)
*Manuel Mekkattu,Mike Y. Michelis,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 介绍了SORS（Soft Over Rigid Simulator），一种用于软体机器人复杂建模的高保真模拟器，通过能量基础框架和有限元方法支持模块化扩展，并使用序列二次规划确保物理一致的接触处理。经实验证明，该模拟器能够以高度物理保真度捕捉材料行为和驱动动态，为下一代软体机器人的原型设计提供了一个经过验证的工具。


<details>
  <summary>Details</summary>
Motivation: 由于软体机器人在多物理环境下的部署需要先进的仿真框架来准确反映真实世界性能，同时考虑到软体机器人因大非线性变形、材料不可压缩性和接触交互作用而带来的独特建模挑战，现有的机器人仿真器往往难以有效地解决这些问题。

Method: 提出了SORS，一个基于能量框架并建立在有限元方法上的多功能高保真模拟器，允许模块化扩展以包含定制设计的材料和驱动模型。此外，为了保证物理上一致的接触处理，集成了基于约束非线性优化的序列二次规划方法。

Result: 通过一系列实际实验验证了SORS的有效性，包括悬臂偏转、软体机械臂的压力驱动以及PokeFlex数据集中的接触交互。此外，还展示了该框架对于软体机器人腿控制优化的潜力。这些测试表明，模拟器能够以高物理保真度捕捉基本材料行为和复杂的驱动动力学。

Conclusion: SORS填补了软体机器人生态系统中可扩展性、保真度和可用性的空白，成为连接仿真与现实之间差距的有效工具，为下一代软体机器人的原型开发提供了强有力的支持。

Abstract: The deployment of complex soft robots in multiphysics environments requires advanced simulation frameworks that not only capture interactions between different types of material, but also translate accurately to real-world performance. Soft robots pose unique modeling challenges due to their large nonlinear deformations, material incompressibility, and contact interactions, which complicate both numerical stability and physical accuracy. Despite recent progress, robotic simulators often struggle with modeling such phenomena in a scalable and application-relevant manner. We present SORS (Soft Over Rigid Simulator), a versatile, high-fidelity simulator designed to handle these complexities for soft robot applications. Our energy-based framework, built on the finite element method, allows modular extensions, enabling the inclusion of custom-designed material and actuation models. To ensure physically consistent contact handling, we integrate a constrained nonlinear optimization based on sequential quadratic programming, allowing for stable and accurate modeling of contact phenomena. We validate our simulator through a diverse set of real-world experiments, which include cantilever deflection, pressure-actuation of a soft robotic arm, and contact interactions from the PokeFlex dataset. In addition, we showcase the potential of our framework for control optimization of a soft robotic leg. These tests confirm that our simulator can capture both fundamental material behavior and complex actuation dynamics with high physical fidelity. By bridging the sim-to-real gap in these challenging domains, our approach provides a validated tool for prototyping next-generation soft robots, filling the gap of extensibility, fidelity, and usability in the soft robotic ecosystem.

</details>


### [3] [dLITE: Differentiable Lighting-Informed Trajectory Evaluation for On-Orbit Inspection](https://arxiv.org/abs/2512.16011)
*Jack Naylor,Raghav Mishra,Nicholas H. Barbara,Donald G. Dansereau*

Main category: cs.RO

TL;DR: 本文提出了∂LITE，一个端到端可微分的仿真管线，用于在轨检查操作。它利用先进的可微分渲染工具和定制轨道推进器来优化基于视觉传感器数据的轨道参数，从而自动设计出能显著提高所获数据质量和实用性的轨迹。


<details>
  <summary>Details</summary>
Motivation: 由于低地球轨道（LEO）环境下的独特挑战，如航天器表面阳光的镜面反射、自遮挡以及动态光照条件等，这极大地影响了在轨期间捕捉到的数据质量。再加上航天器之间的相对运动导致的成像距离与姿态变化，使得规划能够最大化可见性、信息量和数据质量的检查操作变得十分困难。为了解决这些问题并提高近距操作中图像的质量，需要一种新的方法来设计和优化检查轨迹。

Method: 作者们开发了一个名为∂LITE的端到端可微分仿真管线，该系统结合了前沿的可微分渲染技术和专有的轨道传播算法，允许根据视觉传感器获取的数据对轨道参数进行端到端优化。通过这种方式，可以自动产生非直观但高效的飞行路径，以大幅改进收集到的数据品质及其应用价值。

Result: 研究结果表明，使用∂LITE能够自动设计出不那么直观但非常有效的轨迹，这些轨迹能够显著提高获得的数据质量及其实用性。

Conclusion: 本文介绍了一种创新的方法——∂LITE，这是一种用于在轨检查任务的新颖且完全可微分的仿真管线。它代表了将现代计算方法应用于航天器任务规划领域的一项重要进展，并为未来的研究提供了新的视角。

Abstract: Visual inspection of space-borne assets is of increasing interest to spacecraft operators looking to plan maintenance, characterise damage, and extend the life of high-value satellites in orbit. The environment of Low Earth Orbit (LEO) presents unique challenges when planning inspection operations that maximise visibility, information, and data quality. Specular reflection of sunlight from spacecraft bodies, self-shadowing, and dynamic lighting in LEO significantly impact the quality of data captured throughout an orbit. This is exacerbated by the relative motion between spacecraft, which introduces variable imaging distances and attitudes during inspection. Planning inspection trajectories with the aide of simulation is a common approach. However, the ability to design and optimise an inspection trajectory specifically to improve the resulting image quality in proximity operations remains largely unexplored. In this work, we present $\partial$LITE, an end-to-end differentiable simulation pipeline for on-orbit inspection operations. We leverage state-of-the-art differentiable rendering tools and a custom orbit propagator to enable end-to-end optimisation of orbital parameters based on visual sensor data. $\partial$LITE enables us to automatically design non-obvious trajectories, vastly improving the quality and usefulness of attained data. To our knowledge, our differentiable inspection-planning pipeline is the first of its kind and provides new insights into modern computational approaches to spacecraft mission planning. Project page: https://appearance-aware.github.io/dlite/

</details>


### [4] [Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios](https://arxiv.org/abs/2512.16019)
*Qiping Zhang,Nathan Tsoi,Mofeed Nagib,Hao-Tien Lewis Chiang,Marynel Vázquez*

Main category: cs.RO

TL;DR: 本研究通过利用大型语言模型（LLMs）的少量学习能力，提高了机器人预测用户对其表现感知的能力，并在社会导航任务中进行了实验验证。结果显示，与传统监督学习模型相比，LLMs不仅能够匹配甚至超越其性能，而且所需标注实例数量显著减少。此外，研究还探讨了个性化示例对于提高预测准确性的作用。


<details>
  <summary>Details</summary>
Motivation: 理解人类如何评价人机交互过程中的机器人行为对于开发符合人类期望的社会意识型机器人至关重要。虽然传统的做法是通过用户研究来收集这些评估信息，但最近的研究提出了使用机器学习的方法。然而，现有的数据驱动方法需要大量的标记数据，这限制了它们的实际应用。为了填补这一空白，本文提出了一种新方法。

Method: 扩展了SEAN TOGETHER数据集，增加了更多真实世界的人机导航场景和参与者反馈。基于此增强的数据集，评估了几种LLM从少量上下文示例中根据观察到的空间-时间线索预测人类对机器人表现的看法的能力。此外，还进行了消融研究以了解LLM依赖于哪种类型的传感器信息来做这些推断，并探索了个性化示例对于上下文学习的新应用。

Result: 结果表明，与传统监督学习模型相比，LLMs能够在所需标记实例数量少一个数量级的情况下达到或超过其性能。随着更多上下文示例的增加，预测性能也有所提高，证实了该方法的可扩展性。另外，发现来自同一用户的个性化示例可以进一步提高预测准确性。

Conclusion: 这项工作为通过用户中心反馈以可扩展方式改进机器人行为铺平了道路。

Abstract: Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.

</details>


### [5] [Maintaining the Level of a Payload carried by Multi-Robot System on Irregular Surface](https://arxiv.org/abs/2512.16024)
*Rishabh Dev Yadav,Shrey Agrawal,Kamalakar Karlapalem*

Main category: cs.RO

TL;DR: 本文介绍了一种多机器人载荷运输系统，能够通过未知和不平坦的倾斜环境运送载荷，并保持载荷所需的定向。该系统使用开环控制器结合闭环PID控制器来实现目标，并且在多种模拟环境中进行了测试以证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了在未知且不平坦的倾斜环境中运送载荷，并同时保持载荷所需的方向，提出了一个基于多机器人的载荷运输系统。

Method: 采用定制的带有线性执行器（活塞）的机器人，通过连续监测载荷方向并计算每个机器人所需的活塞高度来维持载荷所期望的方向。提出了一种开环控制器与闭环PID控制器相结合的方法来达到这一目的。

Result: 该系统能够在任何未知和不平坦地形及倾斜条件下工作，并且在各种具有多样性和复杂性的模拟环境中成功展示了所提控制器的有效性。

Conclusion: 提出的多机器人载荷运输系统能够有效地应对未知及不平坦地形中的载荷搬运任务，维持载荷所需方向。

Abstract: In this paper, we introduce a multi robot payload transport system to carry payloads through an environment of unknown and uneven inclinations while maintaining the desired orientation of the payload. For this task, we used custom built robots with a linear actuator (pistons) mounted on top of each robot. The system continuously monitors the payload's orientation and computes the required piston height of each robot to maintain the desired orientation of the payload. In this work, we propose an open loop controller coupled with a closed loop PID controller to achieve the goal. As our modelling makes no assumptions on the type of terrain, the system can work on any unknown and uneven terrains and inclinations. We showcase the efficacy of our proposed controller by testing it on various simulated environments with varied and complex terrains.

</details>


### [6] [SWIFT-Nav: Stability-Aware Waypoint-Level TD3 with Fuzzy Arbitration for UAV Navigation in Cluttered Environments](https://arxiv.org/abs/2512.16027)
*Shuaidong Ji,Mahdi Bamdad,Francisco Cruz*

Main category: cs.RO

TL;DR: 提出了一种基于TD3的无人机导航框架SWIFT-Nav，通过结合优先级经验回放、校准探索及模糊安全规则，在复杂环境中实现快速稳定的避障路径收敛。


<details>
  <summary>Details</summary>
Motivation: 在杂乱和动态环境中实现高效可靠的无人机导航仍然具有挑战性。

Method: 提出了SWIFT-Nav：一种稳定性感知的航点级模糊仲裁与TD3集成导航方法。该系统将传感器驱动的感知前端与基于TD3的航点策略相结合：感知模块将激光雷达范围转换为加权安全图和目标提示；TD3策略采用优先经验回放训练，并使用衰减的epsilon-greedy探索计划逐步从探索转向利用。轻量级模糊逻辑层根据径向测量和附近障碍物计算安全分数，控制模式切换并限制不安全动作；同时，任务对齐的奖励塑造结合了目标进度、清除率和开关经济性条款，提供了密集且比例恰当的反馈以加速学习过程。

Result: 在Webots中实现并带有基于接近度的碰撞检测，本方法在轨迹平滑性和对未见布局的泛化能力方面始终优于基线，同时保持实时响应性。

Conclusion: 结合TD3与经验回放优先级、校准探索以及模糊安全规则的方法，为杂乱场景中的无人机导航提供了一个稳健且可部署的解决方案。

Abstract: Efficient and reliable UAV navigation in cluttered and dynamic environments remains challenging. We propose SWIFT-Nav: Stability-aware Waypoint-level Integration of Fuzzy arbitration and TD3 for Navigation, a TD3-based navigation framework that achieves fast, stable convergence to obstacle-aware paths. The system couples a sensor-driven perception front end with a TD3 waypoint policy: the perception module converts LiDAR ranges into a confidence-weighted safety map and goal cues, while the TD3 policy is trained with Prioritised Experience Replay to focus on high-error transitions and a decaying epsilon-greedy exploration schedule that gradually shifts from exploration to exploitation. A lightweight fuzzy-logic layer computes a safety score from radial measurements and near obstacles, gates mode switching and clamps unsafe actions; in parallel, task-aligned reward shaping combining goal progress, clearance, and switch-economy terms provides dense, well-scaled feedback that accelerates learning. Implemented in Webots with proximity-based collision checking, our approach consistently outperforms baselines in trajectory smoothness and generalization to unseen layouts, while preserving real-time responsiveness. These results show that combining TD3 with replay prioritisation, calibrated exploration, and fuzzy-safety rules yields a robust and deployable solution for UAV navigation in cluttered scenes.

</details>


### [7] [A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators](https://arxiv.org/abs/2512.16069)
*Maolin Lei,Edoardo Romiti,Arturo Laurenzi,Rui Dai,Matteo Dalle Vedove,Jiatao Ding,Daniele Fontanelli,Nikos Tsagarakis*

Main category: cs.RO

TL;DR: 提出了一种统一的任务驱动计算框架，集成了不同形态的轨迹规划与形态及安装姿态的协同优化。通过层次模型预测控制策略和CMA-ES算法实现了冗余与非冗余机械臂的动作规划及设计优化，并引入虚拟模块抽象支持双分支结构以扩展工作空间而不增加单个关节模块的容量。


<details>
  <summary>Details</summary>
Motivation: 为了解决模块化机械臂在执行多样化任务时面临的适应性问题，特别是在不违反基座关节扭矩限制的情况下扩展可达范围的问题。

Method: 开发了一个基于层次模型预测控制（HMPC）策略的统一任务驱动计算框架，该框架结合了跨不同形态的轨迹规划与形态及安装姿态的联合优化。使用CMA-ES进行设计优化探索混合搜索空间，并引入虚拟模块概念实现双分支结构。

Result: 仿真和硬件实验表明，所提出的框架能够生成满足运动学和动力学约束同时避免环境碰撞的设计方案；通过定制成本函数可以实现如最大化可操作性、最小化关节努力或减少模块数量等灵活的设计目标；并且可以在不需更强大基础模块的情况下实现大工作空间的操作。

Conclusion: 本研究提出的框架有效解决了模块化机械臂在执行多样化任务时的适应性挑战，特别是通过创新性的双分支设计提高了工作空间利用效率。

Abstract: Modular manipulators composed of pre-manufactured and interchangeable modules offer high adaptability across diverse tasks. However, their deployment requires generating feasible motions while jointly optimizing morphology and mounted pose under kinematic, dynamic, and physical constraints. Moreover, traditional single-branch designs often extend reach by increasing link length, which can easily violate torque limits at the base joint. To address these challenges, we propose a unified task-driven computational framework that integrates trajectory planning across varying morphologies with the co-optimization of morphology and mounted pose. Within this framework, a hierarchical model predictive control (HMPC) strategy is developed to enable motion planning for both redundant and non-redundant manipulators. For design optimization, the CMA-ES is employed to efficiently explore a hybrid search space consisting of discrete morphology configurations and continuous mounted poses. Meanwhile, a virtual module abstraction is introduced to enable bi-branch morphologies, allowing an auxiliary branch to offload torque from the primary branch and extend the achievable workspace without increasing the capacity of individual joint modules. Extensive simulations and hardware experiments on polishing, drilling, and pick-and-place tasks demonstrate the effectiveness of the proposed framework. The results show that: 1) the framework can generate multiple feasible designs that satisfy kinematic and dynamic constraints while avoiding environmental collisions for given tasks; 2) flexible design objectives, such as maximizing manipulability, minimizing joint effort, or reducing the number of modules, can be achieved by customizing the cost functions; and 3) a bi-branch morphology capable of operating in a large workspace can be realized without requiring more powerful basic modules.

</details>


### [8] [A simulation platform calibration method for automated vehicle evaluation: accurate on both vehicle level and traffic flow level](https://arxiv.org/abs/2512.16076)
*Jia Hu,Junqi Li,Xuerun Yan,Jintao Lai,Lianhua An*

Main category: cs.RO

TL;DR: 本文提出了一种新的仿真平台校准方法，旨在提高自动驾驶车辆（AVs）与背景交通互动的准确性。该方法在车辆间交互、精确度保证、效率提升以及流程化校准能力方面具有优势。实验结果显示，该方法在互动复制准确性上提高了83.53%，校准效率提升了76.75%，同时保持了车辆级和交通流级指标上的高精度，整体过程完全自动化。


<details>
  <summary>Details</summary>
Motivation: 现有的校准方法无法有效确保自动驾驶车辆（AVs）与背景交通之间交互的准确再现，这对AVs的可靠评估构成了挑战。因此，研究旨在开发一种新的仿真平台校准方法，以解决这一问题。

Method: 本研究介绍了一种新的仿真平台校准方法，其特点包括：能够对车辆间交互进行校准、保证精确度、提高效率、具备流程化校准能力。此方法与没有校准的基准情况及最先进的校准方法进行了对比测试。

Result: 结果表明，所提出的方法将互动复制的准确性提高了83.53%，校准效率提高了76.75%。此外，在车辆级别和交通流级别的度量标准上都保持了较高的准确性，分别提高了51.9%。值得注意的是，整个校准过程是全自动化的，无需人工干预。

Conclusion: 这项研究成功地开发并验证了一种用于自动驾驶车辆仿真测试的新校准方法，该方法显著提高了仿真中车辆间交互的准确性和校准效率，并且实现了全流程自动化。

Abstract: Simulation testing is a fundamental approach for evaluating automated vehicles (AVs). To ensure its reliability, it is crucial to accurately replicate interactions between AVs and background traffic, which necessitates effective calibration. However, existing calibration methods often fall short in achieving this goal. To address this gap, this study introduces a simulation platform calibration method that ensures high accuracy at both the vehicle and traffic flow levels. The method offers several key features:(1) with the capability of calibration for vehicle-to-vehicle interaction; (2) with accuracy assurance; (3) with enhanced efficiency; (4) with pipeline calibration capability. The proposed method is benchmarked against a baseline with no calibration and a state-of-the-art calibration method. Results show that it enhances the accuracy of interaction replication by 83.53% and boosts calibration efficiency by 76.75%. Furthermore, it maintains accuracy across both vehicle-level and traffic flow-level metrics, with an improvement of 51.9%. Notably, the entire calibration process is fully automated, requiring no human intervention.

</details>


### [9] [ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation](https://arxiv.org/abs/2512.16302)
*Zixuan Chen,Chongkai Gao,Lin Shao,Jieqi Shi,Jing Huo,Yang Gao*

Main category: cs.RO

TL;DR: 提出了ManiLong-Shot框架，通过将长周期任务围绕物理交互事件进行结构化处理，并将问题重新定义为排序交互感知的原始动作而非直接模仿连续轨迹，来改善一次性模仿学习（OSIL）在长周期预抓取操作任务中的应用。此方法不仅能够基于视觉-语言模型（VLM）的高层次推理或根据机器人状态变化的规则启发式驱动原始分解，还能够预测对交互至关重要的不变区域、建立演示与当前观察之间的对应关系以及计算目标末端执行器姿态。仿真实验表明，仅通过10个短周期任务训练的ManiLong-Shot能泛化至20个未见过的长周期任务上，并且相比现有最佳技术提高了22.8%的表现。此外，实际机器人实验也验证了ManiLong-Shot在执行长周期操作任务时的能力和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前的一次性模仿学习方法主要局限于短期任务，限制了其应用于复杂且长期的操作任务。为了克服这一局限性，研究者们提出了一种新的框架ManiLong-Shot，旨在使一次性模仿学习能够有效用于长期预抓取操作任务中。

Method: ManiLong-Shot通过围绕物理交互事件构建长期任务，将问题转化为顺序排列交互意识的基础动作而不是直接模仿连续路径。基础动作的分解可以由视觉-语言模型(VLM)的高层推理或基于机器人状态变化的规则启发式引导。对于每个基础动作，ManiLong-Shot会预测关键的交互不变区域，建立示范与当前观察之间的联系，并计算目标末端执行器姿势，以实现有效的任务执行。

Result: 广泛的模拟实验表明，经过仅10个短期任务训练后，ManiLong-Shot能够推广到20个未见过的长期任务上，在三个难度级别上通过一次性的模仿学习实现了相对于最先进方法22.8%的相对改进。此外，真实机器人实验也证实了ManiLong-Shot能够通过一次性模仿学习稳健地执行三个长期操纵任务，证明了其实用性。

Conclusion: ManiLong-Shot提供了一种新颖的方法，使得一次性模仿学习能够有效地扩展到复杂的长期操作任务中。这种方法不仅提升了机器人的技能学习效率，而且在实际应用场景中展示了良好的鲁棒性和泛化能力。

Abstract: One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.

</details>


### [10] [A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion](https://arxiv.org/abs/2512.16367)
*Sijia Chen,Wei Dong*

Main category: cs.RO

TL;DR: 本文提出了一种地面-空中协作系统，通过整合主动视觉、单点测距、惯性里程计和光流来提高飞行机器人在杂乱环境中的定位鲁棒性。实验表明，该方法在烟雾干扰、光照变化、障碍物遮挡等条件下能够实现鲁棒的在线定位，平均均方根误差约为0.09米。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统方法中固定摄像头观察预装标记的方法受限于有限距离且易受捕捉失败影响的问题，特别是在视觉传感器性能下降时，本文旨在通过一种更全面的方法改进地面-空气定位框架。

Method: 本研究设计了一个安装在地面上车辆上的主动视觉子系统，可以动态旋转以检测并跟踪空中机器人上的红外标记，从而增加视野和目标识别能力。同时，结合了单点测距技术来扩展可行距离，并增强视觉退化条件下的重新捕获能力。此外，在估计过程中采用了基于多项式近似与扩展滑动窗口的降维估计器融合多源测量值，考虑不同传感器的准确性实施了自适应滑动置信度评估算法来评估测量质量并根据移动方差动态调整权重参数。

Result: 广泛的实验结果表明，在遇到烟雾干扰、光照变化、障碍物遮挡、长时间视觉丧失以及扩展操作范围的情况下，所提出的方法实现了鲁棒性的在线定位，平均均方根误差大约为0.09米，并且对于捕捉损失和传感器故障保持了韧性。

Conclusion: 综上所述，通过引入一个综合运用主动视觉、单点测距、惯性里程计及光流的地面-空中协作系统，这项工作显著提高了飞行机器人在复杂环境中面对视觉传感器降级情况下的定位稳定性。

Abstract: It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.

</details>


### [11] [E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion](https://arxiv.org/abs/2512.16446)
*Enis Yalcin,Joshua O'Hara,Maria Stamatopoulou,Chengxu Zhou,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: 本文提出了一种名为E-SDS的框架，该框架通过将视觉-语言模型与实时地形传感器分析相结合，自动生成奖励函数，以促进感知运动策略的训练。E-SDS在不同地形上表现出色，尤其在楼梯下降任务中超越了手动设计奖励或非感知自动基线方法，并且大大减少了人类在设计奖励时所需的努力。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉-语言模型的方法缺乏必要的环境感知能力来应对复杂的地形导航问题。为了解决这个问题并减少手动工程的需要，研究者们开发了E-SDS框架。

Method: E-SDS结合了视觉-语言模型与实时地形传感技术，能够根据示例视频自动生成有利于训练稳健感知移动策略的奖励函数。

Result: 实验表明，在四种不同的地形条件下（简单、有间隙、障碍物、楼梯），E-SDS特别成功地完成了楼梯下降任务，而使用手动设计奖励或非感知自动化基准训练的策略未能完成。此外，所有地形条件下的速度跟踪误差也降低了51.9%-82.6%。

Conclusion: E-SDS框架不仅显著提高了机器人在复杂地形上的导航能力，而且极大程度上简化了奖励函数的设计过程，从数天缩短到了不到两小时。

Abstract: Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially "blind", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.

</details>


### [12] [AG-MPBS: a Mobility-Aware Prediction and Behavior-Based Scheduling Framework for Air-Ground Unmanned Systems](https://arxiv.org/abs/2512.16454)
*Tianhao Shao,Kaixing Zhao,Feng Liu,Lixin Yang,Bin Guo*

Main category: cs.RO

TL;DR: 本文提出了一种名为MPBS的可扩展任务招募框架，该框架通过结合行为分类与时空预测来实时自适应地为最合适的无人设备分配任务。实验表明MPBS显著提高了任务完成效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 随着无人驾驶飞行器(UAV)和无人驾驶地面车辆(UGV)等无人系统在城市感知和应急响应等应用中的重要性日益增加，高效招募这些自主设备执行时间敏感型任务已经成为一个关键挑战。

Method: MPBS集成了三个关键模块：一个行为感知的KNN分类器、一个用于预测设备移动性的时间变化马尔可夫预测模型，以及一个考虑任务紧急性和基站性能的动态优先级调度机制。通过将行为分类与时空预测相结合，MPBS能够实时自适应地给最适合的设备分配任务。

Result: 基于真实世界GeoLife数据集的实验证明，MPBS大大提升了任务完成效率及资源使用率。

Conclusion: 所提出的框架为无人系统的智能协作调度提供了一个预测性的、行为感知的解决方案。

Abstract: As unmanned systems such as Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) become increasingly important to applications like urban sensing and emergency response, efficiently recruiting these autonomous devices to perform time-sensitive tasks has become a critical challenge. This paper presents MPBS (Mobility-aware Prediction and Behavior-based Scheduling), a scalable task recruitment framework that treats each device as a recruitable "user". MPBS integrates three key modules: a behavior-aware KNN classifier, a time-varying Markov prediction model for forecasting device mobility, and a dynamic priority scheduling mechanism that considers task urgency and base station performance. By combining behavioral classification with spatiotemporal prediction, MPBS adaptively assigns tasks to the most suitable devices in real time. Experimental evaluations on the real-world GeoLife dataset show that MPBS significantly improves task completion efficiency and resource utilization. The proposed framework offers a predictive, behavior-aware solution for intelligent and collaborative scheduling in unmanned systems.

</details>


### [13] [Tri-Select: A Multi-Stage Visual Data Selection Framework for Mobile Visual Crowdsensing](https://arxiv.org/abs/2512.16469)
*Jiayu Zhang,Kaixing Zhao,Tianhao Shao,Bin Guo,Liang He*

Main category: cs.RO

TL;DR: 本文提出了Tri-Select框架，一个三阶段的视觉数据选择方法，旨在解决移动视觉众包中数据冗余和异质性的问题。通过元数据过滤、基于空间相似性的谱聚类以及基于最大独立集搜索的视觉特征指导选择，该框架提高了选择效率和数据集质量。


<details>
  <summary>Details</summary>
Motivation: 移动视觉众包允许通过分布式移动设备收集图像来实现大规模、细粒度的环境监测。然而，由于采集视角重叠、分辨率不同及用户行为多样导致的数据冗余性和异质性问题亟待解决。

Method: 提出了一种名为Tri-Select的多阶段视觉数据选择框架，包括三个步骤：1) 基于元数据的过滤以排除无关样本；2) 基于空间相似性的谱聚类组织候选图像；3) 利用基于最大独立集搜索的视觉特征导向选择保留高质量且具代表性的图像。

Result: 在真实世界与公开数据集上的实验表明，Tri-Select不仅提升了选择效率也增强了数据集的质量，非常适合应用于可扩展的众包场景中。

Conclusion: Tri-Select作为一种有效的解决方案，解决了移动视觉众包中存在的数据冗余和质量问题，为提高此类应用中的数据处理效率和质量提供了新的思路。

Abstract: Mobile visual crowdsensing enables large-scale, fine-grained environmental monitoring through the collection of images from distributed mobile devices. However, the resulting data is often redundant and heterogeneous due to overlapping acquisition perspectives, varying resolutions, and diverse user behaviors. To address these challenges, this paper proposes Tri-Select, a multi-stage visual data selection framework that efficiently filters redundant and low-quality images. Tri-Select operates in three stages: (1) metadata-based filtering to discard irrelevant samples; (2) spatial similarity-based spectral clustering to organize candidate images; and (3) a visual-feature-guided selection based on maximum independent set search to retain high-quality, representative images. Experiments on real-world and public datasets demonstrate that Tri-Select improves both selection efficiency and dataset quality, making it well-suited for scalable crowdsensing applications.

</details>


### [14] [A Formal Modular Synthesis Approach for the Coordination of 3-D Robotic Construction with Multi-robots](https://arxiv.org/abs/2512.16555)
*Marcelo Rosa,José E. R. Cury,Fabio L. Baldissera*

Main category: cs.RO

TL;DR: 本研究基于监督控制理论，开发了一种反应式控制器（称为监督器），用于协调多个机器人自主构建预定义的3D结构。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决多机器人系统在构建三维结构时如何有效协作的问题。

Method: 采用监督控制理论，从表示单个机器人和目标结构的模型中合成正确的构造性反应控制器。

Result: 成功设计出一种可以复制给其他机器人的监督器，使得所有机器人能够共同完成目标结构的构建。

Conclusion: 所提出的方法为多机器人系统的3D结构构建提供了一个有效的解决方案。

Abstract: In this paper, we deal with the problem of coordinating multiple robots to build 3-D structures. This problem consists of a set of mobile robots that interact with each other in order to autonomously build a predefined 3-D structure. Our approach is based on Supervisory Control Theory, and it allows us to synthesize from models that represent a single robot and the target structure a correct-by-construction reactive controller, called supervisor. When this supervisor is replicated for the other robots, then the target structure can be completed by all robots

</details>


### [15] [Olaf: Bringing an Animated Character to Life in the Physical World](https://arxiv.org/abs/2512.16705)
*David Müller,Espen Knoop,Dario Mylonopoulos,Agon Serifi,Michael A. Hopkins,Ruben Grandia,Moritz Bächer*

Main category: cs.RO

TL;DR: 该论文通过动画参考指导的强化学习，将虚拟角色Olaf带入物理世界。为实现其行走效果，设计了隐藏在软泡沫裙下的不对称腿，并使用球面和平面连杆来适应内部执行器。为减少行走周期中的撞击声和防止过热问题，引入了额外奖励机制和温度反馈控制。


<details>
  <summary>Details</summary>
Motivation: 为了创新机械设计和风格化运动控制，本研究旨在将具有非物理运动方式和不同于典型步行机器人比例的动画角色Olaf实体化，使其能够以令人信服的方式移动。

Method: 采用强化学习技术结合动画参考进行控制；利用软泡沫裙掩盖不对称双腿模拟脚部沿身体移动；应用球面和平面连杆于手臂、嘴巴及眼睛处以适配内部空间；通过添加特定奖励项减少行走时产生的噪音；以及通过监测温度并调整策略避免过热。

Result: 成功地在仿真环境和实际硬件上验证了模型的有效性，展示了一个装扮机器人物前所未有的逼真度。

Conclusion: 这项工作展示了如何有效地将动画角色转化为现实世界中的互动实体，通过创新的设计和技术克服了传统机器人在外观与动作上的局限性。

Abstract: Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.

</details>


### [16] [VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation](https://arxiv.org/abs/2512.16724)
*Yixiang Chen,Yan Huang,Keji He,Peiyan Li,Liang Wang*

Main category: cs.RO

TL;DR: 提出了一种名为VERM的方法，通过利用基础模型中的知识从构建的3D点云想象出一个虚拟的任务适应视角，以过滤掉多相机设置带来的冗余信息，并准确提取与任务相关的特征。该方法还包括深度感知模块和动态粗到细的过程，以促进3D动作规划和精细操控。实验结果表明，该方法在训练时间和推理速度上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 在执行3D操控任务时，机器人需要基于来自多个固定摄像头的感知来执行动作规划。多摄像头设置引入了大量的冗余性和无关信息，增加了计算成本，并迫使模型花费额外的训练时间来抽取关键的任务相关信息。为了过滤掉这些冗余信息并精确地抽取任务相关特性，提出了本方法。

Method: VERM（Robotic Manipulation用的虚拟眼睛）方法，它利用了基础模型中的知识，从构建的3D点云中想象出一个虚拟的任务自适应视角，有效地捕捉必要信息并减少遮挡问题。此外，还设计了一个深度感知模块和一个动态由粗到细的过程，以促进3D动作规划和细致操控。

Result: 广泛的实验结果，在仿真基准RLBench和真实世界评估中，都证明了所提方法的有效性，不仅超越了以前最先进的方法，而且实现了训练时间1.89倍加速和推理速度1.54倍的提升。

Conclusion: VERM方法通过设想虚拟视图、结合深度感知模块以及采用动态粗至细过程，成功解决了多摄像机设置下信息冗余的问题，提高了3D操纵任务的效率和准确性。

Abstract: When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .

</details>


### [17] [Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future](https://arxiv.org/abs/2512.16760)
*Tianshuai Hu,Xiaolu Liu,Song Wang,Yiyao Zhu,Ao Liang,Lingdong Kong,Guoyang Zhao,Zeying Gong,Jun Cen,Zhiyu Huang,Xiaoshuai Hao,Linfeng Li,Hang Song,Xiangtai Li,Jun Ma,Shaojie Shen,Jianke Zhu,Dacheng Tao,Ziwei Liu,Junwei Liang*

Main category: cs.RO

TL;DR: 本文对自主驾驶中出现的视觉-语言-行动（VLA）框架进行了系统性描述，从早期的视觉-行动模型到现代的VLA方法，并将现有技术分为端到端VLA和双系统VLA两大类。同时，讨论了关键挑战与未来研究方向，旨在为发展人类兼容的自动驾驶系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统的自动驾驶依赖于‘感知-决策-行动’模块化管道，在复杂或长尾场景下容易失效；而视觉-行动（VA）模型虽然尝试解决了一些问题，但仍存在透明度不足、对分布变化敏感以及缺乏结构化推理或遵循指令能力的问题。受到大型语言模型（LLMs）和多模态学习进步的启发，视觉-语言-行动（VLA）框架通过结合视觉理解、语言推理及可执行输出提供了一条通往更可解释、泛化性强且符合人类需求的驾驶策略路径。

Method: 文章首先回顾了从早期视觉-行动方法到当前视觉-语言-行动框架的发展历程，然后基于此将现有的方法论归类为两大主要范式：端到端VLA（在单一模型内整合感知、推理和规划功能）与双系统VLA（通过视觉-语言模型实现慢速深思熟虑过程与通过规划器实现快速安全执行过程相分离）。此外，还进一步区分了文本与数值动作生成器以及显式与隐式指导机制等子类别。

Result: 提供了对于基于VLA的驾驶系统进行评估时所使用的代表性数据集和基准测试概览，并强调了包括鲁棒性、可解释性和指令准确性在内的关键挑战与开放的研究方向。

Conclusion: 通过对新兴VLA领域的结构化表征，这项工作旨在为推进更加人性化设计的自动驾驶系统建立一个连贯的基础。

Abstract: Autonomous driving has long relied on modular "Perception-Decision-Action" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.

</details>


### [18] [PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence](https://arxiv.org/abs/2512.16793)
*Xiaopeng Lin,Shijie Lian,Bin Yu,Ruoqi Yang,Changti Wu,Yuzhuo Miao,Yurun Jin,Yukun Shi,Cong Huang,Bojun Cheng,Kai Chen*

Main category: cs.RO

TL;DR: 本文提出了一种Egocentric2Embodiment转换流程，将第一人称视频转化为多层次、基于模式的VQA监督，并构建了大规模的E2E-3M数据集。通过在该数据集上训练得到的PhysBrain展现了显著提高的第一人称理解能力，特别是在规划任务上，并且能够更有效地从人类第一人称监督转移到机器人控制中。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）主要基于第三人称视角的数据进行训练，这对类人机器人而言存在视角不匹配的问题。同时，扩大机器人第一人称视角的数据收集面临成本高和多样性不足的问题。因此，研究者寻求利用大规模的人类第一人称视频作为替代方案，这种视频自然地捕捉了丰富的交互背景和因果结构。

Method: 提出了Egocentric2Embodiment翻译流程，旨在将原始的第一人称视频转化为结构化且可靠的具身训练监督信息。此流程生成多级、基于模式的视觉问答(VQA)监督内容，强调证据基础与时间一致性，从而支持E2E-3M数据集的大规模建设。

Result: 通过在E2E-3M数据集上训练得到了名为PhysBrain的具有第一人称意识的具身大脑。实验表明，相较于基线方法，PhysBrain在第一人称理解方面表现出明显改进，特别是在EgoThink上的规划任务；并且为后续的VLA微调提供了更好的初始化，提高了SimplerEnv中的成功率至53.9%。

Conclusion: 本研究表明，通过有效转化并利用大规模人类第一人称视频来训练模型，可以显著提升机器人对第一人称视角的理解能力及其在下游控制任务中的表现。

Abstract: Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.

</details>
