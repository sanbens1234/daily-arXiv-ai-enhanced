<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning](https://arxiv.org/abs/2512.16861)
*Zihan Zhou,Animesh Garg,Ajay Mandlekar,Caelan Garrett*

Main category: cs.RO

TL;DR: 提出了ReinforceGen系统，结合任务分解、数据生成、模仿学习和运动规划来解决长期操控问题，并通过强化学习进行微调。在Robosuite数据集上达到了80%的成功率，且消融研究表明其微调方法使性能平均提高了89%。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人领域中长期存在的长期操控挑战。

Method: 使用了任务分解、数据生成、模仿学习以及运动规划相结合的方法形成初步解决方案，并通过基于强化学习的微调来改进每个组件。

Result: 在最高重置范围设置下，对于所有具有视觉-运动控制的任务，ReinforceGen达到了80%的成功率。此外，消融研究显示，所提出的微调方法使得性能平均提升了89%。

Conclusion: ReinforceGen通过整合多种技术并运用强化学习优化，有效地解决了长期操控难题，在实验中展示了显著的效果提升。

Abstract: Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/

</details>


### [2] [PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies](https://arxiv.org/abs/2512.16881)
*Arhan Jain,Mingtong Zhang,Kanav Arora,William Chen,Marcel Torne,Muhammad Zubair Irshad,Sergey Zakharov,Yue Wang,Sergey Levine,Chelsea Finn,Wei-Chiu Ma,Dhruv Shah,Abhishek Gupta,Karl Pertsch*

Main category: cs.RO

TL;DR: 本文介绍了一种名为PolaRiS的新框架，该框架利用神经重建方法将现实世界场景的短视频扫描转换为可交互的模拟环境，以实现高保真度的机器人策略评估。通过与现有模拟基准相比，PolaRiS评价与现实世界中的通用策略表现有更强的相关性，并且可以快速创建多样的模拟环境。


<details>
  <summary>Details</summary>
Motivation: 机器人学习研究面临的一个重要挑战是准确测量和比较机器人策略的表现。由于现实世界测试的随机性、再现性和耗时性质，这在机器人技术中历来是一项难题。对于需要跨各种场景和任务进行评估的通用策略来说，这一问题更加严重。虽然仿真评估提供了对现实世界评估的可扩展补充，但现有仿真基准与现实世界之间的视觉和物理领域差距使其成为政策改进不可靠的信号。此外，构建现实且多样化的模拟环境传统上需要大量的人力和专业知识。

Method: 作者们提出了Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS)框架，它是一个可扩展的真实到模拟框架，用于高保真的模拟机器人评估。PolaRiS使用神经重建方法将真实世界场景的短视频扫描转化为交互式模拟环境。另外，他们还开发了一个简单的模拟数据共同训练方案，以弥合剩余的真实到模拟差距，并能够在未见的模拟环境中进行零样本评估。

Result: 通过广泛的仿真与现实世界的配对评估，研究表明PolaRiS评估与现实世界通用策略性能之间存在比现有模拟基准更高的相关性。其简易性也使得能够迅速创建出多种多样的模拟环境。

Conclusion: 这项工作朝着下一代机器人基础模型的分布式和民主化评估迈出了一步。

Abstract: A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.

</details>


### [3] [Sceniris: A Fast Procedural Scene Generation Framework](https://arxiv.org/abs/2512.16896)
*Jinghuan Shang,Harsh Patel,Ran Gong,Karl Schmeckpeper*

Main category: cs.RO

TL;DR: 本文介绍了一种高效的3D场景生成框架Sceniris，它通过批处理采样和更快的碰撞检测实现了比现有方法至少234倍的速度提升，并且支持多样化的场景需求。


<details>
  <summary>Details</summary>
Motivation: 合成3D场景对于物理AI和生成模型的发展至关重要，但现有的过程生成方法通常输出效率低下，这成为大规模数据集创建中的一个重大瓶颈。

Method: 作者们提出了一种新的高效过程化场景生成框架——Sceniris，该框架针对先前方法的主要性能限制进行了优化设计，利用了批处理采样技术和cuRobo中更快速的碰撞检查技术。

Result: 实验结果表明，Sceniris相比之前的方法Scene Synthesizer在速度上至少提高了234倍，并且能够生成无碰撞的大规模场景变化以及满足机器人任务需求的操作可行场景。

Conclusion: Sceniris为快速生成大规模、无碰撞且符合多样化需求的3D场景提供了一个强大而高效的解决方案。

Abstract: Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris

</details>
