<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 15]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [TRACE: Textual Reasoning for Affordance Coordinate Extraction](https://arxiv.org/abs/2511.01999)
*Sangyun Park,Jin Kim,Yuchen Cui,Matthew S. Brown*

Main category: cs.RO

TL;DR: 本文提出了一种新的方法TRACE，通过将文本推理链整合到可操作性预测过程中来提高视觉-语言模型在机器人操作中的精确度。实验表明，该方法在基准测试中取得了最先进的性能，并且模型的注意力图分析显示了一个可解释的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型难以将高级指令转化为机器人操作所需的精确空间可操作性，而现有的视觉思维链方法往往计算成本较高。

Method: 引入了名为TRACE的新方法，该方法将文本推理链（CoR）集成到可操作性预测过程中，并创建了大规模的TRACE数据集，用于对视觉-语言模型进行微调。

Result: TRACE调整后的模型在主要的Where2Place（W2P）基准上达到了48.1%的准确率，在更具挑战性的W2P(h)子集上达到了55.0%的准确率。消融研究表明，推理数据量与表现直接相关，证实了CoR的有效性。此外，模型注意力图分析揭示了一个动态变化焦点的可解释推理过程。

Conclusion: 训练视觉-语言模型生成文本推理链是一种有效增强基于VLM的机器人控制精度、可靠性和可解释性的策略。

Abstract: Vision-Language Models (VLMs) struggle to translate high-level instructions
into the precise spatial affordances required for robotic manipulation. While
visual Chain-of-Thought (CoT) methods exist, they are often computationally
intensive. In this work, we introduce TRACE (Textual Reasoning for Affordance
Coordinate Extraction), a novel methodology that integrates a textual Chain of
Reasoning (CoR) into the affordance prediction process. We use this methodology
to create the TRACE dataset, a large-scale collection created via an autonomous
pipeline that pairs instructions with explicit textual rationales. By
fine-tuning a VLM on this data, our model learns to externalize its spatial
reasoning before acting. Our experiments show that our TRACE-tuned model
achieves state-of-the-art performance, reaching 48.1% accuracy on the primary
Where2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more
challenging W2P(h) subset. Crucially, an ablation study demonstrates that
performance scales directly with the amount of reasoning data used, confirming
the CoR's effectiveness. Furthermore, analysis of the model's attention maps
reveals an interpretable reasoning process where focus shifts dynamically
across reasoning steps. This work shows that training VLMs to generate a
textual CoR is an effective and robust strategy for enhancing the precision,
reliability, and interpretability of VLM-based robot control. Our dataset and
code are available at https://github.com/jink-ucla/TRACE

</details>


### [2] [Stein-based Optimization of Sampling Distributions in Model Predictive Path Integral Control](https://arxiv.org/abs/2511.02015)
*Jace Aldrich,Odest Chadwicke Jenkins*

Main category: cs.RO

TL;DR: 提出了一种新的方法，即Stein-优化路径积分推理（SOPPI），它通过在MPPI环境步骤之间引入SVGD更新来优化样本生成，以形成更优的轨迹表示。


<details>
  <summary>Details</summary>
Motivation: 传统的模型预测路径积分（MPPI）控制依赖于随机采样的轨迹，这可能导致样本不足，不能充分代表所有可能的轨迹空间，从而导致次优结果。

Method: 通过在MPPI环境步骤间引入Stein变分梯度下降（SVGD）更新，动态地在运行时更新噪声分布，以形成更优的轨迹表示而不大幅增加计算需求。

Result: 在从Cart-Pole到二维双足行走任务的各种系统中展示了该方法的有效性，表明与标准MPPI相比性能有所提高，并且在较低粒子数量下也具有可行性。

Conclusion: 讨论了将这种MPPI/SVGD方法应用于更高自由度系统的可能性，以及其对于最新可微模拟器发展的潜力。

Abstract: This paper presents a novel method for Model Predictive Path Integral (MPPI)
control that optimizes sample generation towards an optimal trajectory through
Stein Variational Gradient Descent (SVGD). MPPI is traditionally reliant on
randomly sampled trajectories, often by a Gaussian distribution. The result can
lead to sample deprivation, under-representing the space of possible
trajectories, and yield suboptimal results. Through introducing SVGD updates in
between MPPI environment steps, we present Stein-Optimized Path-Integral
Inference (SOPPI), an MPPI/SVGD algorithm that can dynamically update noise
distributions at runtime to shape a more optimal representation without an
excessive increase in computational requirements. We demonstrate the efficacy
of our method systems ranging from a Cart-Pole to a two-dimensional bipedal
walking task, indicating improved performance above standard MPPI across a
range of hyper-parameters and demonstrate feasibility at lower particle counts.
We discuss the applicability of this MPPI/SVGD method to higher
degree-of-freedom systems, as well as its potential to new developments in
state-of-the-art differentiable simulators.

</details>


### [3] [TurboMap: GPU-Accelerated Local Mapping for Visual SLAM](https://arxiv.org/abs/2511.02036)
*Parsa Hosseininejad,Kimia Khabiri,Shishir Gopinath,Soudabeh Mohammadhashemi,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: TurboMap, a module for visual SLAM systems, uses GPU and CPU optimizations to speed up the local mapping process while keeping the system's accuracy. It achieves an average speedup of 1.3x in the EuRoC dataset and 1.6x in the TUM-VI dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the performance of the local mapping process in visual SLAM systems by addressing key bottlenecks through GPU and CPU optimizations, aiming to achieve faster processing without sacrificing the accuracy of the SLAM system.

Method: TurboMap offloads map point triangulation and fusion to the GPU, accelerates redundant keyframe culling on the CPU, and incorporates a GPU-accelerated solver for speeding up local bundle adjustment. The implementation is based on ORB-SLAM3 and utilizes CUDA for GPU programming.

Result: Experimental results show that TurboMap attains an average speedup of 1.3x on the EuRoC dataset and 1.6x on the TUM-VI dataset within the local mapping module, with both desktop and embedded platforms, all while maintaining the original system's accuracy.

Conclusion: TurboMap successfully enhances the speed of the local mapping process in visual SLAM systems across different datasets and platforms, proving its effectiveness in accelerating the process without compromising on accuracy.

Abstract: This paper presents TurboMap, a GPU-accelerated and CPU-optimized local
mapping module for visual SLAM systems. We identify key performance bottlenecks
in the local mapping process for visual SLAM and address them through targeted
GPU and CPU optimizations. Specifically, we offload map point triangulation and
fusion to the GPU, accelerate redundant keyframe culling on the CPU, and
integrate a GPU-accelerated solver to speed up local bundle adjustment. Our
implementation is built on top of ORB-SLAM3 and leverages CUDA for GPU
programming. The experimental results show that TurboMap achieves an average
speedup of 1.3x in the EuRoC dataset and 1.6x in the TUM-VI dataset in the
local mapping module, on both desktop and embedded platforms, while maintaining
the accuracy of the original system.

</details>


### [4] [TACO: Trajectory-Aware Controller Optimization for Quadrotors](https://arxiv.org/abs/2511.02060)
*Hersh Sanghvi,Spencer Folk,Vijay Kumar,Camillo Jose Taylor*

Main category: cs.RO

TL;DR: 提出了一种名为TACO的框架，该框架能够根据即将到来的参考轨迹和当前四旋翼状态在线调整控制器参数，从而优化了四旋翼轨迹跟踪性能，并且在实际四旋翼上实现了实时部署。


<details>
  <summary>Details</summary>
Motivation: 传统的控制器参数调优方法依赖于固定的、手动调节的参数，这牺牲了特定任务的表现。为了解决这个问题，研究者们开发了TACO框架，它能够基于即将到来的参考轨迹以及当前的四旋翼状态在线调整控制器参数。

Method: TACO框架使用了一个学习到的预测模型和一个轻量级的优化方案来实时优化控制器增益。此外，还引入了一个并行化的四旋翼模拟器以支持在多种轨迹上的快速数据收集，从而实现大规模训练。

Result: 实验结果表明，TACO不仅优于传统静态参数调优方法，在各种类型的轨迹测试中表现出色，而且其运行速度比黑盒优化基线快几个数量级，使得在物理四旋翼上的实时部署成为可能。另外，通过TACO调整轨迹显著减少了四旋翼的跟踪误差。

Conclusion: TACO提供了一种有效的解决方案，用于提高四旋翼轨迹跟踪任务中的控制器表现，同时保持了实时性。这种方法能够在不影响平滑性约束的情况下改善动态可行性。

Abstract: Controller performance in quadrotor trajectory tracking depends heavily on
parameter tuning, yet standard approaches often rely on fixed, manually tuned
parameters that sacrifice task-specific performance. We present
Trajectory-Aware Controller Optimization (TACO), a framework that adapts
controller parameters online based on the upcoming reference trajectory and
current quadrotor state. TACO employs a learned predictive model and a
lightweight optimization scheme to optimize controller gains in real time with
respect to a broad class of trajectories, and can also be used to adapt
trajectories to improve dynamic feasibility while respecting smoothness
constraints. To enable large-scale training, we also introduce a parallelized
quadrotor simulator supporting fast data collection on diverse trajectories.
Experiments on a variety of trajectory types show that TACO outperforms
conventional, static parameter tuning while operating orders of magnitude
faster than black-box optimization baselines, enabling practical real-time
deployment on a physical quadrotor. Furthermore, we show that adapting
trajectories using TACO significantly reduces the tracking error obtained by
the quadrotor.

</details>


### [5] [Census-Based Population Autonomy For Distributed Robotic Teaming](https://arxiv.org/abs/2511.02147)
*Tyler M. Paine,Anastasia Bizyaeva,Michael R. Benjamin*

Main category: cs.RO

TL;DR: 本文提出了一种多机器人自主性的分层模型，该模型基于普查原则进行集体决策，并通过区间编程实现个体行为的多目标优化。这种模型可以简化为分布式优化和控制的基础算法，同时全模型能够实现现实世界中有用的新类型集体行为。


<details>
  <summary>Details</summary>
Motivation: 协作机器人团队因其在任务执行效率和鲁棒性方面的优势而受到关注，特别是在海洋环境中运行的系统中。然而，如何建模、分析和设计这些多机器人系统以充分利用协作的好处是一个挑战，因为多机器人自主领域既包括集体行为也包括个体行为。

Method: 论文介绍了一种新的多机器人自主性分层模型，其中利用了“普查”原则来进行关于组队的集体决策，并结合了针对动作的个人决策的多目标行为优化。普查组件被表达为非线性意见动态模型，而多目标行为优化则通过区间编程来完成。此外，还介绍了一种新的子群分配分布式优化方法，其中机器人使用梯度下降算法最小化局部已知的成本函数部分，同时受到邻居的意见状态的影响以考虑未观察到的成本。

Result: 这个模型可以还原成分布式优化与控制中的基础算法，而完整模型则能够实现新型的群体行为，这些行为在实际场景中非常有用。为了证明这一点，研究者们在三类不同的实验中验证了该模型的有效性：自适应采样场景、高价值单位保护场景以及一场竞争性的夺旗游戏。

Conclusion: 提出的多机器人自主性分层模型不仅能够支持传统的分布式优化与控制算法，还能促进新类型的群体行为，这在多种实际应用场景中得到了验证。

Abstract: Collaborating teams of robots show promise due in their ability to complete
missions more efficiently and with improved robustness, attributes that are
particularly useful for systems operating in marine environments. A key issue
is how to model, analyze, and design these multi-robot systems to realize the
full benefits of collaboration, a challenging task since the domain of
multi-robot autonomy encompasses both collective and individual behaviors. This
paper introduces a layered model of multi-robot autonomy that uses the
principle of census, or a weighted count of the inputs from neighbors, for
collective decision-making about teaming, coupled with multi-objective behavior
optimization for individual decision-making about actions. The census component
is expressed as a nonlinear opinion dynamics model and the multi-objective
behavior optimization is accomplished using interval programming. This model
can be reduced to recover foundational algorithms in distributed optimization
and control, while the full model enables new types of collective behaviors
that are useful in real-world scenarios. To illustrate these points, a new
method for distributed optimization of subgroup allocation is introduced where
robots use a gradient descent algorithm to minimize portions of the cost
functions that are locally known, while being influenced by the opinion states
from neighbors to account for the unobserved costs. With this method the group
can collectively use the information contained in the Hessian matrix of the
total global cost. The utility of this model is experimentally validated in
three categorically different experiments with fleets of autonomous surface
vehicles: an adaptive sampling scenario, a high value unit protection scenario,
and a competitive game of capture the flag.

</details>


### [6] [Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models](https://arxiv.org/abs/2511.02162)
*Alexander Htet Kyaw,Richa Gupta,Dhruv Shah,Anoop Sinha,Kory Mathewson,Stefanie Pender,Sachin Chitta,Yotto Koga,Faez Ahmed,Lawrence Sass,Randall Davis*

Main category: cs.RO

TL;DR: 该论文提出了一种结合3D生成AI与视觉-语言模型（VLMs）的流程，用于根据自然语言指令组装多组件物体。通过利用VLMs进行零样本、多模态关于几何和功能性的推理，将AI生成的网格分解为使用预定义结构和面板组件组成的多组件3D模型。实验结果表明用户更偏好VLMs生成的组件分配方案。此外，系统还允许用户通过对话反馈来优化组件分配，增强了人类在生成物理对象过程中的控制权和自主性。


<details>
  <summary>Details</summary>
Motivation: 尽管3D生成AI已经能够从文本提示创建实体物品，但在处理涉及多种组件类型的物体时仍存在挑战。为此，研究者旨在开发一种解决方案，能够有效支持基于自然语言描述的多组件物体的机器人装配，并提高最终产物的质量及用户满意度。

Method: 研究团队构建了一个集成3D生成AI与视觉-语言模型（VLMs）的工作流。此方法利用VLMs执行零样本、跨模态的理解任务，以解析AI生成的三维网格，并依据预设的结构件与面板元件将其拆解成多个组件的3D模型。

Result: 评估结果显示，在测试物品上，用户对VLMs产生的组件分配方案的偏好度达到90.6%，远高于规则基础(59.4%)或随机分配(2.5%)的情况。此外，系统还提供了让用户通过会话式反馈调整组件分配的功能。

Conclusion: 本研究表明，通过整合3D生成AI与视觉-语言模型可以有效地实现基于自然语言的多组件物体的机器人装配，不仅提高了用户对于生成物的满意程度，而且通过对话式交互增强了人类在整个制造过程中的参与度和控制力。

Abstract: Advances in 3D generative AI have enabled the creation of physical objects
from text prompts, but challenges remain in creating objects involving multiple
component types. We present a pipeline that integrates 3D generative AI with
vision-language models (VLMs) to enable the robotic assembly of multi-component
objects from natural language. Our method leverages VLMs for zero-shot,
multi-modal reasoning about geometry and functionality to decompose
AI-generated meshes into multi-component 3D models using predefined structural
and panel components. We demonstrate that a VLM is capable of determining which
mesh regions need panel components in addition to structural components, based
on object functionality. Evaluation across test objects shows that users
preferred the VLM-generated assignments 90.6% of the time, compared to 59.4%
for rule-based and 2.5% for random assignment. Lastly, the system allows users
to refine component assignments through conversational feedback, enabling
greater human control and agency in making physical objects with generative AI
and robotics.

</details>


### [7] [Kinematic and Ergonomic Design of a Robotic Arm for Precision Laparoscopic Surgery](https://arxiv.org/abs/2511.02167)
*Tian Hao,Tong Lu,Che Chan*

Main category: cs.RO

TL;DR: 本文研究了一种专为高精度任务设计的7自由度腹腔镜手术机器人手臂，通过结合运动中心和人体工程学考虑来提高外科医生的操作体验。实验结果表明，该优化设计在目标准确性、任务效率以及降低操作者肌肉紧张度方面显著优于传统手动腹腔镜手术。


<details>
  <summary>Details</summary>
Motivation: 为了提高微创手术中的精确度并减少外科医生疲劳，本文旨在探索一种适用于高精度任务的腹腔镜手术机器人手臂的设计原则。

Method: 提出了一种包含远程运动中心（RCM）和人体工程学考量的7-DOF机器人手臂系统，并在一个通用机器人平台上实现了这一设计。通过一系列模拟手术任务来评估其定位准确性、任务效率以及相对于传统手动腹腔镜手术而言的外科医生舒适度。

Result: 实验结果显示，经过优化的机器人设计方案在定位准确性上提高了超过50%，同时缩短了任务完成时间，并显著降低了操作者的肌肉紧张感与不适。

Conclusion: 研究强调了运动学优化（例如增加关节灵活性及颤动过滤）与以人为核心的体工学设计对于提升机器人辅助手术性能的重要性。这些发现可指导下一代外科机器人的开发，以改善手术效果和操作团队的人体工程学。

Abstract: Robotic assistance in minimally invasive surgery can greatly enhance surgical
precision and reduce surgeon fatigue. This paper presents a focused
investigation on the kinematic and ergonomic design principles for a
laparoscopic surgical robotic arm aimed at high-precision tasks. We propose a
7-degree-of-freedom (7-DOF) robotic arm system that incorporates a remote
center of motion (RCM) at the instrument insertion point and ergonomic
considerations to improve surgeon interaction. The design is implemented on a
general-purpose robotic platform, and a series of simulated surgical tasks were
performed to evaluate targeting accuracy, task efficiency, and surgeon comfort
compared to conventional manual laparoscopy. Experimental results demonstrate
that the optimized robotic design achieves significantly improved targeting
accuracy (error reduced by over 50%) and shorter task completion times, while
substantially lowering operator muscle strain and discomfort. These findings
validate the importance of kinematic optimization (such as added articulations
and tremor filtering) and human-centered ergonomic design in enhancing the
performance of robot-assisted surgery. The insights from this work can guide
the development of next-generation surgical robots that improve surgical
outcomes and ergonomics for the operating team.

</details>


### [8] [A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms](https://arxiv.org/abs/2511.02192)
*Linxin Hou,Qirui Wu,Zhihang Qin,Neil Banerjee,Yongxin Guo,Cecilia Laschi*

Main category: cs.RO

TL;DR: 本文比较了集中式和分布式多智能体强化学习在控制模拟中的软机器人手臂时的性能，结果显示随着控制段数n的增加，分布式策略展现出更高的样本效率、成功率及鲁棒性，但集中式策略在训练时间上更有效率。


<details>
  <summary>Details</summary>
Motivation: 为了研究在软体机器人系统中基于强化学习的控制方法，特别是比较集中式与分布式多智能体强化学习架构在不同条件下的表现差异。

Method: 使用PyElastica和OpenAI Gym接口，对一个建模为Cosserat杆的软体机器人手臂进行训练，分别采用全局近端策略优化（PPO）控制器和多智能体PPO（MAPPO），并针对不同数量的受控部分n，在三种场景下评估其性能：默认基线条件、从外部干扰中恢复以及适应执行器故障。

Result: 当控制段数n≤4时，分布式策略没有显著优势；对于非常简单的系统(n≤2)，集中式策略优于分布式；当n增加到4<n≤12时，分布式策略表现出高样本效率，并且在局部可观测条件下具有更强的成功率、韧性和鲁棒性；然而，集中式策略在训练过程中实现了更高的时间效率。

Conclusion: 本研究表明，在软体机器人系统中应用强化学习控制时，集中式与分布式策略之间存在权衡，包括样本效率、成功率、鲁棒性以及训练时间等多方面因素，为未来从模拟到实际应用提供了设计指导。

Abstract: This paper presents a quantitative comparison between centralised and
distributed multi-agent reinforcement learning (MARL) architectures for
controlling a soft robotic arm modelled as a Cosserat rod in simulation. Using
PyElastica and the OpenAI Gym interface, we train both a global Proximal Policy
Optimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical
budgets. Both approaches are based on the arm having $n$ number of controlled
sections. The study systematically varies $n$ and evaluates the performance of
the arm to reach a fixed target in three scenarios: default baseline condition,
recovery from external disturbance, and adaptation to actuator failure.
Quantitative metrics used for the evaluation are mean action magnitude, mean
final distance, mean episode length, and success rate. The results show that
there are no significant benefits of the distributed policy when the number of
controlled sections $n\le4$. In very simple systems, when $n\le2$, the
centralised policy outperforms the distributed one. When $n$ increases to $4<
n\le 12$, the distributed policy shows a high sample efficiency. In these
systems, distributed policy promotes a stronger success rate, resilience, and
robustness under local observability and yields faster convergence given the
same sample size. However, centralised policies achieve much higher time
efficiency during training as it takes much less time to train the same size of
samples. These findings highlight the trade-offs between centralised and
distributed policy in reinforcement learning-based control for soft robotic
systems and provide actionable design guidance for future sim-to-real transfer
in soft rod-like manipulators.

</details>


### [9] [LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation](https://arxiv.org/abs/2511.02239)
*Youngjin Hong,Houjian Yu,Mingen Li,Changhyun Choi*

Main category: cs.RO

TL;DR: 本文提出LACY框架，通过学习语言到动作（L2A）和动作到语言（A2L）的双向映射以及验证两种语言描述之间的一致性（L2C），提高了机器人操作任务的成功率，并增强了语言-动作的理解。


<details>
  <summary>Details</summary>
Motivation: 当前从语言指令到动作的单向范式限制了策略的泛化能力和行为解释能力。为了克服这一局限，作者认为需要开发一种能够将动作反向映射回语言的能力，以形成更丰富的内部表征并开启新的自监督学习范式。

Method: 提出了LACY（Language-Action Cycle）框架，在一个单一视觉-语言模型中学习双向映射。LACY联合训练三项协同任务：从语言生成参数化动作（L2A）、用语言解释观察到的动作（A2L）以及验证两个语言描述之间的语义一致性（L2C）。此外，它还通过主动增强策略自动生成和筛选新训练数据，特别针对低置信度案例，从而无需额外的人工标签即可改善模型。

Result: 在仿真环境和真实世界的取放任务实验中，LACY平均提高了56.46%的任务成功率，并为机器人操作提供了更加稳健的语言-动作基础。

Conclusion: LACY框架通过引入动作到语言的映射能力，不仅提高了机器人执行任务的表现，同时也增强了系统对自身行为的理解和解释能力，为自监督学习开辟了新的方向。

Abstract: Learning generalizable policies for robotic manipulation increasingly relies
on large-scale models that map language instructions to actions (L2A). However,
this one-way paradigm often produces policies that execute tasks without deeper
contextual understanding, limiting their ability to generalize or explain their
behavior. We argue that the complementary skill of mapping actions back to
language (A2L) is essential for developing more holistic grounding. An agent
capable of both acting and explaining its actions can form richer internal
representations and unlock new paradigms for self-supervised learning. We
introduce LACY (Language-Action Cycle), a unified framework that learns such
bidirectional mappings within a single vision-language model. LACY is jointly
trained on three synergistic tasks: generating parameterized actions from
language (L2A), explaining observed actions in language (A2L), and verifying
semantic consistency between two language descriptions (L2C). This enables a
self-improving cycle that autonomously generates and filters new training data
through an active augmentation strategy targeting low-confidence cases, thereby
improving the model without additional human labels. Experiments on
pick-and-place tasks in both simulation and the real world show that LACY
improves task success rates by 56.46% on average and yields more robust
language-action grounding for robotic manipulation. Project page:
https://vla2026.github.io/LACY/

</details>


### [10] [SuckTac: Camera-based Tactile Sucker for Unstructured Surface Perception and Interaction](https://arxiv.org/abs/2511.02294)
*Ruiyong Yuan,Jieji Ren,Zhanxuan Peng,Feifei Chen,Guoying Gu*

Main category: cs.RO

TL;DR: 本文提出了一种新型智能吸盘SuckTac，它将基于摄像头的触觉传感器直接集成到其优化结构中，提供了高密度感知和强大的吸附能力。通过一系列实验验证了该系统的优越性能和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数吸盘缺乏高保真的感知和触觉传感功能，这阻碍了它们解决目标表面的精细几何特征和交互状态的问题。这种局限性限制了它们在处理不规则物体和复杂、非结构化环境中的稳健表现。

Method: 受到头足类动物吸盘自适应结构和高性能感官能力的启发，本文提出了一个名为SuckTac的新颖智能吸盘。通过联合结构设计与优化以及多材料集成铸造技术，将摄像头和光源嵌入吸盘内，实现了对诸如表面形状、纹理和粗糙度等细节的现场高密度感知。此外，通过对吸盘机械设计进行优化——改进外形、增加柔顺唇边及结合表面微结构——进一步增强了其鲁棒性和适应性。

Result: 广泛的实验，包括机器人布料操作和软体移动机器人检查等挑战性任务，证明了所提系统具有优越的性能和广泛的适用性。

Conclusion: SuckTac作为一种集成了高密度感知能力的智能吸盘，在处理不规则对象和应对复杂环境方面展现出色的表现力，为机器人领域带来了新的可能性。

Abstract: Suckers are significant for robots in picking, transferring, manipulation and
locomotion on diverse surfaces. However, most of the existing suckers lack
high-fidelity perceptual and tactile sensing, which impedes them from resolving
the fine-grained geometric features and interaction status of the target
surface. This limits their robust performance with irregular objects and in
complex, unstructured environments. Inspired by the adaptive structure and
high-performance sensory capabilities of cephalopod suckers, in this paper, we
propose a novel, intelligent sucker, named SuckTac, that integrates a
camera-based tactile sensor directly within its optimized structure to provide
high-density perception and robust suction. Specifically, through joint
structure design and optimization and based on a multi-material integrated
casting technique, a camera and light source are embedded into the sucker,
which enables in-situ, high-density perception of fine details like surface
shape, texture and roughness. To further enhance robustness and adaptability,
the sucker's mechanical design is also optimized by refining its profile,
adding a compliant lip, and incorporating surface microstructure. Extensive
experiments, including challenging tasks such as robotic cloth manipulation and
soft mobile robot inspection, demonstrate the superior performance and broad
applicability of the proposed system.

</details>


### [11] [ZJUNlict Extended Team Description Paper 2025](https://arxiv.org/abs/2511.02315)
*Zifei Wu,Lijie Wang,Zhe Yang,Shijie Yang,Liang Wang,Haoran Fu,Yinliang Cai,Rong Xiong*

Main category: cs.RO

TL;DR: ZJUNlict团队在过去一年中在硬件和软件方面都取得了进展，包括将IMU集成到v2023机器人以提高姿态准确性和角速度规划，并优化了策略和CUDA模块来提高决策效率、追球预测及持球预测，从而更好地适应快节奏的比赛。


<details>
  <summary>Details</summary>
Motivation: 为了提高机器人足球比赛中机器人的性能，特别是针对姿态准确性、角速度规划以及应对快速变化的比赛环境下的决策效率与球的追踪和控制能力。

Method: 通过将惯性测量单元（IMU）整合进v2023版本的机器人来增强其姿态感知；同时对软件部分的关键模块如策略制定与CUDA加速计算模块进行了优化升级。

Result: 实现了更精确的姿态控制和角速度规划能力；提高了软件层面关于决策效率、球追踪预测及持球状态预测的表现。

Conclusion: ZJUNlict团队通过硬件上的IMU集成与软件上关键算法模块的优化显著提升了机器人在复杂动态环境下执行任务的能力。

Abstract: This paper presents the ZJUNlict team's work over the past year, covering
both hardware and software advancements. In the hardware domain, the
integration of an IMU into the v2023 robot was completed to enhance posture
accuracy and angular velocity planning. On the software side, key modules were
optimized, including the strategy and CUDA modules, with significant
improvements in decision making efficiency, ball pursuit prediction, and ball
possession prediction to adapt to high-tempo game dynamics.

</details>


### [12] [Whole-body motion planning and safety-critical control for aerial manipulation](https://arxiv.org/abs/2511.02342)
*Lin Yang,Jinwoo Lee,Domenico Campolo,H. Jin Kim,Jeonghyun Byun*

Main category: cs.RO

TL;DR: 本文提出了一种基于超二次曲面的全机身运动规划和安全关键控制框架，用于空中操作器。该方法通过融合Voronoi图和平衡流形公式生成平滑且具有防碰撞意识的轨迹，并设计了一个安全关键控制器来共同执行推力限制和碰撞避免。仿真结果表明，该方法在杂乱环境中比基于采样的规划器产生更快、更安全、更平滑的轨迹，并且在几何保真度上优于基于椭球体的方法。实际实验验证了其可行性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 空中操作结合了多旋翼的机动性和机器人手臂的灵巧性，以在杂乱空间中执行复杂任务。然而，由于全身碰撞避免以及常用几何抽象（如边界框或椭球体）的保守性，规划安全、动态可行的轨迹仍然困难。

Method: 本研究使用SQ-plus-proxy表示法，利用可微分且几何精度高的表面建模车辆和障碍物。通过将Voronoi图与平衡流形公式相结合，引入最大间隙规划器来生成平滑且具有防碰撞意识的轨迹。同时设计了一种安全关键控制器，它通过高阶控制屏障函数共同执行推力限制和碰撞避免。

Result: 仿真结果显示，在杂乱环境中，所提方法相较于基于采样的规划器能生成更快、更安全及更平滑的轨迹，并且在几何保真度方面超越了基于椭球体的方法。实际硬件实验进一步证实了该方法的可行性和鲁棒性。

Conclusion: 这项工作为解决空中操作器在复杂环境中的安全高效导航问题提供了有效解决方案，证明了基于超二次曲面的全机身规划和控制框架的有效性。

Abstract: Aerial manipulation combines the maneuverability of multirotors with the
dexterity of robotic arms to perform complex tasks in cluttered spaces. Yet
planning safe, dynamically feasible trajectories remains difficult due to
whole-body collision avoidance and the conservativeness of common geometric
abstractions such as bounding boxes or ellipsoids. We present a whole-body
motion planning and safety-critical control framework for aerial manipulators
built on superquadrics (SQs). Using an SQ-plus-proxy representation, we model
both the vehicle and obstacles with differentiable, geometry-accurate surfaces.
Leveraging this representation, we introduce a maximum-clearance planner that
fuses Voronoi diagrams with an equilibrium-manifold formulation to generate
smooth, collision-aware trajectories. We further design a safety-critical
controller that jointly enforces thrust limits and collision avoidance via
high-order control barrier functions. In simulation, our approach outperforms
sampling-based planners in cluttered environments, producing faster, safer, and
smoother trajectories and exceeding ellipsoid-based baselines in geometric
fidelity. Actual experiments on a physical aerial-manipulation platform confirm
feasibility and robustness, demonstrating consistent performance across
simulation and hardware settings. The video can be found at
https://youtu.be/hQYKwrWf1Ak.

</details>


### [13] [Non-Contact Manipulation of Induced Magnetic Dipoles](https://arxiv.org/abs/2511.02761)
*Seth Stewart,Joseph Pawelski,Steve Ward,Andrew J. Petruska*

Main category: cs.RO

TL;DR: 该研究展示了通过使用振荡磁场对导电非磁性物体（如铝球）进行闭环位置控制的技术，这为太空垃圾回收等应用开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 扩展磁操作到导电非磁性物体上，使得像太空垃圾这样的材料能够被回收利用成为可能，尤其是在空间环境中，感应磁操作产生的力较小但非常适用。

Method: 基于之前的工作中利用涡电流产生的相反磁矩实现3D开环位置控制的基础上，本研究进一步实现了半浮式铝球的闭环位置控制，并探讨了不同力反演方法的有效性。

Result: 成功展示了对于由感应产生的磁偶极子进行三自由度位置控制的闭环方法，这是朝向更广泛应用迈出的关键第一步。

Conclusion: 通过闭环保证了对导电非磁性物体的位置控制更加精确有效，为将来在更多领域内实现此类物体的3-DOF位置控制奠定了基础。

Abstract: Extending the field of magnetic manipulation to conductive, non-magnetic
objects opens the door for a wide array of applications previously limited to
hard or soft magnetic materials. Of particular interest is the recycling of
space debris through the use of oscillating magnetic fields, which represent a
cache of raw materials in an environment particularly suited to the low forces
generated from inductive magnetic manipulation. Building upon previous work
that demonstrated 3D open-loop position control by leveraging the opposing
dipole moment created from induced eddy currents, this work demonstrates
closed-loop position control of a semi-buoyant aluminum sphere in lab tests,
and the efficacy of varying methods for force inversion is explored. The
closed-loop methods represent a critical first step towards wider applications
for 3-DOF position control of induced magnetic dipoles.

</details>


### [14] [XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations](https://arxiv.org/abs/2511.02776)
*Shichao Fan,Kun Wu,Zhengping Che,Xinhua Wang,Di Wu,Fei Liao,Ning Liu,Yixue Zhang,Zhen Zhao,Zhiyuan Xu,Meng Li,Qingjie Liu,Shanghang Zhang,Min Wan,Jian Tang*

Main category: cs.RO

TL;DR: 本文提出了一种新的框架XR-1，用于跨多种机器人、任务和环境的多功能且可扩展的视觉-语言-动作（VLA）学习。通过引入统一视觉-运动代码（UVMC），该框架解决了从高维观测中生成精确低级动作以及跨越异构数据源域差距的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型在从高维观察中产生精确低级别动作及解决不同数据源之间的领域差异方面仍存在挑战。目前的方法通常仅编码来自视觉动态或机器人动作的潜在变量来指导策略学习，但未能充分利用大规模异构数据集中存在的互补多模态知识。

Method: 提出了XR-1框架，它利用一种称为统一视觉-运动代码(UVMC)的离散潜在表示，这种表示是通过一个双分支VQ-VAE学习得到的，可以联合编码视觉动态与机器人运动。为有效利用UVMC，还设计了一个三阶段训练范式：(i)自监督UVMC学习，(ii)基于大规模跨实体机器人数据集上的UVMC引导预训练，以及(iii)针对特定任务的后训练。

Result: 通过对六个不同机器人实体超过14,000次实验，涵盖了超过120种不同的操作任务，在现实世界实验中验证了XR-1。XR-1在处理新对象、背景变化、干扰因素及光照条件变化时表现出强大的泛化能力，并且始终优于包括π_0.5、π_0、RDT、UniVLA和GR00T-N1.5在内的最先进基线方法。

Conclusion: XR-1提供了一种有效的解决方案，能够克服现有VLA模型面临的两大挑战，即从高维度观察中生成精确的动作指令和弥合来自不同数据源间的领域鸿沟。其通过引入UVMC作为中间表征并采用三阶段训练流程，实现了对多样机器人平台、任务类型和工作环境的良好适应性。

Abstract: Recent progress in large-scale robotic datasets and vision-language models
(VLMs) has advanced research on vision-language-action (VLA) models. However,
existing VLA models still face two fundamental challenges: (i) producing
precise low-level actions from high-dimensional observations, (ii) bridging
domain gaps across heterogeneous data sources, including diverse robot
embodiments and human demonstrations. Existing methods often encode latent
variables from either visual dynamics or robotic actions to guide policy
learning, but they fail to fully exploit the complementary multi-modal
knowledge present in large-scale, heterogeneous datasets. In this work, we
present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable
VLA learning across diverse robots, tasks, and environments. XR-1 introduces
the \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation
learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and
robotic motion. UVMC addresses these challenges by (i) serving as an
intermediate representation between the observations and actions, and (ii)
aligning multimodal dynamic information from heterogeneous data sources to
capture complementary knowledge. To effectively exploit UVMC, we propose a
three-stage training paradigm: (i) self-supervised UVMC learning, (ii)
UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and
(iii) task-specific post-training. We validate XR-1 through extensive
real-world experiments with more than 14,000 rollouts on six different robot
embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently
outperforms state-of-the-art baselines such as $\pi_{0.5}$, $\pi_0$, RDT,
UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel
objects, background variations, distractors, and illumination changes. Our
project is at https://xr-1-vla.github.io/.

</details>


### [15] [TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System](https://arxiv.org/abs/2511.02832)
*Yanjie Ze,Siheng Zhao,Weizhuo Wang,Angjoo Kanazawa,Rocky Duan,Pieter Abbeel,Guanya Shi,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: 本研究引入了TWIST2，一个便携且无需动捕的人形机器人遥操作和数据收集系统，能够实现实时全身人类动作的获取，并支持基于自我中心视觉的全身体灵巧控制。该系统还提出了一种分层视运动策略框架，可以基于自我中心视觉自主控制整个人形机器人的身体。


<details>
  <summary>Details</summary>
Motivation: 人形机器人学缺乏有效的数据收集框架，现有的人形遥操作系统要么使用解耦控制，要么依赖昂贵的动作捕捉设备。为了改进这一状况并提高可扩展性，研究人员开发了新的解决方案。

Method: 通过利用PICO4U VR获得实时全身人类动作，并配备了一个定制的2自由度机器人颈部（成本约250美元）用于自我中心视觉，从而实现从人到人形机器人的全面控制。此外，提出了一个基于自我中心视觉来自主控制整个人形机器人身体的分层视运动策略框架。

Result: 能够展示长时间范围内的灵巧及移动式人形技能，并能在15分钟内以接近100%的成功率收集100个演示。提出的视运动策略成功展示了全身体灵巧操控与动态踢腿任务。

Conclusion: TWIST2提供了一个有效、可扩展的人形机器人遥操作与数据收集方案，有助于推动相关领域的发展。整个系统是完全可复现的，并已开源。

Abstract: Large-scale data has driven breakthroughs in robotics, from language models
to vision-language-action models in bimanual manipulation. However, humanoid
robotics lacks equally effective data collection frameworks. Existing humanoid
teleoperation systems either use decoupled control or depend on expensive
motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid
teleoperation and data collection system that preserves full whole-body control
while advancing scalability. Our system leverages PICO4U VR for obtaining
real-time whole-body human motions, with a custom 2-DoF robot neck (cost around
$250) for egocentric vision, enabling holistic human-to-humanoid control. We
demonstrate long-horizon dexterous and mobile humanoid skills and we can
collect 100 demonstrations in 15 minutes with an almost 100% success rate.
Building on this pipeline, we propose a hierarchical visuomotor policy
framework that autonomously controls the full humanoid body based on egocentric
vision. Our visuomotor policy successfully demonstrates whole-body dexterous
manipulation and dynamic kicking tasks. The entire system is fully reproducible
and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also
open-sourced at https://twist-data.github.io .

</details>
