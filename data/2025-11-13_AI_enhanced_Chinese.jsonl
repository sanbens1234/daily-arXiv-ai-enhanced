{"id": "2511.08694", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08694", "abs": "https://arxiv.org/abs/2511.08694", "authors": ["Leonard Jung", "Alan Papalia", "Kevin Doherty", "Michael Everett"], "title": "Practical and Performant Enhancements for Maximization of Algebraic Connectivity", "comment": "Submitted to ICRA 2026", "summary": "Long-term state estimation over graphs remains challenging as current graph estimation methods scale poorly on large, long-term graphs. To address this, our work advances a current state-of-the-art graph sparsification algorithm, maximizing algebraic connectivity (MAC). MAC is a sparsification method that preserves estimation performance by maximizing the algebraic connectivity, a spectral graph property that is directly connected to the estimation error. Unfortunately, MAC remains computationally prohibitive for online use and requires users to manually pre-specify a connectivity-preserving edge set. Our contributions close these gaps along three complementary fronts: we develop a specialized solver for algebraic connectivity that yields an average 2x runtime speedup; we investigate advanced step size strategies for MAC's optimization procedure to enhance both convergence speed and solution quality; and we propose automatic schemes that guarantee graph connectivity without requiring manual specification of edges. Together, these contributions make MAC more scalable, reliable, and suitable for real-time estimation applications.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6539\u8fdb\u4e00\u79cd\u540d\u4e3aMAC\u7684\u56fe\u7a00\u758f\u5316\u7b97\u6cd5\uff0c\u63d0\u9ad8\u4e86\u957f\u671f\u56fe\u72b6\u6001\u4f30\u8ba1\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002\u6539\u8fdb\u63aa\u65bd\u5305\u62ec\u5f00\u53d1\u4e13\u95e8\u6c42\u89e3\u5668\u3001\u7814\u7a76\u9ad8\u7ea7\u6b65\u957f\u7b56\u7565\u4ee5\u53ca\u63d0\u51fa\u81ea\u52a8\u65b9\u6848\u4fdd\u8bc1\u56fe\u8fde\u901a\u6027\uff0c\u4ece\u800c\u4f7f\u5f97MAC\u66f4\u52a0\u9002\u5408\u5b9e\u65f6\u4f30\u8ba1\u5e94\u7528\u3002", "motivation": "\u957f\u671f\u56fe\u4e0a\u7684\u72b6\u6001\u4f30\u8ba1\u7531\u4e8e\u5f53\u524d\u56fe\u4f30\u8ba1\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u3001\u957f\u671f\u56fe\u4e0a\u8868\u73b0\u4e0d\u4f73\u800c\u9762\u4e34\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u9009\u62e9\u63a8\u8fdb\u4e00\u79cd\u79f0\u4e3a\u6700\u5927\u5316\u4ee3\u6570\u8fde\u901a\u5ea6\uff08MAC\uff09\u7684\u73b0\u6709\u6700\u5148\u8fdb\u7684\u56fe\u7a00\u758f\u5316\u7b97\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u4f5c\u8005\u4ece\u4e09\u4e2a\u65b9\u9762\u8fdb\u884c\u4e86\u6539\u8fdb\uff1a1. \u5f00\u53d1\u4e86\u4e00\u4e2a\u9488\u5bf9\u4ee3\u6570\u8fde\u901a\u5ea6\u7684\u4e13\u4e1a\u6c42\u89e3\u5668\uff0c\u5e73\u5747\u8fd0\u884c\u65f6\u95f4\u7f29\u77ed\u4e86\u4e00\u534a\uff1b2. \u7814\u7a76\u4e86MAC\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u9ad8\u7ea7\u6b65\u957f\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff1b3. \u63d0\u51fa\u4e86\u53ef\u4ee5\u4fdd\u8bc1\u56fe\u8fde\u63a5\u6027\u7684\u81ea\u52a8\u5316\u65b9\u6848\uff0c\u65e0\u9700\u7528\u6237\u624b\u52a8\u6307\u5b9a\u4fdd\u6301\u8fde\u63a5\u6027\u7684\u8fb9\u96c6\u3002", "result": "\u8fd9\u4e9b\u8d21\u732e\u5171\u540c\u4f5c\u7528\u4e0b\uff0cMAC\u53d8\u5f97\u66f4\u52a0\u53ef\u6269\u5c55\u3001\u53ef\u9760\uff0c\u5e76\u4e14\u66f4\u9002\u5408\u4e8e\u5b9e\u65f6\u4f30\u8ba1\u5e94\u7528\u7a0b\u5e8f\u3002", "conclusion": "\u7efc\u4e0a\u6240\u8ff0\uff0c\u901a\u8fc7\u4e0a\u8ff0\u6539\u8fdb\u63aa\u65bd\uff0cMAC\u4e0d\u4ec5\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\uff0c\u800c\u4e14\u4e5f\u53d8\u5f97\u66f4\u4e3a\u7528\u6237\u53cb\u597d\uff0c\u51cf\u5c11\u4e86\u5bf9\u4eba\u5de5\u5e72\u9884\u7684\u9700\u6c42\uff0c\u4ece\u800c\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2511.08732", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08732", "abs": "https://arxiv.org/abs/2511.08732", "authors": ["Marta Lagomarsino", "Elena Merlo", "Andrea Pupa", "Timo Birr", "Franziska Krebs", "Cristian Secchi", "Tamim Asfour", "Arash Ajoudani"], "title": "Intuitive Programming, Adaptive Task Planning, and Dynamic Role Allocation in Human-Robot Collaboration", "comment": "Published in the Annual Review of Control, Robotics, and Autonomous Systems, Volume 9; copyright 2026 the author(s), CC BY 4.0, https://www.annualreviews.org", "summary": "Remarkable capabilities have been achieved by robotics and AI, mastering complex tasks and environments. Yet, humans often remain passive observers, fascinated but uncertain how to engage. Robots, in turn, cannot reach their full potential in human-populated environments without effectively modeling human states and intentions and adapting their behavior. To achieve a synergistic human-robot collaboration (HRC), a continuous information flow should be established: humans must intuitively communicate instructions, share expertise, and express needs. In parallel, robots must clearly convey their internal state and forthcoming actions to keep users informed, comfortable, and in control. This review identifies and connects key components enabling intuitive information exchange and skill transfer between humans and robots. We examine the full interaction pipeline: from the human-to-robot communication bridge translating multimodal inputs into robot-understandable representations, through adaptive planning and role allocation, to the control layer and feedback mechanisms to close the loop. Finally, we highlight trends and promising directions toward more adaptive, accessible HRC.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5b9e\u73b0\u76f4\u89c2\u4fe1\u606f\u4ea4\u6362\u548c\u6280\u80fd\u8f6c\u79fb\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u65e8\u5728\u4fc3\u8fdb\u4eba\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u534f\u540c\u5408\u4f5c\u3002\u6587\u7ae0\u4ece\u591a\u6a21\u6001\u8f93\u5165\u5230\u673a\u5668\u4eba\u53ef\u7406\u89e3\u7684\u8868\u793a\u8f6c\u6362\u3001\u81ea\u9002\u5e94\u89c4\u5212\u548c\u89d2\u8272\u5206\u914d\uff0c\u518d\u5230\u63a7\u5236\u5c42\u548c\u53cd\u9988\u673a\u5236\uff0c\u5168\u9762\u5ba1\u89c6\u4e86\u4ea4\u4e92\u6d41\u7a0b\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u66f4\u9002\u5e94\u6027\u3001\u66f4\u6613\u8bbf\u95ee\u7684\u4eba\u673a\u534f\u4f5c\u8d8b\u52bf\u548c\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u4eba\u6280\u672f\u548c\u4eba\u5de5\u667a\u80fd\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u8457\u6210\u5c31\uff0c\u4f46\u4eba\u7c7b\u5f80\u5f80\u4ecd\u5904\u4e8e\u88ab\u52a8\u89c2\u5bdf\u8005\u7684\u89d2\u8272\uff0c\u800c\u673a\u5668\u4eba\u5728\u6709\u4eba\u7c7b\u5b58\u5728\u7684\u73af\u5883\u4e2d\u4e5f\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\uff0c\u9664\u975e\u80fd\u591f\u6709\u6548\u5730\u5efa\u6a21\u4eba\u7c7b\u7684\u72b6\u6001\u548c\u610f\u56fe\u5e76\u8c03\u6574\u81ea\u8eab\u884c\u4e3a\u3002\u4e3a\u4e86\u8fbe\u5230\u534f\u540c\u7684\u4eba\u673a\u534f\u4f5c\uff08HRC\uff09\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u6301\u7eed\u7684\u4fe1\u606f\u6d41\uff1a\u4eba\u7c7b\u5fc5\u987b\u80fd\u591f\u76f4\u89c2\u5730\u4f20\u8fbe\u6307\u4ee4\u3001\u5206\u4eab\u4e13\u4e1a\u77e5\u8bc6\u5e76\u8868\u8fbe\u9700\u6c42\uff1b\u540c\u65f6\uff0c\u673a\u5668\u4eba\u4e5f\u5fc5\u987b\u6e05\u695a\u5730\u4f20\u8fbe\u5176\u5185\u90e8\u72b6\u6001\u53ca\u5373\u5c06\u91c7\u53d6\u7684\u52a8\u4f5c\uff0c\u4ee5\u786e\u4fdd\u7528\u6237\u611f\u5230\u77e5\u60c5\u3001\u8212\u9002\u4e14\u638c\u63a7\u5c40\u9762\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u56de\u987e\u5206\u6790\u65b9\u6cd5\uff0c\u786e\u5b9a\u5e76\u8fde\u63a5\u8d77\u4f7f\u4eba\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u80fd\u591f\u8fdb\u884c\u76f4\u89c2\u4fe1\u606f\u4ea4\u6362\u548c\u6280\u80fd\u4f20\u9012\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002\u6574\u4e2a\u4e92\u52a8\u6d41\u7a0b\u5305\u62ec\u4ece\u5c06\u591a\u6a21\u5f0f\u8f93\u5165\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u53ef\u7406\u89e3\u7684\u8868\u73b0\u5f62\u5f0f\uff0c\u5230\u81ea\u9002\u5e94\u89c4\u5212\u4e0e\u89d2\u8272\u5206\u914d\uff0c\u76f4\u81f3\u63a7\u5236\u7cfb\u7edf\u4ee5\u53ca\u53cd\u9988\u673a\u5236\u6765\u95ed\u5408\u5faa\u73af\u3002", "result": "\u8bba\u6587\u8bc6\u522b\u51fa\u4e86\u4e00\u7cfb\u5217\u652f\u6301\u76f4\u89c2\u4eba\u673a\u4ea4\u6d41\u7684\u5173\u952e\u8981\u7d20\uff0c\u5e76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u8fd9\u4e9b\u7ec4\u4ef6\u4fc3\u8fdb\u66f4\u81ea\u7136\u3001\u6709\u6548\u7684\u53cc\u5411\u6c9f\u901a\u3002\u6b64\u5916\uff0c\u8fd8\u8ba8\u8bba\u4e86\u5f53\u524d\u9886\u57df\u5185\u7684\u4e3b\u8981\u8d8b\u52bf\u4ee5\u53ca\u5bf9\u4e8e\u66f4\u52a0\u9002\u5e94\u6027\u5f3a\u3001\u6613\u4e8e\u63a5\u89e6\u7684\u4eba\u673a\u5408\u4f5c\u672a\u6765\u7684\u5c55\u671b\u3002", "conclusion": "\u5b9e\u73b0\u9ad8\u6548\u7684\u4eba\u673a\u534f\u4f5c\u4f9d\u8d56\u4e8e\u5efa\u7acb\u4e00\u5957\u5141\u8bb8\u76f4\u89c2\u4fe1\u606f\u4ea4\u6362\u4e0e\u6280\u80fd\u8f6c\u79fb\u7684\u4f53\u7cfb\u3002\u8fd9\u4e0d\u4ec5\u8981\u6c42\u5f00\u53d1\u51fa\u80fd\u591f\u51c6\u786e\u89e3\u6790\u4eba\u7c7b\u610f\u56fe\u7684\u6280\u672f\uff0c\u4e5f\u9700\u8981\u8bbe\u8ba1\u51fa\u80fd\u591f\u8ba9\u673a\u5668\u4eba\u6e05\u6670\u8868\u8fbe\u81ea\u5df1\u72b6\u6001\u4e0e\u8ba1\u5212\u7684\u65b9\u6cd5\u3002\u968f\u7740\u6280\u672f\u8fdb\u6b65\uff0c\u672a\u6765\u7684\u4eba\u673a\u534f\u4f5c\u5c06\u53d8\u5f97\u66f4\u52a0\u7075\u6d3b\u3001\u5305\u5bb9\u3002"}}
{"id": "2511.08741", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.08741", "abs": "https://arxiv.org/abs/2511.08741", "authors": ["Kai S. Yun", "Navid Azizan"], "title": "ATOM-CBF: Adaptive Safe Perception-Based Control under Out-of-Distribution Measurements", "comment": null, "summary": "Ensuring the safety of real-world systems is challenging, especially when they rely on learned perception modules to infer the system state from high-dimensional sensor data. These perception modules are vulnerable to epistemic uncertainty, often failing when encountering out-of-distribution (OoD) measurements not seen during training. To address this gap, we introduce ATOM-CBF (Adaptive-To-OoD-Measurement Control Barrier Function), a novel safe control framework that explicitly computes and adapts to the epistemic uncertainty from OoD measurements, without the need for ground-truth labels or information on distribution shifts. Our approach features two key components: (1) an OoD-aware adaptive perception error margin and (2) a safety filter that integrates this adaptive error margin, enabling the filter to adjust its conservatism in real-time. We provide empirical validation in simulations, demonstrating that ATOM-CBF maintains safety for an F1Tenth vehicle with LiDAR scans and a quadruped robot with RGB images.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aATOM-CBF\u7684\u65b0\u5b89\u5168\u63a7\u5236\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u8ba1\u7b97\u5e76\u9002\u5e94\u611f\u77e5\u6a21\u5757\u4e2d\u7531\u672a\u77e5\u5206\u5e03(OoD)\u6d4b\u91cf\u5f15\u8d77\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u5728\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u6216\u5206\u5e03\u53d8\u5316\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u3002\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u8bc1\u660e\u4e86ATOM-CBF\u5bf9\u4e8e\u88c5\u5907\u6709LiDAR\u7684F1Tenth\u8f66\u8f86\u548c\u4f7f\u7528RGB\u56fe\u50cf\u7684\u56db\u8db3\u673a\u5668\u4eba\u662f\u6709\u6548\u7684\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u4e2d\uff0c\u4f9d\u8d56\u4e8e\u5b66\u4e60\u611f\u77e5\u6a21\u5757\u4ece\u9ad8\u7ef4\u4f20\u611f\u5668\u6570\u636e\u63a8\u65ad\u7cfb\u7edf\u72b6\u6001\u7684\u505a\u6cd5\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9047\u5230\u8bad\u7ec3\u9636\u6bb5\u672a\u66fe\u89c1\u8fc7\u7684\u672a\u77e5\u5206\u5e03(OoD)\u6d4b\u91cf\u65f6\uff0c\u8fd9\u4e9b\u611f\u77e5\u6a21\u5757\u5bb9\u6613\u53d7\u5230\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u8d1f\u9762\u5f71\u54cd\u800c\u5931\u6548\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u5f15\u5165\u4e86\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e86ATOM-CBF\uff08\u81ea\u9002\u5e94\u4e8e\u672a\u77e5\u5206\u5e03\u6d4b\u91cf\u7684\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff09\uff0c\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u63a7\u5236\u6846\u67b6\uff0c\u5b83\u80fd\u660e\u786e\u5730\u8ba1\u7b97\u5e76\u5bf9OoD\u6d4b\u91cf\u5f15\u8d77\u7684\u77e5\u8bc6\u4e0d\u786e\u5b9a\u6027\u505a\u51fa\u53cd\u5e94\uff0c\u4e0d\u9700\u8981\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u6216\u5206\u5e03\u8f6c\u79fb\u7684\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff1a(1) \u4e00\u4e2a\u5bf9OoD\u654f\u611f\u7684\u81ea\u9002\u5e94\u611f\u77e5\u8bef\u5dee\u8303\u56f4\uff1b(2) \u4e00\u4e2a\u6574\u5408\u4e86\u8fd9\u79cd\u81ea\u9002\u5e94\u8bef\u5dee\u8303\u56f4\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u5141\u8bb8\u8fc7\u6ee4\u5668\u5b9e\u65f6\u8c03\u6574\u5176\u4fdd\u5b88\u7a0b\u5ea6\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660eATOM-CBF\u80fd\u591f\u5728\u9762\u5bf9\u88c5\u5907LiDAR\u7684F1Tenth\u8f66\u8f86\u53ca\u4f7f\u7528RGB\u56fe\u50cf\u7684\u56db\u8db3\u673a\u5668\u4eba\u65f6\u4fdd\u6301\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u91c7\u7528ATOM-CBF\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5904\u7406\u56e0\u672a\u77e5\u5206\u5e03\u6570\u636e\u5bfc\u81f4\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u8fdb\u800c\u589e\u5f3a\u57fa\u4e8e\u5b66\u4e60\u7684\u611f\u77e5\u7ec4\u4ef6\u4e4b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2511.08771", "categories": ["cs.RO", "cs.CE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2511.08771", "abs": "https://arxiv.org/abs/2511.08771", "authors": ["Vince Kurtz", "Alejandro Castro"], "title": "CENIC: Convex Error-controlled Numerical Integration for Contact", "comment": "18 pages with 19 figures. Submitted to IEEE Transactions on Robotics (T-RO). The supplemental video is available publicly at https://www.youtube.com/watch?v=9ZZ15MfCgtI", "summary": "State-of-the-art robotics simulators operate in discrete time. This requires users to choose a time step, which is both critical and challenging: large steps can produce non-physical artifacts, while small steps force the simulation to run slowly. Continuous-time error-controlled integration avoids such issues by automatically adjusting the time step to achieve a desired accuracy. But existing error-controlled integrators struggle with the stiff dynamics of contact, and cannot meet the speed and scalability requirements of modern robotics workflows. We introduce CENIC, a new continuous-time integrator that brings together recent advances in convex time-stepping and error-controlled integration, inheriting benefits from both continuous integration and discrete time-stepping. CENIC runs at fast real-time rates comparable to discrete-time robotics simulators like MuJoCo, Drake and Isaac Sim, while also providing guarantees on accuracy and convergence.", "AI": {"tldr": "CENIC, a novel continuous-time integrator, combines the advantages of continuous integration and discrete time-stepping to offer fast, real-time simulation speeds with accuracy and convergence guarantees, addressing the limitations of traditional error-controlled integrators in handling stiff contact dynamics.", "motivation": "The motivation behind this paper is to address the critical challenge faced by state-of-the-art robotics simulators, which run in discrete time, forcing users to select a time step that can either lead to non-physical artifacts or slow down the simulation. The aim is to introduce an integrator capable of automatic time step adjustment for desired accuracy while overcoming the difficulties posed by stiff dynamics of contact, and to meet the speed and scalability demands of modern robotics workflows.", "method": "The method introduced in this paper is CENIC, a new continuous-time integrator that merges recent advancements in convex time-stepping and error-controlled integration. This approach aims to inherit the benefits from both continuous integration (such as adaptive time stepping) and discrete time-stepping (such as computational efficiency and stability).", "result": "CENIC demonstrates fast real-time performance on par with leading discrete-time robotics simulators, including MuJoCo, Drake, and Isaac Sim. It also provides assurances regarding the accuracy and convergence of the simulations, effectively solving the issues related to stiff contact dynamics and maintaining the required speed and scalability for contemporary robotics applications.", "conclusion": "In conclusion, CENIC represents a significant advancement in robotics simulation, offering a solution that not only matches the speed of current discrete-time simulators but also ensures accurate and convergent results, thus providing a robust alternative for simulating complex robotic systems."}}
{"id": "2511.08778", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.08778", "abs": "https://arxiv.org/abs/2511.08778", "authors": ["Richard Cheng", "Peter Werner", "Carolyn Matl"], "title": "Dual-Arm Whole-Body Motion Planning: Leveraging Overlapping Kinematic Chains", "comment": "Published in Humanoids 2025", "summary": "High degree-of-freedom dual-arm robots are becoming increasingly common due to their morphology enabling them to operate effectively in human environments. However, motion planning in real-time within unknown, changing environments remains a challenge for such robots due to the high dimensionality of the configuration space and the complex collision-avoidance constraints that must be obeyed. In this work, we propose a novel way to alleviate the curse of dimensionality by leveraging the structure imposed by shared joints (e.g. torso joints) in a dual-arm robot. First, we build two dynamic roadmaps (DRM) for each kinematic chain (i.e. left arm + torso, right arm + torso) with specific structure induced by the shared joints. Then, we show that we can leverage this structure to efficiently search through the composition of the two roadmaps and largely sidestep the curse of dimensionality. Finally, we run several experiments in a real-world grocery store with this motion planner on a 19 DoF mobile manipulation robot executing a grocery fulfillment task, achieving 0.4s average planning times with 99.9% success rate across more than 2000 motion plans.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u51cf\u5c11\u9ad8\u81ea\u7531\u5ea6\u53cc\u81c2\u673a\u5668\u4eba\u5728\u672a\u77e5\u53d8\u5316\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u96be\u5ea6\uff0c\u901a\u8fc7\u5229\u7528\u5171\u4eab\u5173\u8282\uff08\u5982\u8eaf\u5e72\u5173\u8282\uff09\u7684\u7ed3\u6784\uff0c\u6784\u5efa\u4e86\u4e24\u4e2a\u52a8\u6001\u8def\u7ebf\u56fe\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u6709\u6548\u641c\u7d22\u8fd9\u4e24\u4e2a\u8def\u7ebf\u56fe\u7684\u7ec4\u5408\u4ee5\u907f\u5f00\u7ef4\u5ea6\u707e\u96be\u3002\u5b9e\u9645\u6d4b\u8bd5\u4e2d\uff0c\u5728\u4e00\u4e2a19\u81ea\u7531\u5ea6\u79fb\u52a8\u64cd\u4f5c\u673a\u5668\u4eba\u4e0a\u6267\u884c\u6742\u8d27\u914d\u9001\u4efb\u52a1\u65f6\u8fbe\u5230\u4e86\u5e73\u57470.4\u79d2\u7684\u89c4\u5212\u65f6\u95f4\u548c99.9%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u9ad8\u81ea\u7531\u5ea6\u53cc\u81c2\u673a\u5668\u4eba\u5728\u672a\u77e5\u548c\u53d8\u5316\u73af\u5883\u4e2d\u5b9e\u65f6\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u7531\u4e8e\u914d\u7f6e\u7a7a\u95f4\u7684\u9ad8\u7ef4\u6027\u548c\u5fc5\u987b\u9075\u5b88\u7684\u590d\u6742\u907f\u78b0\u7ea6\u675f\u6240\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u8fd0\u52a8\u94fe\uff08\u5373\u5de6\u81c2+\u8eaf\u5e72\u3001\u53f3\u81c2+\u8eaf\u5e72\uff09\u5efa\u7acb\u5177\u6709\u7531\u5171\u4eab\u5173\u8282\u5f15\u8d77\u7684\u7279\u5b9a\u7ed3\u6784\u7684\u4e24\u4e2a\u52a8\u6001\u8def\u7ebf\u56fe(DRM)\uff0c\u7136\u540e\u5229\u7528\u8fd9\u79cd\u7ed3\u6784\u6709\u6548\u5730\u641c\u7d22\u4e24\u4e2a\u8def\u7ebf\u56fe\u7684\u7ec4\u5408\uff0c\u4ece\u800c\u5927\u5927\u907f\u514d\u4e86\u7ef4\u5ea6\u707e\u96be\u3002", "result": "\u5728\u4e00\u4e2a\u771f\u5b9e\u7684\u6742\u8d27\u5e97\u73af\u5883\u4e2d\u5bf919\u4e2a\u81ea\u7531\u5ea6\u7684\u79fb\u52a8\u64cd\u7eb5\u673a\u5668\u4eba\u8fdb\u884c\u4e86\u591a\u6b21\u5b9e\u9a8c\uff0c\u5b9e\u73b0\u4e86\u5e73\u57470.4\u79d2\u7684\u89c4\u5212\u65f6\u95f4\u4ee5\u53ca\u8d85\u8fc72000\u6b21\u8fd0\u52a8\u8ba1\u5212\u4e2d99.9%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u9ad8\u81ea\u7531\u5ea6\u53cc\u81c2\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u6548\u7387\u4e0e\u6210\u529f\u7387\u3002"}}
{"id": "2511.08863", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.08863", "abs": "https://arxiv.org/abs/2511.08863", "authors": ["Hyesu Jang", "Wooseong Yang", "Ayoung Kim", "Dongje Lee", "Hanguen Kim"], "title": "XPRESS: X-Band Radar Place Recognition via Elliptical Scan Shaping", "comment": "9 pages, 9 figures, Published in IEEE RA-L", "summary": "X-band radar serves as the primary sensor on maritime vessels, however, its application in autonomous navigation has been limited due to low sensor resolution and insufficient information content. To enable X-band radar-only autonomous navigation in maritime environments, this paper proposes a place recognition algorithm specifically tailored for X-band radar, incorporating an object density-based rule for efficient candidate selection and intentional degradation of radar detections to achieve robust retrieval performance. The proposed algorithm was evaluated on both public maritime radar datasets and our own collected dataset, and its performance was compared against state-of-the-art radar place recognition methods. An ablation study was conducted to assess the algorithm's performance sensitivity with respect to key parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3aX\u6ce2\u6bb5\u96f7\u8fbe\u8bbe\u8ba1\u7684\u5730\u70b9\u8bc6\u522b\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u7269\u4f53\u5bc6\u5ea6\u7684\u89c4\u5219\u8fdb\u884c\u9ad8\u6548\u7684\u5019\u9009\u9009\u62e9\uff0c\u5e76\u6709\u610f\u964d\u4f4e\u96f7\u8fbe\u68c0\u6d4b\u7684\u8d28\u91cf\u4ee5\u5b9e\u73b0\u9c81\u68d2\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u4ec5\u4f7f\u7528X\u6ce2\u6bb5\u96f7\u8fbe\u5728\u6d77\u4e8b\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "X\u6ce2\u6bb5\u96f7\u8fbe\u4f5c\u4e3a\u6d77\u4e0a\u8239\u53ea\u7684\u4e3b\u8981\u4f20\u611f\u5668\uff0c\u4f46\u5176\u5728\u81ea\u4e3b\u5bfc\u822a\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u4e86\u4f4e\u5206\u8fa8\u7387\u548c\u4fe1\u606f\u91cf\u4e0d\u8db3\u7684\u9650\u5236\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u5e76\u5b9e\u73b0\u4ec5\u4f9d\u9760X\u6ce2\u6bb5\u96f7\u8fbe\u7684\u6d77\u4e0a\u81ea\u4e3b\u5bfc\u822a\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u7279\u522b\u9488\u5bf9\u8fd9\u79cd\u96f7\u8fbe\u4f18\u5316\u7684\u5730\u70b9\u8bc6\u522b\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5730\u70b9\u8bc6\u522b\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u57fa\u4e8e\u5bf9\u8c61\u5bc6\u5ea6\u7684\u89c4\u5219\u6765\u63d0\u9ad8\u5019\u9009\u70b9\u7684\u9009\u62e9\u6548\u7387\uff0c\u5e76\u4e14\u901a\u8fc7\u6545\u610f\u964d\u4f4e\u96f7\u8fbe\u63a2\u6d4b\u8d28\u91cf\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u5373\u4f7f\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u4e5f\u80fd\u4fdd\u6301\u826f\u597d\u7684\u68c0\u7d22\u6027\u80fd\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u516c\u5f00\u7684\u6d77\u4e8b\u96f7\u8fbe\u6570\u636e\u96c6\u4ee5\u53ca\u7814\u7a76\u56e2\u961f\u81ea\u5df1\u6536\u96c6\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u96f7\u8fbe\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u6d88\u878d\u7814\u7a76\u6765\u5206\u6790\u5173\u952e\u53c2\u6570\u5bf9\u7b97\u6cd5\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u9488\u5bf9X\u6ce2\u6bb5\u96f7\u8fbe\u4f18\u5316\u7684\u5730\u70b9\u8bc6\u522b\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\u5728\u6d77\u4e8b\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4f4e\u5206\u8fa8\u7387\u548c\u4fe1\u606f\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2511.08865", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.08865", "abs": "https://arxiv.org/abs/2511.08865", "authors": ["Cong Tai", "Hansheng Wu", "Haixu Long", "Zhengbin Long", "Zhaoyu Zheng", "Haodong Xiang", "Tao Shen"], "title": "MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror", "comment": null, "summary": "In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePICO\u7684\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u6846\u67b6\uff0c\u80fd\u591f\u4ee5\u4f4e\u6210\u672c\u5b9e\u65f6\u83b7\u53d6\u624b\u52bf\u548c\u59ff\u6001\u6570\u636e\uff0c\u5e76\u4e14\u5929\u7136\u517c\u5bb9RealMirror\u751f\u6001\u7cfb\u7edf\uff0c\u5728Isaac\u6a21\u62df\u73af\u5883\u4e2d\u63d0\u4f9b\u7a33\u5b9a\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u8bb0\u5f55\u529f\u80fd\uff0c\u652f\u6301\u591a\u79cd\u672b\u7aef\u6267\u884c\u5668\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u9065\u63a7\u64cd\u4f5c\uff0c\u65e8\u5728\u964d\u4f4e\u4e0a\u80a2\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7814\u7a76\u6280\u672f\u95e8\u69db\uff0c\u52a0\u901f\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u76f8\u5173\u7814\u7a76\u7684\u53d1\u5c55\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u624b\u52bf\u548c\u59ff\u6001\u6570\u636e\u91c7\u96c6\u7684\u6210\u672c\u6548\u76ca\uff0c\u540c\u65f6\u964d\u4f4e\u4e0a\u80a2\u673a\u5668\u4eba\u64cd\u4f5c\u7814\u7a76\u7684\u6280\u672f\u969c\u788d\uff0c\u52a0\u901f\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\uff08VLA\uff09\u9886\u57df\u5185\u7684\u79d1\u7814\u8fdb\u5c55\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8ePICO\u7684\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e0eRealMirror\u751f\u6001\u7cfb\u7edf\u81ea\u7136\u517c\u5bb9\uff0c\u80fd\u591f\u5728Isaac\u6a21\u62df\u73af\u5883\u4e0b\u7a33\u5b9a\u3001\u7cbe\u786e\u5730\u8bb0\u5f55\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u5e76\u652f\u6301\u914d\u5907\u6709\u7075\u6d3b\u624b\u90e8\u53ca\u673a\u68b0\u5939\u722a\u7b49\u591a\u79cd\u672b\u7aef\u6267\u884c\u5668\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u8fdc\u7a0b\u63a7\u5236\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u6210\u672c\u6548\u76ca\u65b9\u9762\u4f18\u4e8e\u4e3b\u6d41\u7684\u89c6\u89c9\u8ddf\u8e2a\u548c\u8fd0\u52a8\u6355\u6349\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4fc3\u8fdbVLA\u6570\u636e\u96c6\u7684\u6784\u5efa\uff0c\u5e76\u5141\u8bb8\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u673a\u5668\u4eba\u8fdb\u884c\u5b9e\u65f6\u9065\u64cd\u4f5c\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8fd9\u79cd\u65b0\u9896\u7684PICO\u57fa\u7840\u67b6\u6784\uff0c\u7814\u7a76\u8005\u4eec\u5f97\u4ee5\u5229\u7528\u66f4\u52a0\u7ecf\u6d4e\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6536\u96c6\u624b\u90e8\u8fd0\u52a8\u4e0e\u59ff\u6001\u4fe1\u606f\uff0c\u540c\u65f6\u7b80\u5316\u4e86\u590d\u6742\u673a\u5668\u4eba\u64cd\u63a7\u4efb\u52a1\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4e3aVLA\u76f8\u5173\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u652f\u6301\u3002"}}
{"id": "2511.08912", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.08912", "abs": "https://arxiv.org/abs/2511.08912", "authors": ["Jinyu Zhang", "Lijun Han", "Feng Jian", "Lingxi Zhang", "Hesheng Wang"], "title": "A Shared Control Framework for Mobile Robots with Planning-Level Intention Prediction", "comment": null, "summary": "In mobile robot shared control, effectively understanding human motion intention is critical for seamless human-robot collaboration. This paper presents a novel shared control framework featuring planning-level intention prediction. A path replanning algorithm is designed to adjust the robot's desired trajectory according to inferred human intentions. To represent future motion intentions, we introduce the concept of an intention domain, which serves as a constraint for path replanning. The intention-domain prediction and path replanning problems are jointly formulated as a Markov Decision Process and solved through deep reinforcement learning. In addition, a Voronoi-based human trajectory generation algorithm is developed, allowing the model to be trained entirely in simulation without human participation or demonstration data. Extensive simulations and real-world user studies demonstrate that the proposed method significantly reduces operator workload and enhances safety, without compromising task efficiency compared with existing assistive teleoperation approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5171\u4eab\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u9884\u6d4b\u4eba\u7684\u8fd0\u52a8\u610f\u56fe\uff0c\u5e76\u636e\u6b64\u91cd\u65b0\u89c4\u5212\u673a\u5668\u4eba\u8def\u5f84\uff0c\u4ece\u800c\u51cf\u5c11\u64cd\u4f5c\u5458\u7684\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "\u5728\u79fb\u52a8\u673a\u5668\u4eba\u5171\u4eab\u63a7\u5236\u4e2d\uff0c\u6709\u6548\u5730\u7406\u89e3\u4eba\u7c7b\u7684\u8fd0\u52a8\u610f\u56fe\u5bf9\u4e8e\u65e0\u7f1d\u7684\u4eba\u673a\u534f\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u610f\u56fe\u57df\u7684\u6982\u5ff5\u6765\u8868\u793a\u672a\u6765\u7684\u8fd0\u52a8\u610f\u56fe\uff0c\u5e76\u5c06\u610f\u56fe\u57df\u9884\u6d4b\u548c\u8def\u5f84\u91cd\u65b0\u89c4\u5212\u95ee\u9898\u8054\u5408\u8868\u8ff0\u4e3a\u4e00\u4e2a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eVoronoi\u56fe\u7684\u4eba\u7c7b\u8f68\u8ff9\u751f\u6210\u7b97\u6cd5\uff0c\u4f7f\u5f97\u6a21\u578b\u53ef\u4ee5\u5728\u6ca1\u6709\u4eba\u7c7b\u53c2\u4e0e\u6216\u793a\u6559\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b8c\u5168\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u3002", "result": "\u5e7f\u6cdb\u7684\u6a21\u62df\u548c\u5b9e\u9645\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u64cd\u4f5c\u8005\u7684\u5de5\u4f5c\u8d1f\u62c5\u5e76\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4e0e\u73b0\u6709\u7684\u8f85\u52a9\u9065\u64cd\u4f5c\u65b9\u6cd5\u76f8\u6bd4\u4e0d\u4f1a\u727a\u7272\u4efb\u52a1\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u6539\u5584\u4eba\u673a\u5171\u878d\u73af\u5883\u4e0b\u7684\u534f\u4f5c\u8d28\u91cf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u5ea6\u4e92\u52a8\u6027\u7684\u573a\u666f\u3002"}}
{"id": "2511.08935", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08935", "abs": "https://arxiv.org/abs/2511.08935", "authors": ["Ningnan Wang", "Weihuang Chen", "Liming Chen", "Haoxuan Ji", "Zhongyu Guo", "Xuchong Zhang", "Hongbin Sun"], "title": "Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation", "comment": null, "summary": "Embodied visual navigation remains a challenging task, as agents must explore unknown environments with limited knowledge. Existing zero-shot studies have shown that incorporating memory mechanisms to support goal-directed behavior can improve long-horizon planning performance. However, they overlook visual frontier boundaries, which fundamentally dictate future trajectories and observations, and fall short of inferring the relationship between partial visual observations and navigation goals. In this paper, we propose Semantic Cognition Over Potential-based Exploration (SCOPE), a zero-shot framework that explicitly leverages frontier information to drive potential-based exploration, enabling more informed and goal-relevant decisions. SCOPE estimates exploration potential with a Vision-Language Model and organizes it into a spatio-temporal potential graph, capturing boundary dynamics to support long-horizon planning. In addition, SCOPE incorporates a self-reconsideration mechanism that revisits and refines prior decisions, enhancing reliability and reducing overconfident errors. Experimental results on two diverse embodied navigation tasks show that SCOPE outperforms state-of-the-art baselines by 4.6\\% in accuracy. Further analysis demonstrates that its core components lead to improved calibration, stronger generalization, and higher decision quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SCOPE\uff0c\u4e00\u79cd\u96f6\u6837\u672c\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u8fb9\u754c\u4fe1\u606f\u6765\u9a71\u52a8\u57fa\u4e8e\u6f5c\u529b\u7684\u63a2\u7d22\uff0c\u4ece\u800c\u5728\u89c6\u89c9\u5bfc\u822a\u4e2d\u5b9e\u73b0\u66f4\u77e5\u60c5\u548c\u76ee\u6807\u76f8\u5173\u7684\u51b3\u7b56\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSCOPE\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u4e0a\u63d0\u9ad8\u4e864.6%\u3002", "motivation": "\u5f53\u524d\u7684\u96f6\u6837\u672c\u7814\u7a76\u867d\u7136\u901a\u8fc7\u8bb0\u5fc6\u673a\u5236\u589e\u5f3a\u4e86\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u7684\u8868\u73b0\uff0c\u4f46\u5ffd\u7565\u4e86\u5bf9\u89c6\u89c9\u8fb9\u754c\uff08\u5f71\u54cd\u672a\u6765\u8f68\u8ff9\u548c\u89c2\u5bdf\u7684\u5173\u952e\u56e0\u7d20\uff09\u7684\u6709\u6548\u5229\u7528\uff0c\u4ee5\u53ca\u90e8\u5206\u89c6\u89c9\u89c2\u5bdf\u4e0e\u5bfc\u822a\u76ee\u6807\u4e4b\u95f4\u5173\u7cfb\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCOPE (Semantic Cognition Over Potential-based Exploration) \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4f30\u8ba1\u63a2\u7d22\u6f5c\u529b\uff0c\u5e76\u6784\u5efa\u65f6\u7a7a\u6f5c\u529b\u56fe\u6765\u6355\u6349\u8fb9\u754c\u52a8\u6001\uff1b\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u81ea\u6211\u53cd\u601d\u673a\u5236\u4ee5\u91cd\u65b0\u5ba1\u89c6\u548c\u5b8c\u5584\u5148\u524d\u51b3\u5b9a\uff0c\u63d0\u9ad8\u51b3\u7b56\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u5177\u8eab\u5bfc\u822a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cSCOPE\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u9ad8\u51fa4.6%\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u663e\u793a\uff0c\u5176\u6838\u5fc3\u7ec4\u4ef6\u4fc3\u8fdb\u4e86\u66f4\u597d\u7684\u6821\u51c6\u3001\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u9ad8\u7684\u51b3\u7b56\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u660e\u786e\u5730\u5229\u7528\u524d\u6cbf\u4fe1\u606f\u6765\u6307\u5bfc\u57fa\u4e8e\u6f5c\u529b\u7684\u63a2\u7d22\uff0cSCOPE\u80fd\u591f\u505a\u51fa\u66f4\u52a0\u660e\u667a\u4e14\u4e0e\u76ee\u6807\u76f8\u5173\u8054\u7684\u51b3\u5b9a\uff0c\u5728\u957f\u671f\u89c4\u5212\u652f\u6301\u4e0b\u5c55\u73b0\u51fa\u8272\u6027\u80fd\u3002"}}
{"id": "2511.09020", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.09020", "abs": "https://arxiv.org/abs/2511.09020", "authors": ["Mingyang Yu", "Haorui Yang", "Kangning An", "Xinjian Wei", "Xiaoxuan Xu", "Jing Xu"], "title": "A Quantum Tunneling and Bio-Phototactic Driven Enhanced Dwarf Mongoose Optimizer for UAV Trajectory Planning and Engineering Problem", "comment": null, "summary": "With the widespread adoption of unmanned aerial vehicles (UAV), effective path planning has become increasingly important. Although traditional search methods have been extensively applied, metaheuristic algorithms have gained popularity due to their efficiency and problem-specific heuristics. However, challenges such as premature convergence and lack of solution diversity still hinder their performance in complex scenarios. To address these issues, this paper proposes an Enhanced Multi-Strategy Dwarf Mongoose Optimization (EDMO) algorithm, tailored for three-dimensional UAV trajectory planning in dynamic and obstacle-rich environments. EDMO integrates three novel strategies: (1) a Dynamic Quantum Tunneling Optimization Strategy (DQTOS) to enable particles to probabilistically escape local optima; (2) a Bio-phototactic Dynamic Focusing Search Strategy (BDFSS) inspired by microbial phototaxis for adaptive local refinement; and (3) an Orthogonal Lens Opposition-Based Learning (OLOBL) strategy to enhance global exploration through structured dimensional recombination. EDMO is benchmarked on 39 standard test functions from CEC2017 and CEC2020, outperforming 14 advanced algorithms in convergence speed, robustness, and optimization accuracy. Furthermore, real-world validations on UAV three-dimensional path planning and three engineering design tasks confirm its practical applicability and effectiveness in field robotics missions requiring intelligent, adaptive, and time-efficient planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u591a\u7b56\u7565\u4f8f\u5112\u732b\u9f2c\u4f18\u5316(EDMO)\u7b97\u6cd5\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5728\u52a8\u6001\u548c\u969c\u788d\u7269\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u7684\u4e09\u7ef4\u8f68\u8ff9\u89c4\u5212\u3002\u901a\u8fc7\u6574\u5408\u4e09\u79cd\u65b0\u7b56\u7565\uff0cEDMO\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e8614\u79cd\u5148\u8fdb\u7b97\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a(UAV)\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6709\u6548\u7684\u8def\u5f84\u89c4\u5212\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u5c3d\u7ba1\u4f20\u7edf\u641c\u7d22\u65b9\u6cd5\u5df2\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u56e0\u5176\u6548\u7387\u548c\u9488\u5bf9\u7279\u5b9a\u95ee\u9898\u7684\u542f\u53d1\u5f0f\u800c\u53d7\u5230\u6b22\u8fce\u3002\u7136\u800c\uff0c\u5728\u590d\u6742\u573a\u666f\u4e0b\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u4ecd\u9762\u4e34\u8fc7\u65e9\u6536\u655b\u548c\u89e3\u591a\u6837\u6027\u4e0d\u8db3\u7b49\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u591a\u7b56\u7565\u4f8f\u5112\u732b\u9f2c\u4f18\u5316(EDMO)\u7b97\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u52a8\u6001\u53ca\u5145\u6ee1\u969c\u788d\u7269\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u7684\u4e09\u7ef4\u8f68\u8ff9\u89c4\u5212\u3002EDMO\u96c6\u6210\u4e86\u4e09\u79cd\u65b0\u9896\u7b56\u7565\uff1a(1) \u52a8\u6001\u91cf\u5b50\u96a7\u7a7f\u4f18\u5316\u7b56\u7565(DQTOS)\uff0c\u4f7f\u7c92\u5b50\u80fd\u591f\u6982\u7387\u6027\u5730\u9003\u79bb\u5c40\u90e8\u6700\u4f18\uff1b(2) \u53d7\u5fae\u751f\u7269\u8d8b\u5149\u6027\u542f\u53d1\u7684\u751f\u7269\u5149\u654f\u52a8\u6001\u805a\u7126\u641c\u7d22\u7b56\u7565(BDFSS)\uff0c\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u5c40\u90e8\u7cbe\u70bc\uff1b\u4ee5\u53ca(3) \u6b63\u4ea4\u955c\u50cf\u5bf9\u7acb\u5b66\u4e60(OLOBL)\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u7ef4\u5ea6\u91cd\u7ec4\u6765\u52a0\u5f3a\u5168\u5c40\u63a2\u7d22\u3002", "result": "EDMO\u5728CEC2017\u548cCEC2020\u768439\u4e2a\u6807\u51c6\u6d4b\u8bd5\u51fd\u6570\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u5b83\u5728\u6536\u655b\u901f\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u4f18\u5316\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e14\u79cd\u5148\u8fdb\u7684\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5bf9\u65e0\u4eba\u673a\u4e09\u7ef4\u8def\u5f84\u89c4\u5212\u53ca\u4e09\u4e2a\u5de5\u7a0b\u8bbe\u8ba1\u4efb\u52a1\u7684\u5b9e\u9645\u9a8c\u8bc1\uff0c\u8fdb\u4e00\u6b65\u786e\u8ba4\u4e86\u5176\u5728\u9700\u8981\u667a\u80fd\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u65f6\u95f4\u6548\u7387\u9ad8\u7684\u9886\u57df\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u9002\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684EDMO\u7b97\u6cd5\u4e0d\u4ec5\u5728\u7406\u8bba\u6027\u80fd\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u800c\u4e14\u5728\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u65f6\u4e5f\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6f5c\u529b\uff0c\u4e3a\u65e0\u4eba\u673a\u53ca\u5176\u4ed6\u76f8\u5173\u9886\u57df\u7684\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.09080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.09080", "abs": "https://arxiv.org/abs/2511.09080", "authors": ["Shunsuke Ito", "Chaoran Zhao", "Ryo Okamura", "Takuya Azumi"], "title": "D-AWSIM: Distributed Autonomous Driving Simulator for Dynamic Map Generation Framework", "comment": "9 pages. This version includes minor lstlisting configuration adjustments for successful compilation. No changes to content or layout. Originally published at Euromicro DSD 2025", "summary": "Autonomous driving systems have achieved significant advances, and full autonomy within defined operational design domains near practical deployment. Expanding these domains requires addressing safety assurance under diverse conditions. Information sharing through vehicle-to-vehicle and vehicle-to-infrastructure communication, enabled by a Dynamic Map platform built from vehicle and roadside sensor data, offers a promising solution. Real-world experiments with numerous infrastructure sensors incur high costs and regulatory challenges. Conventional single-host simulators lack the capacity for large-scale urban traffic scenarios. This paper proposes D-AWSIM, a distributed simulator that partitions its workload across multiple machines to support the simulation of extensive sensor deployment and dense traffic environments. A Dynamic Map generation framework on D-AWSIM enables researchers to explore information-sharing strategies without relying on physical testbeds. The evaluation shows that D-AWSIM increases throughput for vehicle count and LiDAR sensor processing substantially compared to a single-machine setup. Integration with Autoware demonstrates applicability for autonomous driving research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aD-AWSIM\u7684\u5206\u5e03\u5f0f\u6a21\u62df\u5668\uff0c\u80fd\u591f\u652f\u6301\u5927\u89c4\u6a21\u4f20\u611f\u5668\u90e8\u7f72\u548c\u5bc6\u96c6\u4ea4\u901a\u73af\u5883\u7684\u4eff\u771f\uff0c\u901a\u8fc7\u5728\u591a\u53f0\u673a\u5668\u95f4\u5206\u914d\u5de5\u4f5c\u8d1f\u8f7d\u6765\u63d0\u9ad8\u5904\u7406\u541e\u5410\u91cf\u3002\u8be5\u6a21\u62df\u5668\u96c6\u6210\u4e86\u52a8\u6001\u5730\u56fe\u751f\u6210\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u7814\u7a76\u8005\u63a2\u7d22\u4fe1\u606f\u5171\u4eab\u7b56\u7565\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u7269\u7406\u6d4b\u8bd5\u5e73\u53f0\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65\uff0c\u4f46\u4e3a\u4e86\u6269\u5c55\u5176\u64cd\u4f5c\u9886\u57df\u4ee5\u9002\u5e94\u66f4\u591a\u6837\u5316\u7684\u6761\u4ef6\uff0c\u9700\u8981\u89e3\u51b3\u5b89\u5168\u4fdd\u969c\u95ee\u9898\u3002\u8f66\u8f86\u95f4\u53ca\u8f66\u8f86\u4e0e\u57fa\u7840\u8bbe\u65bd\u4e4b\u95f4\u7684\u4fe1\u606f\u5171\u4eab\u662f\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u4e4b\u4e00\uff0c\u4f46\u7531\u4e8e\u73b0\u5b9e\u4e16\u754c\u5b9e\u9a8c\u7684\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u76d1\u7ba1\u6311\u6218\uff0c\u4ee5\u53ca\u4f20\u7edf\u5355\u4e00\u4e3b\u673a\u6a21\u62df\u5668\u65e0\u6cd5\u80dc\u4efb\u5927\u89c4\u6a21\u57ce\u5e02\u4ea4\u901a\u573a\u666f\u7684\u5c40\u9650\u6027\uff0c\u4fc3\u4f7f\u4e86\u5bf9\u65b0\u65b9\u6cd5\u7684\u9700\u6c42\u3002", "method": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aD-AWSIM\u7684\u65b0\u5f0f\u5206\u5e03\u5f0f\u6a21\u62df\u5668\uff0c\u5b83\u901a\u8fc7\u8de8\u591a\u4e2a\u8ba1\u7b97\u8282\u70b9\u5206\u914d\u4efb\u52a1\u6765\u514b\u670d\u5355\u673a\u6a21\u62df\u5668\u7684\u9650\u5236\u3002\u6b64\u5916\uff0cD-AWSIM\u8fd8\u5305\u542b\u4e86\u4e00\u4e2a\u52a8\u6001\u5730\u56fe\u751f\u6210\u6846\u67b6\uff0c\u8fd9\u4f7f\u5f97\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5728\u4e0d\u4f7f\u7528\u5b9e\u9645\u6d4b\u8bd5\u8bbe\u65bd\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u4e0d\u540c\u7684\u4fe1\u606f\u5171\u4eab\u7b56\u7565\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u8f83\u4e8e\u5355\u673a\u8bbe\u7f6e\uff0c\u5728\u5904\u7406\u8f66\u8f86\u6570\u91cf\u548c\u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u5668\u6570\u636e\u65b9\u9762\uff0cD-AWSIM\u80fd\u591f\u5927\u5e45\u63d0\u9ad8\u541e\u5410\u91cf\u3002\u540c\u65f6\uff0c\u4e0eAutoware\u7cfb\u7edf\u7684\u96c6\u6210\u8bc1\u660e\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "D-AWSIM\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u8003\u8651\u5927\u89c4\u6a21\u4f20\u611f\u5668\u90e8\u7f72\u548c\u590d\u6742\u4ea4\u901a\u72b6\u51b5\u7684\u4fe1\u606f\u5171\u4eab\u7b56\u7565\u7814\u7a76\u4e2d\u3002"}}
{"id": "2511.09091", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.09091", "abs": "https://arxiv.org/abs/2511.09091", "authors": ["Shivam Sood", "Laukik Nakhwa", "Sun Ge", "Yuhong Cao", "Jin Cheng", "Fatemah Zargarbashi", "Taerim Yoon", "Sungjoon Choi", "Stelian Coros", "Guillaume Sartoretti"], "title": "APEX: Action Priors Enable Efficient Exploration for Robust Motion Tracking on Legged Robots", "comment": null, "summary": "Learning natural, animal-like locomotion from demonstrations has become a core paradigm in legged robotics. Despite the recent advancements in motion tracking, most existing methods demand extensive tuning and rely on reference data during deployment, limiting adaptability. We present APEX (Action Priors enable Efficient Exploration), a plug-and-play extension to state-of-the-art motion tracking algorithms that eliminates any dependence on reference data during deployment, improves sample efficiency, and reduces parameter tuning effort. APEX integrates expert demonstrations directly into reinforcement learning (RL) by incorporating decaying action priors, which initially bias exploration toward expert demonstrations but gradually allow the policy to explore independently. This is combined with a multi-critic framework that balances task performance with motion style. Moreover, APEX enables a single policy to learn diverse motions and transfer reference-like styles across different terrains and velocities, while remaining robust to variations in reward design. We validate the effectiveness of our method through extensive experiments in both simulation and on a Unitree Go2 robot. By leveraging demonstrations to guide exploration during RL training, without imposing explicit bias toward them, APEX enables legged robots to learn with greater stability, efficiency, and generalization. We believe this approach paves the way for guidance-driven RL to boost natural skill acquisition in a wide array of robotic tasks, from locomotion to manipulation. Website and code: https://marmotlab.github.io/APEX/.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86APEX\uff0c\u4e00\u79cd\u7ed3\u5408\u4e86\u4e13\u5bb6\u6f14\u793a\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u817f\u90e8\u673a\u5668\u4eba\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u5b66\u4e60\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u53c2\u8003\u6570\u636e\u548c\u53c2\u6570\u8c03\u6574\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u7684\u817f\u90e8\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8c03\u53c2\u5de5\u4f5c\uff0c\u5e76\u4e14\u5728\u90e8\u7f72\u65f6\u4e25\u91cd\u4f9d\u8d56\u53c2\u8003\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u9010\u6e10\u8870\u51cf\u7684\u52a8\u4f5c\u5148\u9a8c\uff08action priors\uff09\uff0cAPEX\u5c06\u4e13\u5bb6\u6f14\u793a\u76f4\u63a5\u6574\u5408\u8fdb\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u540c\u65f6\u91c7\u7528\u591a\u8bc4\u5224\u6846\u67b6\u6765\u5e73\u8861\u4efb\u52a1\u6267\u884c\u4e0e\u52a8\u4f5c\u98ce\u683c\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65e0\u8bba\u662f\u6a21\u62df\u73af\u5883\u8fd8\u662f\u5b9e\u9645\u5e94\u7528\u4e8eUnitree Go2\u673a\u5668\u4eba\u4e0a\uff0cAPEX\u90fd\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u7684\u7a33\u5b9a\u6027\u3001\u5b66\u4e60\u6548\u7387\u4ee5\u53ca\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6307\u5bfc\u9a71\u52a8\u578b\u5f3a\u5316\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\uff0c\u6709\u671b\u4fc3\u8fdb\u4ece\u79fb\u52a8\u5230\u64cd\u4f5c\u7b49\u5e7f\u6cdb\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u81ea\u7136\u6280\u80fd\u7684\u5b66\u4e60\u3002"}}
{"id": "2511.09104", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.09104", "abs": "https://arxiv.org/abs/2511.09104", "authors": ["Amirhossein Kazemipour", "Robert K. Katzschmann"], "title": "Decoupling Torque and Stiffness: A Unified Modeling and Control Framework for Antagonistic Artificial Muscles", "comment": null, "summary": "Antagonistic soft actuators built from artificial muscles (PAMs, HASELs, DEAs) promise plant-level torque-stiffness decoupling, yet existing controllers for soft muscles struggle to maintain independent control through dynamic contact transients. We present a unified framework enabling independent torque and stiffness commands in real-time for diverse soft actuator types. Our unified force law captures diverse soft muscle physics in a single model with sub-ms computation, while our cascaded controller with analytical inverse dynamics maintains decoupling despite model errors and disturbances. Using co-contraction/bias coordinates, the controller independently modulates torque via bias and stiffness via co-contraction-replicating biological impedance strategies. Simulation-based validation through contact experiments demonstrates maintained independence: 200x faster settling on soft surfaces, 81% force reduction on rigid surfaces, and stable interaction vs 22-54% stability for fixed policies. This framework provides a foundation for enabling musculoskeletal antagonistic systems to execute adaptive impedance control for safe human-robot interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u65f6\u72ec\u7acb\u63a7\u5236\u4e0d\u540c\u7c7b\u578b\u8f6f\u6267\u884c\u5668\u7684\u626d\u77e9\u548c\u521a\u5ea6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5171\u6536\u7f29/\u504f\u7f6e\u5750\u6807\u6765\u6a21\u62df\u751f\u7269\u963b\u6297\u7b56\u7565\uff0c\u4f7f\u5f97\u63a7\u5236\u5668\u53ef\u4ee5\u72ec\u7acb\u8c03\u8282\u626d\u77e9\u548c\u521a\u5ea6\u3002\u4eff\u771f\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u63a5\u89e6\u77ac\u6001\u4e0b\u4fdd\u6301\u4e86\u826f\u597d\u7684\u89e3\u8026\u6027\u80fd\uff0c\u5e76\u4e14\u6bd4\u56fa\u5b9a\u7b56\u7565\u66f4\u7a33\u5b9a\u3002", "motivation": "\u73b0\u6709\u7684\u8f6f\u808c\u8089\u63a7\u5236\u5668\u96be\u4ee5\u5728\u52a8\u6001\u63a5\u89e6\u77ac\u6001\u4e2d\u4fdd\u6301\u72ec\u7acb\u63a7\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u4e0d\u540c\u7c7b\u578b\u7684\u8f6f\u6267\u884c\u5668\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u72ec\u7acb\u63a7\u5236\u626d\u77e9\u548c\u521a\u5ea6\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u529b\u91cf\u5b9a\u5f8b\u6a21\u578b\uff0c\u80fd\u591f\u4ee5\u4e9a\u6beb\u79d2\u7ea7\u7684\u901f\u5ea6\u8ba1\u7b97\u591a\u79cd\u8f6f\u808c\u8089\u7269\u7406\u7279\u6027\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ea7\u8054\u63a7\u5236\u5668\uff0c\u5229\u7528\u5206\u6790\u9006\u52a8\u529b\u5b66\u5373\u4f7f\u5728\u5b58\u5728\u6a21\u578b\u8bef\u5dee\u548c\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u6301\u89e3\u8026\u3002\u901a\u8fc7\u4f7f\u7528\u5171\u6536\u7f29/\u504f\u7f6e\u5750\u6807\u7cfb\uff0c\u63a7\u5236\u5668\u6a21\u4eff\u4e86\u751f\u7269\u963b\u6297\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5bf9\u626d\u77e9\u548c\u521a\u5ea6\u7684\u72ec\u7acb\u8c03\u8282\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u63a7\u5236\u5668\u5728\u8f6f\u8868\u9762\u4e0a\u7684\u54cd\u5e94\u901f\u5ea6\u63d0\u9ad8\u4e86200\u500d\uff0c\u5728\u786c\u8868\u9762\u4e0a\u4f5c\u7528\u529b\u51cf\u5c11\u4e8681%\uff0c\u5e76\u4e14\u76f8\u6bd4\u56fa\u5b9a\u7b56\u7565\uff08\u7a33\u5b9a\u6027\u4e3a22-54%\uff09\uff0c\u5176\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u66f4\u52a0\u7a33\u5b9a\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5b9e\u73b0\u5b89\u5168\u7684\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4f7f\u808c\u8089\u9aa8\u9abc\u5bf9\u6297\u7cfb\u7edf\u80fd\u591f\u6267\u884c\u9002\u5e94\u6027\u963b\u6297\u63a7\u5236\u3002"}}
{"id": "2511.09119", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.09119", "abs": "https://arxiv.org/abs/2511.09119", "authors": ["Jiahao Xiao", "Bowen Yan", "Jianbo Zhang", "Jia Wang", "Chunyi Li", "Zhengxue Cheng", "Guangtao Zhai"], "title": "Data Assessment for Embodied Intelligence", "comment": null, "summary": "In embodied intelligence, datasets play a pivotal role, serving as both a knowledge repository and a conduit for information transfer. The two most critical attributes of a dataset are the amount of information it provides and how easily this information can be learned by models. However, the multimodal nature of embodied data makes evaluating these properties particularly challenging. Prior work has largely focused on diversity, typically counting tasks and scenes or evaluating isolated modalities, which fails to provide a comprehensive picture of dataset diversity. On the other hand, the learnability of datasets has received little attention and is usually assessed post-hoc through model training, an expensive, time-consuming process that also lacks interpretability, offering little guidance on how to improve a dataset. In this work, we address both challenges by introducing two principled, data-driven tools. First, we construct a unified multimodal representation for each data sample and, based on it, propose diversity entropy, a continuous measure that characterizes the amount of information contained in a dataset. Second, we introduce the first interpretable, data-driven algorithm to efficiently quantify dataset learnability without training, enabling researchers to assess a dataset's learnability immediately upon its release. We validate our algorithm on both simulated and real-world embodied datasets, demonstrating that it yields faithful, actionable insights that enable researchers to jointly improve diversity and learnability. We hope this work provides a foundation for designing higher-quality datasets that advance the development of embodied intelligence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u6570\u636e\u9a71\u52a8\u7684\u5de5\u5177\u6765\u89e3\u51b3\u6570\u636e\u96c6\u591a\u6837\u6027\u548c\u53ef\u5b66\u4e60\u6027\u8bc4\u4f30\u7684\u95ee\u9898\u3002\u9996\u5148\uff0c\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u5e76\u63d0\u51fa\u591a\u6837\u6027\u71b5\u4f5c\u4e3a\u4fe1\u606f\u91cf\u7684\u8fde\u7eed\u5ea6\u91cf\uff1b\u5176\u6b21\uff0c\u5f15\u5165\u4e86\u9996\u4e2a\u53ef\u89e3\u91ca\u7684\u6570\u636e\u9a71\u52a8\u7b97\u6cd5\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u91cf\u5316\u6570\u636e\u96c6\u7684\u53ef\u5b66\u4e60\u6027\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u901a\u8fc7\u8ba1\u7b97\u4efb\u52a1\u548c\u573a\u666f\u6570\u91cf\u6216\u8bc4\u4f30\u5b64\u7acb\u6a21\u6001\u6765\u8861\u91cf\u6570\u636e\u96c6\u591a\u6837\u6027\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u5168\u9762\u89c6\u89d2\u3002\u540c\u65f6\uff0c\u6570\u636e\u96c6\u7684\u53ef\u5b66\u4e60\u6027\u5f80\u5f80\u88ab\u5ffd\u89c6\uff0c\u901a\u5e38\u5728\u6a21\u578b\u8bad\u7ec3\u540e\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u4e0d\u4ec5\u8017\u65f6\u4e14\u7f3a\u4e4f\u89e3\u91ca\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86\u65b0\u7684\u65b9\u6cd5\u6765\u66f4\u597d\u5730\u8bc4\u4f30\u548c\u7406\u89e3\u6570\u636e\u96c6\u7279\u6027\u3002", "method": "1. \u4e3a\u6bcf\u4e2a\u6570\u636e\u6837\u672c\u6784\u5efa\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u591a\u6837\u6027\u71b5\uff0c\u8fd9\u662f\u4e00\u79cd\u8868\u5f81\u6570\u636e\u96c6\u4e2d\u5305\u542b\u7684\u4fe1\u606f\u91cf\u7684\u8fde\u7eed\u5ea6\u91cf\u3002\n2. \u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u53ef\u89e3\u91ca\u7684\u6570\u636e\u9a71\u52a8\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u5730\u91cf\u5316\u6570\u636e\u96c6\u7684\u53ef\u5b66\u4e60\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u5b83\u53ef\u4ee5\u63d0\u4f9b\u5fe0\u5b9e\u800c\u53ef\u884c\u7684\u89c1\u89e3\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u540c\u65f6\u63d0\u9ad8\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u548c\u53ef\u5b66\u4e60\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u80fd\u591f\u4fc3\u8fdb\u5177\u8eab\u667a\u80fd\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.09142", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.09142", "abs": "https://arxiv.org/abs/2511.09142", "authors": ["Eungchang Mason Lee", "Kevin Christiansen Marsim", "Hyun Myung"], "title": "LODESTAR: Degeneracy-Aware LiDAR-Inertial Odometry with Adaptive Schmidt-Kalman Filter and Data Exploitation", "comment": "8 pages, 5 figures, 6 tables, accepted for the publication in IEEE Robotics and Automation Letters", "summary": "LiDAR-inertial odometry (LIO) has been widely used in robotics due to its high accuracy. However, its performance degrades in degenerate environments, such as long corridors and high-altitude flights, where LiDAR measurements are imbalanced or sparse, leading to ill-posed state estimation. In this letter, we present LODESTAR, a novel LIO method that addresses these degeneracies through two key modules: degeneracy-aware adaptive Schmidt-Kalman filter (DA-ASKF) and degeneracy-aware data exploitation (DA-DE). DA-ASKF employs a sliding window to utilize past states and measurements as additional constraints. Specifically, it introduces degeneracy-aware sliding modes that adaptively classify states as active or fixed based on their degeneracy level. Using Schmidt-Kalman update, it partially optimizes active states while preserving fixed states. These fixed states influence the update of active states via their covariances, serving as reference anchors--akin to a lodestar. Additionally, DA-DE prunes less-informative measurements from active states and selectively exploits measurements from fixed states, based on their localizability contribution and the condition number of the Jacobian matrix. Consequently, DA-ASKF enables degeneracy-aware constrained optimization and mitigates measurement sparsity, while DA-DE addresses measurement imbalance. Experimental results show that LODESTAR outperforms existing LiDAR-based odometry methods and degeneracy-aware modules in terms of accuracy and robustness under various degenerate conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LiDAR\u60ef\u6027\u91cc\u7a0b\u8ba1\u65b9\u6cd5LODESTAR\uff0c\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u9000\u5316\u611f\u77e5\u81ea\u9002\u5e94Schmidt-Kalman\u6ee4\u6ce2\u5668\uff08DA-ASKF\uff09\u548c\u9000\u5316\u611f\u77e5\u6570\u636e\u5229\u7528\uff08DA-DE\uff09\uff0c\u89e3\u51b3\u4e86\u5728\u957f\u8d70\u5eca\u548c\u9ad8\u7a7a\u98de\u884c\u7b49\u9000\u5316\u73af\u5883\u4e2d\u7531\u4e8e\u6fc0\u5149\u96f7\u8fbe\u6d4b\u91cf\u4e0d\u5e73\u8861\u6216\u7a00\u758f\u5bfc\u81f4\u7684\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLODESTAR\u5728\u5404\u79cd\u9000\u5316\u6761\u4ef6\u4e0b\u6bd4\u73b0\u6709\u7684\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe\u7684\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6fc0\u5149\u96f7\u8fbe-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08LIO\uff09\u5728\u957f\u8d70\u5eca\u3001\u9ad8\u7a7a\u98de\u884c\u7b49\u9000\u5316\u73af\u5883\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u73af\u5883\u7279\u5f81\u8868\u73b0\u4e3a\u6fc0\u5149\u96f7\u8fbe\u6d4b\u91cf\u4e0d\u5e73\u8861\u6216\u7a00\u758f\uff0c\u8fdb\u800c\u5bfc\u81f4\u72b6\u6001\u4f30\u8ba1\u4e0d\u826f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86LODESTAR\uff0c\u4e00\u79cd\u65b0\u9896\u7684LIO\u65b9\u6cd5\uff0c\u5b83\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u9000\u5316\u611f\u77e5\u81ea\u9002\u5e94Schmidt-Kalman\u6ee4\u6ce2\u5668(DA-ASKF)\u548c\u9000\u5316\u611f\u77e5\u6570\u636e\u5229\u7528(DA-DE)\u3002DA-ASKF\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u6280\u672f\u6765\u5229\u7528\u8fc7\u53bb\u7684\u72b6\u6001\u548c\u6d4b\u91cf\u4f5c\u4e3a\u989d\u5916\u7ea6\u675f\uff0c\u5e76\u5f15\u5165\u4e86\u80fd\u591f\u6839\u636e\u9000\u5316\u7a0b\u5ea6\u81ea\u9002\u5e94\u5730\u533a\u5206\u6d3b\u52a8\u72b6\u6001\u4e0e\u56fa\u5b9a\u72b6\u6001\u7684\u673a\u5236\u3002DA-DE\u5219\u4ece\u6d3b\u52a8\u72b6\u6001\u4e2d\u5254\u9664\u4fe1\u606f\u91cf\u8f83\u5c11\u7684\u6d4b\u91cf\u503c\uff0c\u5e76\u57fa\u4e8e\u5c40\u90e8\u53ef\u5b9a\u4f4d\u8d21\u732e\u5ea6\u53ca\u96c5\u53ef\u6bd4\u77e9\u9635\u6761\u4ef6\u6570\u9009\u62e9\u6027\u5730\u4f7f\u7528\u6765\u81ea\u56fa\u5b9a\u72b6\u6001\u7684\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u591a\u79cd\u9000\u5316\u6761\u4ef6\u4e0b\uff0cLODESTAR\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe\u7684\u91cc\u7a0b\u8ba1\u7b97\u6cd5\u4ee5\u53ca\u9488\u5bf9\u9000\u5316\u7684\u7279\u5b9a\u6a21\u5757\u800c\u8a00\uff0c\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "LODESTAR\u4f5c\u4e3a\u4e00\u79cd\u521b\u65b0\u6027\u7684LIO\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u5730\u5e94\u5bf9\u4e86\u7531\u4e8e\u73af\u5883\u56e0\u7d20\u5f15\u8d77\u7684\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u7a00\u758f\u6027\u548c\u4e0d\u5747\u8861\u6027\u6311\u6218\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u5b9a\u4f4d\u51c6\u786e\u6027\u4e0e\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.09241", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.09241", "abs": "https://arxiv.org/abs/2511.09241", "authors": ["Yuxi Wei", "Zirui Wang", "Kangning Yin", "Yue Hu", "Jingbo Wang", "Siheng Chen"], "title": "Unveiling the Impact of Data and Model Scaling on High-Level Control for Humanoid Robots", "comment": null, "summary": "Data scaling has long remained a critical bottleneck in robot learning. For humanoid robots, human videos and motion data are abundant and widely available, offering a free and large-scale data source. Besides, the semantics related to the motions enable modality alignment and high-level robot control learning. However, how to effectively mine raw video, extract robot-learnable representations, and leverage them for scalable learning remains an open problem. To address this, we introduce Humanoid-Union, a large-scale dataset generated through an autonomous pipeline, comprising over 260 hours of diverse, high-quality humanoid robot motion data with semantic annotations derived from human motion videos. The dataset can be further expanded via the same pipeline. Building on this data resource, we propose SCHUR, a scalable learning framework designed to explore the impact of large-scale data on high-level control in humanoid robots. Experimental results demonstrate that SCHUR achieves high robot motion generation quality and strong text-motion alignment under data and model scaling, with 37\\% reconstruction improvement under MPJPE and 25\\% alignment improvement under FID comparing with previous methods. Its effectiveness is further validated through deployment in real-world humanoid robot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Humanoid-Union\uff0c\u4e00\u4e2a\u901a\u8fc7\u81ea\u4e3b\u7ba1\u9053\u751f\u6210\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7260\u5c0f\u65f6\u7684\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u6570\u636e\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u5b66\u4e60\u6846\u67b6SCHUR\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u548c\u6a21\u578b\u6269\u5c55\u4e0b\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\u7684\u8d28\u91cf\u548c\u6587\u672c-\u52a8\u4f5c\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u957f\u4e45\u4ee5\u6765\uff0c\u6570\u636e\u6269\u5c55\u4e00\u76f4\u662f\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u74f6\u9888\u3002\u5bf9\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u6765\u8bf4\uff0c\u4eba\u7c7b\u89c6\u9891\u548c\u8fd0\u52a8\u6570\u636e\u4e30\u5bcc\u4e14\u5e7f\u6cdb\u53ef\u7528\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u514d\u8d39\u4e14\u5927\u89c4\u6a21\u7684\u6570\u636e\u6e90\u3002\u6b64\u5916\uff0c\u4e0e\u8fd0\u52a8\u76f8\u5173\u7684\u8bed\u4e49\u80fd\u591f\u5b9e\u73b0\u6a21\u6001\u5bf9\u9f50\u548c\u9ad8\u7ea7\u673a\u5668\u4eba\u63a7\u5236\u5b66\u4e60\u3002\u7136\u800c\uff0c\u5982\u4f55\u6709\u6548\u5730\u6316\u6398\u539f\u59cb\u89c6\u9891\u3001\u63d0\u53d6\u53ef\u4f9b\u673a\u5668\u4eba\u5b66\u4e60\u7684\u8868\u793a\uff0c\u5e76\u5229\u7528\u5b83\u4eec\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u5f15\u5165\u4e86Humanoid-Union\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u751f\u6210\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5305\u62ec\u8d85\u8fc7260\u5c0f\u65f6\u591a\u6837\u5316\u7684\u9ad8\u8d28\u91cf\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u5e26\u6709\u4ece\u4eba\u7c7b\u8fd0\u52a8\u89c6\u9891\u4e2d\u6d3e\u751f\u51fa\u7684\u8bed\u4e49\u6ce8\u91ca\u3002\u57fa\u4e8e\u8fd9\u4e00\u6570\u636e\u8d44\u6e90\uff0c\u4ed6\u4eec\u63d0\u51fa\u4e86SCHUR\uff0c\u4e00\u79cd\u65e8\u5728\u63a2\u7d22\u5927\u89c4\u6a21\u6570\u636e\u5bf9\u4eba\u5f62\u673a\u5668\u4eba\u9ad8\u5c42\u6b21\u63a7\u5236\u5f71\u54cd\u7684\u53ef\u6269\u5c55\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSCHUR\u5728\u6570\u636e\u548c\u6a21\u578b\u6269\u5c55\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86\u9ad8\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\u8d28\u91cf\u4ee5\u53ca\u5f3a\u6587\u672c-\u52a8\u4f5c\u5bf9\u9f50\u6548\u679c\uff0c\u76f8\u8f83\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u5728MPJPE\u4e0a\u91cd\u5efa\u6539\u8fdb\u4e8637%\uff0c\u5728FID\u4e0a\u7684\u5bf9\u9f50\u6539\u8fdb\u4e8625%\u3002\u5176\u6709\u6548\u6027\u8fd8\u901a\u8fc7\u90e8\u7f72\u5230\u771f\u5b9e\u4e16\u754c\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5f97\u5230\u4e86\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u6709\u6548\u5229\u7528\u5927\u89c4\u6a21\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5b66\u4e60\u6027\u80fd\u3002\u63d0\u51fa\u7684SCHUR\u6846\u67b6\u4e0d\u4ec5\u8bc1\u660e\u4e86\u4f7f\u7528\u5927\u89c4\u6a21\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u7684\u597d\u5904\uff0c\u540c\u65f6\u4e5f\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.09302", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.09302", "abs": "https://arxiv.org/abs/2511.09302", "authors": ["Yan Huang", "Shoujie Li", "Xingting Li", "Wenbo Ding"], "title": "UMIGen: A Unified Framework for Egocentric Point Cloud Generation and Cross-Embodiment Robotic Imitation Learning", "comment": null, "summary": "Data-driven robotic learning faces an obvious dilemma: robust policies demand large-scale, high-quality demonstration data, yet collecting such data remains a major challenge owing to high operational costs, dependence on specialized hardware, and the limited spatial generalization capability of current methods. The Universal Manipulation Interface (UMI) relaxes the strict hardware requirements for data collection, but it is restricted to capturing only RGB images of a scene and omits the 3D geometric information on which many tasks rely. Inspired by DemoGen, we propose UMIGen, a unified framework that consists of two key components: (1) Cloud-UMI, a handheld data collection device that requires no visual SLAM and simultaneously records point cloud observation-action pairs; and (2) a visibility-aware optimization mechanism that extends the DemoGen pipeline to egocentric 3D observations by generating only points within the camera's field of view. These two components enable efficient data generation that aligns with real egocentric observations and can be directly transferred across different robot embodiments without any post-processing. Experiments in both simulated and real-world settings demonstrate that UMIGen supports strong cross-embodiment generalization and accelerates data collection in diverse manipulation tasks.", "AI": {"tldr": "UMIGen, a framework that combines Cloud-UMI for handheld data collection and a visibility-aware optimization to generate 3D observation-action pairs, enhances cross-robot generalization and streamlines the collection of manipulation task data.", "motivation": "The motivation is to address the challenge of collecting large-scale, high-quality demonstration data for robotic learning, which is often hindered by high operational costs, reliance on specialized hardware, and limited spatial generalization. UMIGen aims to make data collection more efficient and transferable across different robot platforms.", "method": "UMIGen consists of two main parts: (1) Cloud-UMI, a device for handheld data collection that does not require visual SLAM and records point cloud observation-action pairs, and (2) a visibility-aware optimization mechanism that modifies the DemoGen pipeline to work with egocentric 3D observations, generating only points within the camera's field of view.", "result": "Experiments in both simulated and real-world settings show that UMIGen can support strong cross-embodiment generalization and accelerate the process of data collection for a variety of manipulation tasks.", "conclusion": "UMIGen offers an effective solution for improving the efficiency and generalizability of data collection in data-driven robotic learning, particularly for manipulation tasks, by facilitating the generation of 3D data that closely matches real-world egocentric observations and can be easily transferred between different robot embodiments."}}
{"id": "2511.09484", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09484", "abs": "https://arxiv.org/abs/2511.09484", "authors": ["Chaoyi Pan", "Changhao Wang", "Haozhi Qi", "Zixi Liu", "Homanga Bharadhwaj", "Akash Sharma", "Tingfan Wu", "Guanya Shi", "Jitendra Malik", "Francois Hogan"], "title": "SPIDER: Scalable Physics-Informed Dexterous Retargeting", "comment": "Project website: https://jc-bao.github.io/spider-project/", "summary": "Learning dexterous and agile policy for humanoid and dexterous hand control requires large-scale demonstrations, but collecting robot-specific data is prohibitively expensive. In contrast, abundant human motion data is readily available from motion capture, videos, and virtual reality, which could help address the data scarcity problem. However, due to the embodiment gap and missing dynamic information like force and torque, these demonstrations cannot be directly executed on robots. To bridge this gap, we propose Scalable Physics-Informed DExterous Retargeting (SPIDER), a physics-based retargeting framework to transform and augment kinematic-only human demonstrations to dynamically feasible robot trajectories at scale. Our key insight is that human demonstrations should provide global task structure and objective, while large-scale physics-based sampling with curriculum-style virtual contact guidance should refine trajectories to ensure dynamical feasibility and correct contact sequences. SPIDER scales across diverse 9 humanoid/dexterous hand embodiments and 6 datasets, improving success rates by 18% compared to standard sampling, while being 10X faster than reinforcement learning (RL) baselines, and enabling the generation of a 2.4M frames dynamic-feasible robot dataset for policy learning. As a universal physics-based retargeting method, SPIDER can work with diverse quality data and generate diverse and high-quality data to enable efficient policy learning with methods like RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u91cd\u5b9a\u5411\u6846\u67b6SPIDER\uff0c\u7528\u4e8e\u5c06\u4ec5\u542b\u6709\u4eba\u7c7b\u8fd0\u52a8\u5b66\u6570\u636e\u7684\u6f14\u793a\u8f6c\u6362\u4e3a\u52a8\u6001\u53ef\u884c\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5927\u89c4\u6a21\u57fa\u4e8e\u7269\u7406\u7684\u91c7\u6837\u548c\u8bfe\u7a0b\u5f0f\u7684\u865a\u62df\u63a5\u89e6\u6307\u5bfc\u6765\u7ec6\u5316\u8f68\u8ff9\uff0c\u786e\u4fdd\u52a8\u529b\u5b66\u53ef\u884c\u6027\u5e76\u4fee\u6b63\u63a5\u89e6\u987a\u5e8f\u3002", "motivation": "\u5b66\u4e60\u7075\u5de7\u4e14\u654f\u6377\u7684\u7c7b\u4eba\u673a\u5668\u4eba\u548c\u7075\u5de7\u624b\u63a7\u5236\u7b56\u7565\u9700\u8981\u5927\u89c4\u6a21\u7684\u793a\u8303\u6570\u636e\uff0c\u4f46\u6536\u96c6\u7279\u5b9a\u4e8e\u673a\u5668\u4eba\u7684\u6570\u636e\u6210\u672c\u8fc7\u9ad8\u3002\u76f8\u53cd\uff0c\u6765\u81ea\u52a8\u4f5c\u6355\u6349\u3001\u89c6\u9891\u548c\u865a\u62df\u73b0\u5b9e\u7684\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u4e30\u5bcc\u6613\u5f97\uff0c\u53ef\u4ee5\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8eab\u4f53\u5dee\u5f02\u4ee5\u53ca\u7f3a\u4e4f\u529b\u548c\u626d\u77e9\u7b49\u52a8\u6001\u4fe1\u606f\uff0c\u8fd9\u4e9b\u6f14\u793a\u65e0\u6cd5\u76f4\u63a5\u5728\u673a\u5668\u4eba\u4e0a\u6267\u884c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSPIDER\uff08\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u7269\u7406\u7684\u7075\u5de7\u91cd\u5b9a\u5411\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5c06\u4ec5\u6709\u8fd0\u52a8\u5b66\u4fe1\u606f\u7684\u4eba\u7c7b\u6f14\u793a\u8f6c\u6362\u6210\u5bf9\u4e8e\u673a\u5668\u4eba\u6765\u8bf4\u52a8\u6001\u4e0a\u53ef\u884c\u7684\u8f68\u8ff9\uff0c\u5e76\u4e14\u80fd\u591f\u5927\u89c4\u6a21\u5730\u8fdb\u884c\u8fd9\u79cd\u8f6c\u6362\u3002\u4e3b\u8981\u601d\u8def\u662f\u5229\u7528\u4eba\u7c7b\u6f14\u793a\u63d0\u4f9b\u5168\u5c40\u4efb\u52a1\u7ed3\u6784\u4e0e\u76ee\u6807\uff0c\u540c\u65f6\u901a\u8fc7\u5927\u89c4\u6a21\u57fa\u4e8e\u7269\u7406\u7684\u91c7\u6837\u52a0\u4e0a\u8bfe\u7a0b\u5f0f\u865a\u62df\u63a5\u89e6\u5f15\u5bfc\u6765\u4f18\u5316\u8f68\u8ff9\uff0c\u4ee5\u4fdd\u8bc1\u52a8\u529b\u5b66\u4e0a\u7684\u53ef\u884c\u6027\u53ca\u6b63\u786e\u7684\u63a5\u89e6\u5e8f\u5217\u3002", "result": "SPIDER\u80fd\u591f\u5728\u4e0d\u540c\u76849\u4e2a\u7c7b\u4eba/\u7075\u5de7\u624b\u5b9e\u4f8b\u548c6\u4e2a\u6570\u636e\u96c6\u4e0a\u5de5\u4f5c\uff0c\u76f8\u6bd4\u6807\u51c6\u91c7\u6837\u65b9\u6cd5\u63d0\u9ad8\u4e8618%\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u6bd4\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u5feb\u4e8610\u500d\uff0c\u5e76\u80fd\u751f\u6210240\u4e07\u5e27\u7684\u52a8\u529b\u5b66\u53ef\u884c\u673a\u5668\u4eba\u6570\u636e\u96c6\u7528\u4e8e\u7b56\u7565\u5b66\u4e60\u3002", "conclusion": "\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u7684\u57fa\u4e8e\u7269\u7406\u7684\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0cSPIDER\u53ef\u4ee5\u5904\u7406\u4e0d\u540c\u8d28\u91cf\u7684\u6570\u636e\uff0c\u5e76\u751f\u6210\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u6765\u652f\u6301\u50cf\u5f3a\u5316\u5b66\u4e60\u8fd9\u6837\u7684\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u7b56\u7565\u5b66\u4e60\u3002"}}
{"id": "2511.09516", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09516", "abs": "https://arxiv.org/abs/2511.09516", "authors": ["Runhao Li", "Wenkai Guo", "Zhenyu Wu", "Changyuan Wang", "Haoyuan Deng", "Zhenyu Weng", "Yap-Peng Tan", "Ziwei Wang"], "title": "MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation", "comment": null, "summary": "Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAP-VLA\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5386\u53f2\u6f14\u793a\u4e2d\u6784\u5efa\u8bb0\u5fc6\u5e93\u5e76\u4f7f\u7528\u5b66\u4e60\u7684\u8f6f\u63d0\u793a\u6765\u589e\u5f3a\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u4ee5\u6539\u8fdb\u957f\u65f6\u5e8f\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u673a\u5668\u4eba\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u957f\u671f\u89c4\u5212\u7684\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u8bb0\u5fc6\u80fd\u529b\u4e14\u53ea\u4f9d\u8d56\u5373\u65f6\u611f\u77e5\u8f93\u5165\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u589e\u52a0\u8bb0\u5fc6\u63d0\u793a\u6765\u63d0\u9ad8\u8fd9\u4e9b\u6a21\u578b\u5728\u6267\u884c\u957f\u65f6\u5e8f\u4efb\u52a1\u65f6\u7684\u80fd\u529b\u3002", "method": "MAP-VLA\u9996\u5148\u4ece\u5386\u53f2\u6f14\u793a\u4e2d\u521b\u5efa\u4e00\u4e2a\u8bb0\u5fc6\u5e93\uff0c\u6bcf\u4e2a\u8bb0\u5fc6\u5355\u5143\u90fd\u6355\u6349\u4e86\u4efb\u52a1\u7279\u5b9a\u9636\u6bb5\u7684\u4fe1\u606f\uff0c\u5e76\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7684\u8f6f\u63d0\u793a\u5b9e\u73b0\u3002\u7136\u540e\uff0c\u5728\u5b9e\u65f6\u4efb\u52a1\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u901a\u8fc7\u8f68\u8ff9\u76f8\u4f3c\u6027\u5339\u914d\u68c0\u7d22\u76f8\u5173\u8bb0\u5fc6\uff0c\u5e76\u5c06\u5176\u52a8\u6001\u6574\u5408\u5230VLA\u6a21\u578b\u4e2d\u4ee5\u751f\u6210\u589e\u5f3a\u7684\u52a8\u4f5c\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMAP-VLA\u5728\u6a21\u62df\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe7.0%\u7684\u7edd\u5bf9\u6027\u80fd\u589e\u76ca\uff0c\u5728\u5b9e\u9645\u673a\u5668\u4eba\u8bc4\u4f30\u4e2d\u9488\u5bf9\u957f\u65f6\u5e8f\u4efb\u52a1\u8fbe\u5230\u4e8625.0%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8fc7\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "MAP-VLA\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d2\u4ef6\u5f0f\u8bbe\u8ba1\u589e\u5f3a\u4e86\u51bb\u7ed3\u72b6\u6001\u4e0b\u7684VLA\u6a21\u578b\u5904\u7406\u590d\u6742\u3001\u957f\u65f6\u5e8f\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u663e\u793a\u51fa\u5176\u5728\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
