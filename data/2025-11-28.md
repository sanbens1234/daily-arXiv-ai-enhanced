<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Uncertainty Quantification for Visual Object Pose Estimation](https://arxiv.org/abs/2511.21666)
*Lorenzo Shaikewitz,Charis Georgiou,Luca Carlone*

Main category: cs.RO

TL;DR: 本研究开发了一种名为SLUE的方法，用于在单目设置中生成物体姿态估计的无分布不确定性边界。SLUE通过解决最小体积包围椭球问题的一个松弛版本来提供一个单一的椭球不确定性边界，该边界以高概率包含真实物体姿态。此外，还提出了一个求和平方松弛层次结构来获得更紧致的不确定性边界。实验表明，与先前工作相比，SLUE能生成显著较小的平移边界和具有竞争力的方向边界。


<details>
  <summary>Details</summary>
Motivation: 尽管姿态估计是机器人学中一个被广泛研究的问题，但在没有严格的分布假设下为其附加统计上严谨的不确定性仍不明确。因此，对于稳健控制和规划来说，量化物体姿态估计中的不确定性至关重要。

Method: 研究人员开发了SLUE（S-Lemma Uncertainty Estimation），一种凸程序方法，它基于2D语义关键点像素检测上的高概率噪声边界来推导出关于给定姿态估计的分布无关的姿态不确定性边界。SLUE通过解决受著名S-lemma启发的最小体积包围椭球问题的一种放松形式，将非凸集合简化为单一椭球形不确定性边界。为了在同一置信度下获得更紧凑的不确定性边界，还扩展了SLUE至一个保证收敛到给定关键点约束集的最小体积椭球不确定性边界的求和平方松弛层次结构。

Result: SLUE不需要对边界形状或大小的初始猜测，并且保证以高概率包含真实物体姿态。此外，该方法能够轻松地将姿态不确定性边界投影到独立的平移和轴角方向边界上。在两个姿态估计数据集和一个现实世界无人机跟踪场景上的评估显示，与先前的工作相比，SLUE生成了明显更小的平移边界以及具有竞争力的方向边界。

Conclusion: 本研究提出的SLUE方法为物体姿态估计提供了有效的无分布不确定性边界，在保持高置信度的同时实现了更紧凑的姿态不确定性表示。这对于需要精确姿态信息的应用领域，如无人机跟踪等，尤为重要。

Abstract: Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.

</details>


### [2] [TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos](https://arxiv.org/abs/2511.21690)
*Seungjae Lee,Yoonkyo Jung,Inkook Chun,Yao-Chih Lee,Zikui Cai,Hongjia Huang,Aayush Talreja,Tan Dat Dao,Yongyuan Liang,Jia-Bin Huang,Furong Huang*

Main category: cs.RO

TL;DR: 本文提出了一种新的方法TraceGen，通过构建一个统一的符号表示——3D“轨迹空间”来解决机器人从少量演示中学习新任务的问题。这种方法能够跨实体、环境和任务学习。通过TraceForge数据管道将多样化的视频转换为一致的3D轨迹，使得TraceGen能够在仅有五个目标机器人视频的情况下达到80%的成功率，并且在使用手持电话拍摄的人类演示视频时也能达到67.5%的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前机器人从少量演示中学到新任务面临挑战，尤其是在不同实体（如人类和其他机器人）、相机视角及环境差异的影响下直接利用这些视频变得更加困难。为了解决这一小样本问题，提出了新的解决方案。

Method: 提出了TraceGen，这是一种世界模型，它预测的是轨迹空间而非像素空间中的未来运动，从而抽象出外观同时保留了操作所需的几何结构。为了大规模训练TraceGen，开发了一个名为TraceForge的数据处理流程，可以将不同类型的人类和机器人视频转化为一致性的3D轨迹，生成了包含123K个视频和1.8M观察-轨迹-语言三元组的数据集。

Result: 预训练后的TraceGen能够在只有五个目标机器人视频的情况下，在四个任务上达到80%的成功率，而且推理速度比最新的基于视频的世界模型快50至600倍。即使是在用手机随意拍摄的五个人类演示视频，它也能够在一个真实机器人上实现67.5%的成功率。

Conclusion: 本研究提出的TraceGen方法展示了强大的适应性和效率，能够在不同实体间转移知识而不需要依赖于物体检测器或复杂的像素级生成技术，为机器人快速学习新技能提供了有效途径。

Abstract: Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.

</details>
