<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors](https://arxiv.org/abs/2512.09065)
*Shivendra Agrawal,Jake Brawer,Ashutosh Naik,Alessandro Roncone,Bradley Hayes*

Main category: cs.RO

TL;DR: ShelfAware是一个基于语义粒子滤波器的全局定位系统，它利用物体类别级别的统计证据而非固定地标来处理室内工作空间中的重复几何、动态杂乱和感知噪声。该方法通过融合深度似然与以类别为中心的语义相似度，并采用预计算的语义视点库执行MCL内的逆向语义提议，从而在低成本视觉硬件上实现快速目标假设生成。实验表明，ShelfAware在零售环境中实现了96%的成功率，平均收敛时间为1.91秒，且在所有条件下都保持了最低的平移RMSE。此外，它还能作为无基础设施组件集成到移动机器人中，支持仓库、实验室及零售环境下的应用，并为视觉障碍者提供辅助导航功能。


<details>
  <summary>Details</summary>
Motivation: 针对室内工作空间中存在的准静态特性——即整体布局稳定但局部语义不断变化的情况，导致出现重复几何形状、动态障碍物以及对基于视觉定位方法构成挑战的感知噪音问题。为了克服这些挑战并提高定位准确性，提出了ShelfAware系统。

Method: ShelfAware采用了结合深度似然和以对象类别为中心的语义相似性的方法，并利用预先计算好的一系列语义视角来进行蒙特卡洛定位（MCL）框架内的逆向语义提案。这种方法允许在仅使用视觉传感器和视觉惯性里程计(VIO)的情况下，在低成本硬件上实现快速而有针对性的假设生成。

Result: 通过对四个不同条件下的100次全局定位试验分析发现，ShelfAware在零售环境中达到了96%的成功率，平均收敛时间为1.91秒，并且在所有测试条件下都获得了最低的平移均方根误差(RMSE)。此外，该系统能够在80%的测试序列中维持稳定的跟踪效果。

Conclusion: ShelfAware通过在类别级别建模语义分布并利用逆向提案技术有效解决了准静态领域内常见的几何别名和语义漂移问题。由于只需要视觉传感器和VIO即可运行，因此它可以作为无需额外基础设施的支持模块应用于仓库、实验室和零售等场景中的移动机器人上；同时，也为开发面向视觉障碍人士的辅助导航设备提供了可能。

Abstract: Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.

</details>


### [2] [Inferring Operator Emotions from a Motion-Controlled Robotic Arm](https://arxiv.org/abs/2512.09086)
*Xinyu Qi,Zeyu Deng,Shaun Alexander Macdonald,Liying Li,Chen Wang,Muhammad Ali Imran,Philip G. Zhao*

Main category: cs.RO

TL;DR: 该研究通过机器学习系统，利用远程控制机器人化身的功能性动作来推断人类操作员的情绪状态，而无需依赖读取生命体征或肢体语言等传统方法。实验结果表明，该系统在识别用户情绪状态方面达到了83.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 远程机器人操作者的情感状态对机器人运动有着显著影响，可能导致意外后果。然而，在远程机器人控制场景中识别用户操作者的情感状态尚未得到充分探索。当前的情绪识别方法依赖于读取用户的生理信号或身体语言，但这需要额外设备和用户参与，给远程机器人控制带来了限制。

Method: 研究人员开发了一种基于机器学习的系统，能够通过分析非为情感表达设计的远程控制机器人化身的动作来推测人类操作者的情绪状态。这些动作是由操作者的手部动作引起的。

Result: 该系统在识别由机器人动作所表现出来的用户情绪状态上达到了83.3%的准确度。

Conclusion: 这项研究表明，即使使用不专为情感表达设计的机器人，也能有效识别远程控制情境下人类操作者的情绪状态。这对当前及未来的远程机器人操作以及情感机器人领域具有重要意义。

Abstract: A remote robot operator's affective state can significantly impact the resulting robot's motions leading to unexpected consequences, even when the user follows protocol and performs permitted tasks. The recognition of a user operator's affective states in remote robot control scenarios is, however, underexplored. Current emotion recognition methods rely on reading the user's vital signs or body language, but the devices and user participation these measures require would add limitations to remote robot control. We demonstrate that the functional movements of a remote-controlled robotic avatar, which was not designed for emotional expression, can be used to infer the emotional state of the human operator via a machine-learning system. Specifically, our system achieved 83.3$\%$ accuracy in recognizing the user's emotional state expressed by robot movements, as a result of their hand motions. We discuss the implications of this system on prominent current and future remote robot operation and affective robotic contexts.

</details>


### [3] [Masked Generative Policy for Robotic Control](https://arxiv.org/abs/2512.09101)
*Lipeng Zhuang,Shiyu Fan,Florent P. Audonnet,Yingdong Ru,Gerardo Aragon Camarasa,Paul Henderson*

Main category: cs.RO

TL;DR: 本文提出了一种新的视觉运动模仿学习框架——Masked Generative Policy (MGP)，通过将动作表示为离散的token，并训练一个条件遮罩变换器来并行生成这些token，然后快速仅对低置信度的token进行细化。MGP-Short适用于马尔可夫任务，而MGP-Long则能够在单次传递中预测完整轨迹，并基于新观察动态地改进低置信度的动作token。实验表明，MGP在150个机器人操作任务上实现了更快的推理速度和更高的成功率。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法难以处理复杂且非马尔科夫任务的问题，作者旨在开发一种新的框架，以实现全局一致性的预测和鲁棒自适应执行能力，从而提高机器人在模仿学习中的性能。

Method: 提出了Masked Generative Policy (MGP)框架，该框架采用离散token表示动作并通过条件遮罩变压器进行处理。此外，还引入了两种新的采样范式：MGP-Short和MGP-Long，分别适用于不同类型的控制任务。

Result: 在Meta-World和LIBERO基准测试覆盖的150个机器人操作任务上进行了广泛评估，结果显示MGP相比最先进的扩散和自回归策略，在保持快速推理的同时提高了平均成功率。特别地，在动态及缺失观测环境中，MGP进一步提升了平均成功率。

Conclusion: MGP框架证明了其在提升机器人模仿学习效率与准确性方面的潜力，特别是在处理复杂、非马尔科夫任务时表现出色。

Abstract: We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.

</details>


### [4] [Cognitive Trust in HRI: "Pay Attention to Me and I'll Trust You Even if You are Wrong"](https://arxiv.org/abs/2512.09105)
*Adi Manor,Dan Cohen,Ziv Keidar,Avi Parush,Hadas Erel*

Main category: cs.RO

TL;DR: 该研究探讨了机器人能力和注意力对人机交互中认知信任的影响，发现高注意力可以弥补低能力带来的不足，表明建立认知信任的过程比以前认为的更复杂，涉及通常被忽视的情感过程。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索影响人机互动中认知信任的因素，特别是机器人能力和其注意力之间的相互作用，以及它们如何共同构建用户对机器人的信任感。

Method: 通过2x2实验设计（能力高低和注意力高低），参与者与一只机器人狗完成搜索任务，以此来评估不同组合下机器人能力与注意力对参与者认知信任水平的影响。

Result: 结果显示，当机器人表现出高度注意力时，即使它的表现不佳，参与者的信任度也能保持在一个较高的水平；而缺乏注意力的情况下，低能力显著降低了认知信任。

Conclusion: 结论指出，在人类与机器人交互过程中建立认知信任可能涉及到情感补偿机制，这为理解认知信任提供了新的视角，即除了传统的基于能力的信任模型外还应考虑情感因素的作用。

Abstract: Cognitive trust and the belief that a robot is capable of accurately performing tasks, are recognized as central factors in fostering high-quality human-robot interactions. It is well established that performance factors such as the robot's competence and its reliability shape cognitive trust. Recent studies suggest that affective factors, such as robotic attentiveness, also play a role in building cognitive trust. This work explores the interplay between these two factors that shape cognitive trust. Specifically, we evaluated whether different combinations of robotic competence and attentiveness introduce a compensatory mechanism, where one factor compensates for the lack of the other. In the experiment, participants performed a search task with a robotic dog in a 2x2 experimental design that included two factors: competence (high or low) and attentiveness (high or low). The results revealed that high attentiveness can compensate for low competence. Participants who collaborated with a highly attentive robot that performed poorly reported trust levels comparable to those working with a highly competent robot. When the robot did not demonstrate attentiveness, low competence resulted in a substantial decrease in cognitive trust. The findings indicate that building cognitive trust in human-robot interaction may be more complex than previously believed, involving emotional processes that are typically overlooked. We highlight an affective compensatory mechanism that adds a layer to consider alongside traditional competence-based models of cognitive trust.

</details>


### [5] [Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous](https://arxiv.org/abs/2512.09111)
*Yuji Takubo,Arpit Dwivedi,Sukeerth Ramkumar,Luis A. Pabon,Daniele Gammelli,Marco Pavone,Simone D'Amico*

Main category: cs.RO

TL;DR: 本文介绍了一种名为SAGES的航天器轨迹生成框架，它能够将自然语言命令转化为符合高级意图且遵守非凸约束条件的航天器轨迹。实验表明，SAGES能够在多种行为模式下与人类指令保持高度一致，为通过直观自然语言命令交互式地指导航天器的安全和行为提供了可能。


<details>
  <summary>Details</summary>
Motivation: 现有自主航天器实时轨迹生成方法需要大量专家输入（如航点、约束条件、任务时间线等），这限制了其在实际对接任务中的操作可扩展性。为了减少对专家依赖并提高系统适应性，提出了新的解决方案。

Method: 开发了SAGES (Semantic Autonomous Guidance Engine for Space) 框架，该框架能将自然语言指令转换成反映高层次意图同时遵守非凸约束条件的航天器轨迹。

Result: 实验结果表明，在容错近距离操作及连续时间约束执行、自由飞行机器人平台两个场景中，SAGES能够可靠地产生与人类命令相一致的轨迹，实现了超过90%的行为一致性。

Conclusion: 这项工作代表了朝向语言条件化、约束感知的航天器轨迹生成迈出的第一步，使操作员能够通过直观的自然语言命令以较少的专业负担来互动地指导安全性和行为。

Abstract: Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous missions.This paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90\% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden.

</details>


### [6] [UPETrack: Unidirectional Position Estimation for Tracking Occluded Deformable Linear Objects](https://arxiv.org/abs/2512.09283)
*Fan Wu,Chenguang Yang,Haibin Yang,Shuo Wang,Yanrui Xu,Xing Zhou,Meng Gao,Yaoqi Xian,Zhihong Zhu,Shifeng Huang*

Main category: cs.RO

TL;DR: 提出了一种基于单向位置估计（UPE）的几何驱动框架UPETrack，用于实时跟踪可变形线性物体（DLOs），无需物理建模、虚拟仿真或视觉标记。该方法通过高斯混合模型（GMM）和期望最大化（EM）算法追踪可见段，并使用UPE算法预测遮挡区域。实验表明，UPETrack在定位准确性和计算效率上优于现有的两种先进跟踪算法。


<details>
  <summary>Details</summary>
Motivation: 为了解决工业装配、医疗程序及日常生活应用中对可变形线性物体(DLOs)进行机器人操作时面临的高维配置空间、非线性动力学以及频繁部分遮挡等挑战，从而实现鲁棒的实时DLO跟踪。

Method: 开发了名为UPETrack的新框架，它基于单向位置估计(UPE)，分为两个阶段运作：1) 利用通过期望最大化(EM)算法拟合的高斯混合模型(GMM)来追踪可见段；2) 使用我们提出的UPE算法来进行遮挡区域预测。UPE利用DLO形状中的几何连续性及其时间演变模式，通过三个主要机制建立封闭形式的位置估计器：局部线性组合位移项、邻近线性约束项和历史曲率项。

Result: 实验结果证明，UPETrack在定位精度与计算效率方面都超越了包括TrackDLO和CDCPD2在内的两种最先进的跟踪算法。

Conclusion: UPETrack提供了一种有效且高效的解决方案，以应对DLOs实时状态跟踪中存在的挑战，特别是针对高维配置空间、非线性动态行为及常见遮挡问题，而无需依赖物理建模、虚拟仿真或视觉标记。

Abstract: Real-time state tracking of Deformable Linear Objects (DLOs) is critical for enabling robotic manipulation of DLOs in industrial assembly, medical procedures, and daily-life applications. However, the high-dimensional configuration space, nonlinear dynamics, and frequent partial occlusions present fundamental barriers to robust real-time DLO tracking. To address these limitations, this study introduces UPETrack, a geometry-driven framework based on Unidirectional Position Estimation (UPE), which facilitates tracking without the requirement for physical modeling, virtual simulation, or visual markers. The framework operates in two phases: (1) visible segment tracking is based on a Gaussian Mixture Model (GMM) fitted via the Expectation Maximization (EM) algorithm, and (2) occlusion region prediction employing UPE algorithm we proposed. UPE leverages the geometric continuity inherent in DLO shapes and their temporal evolution patterns to derive a closed-form positional estimator through three principal mechanisms: (i) local linear combination displacement term, (ii) proximal linear constraint term, and (iii) historical curvature term. This analytical formulation allows efficient and stable estimation of occluded nodes through explicit linear combinations of geometric components, eliminating the need for additional iterative optimization. Experimental results demonstrate that UPETrack surpasses two state-of-the-art tracking algorithms, including TrackDLO and CDCPD2, in both positioning accuracy and computational efficiency.

</details>


### [7] [Scene-agnostic Hierarchical Bimanual Task Planning via Visual Affordance Reasoning](https://arxiv.org/abs/2512.09310)
*Kwang Bin Lee,Jiho Kang,Sung-Hee Lee*

Main category: cs.RO

TL;DR: 本文提出了一种适用于未知场景的双手机器人任务规划框架，通过视觉点定位、双目标子规划和基于交互点的双手机器人提示三个模块，实现了从高层指令到具体执行动作的有效转换。实验表明该方法能够生成连贯、可行且紧凑的双手计划，并在杂乱环境中具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人任务规划系统主要针对单手操作设计，难以应对复杂环境下双手协调操作的空间、几何及协调性挑战。为了填补这一空白，作者旨在开发一种能够在任何场景下有效实现双手任务规划的新方法。

Method: 本研究构建了一个整合了视觉点定位（VPG）、双目标子规划（BSP）以及基于交互点的双手机器人提示（IPBP）三大核心组件的统一框架。其中，VPG负责从图像中识别物体并确定世界坐标下的互动点；BSP则根据空间邻近性和跨物体可达性来制定简明的动作子目标；IPBP将这些子目标与技能库相结合，生成满足手部状态和功能要求的动作序列。

Result: 实验结果表明，所提方法不仅能够产生逻辑清晰、实际可执行且高效紧凑的双手操作方案，而且无需额外训练即可适应于各种复杂环境。这证明了该框架对于未知场景中的双手任务具备强大的适应能力和合理的物理可行性。

Conclusion: 综上所述，本文介绍了一种新的双手机器人任务规划解决方案，成功地将高级语义理解与低级物理执行相结合，为解决现实世界中复杂的双手操作问题提供了强有力的支持。

Abstract: Embodied agents operating in open environments must translate high-level instructions into grounded, executable behaviors, often requiring coordinated use of both hands. While recent foundation models offer strong semantic reasoning, existing robotic task planners remain predominantly unimanual and fail to address the spatial, geometric, and coordination challenges inherent to bimanual manipulation in scene-agnostic settings. We present a unified framework for scene-agnostic bimanual task planning that bridges high-level reasoning with 3D-grounded two-handed execution. Our approach integrates three key modules. Visual Point Grounding (VPG) analyzes a single scene image to detect relevant objects and generate world-aligned interaction points. Bimanual Subgoal Planner (BSP) reasons over spatial adjacency and cross-object accessibility to produce compact, motion-neutralized subgoals that exploit opportunities for coordinated two-handed actions. Interaction-Point-Driven Bimanual Prompting (IPBP) binds these subgoals to a structured skill library, instantiating synchronized unimanual or bimanual action sequences that satisfy hand-state and affordance constraints. Together, these modules enable agents to plan semantically meaningful, physically feasible, and parallelizable two-handed behaviors in cluttered, previously unseen scenes. Experiments show that it produces coherent, feasible, and compact two-handed plans, and generalizes to cluttered scenes without retraining, demonstrating robust scene-agnostic affordance reasoning for bimanual tasks.

</details>


### [8] [Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances](https://arxiv.org/abs/2512.09377)
*Lidan Xu,Dadong Fan,Junhong Wang,Wenshuo Li,Hao Lu,Jianzhong Qiao*

Main category: cs.RO

TL;DR: 研究证明了在仅有两架无人机的里程计信息下，当存在两种或更少类型的综合扰动时，整个系统（包括负载姿态）是可观测的。基于此结论，提出了一种复合扰动滤波方案，通过设计一种基于扰动观测器的误差状态扩展卡尔曼滤波器来同时估计状态和扰动，从而提高了系统的估计性能。


<details>
  <summary>Details</summary>
Motivation: 合作悬浮空中运输对诸如空气动力效应和推力不确定性等多源干扰非常敏感。为了实现精确的负载操作，现有方法通常依赖额外的传感器来测量缆绳方向或负载的姿态，这增加了系统成本和复杂性。因此，一个基本的问题是：是否仅使用无人机的里程计信息，在多源干扰下就能观察到负载的姿态？

Method: 本工作专注于双无人机-杆系统，并利用可观测性秩标准证明了当只有两种或更少类型的综合扰动存在时，整个系统是可观测的。为验证这一分析，考虑了扰动只作用于无人机的情况，并开发了一种复合扰动滤波方案。设计了一种基于扰动观测器的误差状态扩展卡尔曼滤波器，用于状态和扰动估计。

Result: 仿真和实验测试验证了仅凭无人机的里程计信息就完全有可能估计出系统的状态和扰动。

Conclusion: 这项工作的发现为构建更具成本效益且鲁棒性强的系统铺平了道路，通过减少所需的传感器套件数量降低了系统复杂度。

Abstract: Cooperative suspended aerial transportation is highly susceptible to multi-source disturbances such as aerodynamic effects and thrust uncertainties. To achieve precise load manipulation, existing methods often rely on extra sensors to measure cable directions or the payload's pose, which increases the system cost and complexity. A fundamental question remains: is the payload's pose observable under multi-source disturbances using only the drones' odometry information? To answer this question, this work focuses on the two-drone-bar system and proves that the whole system is observable when only two or fewer types of lumped disturbances exist by using the observability rank criterion. To the best of our knowledge, we are the first to present such a conclusion and this result paves the way for more cost-effective and robust systems by minimizing their sensor suites. Next, to validate this analysis, we consider the situation where the disturbances are only exerted on the drones, and develop a composite disturbance filtering scheme. A disturbance observer-based error-state extended Kalman filter is designed for both state and disturbance estimation, which renders improved estimation performance for the whole system evolving on the manifold $(\mathbb{R}^3)^2\times(TS^2)^3$. Our simulation and experimental tests have validated that it is possible to fully estimate the state and disturbance of the system with only odometry information of the drones.

</details>


### [9] [H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos](https://arxiv.org/abs/2512.09406)
*Hai Ci,Xiaokang Liu,Pei Yang,Yiren Song,Mike Zheng Shou*

Main category: cs.RO

TL;DR: 该论文提出了一种视频到视频的转换框架，能够将普通的人与物体互动的视频转化为具有真实物理交互效果的机器人操作视频。通过在训练视频中对机器人手臂进行修补以获得干净背景，并覆盖一个简单的视觉提示（标记和箭头指示夹具的位置和方向），可以训练生成模型将机器人手臂重新插入场景。实验结果显示，此方法相比基线能产生更逼真且基于实际物理的机器人动作，为从未标记的人类视频中扩展机器人学习提供了一个有前景的方向。


<details>
  <summary>Details</summary>
Motivation: 作者旨在开发一种能够让机器人从日常人类视频中学习操作技能的方法，从而避免繁琐的机器人数据收集过程。通过这种方法，希望机器人能够获取广泛的能力。

Method: 提出了一种视频到视频的转换框架，该框架不需要任何配对的人-机器人视频，只需要一组未配对的机器人视频即可进行训练。引入了一种可转移表示来桥接实体差距：通过对训练视频中的机器人手臂进行修补以得到干净背景，并叠加一个简单视觉提示（标记及箭头显示夹具位置与朝向），条件化生成模型将机器人手臂重新插入场景之中。测试时，对人类视频采用相同处理方式（修补人物并叠加人体姿态线索）来生成高质量模仿人类动作的机器人视频。此外，还微调了SOTA视频扩散模型（Wan 2.2版）以确保时间一致性，并利用其丰富的先验知识。

Result: 实证结果表明，所提方法比基线方法产生了显著更加逼真、基于物理的机器人运动，这为从无标签的人类视频中扩大机器人学习提供了很有前途的方向。

Conclusion: 本研究展示了一种新的途径，即通过视频到视频的转换技术使机器人能够从大量容易获取的人类活动视频中学习有用的操作技能，而无需专门收集机器人特定的数据集。

Abstract: Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/

</details>


### [10] [Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation](https://arxiv.org/abs/2512.09410)
*Jialin Ying,Zhihao Li,Zicheng Dong,Guohua Wu,Yihuan Liao*

Main category: cs.RO

TL;DR: 提出了一种名为PGF-MAPPO的新框架，通过结合拓扑规划与反应控制来解决多智能体在复杂环境中协作追捕的问题。该方法利用A*算法的势场进行密集奖励塑形，并引入方向前沿分配策略以促进空间分散和加速覆盖。实验表明，相比其他基线方法，PGF-MAPPO能够更高效地捕捉更快的逃避者，并且具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在复杂的环境中执行协作追逐-逃避任务面临稀疏奖励和视野受限等问题。传统的多智能体强化学习方法往往探索效率低下，难以扩展至大规模场景。

Method: 提出了PGF-MAPPO（路径引导前沿MAPPO），一个将拓扑规划与反应式控制相结合的分层框架。为了解决局部极小值和稀疏奖励问题，集成基于A*算法的势场来进行密集奖励塑形。此外，还引入了方向前沿分配策略，结合最远点采样与几何角度抑制技术，强制实现空间分散并加快覆盖速度。该架构采用参数共享的去中心化批评家设计，保持O(1)模型复杂度，适合于机器人集群应用。

Result: 实验结果表明，PGF-MAPPO对更快的逃避者表现出更高的捕捉效率。在10x10地图上训练的策略能够在未见过的20x20环境中有很好的零样本泛化表现，显著优于基于规则和其他学习方法的基线。

Conclusion: 本研究提出的PGF-MAPPO不仅解决了多智能体在复杂环境中协作追捕所面临的挑战，而且展示了强大的性能以及出色的泛化能力，为未来的研究提供了新的视角。

Abstract: Collaborative pursuit-evasion in cluttered environments presents significant challenges due to sparse rewards and constrained Fields of View (FOV). Standard Multi-Agent Reinforcement Learning (MARL) often suffers from inefficient exploration and fails to scale to large scenarios. We propose PGF-MAPPO (Path-Guided Frontier MAPPO), a hierarchical framework bridging topological planning with reactive control. To resolve local minima and sparse rewards, we integrate an A*-based potential field for dense reward shaping. Furthermore, we introduce Directional Frontier Allocation, combining Farthest Point Sampling (FPS) with geometric angle suppression to enforce spatial dispersion and accelerate coverage. The architecture employs a parameter-shared decentralized critic, maintaining O(1) model complexity suitable for robotic swarms. Experiments demonstrate that PGF-MAPPO achieves superior capture efficiency against faster evaders. Policies trained on 10x10 maps exhibit robust zero-shot generalization to unseen 20x20 environments, significantly outperforming rule-based and learning-based baselines.

</details>


### [11] [D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM](https://arxiv.org/abs/2512.09411)
*Siting Zhu,Yuxiang Huang,Wenhua Wu,Chaokang Jiang,Yongbo Chen,I-Ming Chen,Hesheng Wang*

Main category: cs.RO

TL;DR: 本文提出了一种新的动态SLAM系统D$^2$GSLAM，利用高斯表示同时执行准确的动态重建和在动态环境中的鲁棒跟踪。该系统由四个关键组件组成：几何提示的动态分离方法、动态-静态复合表示、渐进式姿态细化策略以及运动一致性损失。


<details>
  <summary>Details</summary>
Motivation: 尽管密集SLAM在静态环境中表现出色，但在动态环境下的应用仍面临挑战。现有方法通常直接移除动态物体，只关注静态场景重建，忽略了这些动态物体中包含的运动信息。为解决这一问题，研究提出了D$^2$GSLAM系统。

Method: 1. 提出了一种基于几何一致性的动态分离方法，用于区分场景中的静态与动态元素，并通过此方法获得粗略动态区域。
2. 引入了动态-静态复合表示，将静态3D高斯与动态4D高斯相结合，以建模场景中对象从静态到动态状态之间的转变。
3. 采用渐进式姿态细化策略，结合静态场景几何多视图一致性及动态物体运动信息来实现精确相机追踪。
4. 设计了一种运动一致性损失，利用物体运动的时间连续性来进行准确的动态建模。

Result: D$^2$GSLAM在动态场景中的地图构建和跟踪准确性方面表现优异，并且能够进行精确的动态建模。

Conclusion: D$^2$GSLAM系统通过其独特的设计有效解决了动态环境下的SLAM问题，不仅提高了动态场景的地图构建和跟踪精度，还展示了在动态建模方面的潜力。

Abstract: Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments. However, dense SLAM in dynamic environments remains challenging. Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects. In this paper, we present D$^2$GSLAM, a novel dynamic SLAM system utilizing Gaussian representation, which simultaneously performs accurate dynamic reconstruction and robust tracking within dynamic environments. Our system is composed of four key components: (i) We propose a geometric-prompt dynamic separation method to distinguish between static and dynamic elements of the scene. This approach leverages the geometric consistency of Gaussian representation and scene geometry to obtain coarse dynamic regions. The regions then serve as prompts to guide the refinement of the coarse mask for achieving accurate motion mask. (ii) To facilitate accurate and efficient mapping of the dynamic scene, we introduce dynamic-static composite representation that integrates static 3D Gaussians with dynamic 4D Gaussians. This representation allows for modeling the transitions between static and dynamic states of objects in the scene for composite mapping and optimization. (iii) We employ a progressive pose refinement strategy that leverages both the multi-view consistency of static scene geometry and motion information from dynamic objects to achieve accurate camera tracking. (iv) We introduce a motion consistency loss, which leverages the temporal continuity in object motions for accurate dynamic modeling. Our D$^2$GSLAM demonstrates superior performance on dynamic scenes in terms of mapping and tracking accuracy, while also showing capability in accurate dynamic modeling.

</details>


### [12] [A Hierarchical, Model-Based System for High-Performance Humanoid Soccer](https://arxiv.org/abs/2512.09431)
*Quanyou Wang,Mingzhang Zhu,Ruochen Hou,Kay Gillespie,Alvin Zhu,Shiqi Wang,Yicheng Wang,Gaberiel I. Fernandez,Yeting Liu,Colin Togashi,Hyunwoo Nam,Aditya Navghare,Alex Xu,Taoyuanmin Zhu,Min Sung Ahn,Arturo Flores Alvarez,Justin Quan,Ethan Hong,Dennis W. Hong*

Main category: cs.RO

TL;DR: 本文介绍了我们团队在2024年成人尺寸人形足球比赛中获胜所依赖的硬件和软件创新，包括轻量化设计、高扭矩执行器、集成感知定位框架等，并展示了这些子系统的无缝集成如何在动态对抗条件下实现快速、精准且战术有效的比赛表现。


<details>
  <summary>Details</summary>
Motivation: 随着驱动、感知和控制技术的进步，运动型人形机器人的发展引起了广泛关注。RoboCup作为一个完全自主的人形机器人国际竞赛，为这类系统提供了一个独特挑战性的基准，其长远目标是在2050年前与人类足球运动员竞争。本文旨在介绍支持我们团队赢得2024年成人尺寸人形足球比赛冠军的关键技术和策略。

Method: 硬件方面：开发了采用轻量级结构组件、高扭矩准直驱执行器及特殊脚部设计的成人尺寸人形平台；软件方面：构建了一个结合立体视觉、物体检测以及基于地标融合的综合感知与定位框架，同时开发了一套考虑碰撞避免的动态可行路径生成算法，以及一个基于游戏状态演化的集中式行为管理器来协调高层决策、角色选择和踢球动作。

Result: 通过上述硬件与软件子系统的无缝整合，实现了在真实比赛中的快速反应、精确操作以及战术有效性，从而确保了在动态变化和对抗环境下仍能保持稳定表现。

Conclusion: 文章总结了ARTEMIS作为2024年人形足球锦标赛成年组冠军背后的设计原则、系统架构及实验结果，展示了如何通过技术创新克服复杂环境下的挑战。

Abstract: The development of athletic humanoid robots has gained significant attention as advances in actuation, sensing, and control enable increasingly dynamic, real-world capabilities. RoboCup, an international competition of fully autonomous humanoid robots, provides a uniquely challenging benchmark for such systems, culminating in the long-term goal of competing against human soccer players by 2050. This paper presents the hardware and software innovations underlying our team's victory in the RoboCup 2024 Adult-Sized Humanoid Soccer Competition. On the hardware side, we introduce an adult-sized humanoid platform built with lightweight structural components, high-torque quasi-direct-drive actuators, and a specialized foot design that enables powerful in-gait kicks while preserving locomotion robustness. On the software side, we develop an integrated perception and localization framework that combines stereo vision, object detection, and landmark-based fusion to provide reliable estimates of the ball, goals, teammates, and opponents. A mid-level navigation stack then generates collision-aware, dynamically feasible trajectories, while a centralized behavior manager coordinates high-level decision making, role selection, and kick execution based on the evolving game state. The seamless integration of these subsystems results in fast, precise, and tactically effective gameplay, enabling robust performance under the dynamic and adversarial conditions of real matches. This paper presents the design principles, system architecture, and experimental results that contributed to ARTEMIS's success as the 2024 Adult-Sized Humanoid Soccer champion.

</details>


### [13] [Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments](https://arxiv.org/abs/2512.09447)
*Jaehyun Kim,Seungwon Choi,Tae-Wan Kim*

Main category: cs.RO

TL;DR: 提出了一种描述符无关的、多帧闭环验证方法，利用截断的顺序概率比测试(SPRT)来改善室内结构重复环境中的定位精度。


<details>
  <summary>Details</summary>
Motivation: 在结构重复的室内环境中，传统的单帧描述符比较或固定阈值加后期ICP校验的方法容易产生误报，因此需要一种新的方法来提高定位准确性并减少误报。

Method: 该方法通过累积查询与每个候选之间的短时描述符相似性流，并根据用户指定的一类/二类错误设计目标自适应地做出接受/拒绝决策。采用了先精度后策略以抑制室内结构重复环境下的假阳性结果。

Result: 评估表明，与单帧和启发式多帧基线相比，该序列验证器一致提高了精度，并减少了别名环路的影响。

Conclusion: 所提出的多帧闭环验证方法能够有效提高室内重复结构环境下的定位精度，并且对于不同类型的LiDAR全局描述符均表现出色。

Abstract: We propose a descriptor-agnostic, multi-frame loop closure verification method that formulates LiDAR loop closure as a truncated Sequential Probability Ratio Test (SPRT). Instead of deciding from a single descriptor comparison or using fixed thresholds with late-stage Iterative Closest Point (ICP) vetting, the verifier accumulates a short temporal stream of descriptor similarities between a query and each candidate. It then issues an accept/reject decision adaptively once sufficient multi-frame evidence has been observed, according to user-specified Type-I/II error design targets. This precision-first policy is designed to suppress false positives in structurally repetitive indoor environments. We evaluate the verifier on a five-sequence library dataset, using a fixed retrieval front-end with several representative LiDAR global descriptors. Performance is assessed via segment-level K-hit precision-recall and absolute trajectory error (ATE) and relative pose error (RPE) after pose graph optimization. Across descriptors, the sequential verifier consistently improves precision and reduces the impact of aliased loops compared with single-frame and heuristic multi-frame baselines. Our implementation and dataset will be released at: https://github.com/wanderingcar/snu_library_dataset.

</details>


### [14] [Development of a Compliant Gripper for Safe Robot-Assisted Trouser Dressing-Undressing](https://arxiv.org/abs/2512.09462)
*Jayant Unde,Takumi Inden,Yuki Wakayama,Jacinto Colan,Yaonan Zhu,Tadayoshi Aoyama,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 本文介绍了一种专为老年人或偏瘫患者设计的夹持系统，该系统旨在辅助穿脱裤子，通过实验评估显示了在狭小空间内高成功率地完成任务的能力，从而提高了这类人群的生活质量。


<details>
  <summary>Details</summary>
Motivation: 随着包括日本在内的许多国家人口老龄化加剧，维持老年人生活质量成为重要议题。对于身体机能受损的老年人来说，如厕支持尤为重要。

Method: 设计、开发并实验评估了一个夹持系统，该系统特别针对帮助老年人或偏瘫者穿脱裤子的需求和挑战进行了优化。夹持器被集成到一个定制的机械臂系统中，以提供全面的支持解决方案。

Result: 实验评价表明，所提出的夹持器能够以较高的成功率辅助用户在狭小空间内进行裤子的穿脱动作，并且与现有研究相比表现良好。

Conclusion: 这项研究促进了辅助机器人技术的发展，有助于老年人及身体障碍者保持独立性，提高他们的生活质量。

Abstract: In recent years, many countries, including Japan, have rapidly aging populations, making the preservation of seniors' quality of life a significant concern. For elderly people with impaired physical abilities, support for toileting is one of the most important issues. This paper details the design, development, experimental assessment, and potential application of the gripper system, with a focus on the unique requirements and obstacles involved in aiding elderly or hemiplegic individuals in dressing and undressing trousers. The gripper we propose seeks to find the right balance between compliance and grasping forces, ensuring precise manipulation while maintaining a safe and compliant interaction with the users. The gripper's integration into a custom--built robotic manipulator system provides a comprehensive solution for assisting hemiplegic individuals in their dressing and undressing tasks. Experimental evaluations and comparisons with existing studies demonstrate the gripper's ability to successfully assist in both dressing and dressing of trousers in confined spaces with a high success rate. This research contributes to the advancement of assistive robotics, empowering elderly, and physically impaired individuals to maintain their independence and improve their quality of life.

</details>


### [15] [ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics](https://arxiv.org/abs/2512.09510)
*Donato Caramia,Florian T. Pokorny,Giuseppe Triggiani,Denis Ruffino,David Naso,Paolo Roberto Massenio*

Main category: cs.RO

TL;DR: ViTA-Seg, a class-agnostic Vision Transformer framework, supports real-time amodal segmentation for robotic bin picking, using global attention to predict complete object masks. It includes two architectures and is supported by ViTA-SimData, a synthetic dataset. The Dual-Head version of ViTA-Seg shows high accuracy and efficiency in amodal and occlusion segmentation, suitable for real-time robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenge of occlusions in robotic bin picking, which affects the accuracy and reliability of grasp planning. By proposing a new framework that can predict complete object masks, even for hidden parts, the authors aim to improve the performance of robotic systems in these scenarios.

Method: The method involves the development of ViTA-Seg, a Vision Transformer-based framework designed for amodal segmentation. This framework comes in two architectures: Single-Head for predicting amodal masks only, and Dual-Head for both amodal and occluded mask prediction. Additionally, the researchers created ViTA-SimData, a synthetic dataset that mimics industrial bin-picking situations, to support their model's training and evaluation.

Result: The results show that the Dual-Head version of ViTA-Seg achieves significant accuracy in amodal and occlusion segmentation when tested on COOCA and KINS benchmarks. It also demonstrates computational efficiency, making it a suitable solution for real-time applications in robotic manipulation.

Conclusion: In conclusion, ViTA-Seg, especially its Dual-Head architecture, provides an effective and efficient solution for amodal and occlusion segmentation, which is critical for improving the robustness and reliability of robotic bin picking operations.

Abstract: Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation.

</details>


### [16] [Mastering Diverse, Unknown, and Cluttered Tracks for Robust Vision-Based Drone Racing](https://arxiv.org/abs/2512.09571)
*Feng Yu,Yu Hu,Yang Su,Yang Deng,Linzuo Zhang,Danping Zou*

Main category: cs.RO

TL;DR: 提出了一种两阶段学习框架，结合软碰撞训练和硬碰撞精炼来解决无人机竞速中的障碍物避免问题，并通过自适应噪声增强课程、非对称演员-评论家架构、Lipschitz约束以及轨迹原语生成器来提高策略探索、鲁棒性及跨环境泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的无人机竞速方法主要针对固定且无障碍物的赛道，而缺乏对未知复杂环境的泛化能力。这主要是由于需要平衡竞速速度与避障需求、有限的可行空间导致策略探索陷入局部最优，以及深度图中门和障碍物之间的感知模糊性。

Method: 一种两阶段学习框架：首先进行软碰撞训练以保持策略探索并实现高速飞行，然后是硬碰撞精炼阶段加强障碍物避免。该框架还包括一个自适应噪声增强课程表、不对称的行动者-评论家架构，逐步将策略依赖从特权门状态信息转移到基于深度的视觉输入上。此外，还施加了Lipschitz约束并集成了轨道原语生成器以增强运动稳定性和跨环境通用性。

Result: 通过广泛的模拟和消融研究对该框架进行了评估，并在一个计算资源受限的四旋翼无人机上进行了实际实验验证。系统能够实现敏捷飞行同时对于门位置误差具有鲁棒性，并能在多样化、部分未知和杂乱环境中运行。

Conclusion: 所提出的框架成功地解决了无人机在未知复杂环境下竞速时面临的挑战，包括平衡速度与避障、克服有限空间内策略探索局限以及减少感知模糊性等问题，从而开发出一种能够在多种不同环境中运作的泛化无人机竞速系统。

Abstract: Most reinforcement learning(RL)-based methods for drone racing target fixed, obstacle-free tracks, leaving the generalization to unknown, cluttered environments largely unaddressed. This challenge stems from the need to balance racing speed and collision avoidance, limited feasible space causing policy exploration trapped in local optima during training, and perceptual ambiguity between gates and obstacles in depth maps-especially when gate positions are only coarsely specified. To overcome these issues, we propose a two-phase learning framework: an initial soft-collision training phase that preserves policy exploration for high-speed flight, followed by a hard-collision refinement phase that enforces robust obstacle avoidance. An adaptive, noise-augmented curriculum with an asymmetric actor-critic architecture gradually shifts the policy's reliance from privileged gate-state information to depth-based visual input. We further impose Lipschitz constraints and integrate a track-primitive generator to enhance motion stability and cross-environment generalization. We evaluate our framework through extensive simulation and ablation studies, and validate it in real-world experiments on a computationally constrained quadrotor. The system achieves agile flight while remaining robust to gate-position errors, developing a generalizable drone racing framework with the capability to operate in diverse, partially unknown and cluttered environments. https://yufengsjtu.github.io/MasterRacing.github.io/

</details>


### [17] [GLaD: Geometric Latent Distillation for Vision-Language-Action Models](https://arxiv.org/abs/2512.09619)
*Minghao Guo,Meng Cao,Jiachen Tao,Rongtao Xu,Yan Yan,Xiaodan Liang,Ivan Laptev,Xiaojun Chang*

Main category: cs.RO

TL;DR: 本文提出了一种几何感知的视觉-语言-动作框架GLaD，通过知识蒸馏在预训练过程中整合3D几何先验，提升了空间推理与策略泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大部分视觉-语言-动作模型主要依赖RGB信息，而忽略了对空间推理和操作至关重要的几何线索。

Method: 引入了GLaD框架，该框架通过知识蒸馏将3D几何先验融入预训练过程；不仅将几何特征蒸馏到视觉编码器中，还将LLM对应于视觉标记的隐藏状态与冻结的几何感知视觉转换器（VGGT）的特征对齐，确保几何理解被深度整合进驱动动作预测的多模态表示中。

Result: 在Bridge数据集上预训练后，GLaD在四个LIBERO任务套件中实现了94.1%的平均成功率，优于使用相同预训练数据的UniVLA(92.5%)。

Conclusion: 几何感知预训练能够增强空间推理能力和策略泛化性，而且不需要显式的深度传感器或3D注释。

Abstract: Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.

</details>


### [18] [ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat](https://arxiv.org/abs/2512.09656)
*Nicolas Marticorena,Tobias Fischer,Niko Suenderhauf*

Main category: cs.RO

TL;DR: 本文提出了一种基于二次规划的反应式控制器ReMoSPLAT，用于移动操作中的避障。通过将高斯点表示法应用于碰撞避免，并在优化公式中加入额外的约束和成本，该控制器使移动操作平台能够在杂乱环境中达到目标末端执行器姿态的同时避开障碍物。实验表明，该方法与依赖完美地面真实信息的控制器相比，具有相当的性能。


<details>
  <summary>Details</summary>
Motivation: 如何在不涉及昂贵规划的情况下，将环境的准确表示纳入反应式控制以实现避障是一个难题。

Method: 提出了ReMoSPLAT，一种利用高斯点表示进行碰撞避免的反应式控制器。通过在优化公式中整合额外约束和成本，使得移动操纵平台能够到达目标终端执行器位置同时避开障碍物。此外，研究了两种有效计算机器人-障碍物距离的方法之间的权衡：纯几何方法与光栅化方法。

Result: 仿真实验显示，无论是合成数据还是真实世界扫描，所提方法都表现出可行性，且其表现可与依赖于完美地面实况信息的控制器相媲美。

Conclusion: ReMoSPLAT为移动操作提供了一种有效的避障解决方案，即使在复杂的环境下也能确保机器人安全地完成任务。

Abstract: Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information.

</details>


### [19] [High-Resolution Water Sampling via a Solar-Powered Autonomous Surface Vehicle](https://arxiv.org/abs/2512.09798)
*Misael Mamani,Mariel Fernandez,Grace Luna,Steffani Limachi,Leonel Apaza,Carolina Montes-Dávalos,Marcelo Herrera,Edwin Salcedo*

Main category: cs.RO

TL;DR: 本研究介绍了一种太阳能驱动的全自主水面无人船(USV)，该无人船采用新颖的注射器采样架构，能够单次任务采集72个离散且污染最小化的水样本。结合ROS 2自动驾驶系统、GPS-RTK导航、LiDAR和立体视觉障碍检测、基于Nav2的任务规划以及远程LoRa监督，实现了在非结构化环境中的可靠采样路线执行。现场试验表明，该平台可以实现高精度的定点采样、稳定的自主航行，并获取与手动采集相当的水质物理化学测量结果。


<details>
  <summary>Details</summary>
Motivation: 准确评估水质需要空间分辨度高的采样数据，但大多数现有水面无人船（USV）只能收集有限数量的样本或依赖于代表性差的单点传感器。为了克服这些限制，研究人员开发了一个新的采样平台，旨在提供更高效、更具代表性的水质采样方法。

Method: 本研究设计并实现了一种具备自主导航能力的太阳能动力水面无人船，该船配备了基于注射器的独特采样系统，能够在一次任务中收集多达72份独立水样。此外，它还集成了包括ROS 2自动驾驶堆栈、高精度GPS-RTK定位、LiDAR及立体视觉避障技术在内的多种先进技术，以支持复杂环境下的精确导航与操作。

Result: 实地测试显示，该无人船达到了87%的航路点精度，证明了其在自然条件下稳定自主航行的能力；同时，通过比较自动采集样品与人工采集样品之间的温度、pH值、电导率及总溶解固体等关键指标，发现两者之间具有良好的一致性。

Conclusion: 实验结果表明，所提出的平台能够有效地进行高分辨率水质采样并自动完成任务，为偏远地区水域监测提供了一个可扩展的解决方案。

Abstract: Accurate water quality assessment requires spatially resolved sampling, yet most unmanned surface vehicles (USVs) can collect only a limited number of samples or rely on single-point sensors with poor representativeness. This work presents a solar-powered, fully autonomous USV featuring a novel syringe-based sampling architecture capable of acquiring 72 discrete, contamination-minimized water samples per mission. The vehicle incorporates a ROS 2 autonomy stack with GPS-RTK navigation, LiDAR and stereo-vision obstacle detection, Nav2-based mission planning, and long-range LoRa supervision, enabling dependable execution of sampling routes in unstructured environments. The platform integrates a behavior-tree autonomy architecture adapted from Nav2, enabling mission-level reasoning and perception-aware navigation. A modular 6x12 sampling system, controlled by distributed micro-ROS nodes, provides deterministic actuation, fault isolation, and rapid module replacement, achieving spatial coverage beyond previously reported USV-based samplers. Field trials in Achocalla Lagoon (La Paz, Bolivia) demonstrated 87% waypoint accuracy, stable autonomous navigation, and accurate physicochemical measurements (temperature, pH, conductivity, total dissolved solids) comparable to manually collected references. These results demonstrate that the platform enables reliable high-resolution sampling and autonomous mission execution, providing a scalable solution for aquatic monitoring in remote environments.

</details>


### [20] [Bridging the Basilisk Astrodynamics Framework with ROS 2 for Modular Spacecraft Simulation and Hardware Integration](https://arxiv.org/abs/2512.09833)
*Elias Krantz,Ngai Nam Chan,Gunnar Tibert,Huina Mao,Christer Fuglesang*

Main category: cs.RO

TL;DR: 本文介绍了一种轻量级开源通信桥，连接Basilisk航天动力学模拟器和ROS 2，实现航天器控制中的实时双向数据交换。通过在仿真环境和ATMOS平面微重力测试平台上部署相同的领导-跟随编队飞行场景，展示了该桥的使用。


<details>
  <summary>Details</summary>
Motivation: 整合高保真航天器模拟器与模块化机器人框架对于自主性开发来说仍然是一个挑战。

Method: 开发了一个轻量级、开源的通信桥，用于连接Basilisk航天动力学模拟器和ROS 2，实现了无需修改Basilisk核心部分的实时双向数据交换，并且能够无缝集成到ROS 2节点中。

Result: 通过展示在一个领导-跟随编队飞行情境下的应用，验证了该桥接方案的有效性。此场景同时在仿真环境中及ATMOS平面微重力测试台上进行了部署。

Conclusion: 所提出的通信桥为模块化的航天器自主性提供了一个灵活可扩展的平台，并支持快速开发、硬件在环测试以及从仿真到硬件的平滑过渡。

Abstract: Integrating high-fidelity spacecraft simulators with modular robotics frameworks remains a challenge for autonomy development. This paper presents a lightweight, open-source communication bridge between the Basilisk astrodynamics simulator and the Robot Operating System 2 (ROS 2), enabling real-time, bidirectional data exchange for spacecraft control. The bridge requires no changes to Basilisk's core and integrates seamlessly with ROS 2 nodes. We demonstrate its use in a leader-follower formation flying scenario using nonlinear model predictive control, deployed identically in both simulation and on the ATMOS planar microgravity testbed. This setup supports rapid development, hardware-in-the-loop testing, and seamless transition from simulation to hardware. The bridge offers a flexible and scalable platform for modular spacecraft autonomy and reproducible research workflows.

</details>


### [21] [Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation](https://arxiv.org/abs/2512.09851)
*Yuyang Li,Yinghan Chen,Zihang Zhao,Puhao Li,Tengyu Liu,Siyuan Huang,Yixin Zhu*

Main category: cs.RO

TL;DR: 本研究提出了TacThru，一种能够同时提供视觉感知和可靠触觉信号提取的透视皮肤（STS）传感器，以及TacThru-UMI，一种利用这些多模态信号进行操作的模仿学习框架。通过五项具有挑战性的实际任务测试表明，该系统在处理包括与薄软物体接触检测及需要多模态协调的精密操作等关键场景时表现优异，平均成功率达到了85.5%，显著优于仅使用视觉或交替使用触觉-视觉的方法。


<details>
  <summary>Details</summary>
Motivation: 机器人操控既需要丰富的多模态感知能力，也需要有效的学习框架来应对复杂的现实世界任务。然而，现有的透视皮肤（STS）传感器设计无法同时实现多模态感知，并且触觉追踪不够可靠。此外，如何将这些丰富的多模态信号整合到基于学习的操作流程中仍然是一个开放性问题。

Method: 研究人员开发了一种名为TacThru的新型STS传感器，它具备全透明弹性体、持久照明、新颖的关键线标记以及高效的跟踪机制。同时，他们还提出了一种名为TacThru-UMI的模仿学习框架，该框架通过基于Transformer的扩散策略整合了视觉和触觉信息。

Result: 实验结果表明，在五个具有挑战性的现实世界任务上，TacThru-UMI系统的平均成功率达到85.5%，远高于仅依赖视觉（55.4%）或采用触觉-视觉交替方法（66.3%）的基线模型。特别是在涉及薄软物体接触检测及需要高度精确度和多模态协调的任务中，该系统展现了卓越性能。

Conclusion: 这项研究表明，结合同时多模态感知与现代学习框架可以实现更精确、更适应性强的机器人操控。

Abstract: Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.

</details>


### [22] [YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos](https://arxiv.org/abs/2512.09903)
*Ryan Meegan,Adam D'Souza,Bryan Bo Cao,Shubham Jain,Kristin Dana*

Main category: cs.RO

TL;DR: 提出了YOPO-Nav方法，利用探索视频作为视觉参考，在不依赖度量地图的情况下让机器人重走已探索的轨迹。该方法将环境编码为由局部3D高斯喷涂模型组成的紧凑空间表示，并通过视觉定位和局部模型细化来预测动作指导导航。此外还引入了YOPO-Campus数据集用于评估方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人导航系统依赖于详细的映射和路径规划，但构建和维护3D地图通常计算成本高且内存密集。本研究旨在当大型环境的探索视频可用时，解决基于视觉的导航问题，使得机器人能够依靠这些视觉信息重新追踪被探索过的路径，而无需依赖度量地图。

Method: YOPO-Nav方法通过将环境编码为一系列互相连接的局部3D高斯喷涂(3DGS)模型的空间表示来实现。在导航过程中，框架会比对机器人的当前视觉观察与这个表示，并据此预测出引导机器人返回到演示轨迹的动作。此方法采用分层设计：视觉地点识别(VPR)模块提供粗略定位，而局部3DGS模型则负责细化目标点及中间姿态以生成控制指令。

Result: 实验结果表明，YOPO-Nav方法在真实世界场景下的图像目标导航中表现优异，特别是在物理机器人上应用时。通过使用Clearpath Jackal机器人在YOPO-Campus数据集上的轨迹进行测试，进一步验证了近期几种视觉导航方法的效果。

Conclusion: YOPO-Nav是一种有效的视觉导航方法，它能够在没有详细3D地图的情况下，仅凭探索视频就让机器人成功地重走探索过的路线。同时，新引入的YOPO-Campus数据集也为视觉导航及相关领域研究提供了宝贵的资源。

Abstract: Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.

</details>


### [23] [Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models](https://arxiv.org/abs/2512.09927)
*Yifan Ye,Jiaqi Ma,Jun Cen,Zhihe Lu*

Main category: cs.RO

TL;DR: 本文提出了一种无需训练的令牌压缩框架TEAM-VLA，旨在加速大规模视觉-语言-动作模型的推理过程，同时保持任务性能。通过动态令牌扩展机制和有选择性的合并策略，该方法在效率与效果之间达成了良好的平衡，并且在LIBERO基准测试中显示出显著的推理速度提升及任务成功率。


<details>
  <summary>Details</summary>
Motivation: 针对大规模预训练视觉-语言-动作（VLA）模型由于参数量巨大导致实时部署时面临计算成本高昂和延迟敏感的问题，研究提出了一个新的解决方案以加速这些模型的推理速度而不牺牲任务表现。

Method: 提出了一种名为Token Expand-and-Merge-VLA (TEAM-VLA) 的无训练令牌压缩框架。该框架包含一个动态令牌扩展机制，能够识别并在注意力高亮区域附近采样额外的信息令牌，增强上下文完整性；随后，在更深的层次下，基于动作感知指导有选择地合并这些扩展令牌，减少冗余同时保持语义一致性。整个过程在一个前馈传递中完成，无需重新训练或更新参数。

Result: 广泛的实验表明，相比完整的VLA模型，TEAM-VLA不仅能够持续提高推理速度，还能保持甚至超越原有的任务成功率。

Conclusion: TEAM-VLA提供了一个有效的方法来加速大规模VLA模型的推理过程，同时确保了足够的任务性能，为实现实时机器人感知和控制开辟了新的途径。

Abstract: Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}

</details>


### [24] [HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models](https://arxiv.org/abs/2512.09928)
*Minghui Lin,Pengxiang Ding,Shu Wang,Zifeng Zhuang,Yang Liu,Xinyang Tong,Wenxuan Song,Shangke Lyu,Siteng Huang,Donglin Wang*

Main category: cs.RO

TL;DR: 提出了HiF-VLA框架，通过利用运动来进行双向时间推理，从而改善了视觉-语言-动作模型在长时操作中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作(Vision-Language-Action, VLA)模型大多假设马尔可夫性质，仅依赖当前观察而忽略了时间背景信息，导致长期一致性降低。

Method: 提出了一种名为HiF-VLA的新方法，该方法通过利用运动作为更紧凑且信息丰富的表示来捕捉状态间变化，并过滤静态噪声。它整合了后见之明（hindsight）和预见（foresight）的推理能力，以支持长时间跨度的操作任务。

Result: HiF-VLA在LIBERO-Long和CALVIN ABC-D基准测试中超越了强大的基线模型，同时实际增加了非常小的推理延迟。此外，在现实世界的长时操作任务中也取得了显著改进。

Conclusion: HiF-VLA提供了一种有效的方法来增强VLA模型的时间推理能力，对于提高机器人在复杂环境下的执行效率具有重要意义。

Abstract: Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.

</details>
