{"id": "2601.02379", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02379", "abs": "https://arxiv.org/abs/2601.02379", "authors": ["Nolan B. Gutierrez", "William J. Beksi"], "title": "Movement Primitives in Robotics: A Comprehensive Survey", "comment": "105 pages, 3 figures, and 6 tables", "summary": "Biological systems exhibit a continuous stream of movements, consisting of sequential segments, that allow them to perform complex tasks in a creative and versatile fashion. This observation has led researchers towards identifying elementary building blocks of motion known as movement primitives, which are well-suited for generating motor commands in autonomous systems, such as robots. In this survey, we provide an encyclopedic overview of movement primitive approaches and applications in chronological order. Concretely, we present movement primitive frameworks as a way of representing robotic control trajectories acquired through human demonstrations. Within the area of robotics, movement primitives can encode basic motions at the trajectory level, such as how a robot would grasp a cup or the sequence of motions necessary to toss a ball. Furthermore, movement primitives have been developed with the desirable analytical properties of a spring-damper system, probabilistic coupling of multiple demonstrations, using neural networks in high-dimensional systems, and more, to address difficult challenges in robotics. Although movement primitives have widespread application to a variety of fields, the goal of this survey is to inform practitioners on the use of these frameworks in the context of robotics. Specifically, we aim to (i) present a systematic review of major movement primitive frameworks and examine their strengths and weaknesses; (ii) highlight applications that have successfully made use of movement primitives; and (iii) examine open questions and discuss practical challenges when applying movement primitives in robotics.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u6587\u7ae0\u5168\u9762\u6982\u8ff0\u4e86\u8fd0\u52a8\u57fa\u5143\u65b9\u6cd5\u53ca\u5176\u5728\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u8fd9\u4e9b\u6846\u67b6\u5728\u673a\u5668\u4eba\u4e0a\u4e0b\u6587\u4e2d\u7684\u4f7f\u7528\u4fe1\u606f\u3002", "motivation": "\u7814\u7a76\u8005\u4eec\u53d1\u73b0\u751f\u7269\u7cfb\u7edf\u901a\u8fc7\u4e00\u7cfb\u5217\u8fde\u7eed\u7684\u8fd0\u52a8\u7247\u6bb5\u6765\u6267\u884c\u590d\u6742\u4efb\u52a1\uff0c\u8fd9\u542f\u53d1\u4e86\u4ed6\u4eec\u8bc6\u522b\u51fa\u9002\u5408\u4e8e\u81ea\u52a8\u751f\u6210\u81ea\u4e3b\u7cfb\u7edf\uff08\u5982\u673a\u5668\u4eba\uff09\u8fd0\u52a8\u6307\u4ee4\u7684\u57fa\u672c\u6784\u5efa\u5757\u2014\u2014\u8fd0\u52a8\u57fa\u5143\u3002", "method": "\u672c\u6587\u6309\u7167\u65f6\u95f4\u987a\u5e8f\u63d0\u4f9b\u4e86\u8fd0\u52a8\u57fa\u5143\u65b9\u6cd5\u53ca\u5e94\u7528\u7684\u767e\u79d1\u5168\u4e66\u5f0f\u6982\u89c8\uff0c\u5e76\u5c06\u8fd0\u52a8\u57fa\u5143\u6846\u67b6\u4f5c\u4e3a\u8868\u793a\u901a\u8fc7\u4eba\u7c7b\u6f14\u793a\u83b7\u5f97\u7684\u673a\u5668\u4eba\u63a7\u5236\u8f68\u8ff9\u7684\u4e00\u79cd\u65b9\u5f0f\u6765\u4ecb\u7ecd\u3002", "result": "\u8fd0\u52a8\u57fa\u5143\u80fd\u591f\u5728\u8f68\u8ff9\u7ea7\u522b\u4e0a\u7f16\u7801\u57fa\u672c\u52a8\u4f5c\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u96be\u9898\uff1b\u540c\u65f6\uff0c\u8be5\u7efc\u8ff0\u4e5f\u63a2\u8ba8\u4e86\u6210\u529f\u8fd0\u7528\u8fd0\u52a8\u57fa\u5143\u7684\u5e94\u7528\u5b9e\u4f8b\u4ee5\u53ca\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9047\u5230\u7684\u95ee\u9898\u4e0e\u6311\u6218\u3002", "conclusion": "\u7efc\u8ff0\u65e8\u5728\u5bf9\u4e3b\u8981\u7684\u8fd0\u52a8\u57fa\u5143\u6846\u67b6\u8fdb\u884c\u7cfb\u7edf\u6027\u56de\u987e\u5e76\u5206\u6790\u5176\u4f18\u7f3a\u70b9\u3001\u7a81\u51fa\u6210\u529f\u7684\u5e94\u7528\u6848\u4f8b\uff0c\u5e76\u8ba8\u8bba\u5728\u673a\u5668\u4eba\u6280\u672f\u9886\u57df\u5185\u5e94\u7528\u8fd0\u52a8\u57fa\u5143\u65f6\u5b58\u5728\u7684\u5f00\u653e\u95ee\u9898\u548c\u5b9e\u8df5\u6311\u6218\u3002"}}
{"id": "2601.02456", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02456", "abs": "https://arxiv.org/abs/2601.02456", "authors": ["Junhao Cai", "Zetao Cai", "Jiafei Cao", "Yilun Chen", "Zeyu He", "Lei Jiang", "Hang Li", "Hengjie Li", "Yang Li", "Yufei Liu", "Yanan Lu", "Qi Lv", "Haoxiang Ma", "Jiangmiao Pang", "Yu Qiao", "Zherui Qiu", "Yanqing Shen", "Xu Shi", "Yang Tian", "Bolun Wang", "Hanqing Wang", "Jiaheng Wang", "Tai Wang", "Xueyuan Wei", "Chao Wu", "Yiman Xie", "Boyang Xing", "Yuqiang Yang", "Yuyin Yang", "Qiaojun Yu", "Feng Yuan", "Jia Zeng", "Jingjing Zhang", "Shenghan Zhang", "Shi Zhang", "Zhuoma Zhaxi", "Bowen Zhou", "Yuanzhen Zhou", "Yunsong Zhou", "Hongrui Zhu", "Yangkun Zhu", "Yuchen Zhu"], "title": "InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation", "comment": "Homepage: https://internrobotics.github.io/internvla-a1.github.io/", "summary": "Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\\% improvement in daily tasks and a 40\\%-73.3\\% boost in dynamic settings, such as conveyor belt sorting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInternVLA-A1\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684Mixture-of-Transformers\u67b6\u6784\u534f\u8c03\u4e09\u4e2a\u4e13\u5bb6\uff08\u573a\u666f\u7406\u89e3\u3001\u89c6\u89c9\u9884\u89c1\u751f\u6210\u548c\u52a8\u4f5c\u6267\u884c\uff09\uff0c\u65e8\u5728\u5c06\u8bed\u4e49\u7406\u89e3\u548c\u52a8\u6001\u9884\u6d4b\u80fd\u529b\u7ed3\u5408\u8d77\u6765\u3002\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4efb\u52a1\u548c\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cInternVLA-A1\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684Vision-Language-Action (VLA) \u6a21\u578b\u867d\u7136\u5728\u8bed\u4e49\u7406\u89e3\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u63a8\u65ad\u7269\u7406\u4e16\u754c\u52a8\u6001\u7684\u80fd\u529b\uff1b\u800c\u57fa\u4e8eWorld Models\u7684\u65b9\u6cd5\u5c3d\u7ba1\u5c1d\u8bd5\u901a\u8fc7\u89c6\u9891\u9884\u6d4b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5374\u5f80\u5f80\u7f3a\u4e4f\u8bed\u4e49\u57fa\u7840\u4e14\u5904\u7406\u9884\u6d4b\u9519\u8bef\u65f6\u8868\u73b0\u8106\u5f31\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u8bed\u4e49\u7406\u89e3\u548c\u52a8\u6001\u9884\u6d4b\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u6a21\u578b\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684Mixture-of-Transformers\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u534f\u8c03\u4e86\u4e09\u4e2a\u4e13\u5bb6\uff1a\u4e00\u4e2a\u8d1f\u8d23\u573a\u666f\u7406\u89e3\uff0c\u4e00\u4e2a\u8d1f\u8d23\u89c6\u89c9\u9884\u89c1\u751f\u6210\uff0c\u53e6\u4e00\u4e2a\u8d1f\u8d23\u52a8\u4f5c\u6267\u884c\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u901a\u8fc7\u7edf\u4e00\u7684\u63a9\u7801\u81ea\u6ce8\u610f\u529b\u673a\u5236\u65e0\u7f1d\u4ea4\u4e92\u3002\u57fa\u4e8eInternVL3\u548cQwen3-VL\uff0c\u4ee52B\u548c3B\u53c2\u6570\u89c4\u6a21\u5b9e\u4f8b\u5316\u4e86InternVLA-A1\u6a21\u578b\u3002\u4f7f\u7528\u6df7\u5408\u5408\u6210-\u771f\u5b9e\u6570\u636e\u96c6\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u6db5\u76d612\u4e2a\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4efb\u52a1\u548c\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8bc4\u4f30\u663e\u793a\uff0cInternVLA-A1\u663e\u8457\u4f18\u4e8e\u9886\u5148\u7684\u6a21\u578b\u5982pi0\u548cGR00T N1.5\uff0c\u5728\u65e5\u5e38\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e8614.5%\uff0c\u5728\u52a8\u6001\u8bbe\u7f6e\u4e0b\u5982\u4f20\u9001\u5e26\u5206\u62e3\u4efb\u52a1\u4e2d\u63d0\u5347\u4e8640%-73.3%\u3002", "conclusion": "InternVLA-A1\u6210\u529f\u5730\u5c06\u9ad8\u7ea7\u8bed\u4e49\u7406\u89e3\u548c\u5f3a\u5927\u7684\u52a8\u6001\u9884\u6d4b\u80fd\u529b\u7ed3\u5408\u8d77\u6765\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u591a\u79cd\u5e94\u7528\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.02505", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02505", "abs": "https://arxiv.org/abs/2601.02505", "authors": ["Jiazhen Liu", "Glen Neville", "Jinwoo Park", "Sonia Chernova", "Harish Ravichandar"], "title": "Learning and Optimizing the Efficacy of Spatio-Temporal Task Allocation under Temporal and Resource Constraints", "comment": "The journal extension version of our conference paper: arXiv:2404.07902, which has been accepted by ISRR 2024", "summary": "Complex multi-robot missions often require heterogeneous teams to jointly optimize task allocation, scheduling, and path planning to improve team performance under strict constraints. We formalize these complexities into a new class of problems, dubbed Spatio-Temporal Efficacy-optimized Allocation for Multi-robot systems (STEAM). STEAM builds upon trait-based frameworks that model robots using their capabilities (e.g., payload and speed), but goes beyond the typical binary success-failure model by explicitly modeling the efficacy of allocations as trait-efficacy maps. These maps encode how the aggregated capabilities assigned to a task determine performance. Further, STEAM accommodates spatio-temporal constraints, including a user-specified time budget (i.e., maximum makespan). To solve STEAM problems, we contribute a novel algorithm named Efficacy-optimized Incremental Task Allocation Graph Search (E-ITAGS) that simultaneously optimizes task performance and respects time budgets by interleaving task allocation, scheduling, and path planning. Motivated by the fact that trait-efficacy maps are difficult, if not impossible, to specify, E-ITAGS efficiently learns them using a realizability-aware active learning module. Our approach is realizability-aware since it explicitly accounts for the fact that not all combinations of traits are realizable by the robots available during learning. Further, we derive experimentally-validated bounds on E-ITAGS' suboptimality with respect to efficacy. Detailed numerical simulations and experiments using an emergency response domain demonstrate that E-ITAGS generates allocations of higher efficacy compared to baselines, while respecting resource and spatio-temporal constraints. We also show that our active learning approach is sample efficient and establishes a principled tradeoff between data and computational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u95ee\u9898\u5206\u7c7b\u2014\u2014STEAM\uff0c\u4ee5\u53ca\u4e00\u79cd\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u65b0\u7b97\u6cd5E-ITAGS\u3002E-ITAGS\u901a\u8fc7\u4ea4\u9519\u4efb\u52a1\u5206\u914d\u3001\u8c03\u5ea6\u548c\u8def\u5f84\u89c4\u5212\u6765\u4f18\u5316\u4efb\u52a1\u6027\u80fd\u5e76\u9075\u5b88\u65f6\u95f4\u9884\u7b97\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u5229\u7528\u4e86\u53ef\u5b9e\u73b0\u6027\u611f\u77e5\u7684\u4e3b\u52a8\u5b66\u4e60\u6a21\u5757\u6709\u6548\u5730\u5b66\u4e60\u7279\u5f81\u6548\u80fd\u56fe\u3002\u5b9e\u9a8c\u8868\u660e\uff0cE-ITAGS\u5728\u6ee1\u8db3\u8d44\u6e90\u548c\u65f6\u7a7a\u7ea6\u675f\u7684\u540c\u65f6\u751f\u6210\u4e86\u6bd4\u57fa\u7ebf\u66f4\u9ad8\u7684\u6548\u80fd\u5206\u914d\uff0c\u5e76\u4e14\u5176\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5efa\u7acb\u4e86\u539f\u5219\u6027\u7684\u6743\u8861\u3002", "motivation": "\u590d\u6742\u7684\u591a\u673a\u5668\u4eba\u4efb\u52a1\u9700\u8981\u5f02\u6784\u56e2\u961f\u5171\u540c\u4f18\u5316\u4efb\u52a1\u5206\u914d\u3001\u8c03\u5ea6\u548c\u8def\u5f84\u89c4\u5212\u4ee5\u63d0\u9ad8\u56e2\u961f\u8868\u73b0\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u4e8c\u5143\u6210\u529f-\u5931\u8d25\u6a21\u578b\uff0c\u4f46\u672a\u5145\u5206\u8003\u8651\u7279\u5b9a\u4efb\u52a1\u5206\u914d\u5bf9\u6027\u80fd\u5f71\u54cd\u7684\u5177\u4f53\u7a0b\u5ea6\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86STEAM\u95ee\u9898\u5206\u7c7b\uff0c\u65e8\u5728\u660e\u786e\u5efa\u6a21\u4e0d\u540c\u80fd\u529b\u7ec4\u5408\u5bf9\u4e8e\u4efb\u52a1\u6267\u884c\u6548\u679c\u7684\u5f71\u54cd\uff08\u5373\u7279\u5f81\u6548\u80fd\u56fe\uff09\uff0c\u540c\u65f6\u8003\u8651\u7a7a\u95f4\u4e0e\u65f6\u95f4\u4e0a\u7684\u9650\u5236\u6761\u4ef6\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3STEAM\u95ee\u9898\uff0c\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aEfficacy-optimized Incremental Task Allocation Graph Search (E-ITAGS)\u7684\u65b0\u7b97\u6cd5\u3002E-ITAGS\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u4efb\u52a1\u6307\u6d3e\u3001\u8ba1\u5212\u5b89\u6392\u53ca\u8def\u7ebf\u89c4\u5212\u6765\u8fbe\u5230\u6700\u4f73\u5316\u4efb\u52a1\u6267\u884c\u6548\u679c\u7684\u76ee\u7684\uff0c\u5e76\u786e\u4fdd\u4e0d\u8d85\u8fc7\u7ed9\u5b9a\u7684\u65f6\u95f4\u4e0a\u9650\u3002\u8003\u8651\u5230\u76f4\u63a5\u5b9a\u4e49\u7279\u5f81\u6548\u80fd\u56fe\u53ef\u80fd\u5b58\u5728\u56f0\u96be\uff0cE-ITAGS\u5185\u5d4c\u4e86\u4e00\u4e2a\u80fd\u591f\u610f\u8bc6\u5230\u5b9e\u9645\u53ef\u884c\u6027\u7684\u4e3b\u52a8\u5b66\u4e60\u7ec4\u4ef6\uff0c\u7528\u4ee5\u9ad8\u6548\u5730\u4f30\u8ba1\u8fd9\u4e9b\u6548\u80fd\u56fe\u3002", "result": "\u901a\u8fc7\u8be6\u7ec6\u7684\u6570\u503c\u4eff\u771f\u53ca\u5728\u7d27\u6025\u54cd\u5e94\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0cE-ITAGS\u76f8\u6bd4\u5176\u4ed6\u57fa\u51c6\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u66f4\u9ad8\u6548\u7684\u5206\u914d\u65b9\u6848\uff0c\u540c\u65f6\u4e25\u683c\u9075\u5b88\u4e86\u8d44\u6e90\u9650\u5236\u548c\u65f6\u7a7a\u7ea6\u675f\u6761\u4ef6\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8868\u660e\u6240\u63d0\u51fa\u7684\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u4e0d\u4ec5\u6837\u672c\u6548\u7387\u9ad8\uff0c\u800c\u4e14\u5728\u6570\u636e\u6536\u96c6\u91cf\u4e0e\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u627e\u5230\u4e86\u5408\u7406\u7684\u5e73\u8861\u70b9\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86E-ITAGS\u7b97\u6cd5\u5728\u5904\u7406\u590d\u6742\u591a\u673a\u5668\u4eba\u4efb\u52a1\u65f6\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4e25\u683c\u7684\u65f6\u7a7a\u7ea6\u675f\u6761\u4ef6\u4e0b\u5bfb\u627e\u6700\u4f18\u89e3\u65b9\u9762\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u7684\u8d28\u91cf\uff0c\u8fd8\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u667a\u80fd\u5b66\u4e60\u673a\u5236\u964d\u4f4e\u5bf9\u5148\u9a8c\u77e5\u8bc6\u7684\u9700\u6c42\uff0c\u4ece\u800c\u4f7f\u5f97\u7cfb\u7edf\u66f4\u52a0\u7075\u6d3b\u9002\u5e94\u5404\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.02645", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02645", "abs": "https://arxiv.org/abs/2601.02645", "authors": ["Samarth Kalluraya", "Yiannis Kantaros"], "title": "Making Infeasible Tasks Feasible: Planning to Reconfigure Disconnected 3D Environments with Movable Objects", "comment": null, "summary": "Several planners have been developed to compute dynamically feasible, collision-free robot paths from an initial to a goal configuration. A key assumption in these works is that the goal region is reachable; an assumption that often fails in practice when environments are disconnected. Motivated by this limitation, we consider known 3D environments comprising objects, also called blocks, that form distinct navigable support surfaces (planes), and that are either non-movable (e.g., tables) or movable (e.g., boxes). These surfaces may be mutually disconnected due to height differences, holes, or lateral separations. Our focus is on tasks where the robot must reach a goal region residing on an elevated plane that is unreachable. Rather than declaring such tasks infeasible, an effective strategy is to enable the robot to interact with the environment, rearranging movable objects to create new traversable connections; a problem known as Navigation Among Movable Objects (NAMO). Existing NAMO planners typically address 2D environments, where obstacles are pushed aside to clear a path. These methods cannot directly handle the considered 3D setting; in such cases, obstacles must be placed strategically to bridge these physical disconnections. We address this challenge by developing BRiDGE (Block-based Reconfiguration in Disconnected 3D Geometric Environments), a sampling-based planner that incrementally builds trees over robot and object configurations to compute feasible plans specifying which objects to move, where to place them, and in what order, while accounting for a limited number of movable objects. To accelerate planning, we introduce non-uniform sampling strategies. We show that our method is probabilistically complete and we provide extensive numerical and hardware experiments validating its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBRiDGE\u7684\u89c4\u5212\u5668\uff0c\u5b83\u80fd\u57283D\u73af\u5883\u4e2d\u901a\u8fc7\u79fb\u52a8\u7269\u4f53\u521b\u5efa\u65b0\u7684\u53ef\u901a\u884c\u8def\u5f84\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u5728\u4e0d\u8fde\u901a\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5047\u8bbe\u76ee\u6807\u533a\u57df\u662f\u53ef\u8fbe\u7684\uff0c\u4f46\u5728\u5b9e\u9645\u4e2d\uff0c\u5f53\u73af\u5883\u4e0d\u8fde\u901a\u65f6\u8fd9\u4e00\u5047\u8bbe\u5f80\u5f80\u5931\u8d25\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u4eec\u8003\u8651\u4e86\u7531\u5f62\u6210\u4e0d\u540c\u53ef\u5bfc\u822a\u652f\u6491\u9762\u7684\u5bf9\u8c61\u7ec4\u6210\u7684\u5df2\u77e53D\u73af\u5883\uff0c\u5e76\u4e14\u8fd9\u4e9b\u5bf9\u8c61\u53ef\u80fd\u662f\u4e0d\u53ef\u79fb\u52a8\u6216\u53ef\u79fb\u52a8\u7684\u3002\u4e3a\u4e86\u5b8c\u6210\u673a\u5668\u4eba\u5fc5\u987b\u5230\u8fbe\u4f4d\u4e8e\u65e0\u6cd5\u8fbe\u5230\u7684\u9ad8\u5e73\u9762\u7684\u76ee\u6807\u533a\u57df\u7684\u4efb\u52a1\uff0c\u9700\u8981\u8ba9\u673a\u5668\u4eba\u4e0e\u73af\u5883\u4e92\u52a8\uff0c\u91cd\u65b0\u6392\u5217\u53ef\u79fb\u52a8\u7269\u4f53\u4ee5\u521b\u5efa\u65b0\u7684\u53ef\u901a\u884c\u8fde\u63a5\u3002", "method": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86BRiDGE\uff08\u57fa\u4e8e\u5757\u7684\u4e0d\u8fde\u901a3D\u51e0\u4f55\u73af\u5883\u4e2d\u7684\u91cd\u914d\u7f6e\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u89c4\u5212\u5668\uff0c\u80fd\u591f\u9010\u6b65\u6784\u5efa\u8de8\u8d8a\u673a\u5668\u4eba\u548c\u7269\u4f53\u914d\u7f6e\u7684\u6811\uff0c\u8ba1\u7b97\u51fa\u53ef\u884c\u8ba1\u5212\uff0c\u6307\u5b9a\u8981\u79fb\u52a8\u54ea\u4e9b\u7269\u4f53\u3001\u653e\u7f6e\u5728\u54ea\u91cc\u4ee5\u53ca\u987a\u5e8f\uff0c\u540c\u65f6\u8003\u8651\u5230\u53ef\u79fb\u52a8\u7269\u4f53\u6570\u91cf\u6709\u9650\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u975e\u5747\u5300\u91c7\u6837\u7b56\u7565\u6765\u52a0\u901f\u89c4\u5212\u8fc7\u7a0b\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6570\u503c\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6982\u7387\u5b8c\u6574\u6027\u3002", "conclusion": "BRiDGE\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u74063D\u73af\u5883\u4e2d\u7531\u4e8e\u7269\u7406\u65ad\u5f00\u800c\u96be\u4ee5\u76f4\u63a5\u89e3\u51b3\u7684\u5bfc\u822a\u4efb\u52a1\uff0c\u901a\u8fc7\u667a\u80fd\u5730\u91cd\u65b0\u5b89\u6392\u53ef\u79fb\u52a8\u969c\u788d\u7269\u6765\u521b\u5efa\u65b0\u7684\u8def\u5f84\u3002"}}
{"id": "2601.02649", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02649", "abs": "https://arxiv.org/abs/2601.02649", "authors": ["Jiangyi Fang", "Bowen Zhou", "Haotian Wang", "Xin Zhu", "Leye Wang"], "title": "Effective Online 3D Bin Packing with Lookahead Parcels Using Monte Carlo Tree Search", "comment": null, "summary": "Online 3D Bin Packing (3D-BP) with robotic arms is crucial for reducing transportation and labor costs in modern logistics. While Deep Reinforcement Learning (DRL) has shown strong performance, it often fails to adapt to real-world short-term distribution shifts, which arise as different batches of goods arrive sequentially, causing performance drops. We argue that the short-term lookahead information available in modern logistics systems is key to mitigating this issue, especially during distribution shifts. We formulate online 3D-BP with lookahead parcels as a Model Predictive Control (MPC) problem and adapt the Monte Carlo Tree Search (MCTS) framework to solve it. Our framework employs a dynamic exploration prior that automatically balances a learned RL policy and a robust random policy based on the lookahead characteristics. Additionally, we design an auxiliary reward to penalize long-term spatial waste from individual placements. Extensive experiments on real-world datasets show that our method consistently outperforms state-of-the-art baselines, achieving over 10\\% gains under distributional shifts, 4\\% average improvement in online deployment, and up to more than 8\\% in the best case--demonstrating the effectiveness of our framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5728\u7ebf3D\u88c5\u7bb1\u95ee\u9898\u3002\u901a\u8fc7\u5229\u7528\u524d\u77bb\u4fe1\u606f\u5e76\u8bbe\u8ba1\u8f85\u52a9\u5956\u52b1\u6765\u60e9\u7f5a\u957f\u671f\u7a7a\u95f4\u6d6a\u8d39\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u53d8\u5316\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u8d85\u8fc710%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u57283D\u88c5\u7bb1\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u96be\u4ee5\u9002\u5e94\u56e0\u4e0d\u540c\u6279\u6b21\u8d27\u7269\u8fde\u7eed\u5230\u8fbe\u800c\u4ea7\u751f\u7684\u77ed\u671f\u5206\u5e03\u53d8\u5316\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u6587\u7ae0\u8ba4\u4e3a\u5229\u7528\u73b0\u4ee3\u7269\u6d41\u7cfb\u7edf\u4e2d\u7684\u77ed\u671f\u524d\u77bb\u4fe1\u606f\u662f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u5173\u952e\u3002", "method": "\u5c06\u5e26\u6709\u524d\u77bb\u5305\u88f9\u4fe1\u606f\u7684\u5728\u7ebf3D\u88c5\u7bb1\u95ee\u9898\u5efa\u6a21\u4e3a\u4e00\u4e2a\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u4fee\u6539\u540e\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u6846\u67b6\u8fdb\u884c\u6c42\u89e3\u3002\u6846\u67b6\u4e2d\u4f7f\u7528\u4e86\u52a8\u6001\u63a2\u7d22\u5148\u9a8c\u673a\u5236\u6765\u81ea\u9002\u5e94\u5730\u5e73\u8861\u5b66\u4e60\u5230\u7684RL\u7b56\u7565\u4e0e\u7a33\u5065\u968f\u673a\u7b56\u7565\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b\u540c\u65f6\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u989d\u5916\u5956\u52b1\u9879\u4ee5\u51cf\u5c11\u5355\u4e2a\u653e\u7f6e\u52a8\u4f5c\u5f15\u8d77\u7684\u957f\u8fdc\u7a7a\u95f4\u6d6a\u8d39\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u8f83\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u800c\u8a00\uff0c\u4e8e\u5206\u5e03\u53d8\u5316\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u8d85\u8fc710%\u7684\u589e\u76ca\u3001\u5728\u7ebf\u90e8\u7f72\u65f6\u5e73\u5747\u6539\u8fdb\u4e864%\uff0c\u6700\u597d\u60c5\u51b5\u4e0b\u751a\u81f3\u8d85\u8fc7\u4e868%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7ed3\u5408MPC\u548cMCTS\u4ee5\u53ca\u5f15\u5165\u524d\u77bb\u4fe1\u606f\u548c\u7279\u5b9a\u5956\u52b1\u673a\u5236\u7684\u65b0\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u5728\u7ebf3D\u88c5\u7bb1\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u5e94\u5bf9\u8d27\u7269\u6279\u6b21\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u65f6\u5c55\u73b0\u51fa\u66f4\u5f3a\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.02686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02686", "abs": "https://arxiv.org/abs/2601.02686", "authors": ["Haixin Jin", "Nikhil Uday Shinde", "Soofiyan Atar", "Hongzhan Yu", "Dylan Hirsch", "Sicun Gao", "Michael C. Yip", "Sylvia Herbert"], "title": "Learning to Nudge: A Scalable Barrier Function Framework for Safe Robot Interaction in Dense Clutter", "comment": null, "summary": "Robots operating in everyday environments must navigate and manipulate within densely cluttered spaces, where physical contact with surrounding objects is unavoidable. Traditional safety frameworks treat contact as unsafe, restricting robots to collision avoidance and limiting their ability to function in dense, everyday settings. As the number of objects grows, model-based approaches for safe manipulation become computationally intractable; meanwhile, learned methods typically tie safety to the task at hand, making them hard to transfer to new tasks without retraining. In this work we introduce Dense Contact Barrier Functions(DCBF). Our approach bypasses the computational complexity of explicitly modeling multi-object dynamics by instead learning a composable, object-centric function that implicitly captures the safety constraints arising from physical interactions. Trained offline on interactions with a few objects, the learned DCBFcomposes across arbitrary object sets at runtime, producing a single global safety filter that scales linearly and transfers across tasks without retraining. We validate our approach through simulated experiments in dense clutter, demonstrating its ability to enable collision-free navigation and safe, contact-rich interaction in suitable settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bc6\u96c6\u63a5\u89e6\u5c4f\u969c\u51fd\u6570\uff08Dense Contact Barrier Functions, DCBF\uff09\uff0c\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u53ef\u7ec4\u5408\u7684\u5bf9\u8c61\u4e2d\u5fc3\u51fd\u6570\u6765\u9690\u5f0f\u5730\u6355\u6349\u7269\u7406\u4ea4\u4e92\u4e2d\u4ea7\u751f\u7684\u5b89\u5168\u7ea6\u675f\uff0c\u4ece\u800c\u7ed5\u8fc7\u4e86\u663e\u5f0f\u5efa\u6a21\u591a\u5bf9\u8c61\u52a8\u529b\u5b66\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002\u6b64\u65b9\u6cd5\u80fd\u591f\u5728\u7ebf\u6027\u6269\u5c55\u7684\u540c\u65f6\u8de8\u4efb\u52a1\u8f6c\u79fb\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5e76\u5728\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u5b89\u5168\u6846\u67b6\u5c06\u63a5\u89e6\u89c6\u4e3a\u4e0d\u5b89\u5168\u56e0\u7d20\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u65e5\u5e38\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\u3002\u968f\u7740\u7269\u4f53\u6570\u91cf\u7684\u589e\u52a0\uff0c\u57fa\u4e8e\u6a21\u578b\u7684\u5b89\u5168\u64cd\u4f5c\u65b9\u6cd5\u53d8\u5f97\u96be\u4ee5\u8ba1\u7b97\uff1b\u800c\u5b66\u4e60\u7684\u65b9\u6cd5\u901a\u5e38\u5c06\u5b89\u5168\u6027\u4e0e\u624b\u5934\u7684\u4efb\u52a1\u7ed1\u5b9a\u5728\u4e00\u8d77\uff0c\u4f7f\u5f97\u5b83\u4eec\u5f88\u96be\u5728\u4e0d\u8fdb\u884c\u518d\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8f6c\u79fb\u5230\u65b0\u4efb\u52a1\u4e0a\u3002", "method": "\u7814\u7a76\u8005\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u79f0\u4e3a\u5bc6\u96c6\u63a5\u89e6\u5c4f\u969c\u51fd\u6570(DCBF)\u7684\u65b0\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u5c11\u91cf\u7269\u4f53\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u6765\u5b66\u4e60\u4e00\u4e2a\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u3001\u53ef\u4ee5\u7ec4\u5408\u4f7f\u7528\u7684\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u80fd\u591f\u9690\u542b\u5730\u6355\u83b7\u7531\u7269\u7406\u4ea4\u4e92\u5f15\u8d77\u7684\u5b89\u5168\u7ea6\u675f\u6761\u4ef6\u3002\u5728\u8fd0\u884c\u65f6\uff0c\u8fd9\u79cd\u5b66\u5230\u7684DCBF\u53ef\u4ee5\u5728\u4efb\u610f\u7269\u4f53\u96c6\u5408\u4e4b\u95f4\u7ec4\u5408\uff0c\u751f\u6210\u4e00\u4e2a\u5168\u5c40\u6027\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u5b83\u80fd\u591f\u7ebf\u6027\u6269\u5c55\u5e76\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8de8\u4efb\u52a1\u4f7f\u7528\u3002", "result": "\u901a\u8fc7\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u5728\u5bc6\u96c6\u6742\u4e71\u73af\u5883\u4e0b\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u80fd\u591f\u5728\u9002\u5408\u7684\u573a\u666f\u4e0b\u5b9e\u73b0\u65e0\u78b0\u649e\u5bfc\u822a\u548c\u5b89\u5168\u7684\u9ad8\u63a5\u89e6\u5ea6\u4ea4\u4e92\u3002", "conclusion": "Dense Contact Barrier Functions \u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9014\u5f84\uff0c\u8ba9\u673a\u5668\u4eba\u80fd\u591f\u5728\u5bc6\u96c6\u4e14\u5145\u6ee1\u969c\u788d\u7269\u7684\u73af\u5883\u4e2d\u5b89\u5168\u5730\u5bfc\u822a\u548c\u64cd\u4f5c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u8de8\u4efb\u52a1\u9002\u5e94\u6027\u3002"}}
{"id": "2601.02704", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02704", "abs": "https://arxiv.org/abs/2601.02704", "authors": ["Kento Kawaharazuka", "Keita Yoneda", "Takahiro Hattori", "Shintaro Inoue", "Kei Okada"], "title": "Analysis of Various Manipulator Configurations Based on Multi-Objective Black-Box Optimization", "comment": "Accepted to Advanced Robotics, website: https://haraduka.github.io/bbo-manip-design", "summary": "Various 6-degree-of-freedom (DOF) and 7-DOF manipulators have been developed to date. Over a long history, their joint configurations and link length ratios have been determined empirically. In recent years, the development of robotic foundation models has become increasingly active, leading to the continuous proposal of various manipulators to support these models. However, none of these manipulators share exactly the same structure, as the order of joints and the ratio of link lengths differ among robots. Therefore, in order to discuss the optimal structure of a manipulator, we performed multi-objective optimization from the perspectives of end-effector reachability and joint torque. We analyze where existing manipulator structures stand within the sampling results of the optimization and provide insights for future manipulator design.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5bf9\u672b\u7aef\u6267\u884c\u5668\u53ef\u8fbe\u6027\u548c\u5173\u8282\u626d\u77e9\u7684\u591a\u76ee\u6807\u4f18\u5316\uff0c\u63a2\u8ba8\u4e86\u64cd\u4f5c\u81c2\u7684\u6700\u4f73\u7ed3\u6784\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709\u64cd\u4f5c\u81c2\u7ed3\u6784\u5728\u4f18\u5316\u91c7\u6837\u7ed3\u679c\u4e2d\u7684\u4f4d\u7f6e\uff0c\u4e3a\u672a\u6765\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u76846\u81ea\u7531\u5ea6\u548c7\u81ea\u7531\u5ea6\u64cd\u4f5c\u81c2\u7684\u5173\u8282\u914d\u7f6e\u548c\u8fde\u6746\u957f\u5ea6\u6bd4\u4f8b\u662f\u57fa\u4e8e\u7ecf\u9a8c\u786e\u5b9a\u7684\uff0c\u800c\u4e14\u4e0d\u540c\u673a\u5668\u4eba\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u6765\u8ba8\u8bba\u54ea\u79cd\u7ed3\u6784\u662f\u6700\u4f18\u7684\u64cd\u4f5c\u81c2\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\uff0c\u4ece\u672b\u7aef\u6267\u884c\u5668\u53ef\u8fbe\u6027\u548c\u5173\u8282\u626d\u77e9\u4e24\u4e2a\u89d2\u5ea6\u51fa\u53d1\uff0c\u5bfb\u627e\u6700\u4f18\u64cd\u4f5c\u81c2\u7ed3\u6784\u3002", "result": "\u5f97\u5230\u4e86\u4e00\u7cfb\u5217\u4f18\u5316\u540e\u7684\u64cd\u4f5c\u81c2\u7ed3\u6784\u6837\u672c\uff0c\u5e76\u5c06\u73b0\u6709\u64cd\u4f5c\u81c2\u7ed3\u6784\u4e0e\u4e4b\u5bf9\u6bd4\uff0c\u660e\u786e\u4e86\u73b0\u6709\u8bbe\u8ba1\u5728\u4f18\u5316\u7a7a\u95f4\u4e2d\u7684\u4f4d\u7f6e\u3002", "conclusion": "\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u4e0d\u540c\u64cd\u4f5c\u81c2\u7ed3\u6784\u4e4b\u95f4\u7684\u5dee\u5f02\u4ee5\u53ca\u5b83\u4eec\u76f8\u5bf9\u4e8e\u6700\u4f18\u89e3\u7684\u4f4d\u7f6e\uff0c\u4e3a\u672a\u6765\u64cd\u4f5c\u81c2\u7684\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2601.02723", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02723", "abs": "https://arxiv.org/abs/2601.02723", "authors": ["Wenzheng Zhang", "Kazuki Adachi", "Yoshitaka Hara", "Sousuke Nakamura"], "title": "Loop Closure using AnyLoc Visual Place Recognition in DPV-SLAM", "comment": "Accepted at IEEE/SICE International Symposium on System Integration(SII) 2026. 6 pages, 14 figures", "summary": "Loop closure is crucial for maintaining the accuracy and consistency of visual SLAM. We propose a method to improve loop closure performance in DPV-SLAM. Our approach integrates AnyLoc, a learning-based visual place recognition technique, as a replacement for the classical Bag of Visual Words (BoVW) loop detection method. In contrast to BoVW, which relies on handcrafted features, AnyLoc utilizes deep feature representations, enabling more robust image retrieval across diverse viewpoints and lighting conditions. Furthermore, we propose an adaptive mechanism that dynamically adjusts similarity threshold based on environmental conditions, removing the need for manual tuning. Experiments on both indoor and outdoor datasets demonstrate that our method significantly outperforms the original DPV-SLAM in terms of loop closure accuracy and robustness. The proposed method offers a practical and scalable solution for enhancing loop closure performance in modern SLAM systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdbDPV-SLAM\u4e2d\u56de\u73af\u95ed\u5408\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u5b66\u4e60\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u6280\u672fAnyLoc\u6765\u66ff\u4ee3\u4f20\u7edf\u7684Bag of Visual Words\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u673a\u5236\u8c03\u6574\u76f8\u4f3c\u6027\u9608\u503c\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5ba4\u5185\u548c\u5ba4\u5916\u6570\u636e\u96c6\u4e0a\u90fd\u663e\u8457\u63d0\u9ad8\u4e86\u56de\u73af\u95ed\u5408\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u89c6\u89c9SLAM\u7cfb\u7edf\u4e2d\u4f7f\u7528\u7684\u624b\u5de5\u7279\u5f81\u4f9d\u8d56\u6027\u5f3a\u3001\u5bf9\u89c6\u70b9\u53d8\u5316\u548c\u5149\u7167\u6761\u4ef6\u654f\u611f\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u65b0\u7684\u65b9\u6848\u4ee5\u63d0\u9ad8\u56de\u73af\u95ed\u5408\u7684\u51c6\u786e\u6027\u4e0e\u4e00\u81f4\u6027\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5c06\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4f4d\u7f6e\u8bc6\u522b\u6280\u672fAnyLoc\u6574\u5408\u8fdbDPV-SLAM\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u7528\u4ee5\u53d6\u4ee3\u539f\u6709\u7684BoVW\u56de\u73af\u68c0\u6d4b\u65b9\u5f0f\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u6839\u636e\u73af\u5883\u72b6\u51b5\u81ea\u52a8\u8c03\u8282\u76f8\u4f3c\u5ea6\u9608\u503c\u7684\u81ea\u9002\u5e94\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u573a\u666f\u4e0b\uff0c\u6240\u63d0\u65b9\u6cd5\u76f8\u8f83\u4e8e\u539f\u59cbDPV-SLAM\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u7684\u56de\u73af\u95ed\u5408\u7cbe\u5ea6\u53ca\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ed3\u5408AnyLoc\u4e0e\u52a8\u6001\u9608\u503c\u8c03\u6574\u673a\u5236\u7684\u65b9\u6cd5\u4e3a\u73b0\u4ee3SLAM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56de\u73af\u95ed\u5408\u6027\u80fd\u3002"}}
{"id": "2601.02738", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02738", "abs": "https://arxiv.org/abs/2601.02738", "authors": ["Kexin Guo", "Zihan Yang", "Yuhang Liu", "Jindou Jia", "Xiang Yu"], "title": "Optimizing Control-Friendly Trajectories with Self-Supervised Residual Learning", "comment": "10 pages, 9 figures", "summary": "Real-world physics can only be analytically modeled with a certain level of precision for modern intricate robotic systems. As a result, tracking aggressive trajectories accurately could be challenging due to the existence of residual physics during controller synthesis. This paper presents a self-supervised residual learning and trajectory optimization framework to address the aforementioned challenges. At first, unknown dynamic effects on the closed-loop model are learned and treated as residuals of the nominal dynamics, jointly forming a hybrid model. We show that learning with analytic gradients can be achieved using only trajectory-level data while enjoying accurate long-horizon prediction with an arbitrary integration step size. Subsequently, a trajectory optimizer is developed to compute the optimal reference trajectory with the residual physics along it minimized. It ends up with trajectories that are friendly to the following control level. The agile flight of quadrotors illustrates that by utilizing the hybrid dynamics, the proposed optimizer outputs aggressive motions that can be precisely tracked.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6b8b\u5dee\u5b66\u4e60\u548c\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u4ee3\u590d\u6742\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7531\u4e8e\u5b58\u5728\u6b8b\u4f59\u7269\u7406\u6548\u5e94\u800c\u96be\u4ee5\u51c6\u786e\u8ddf\u8e2a\u6fc0\u8fdb\u8f68\u8ff9\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5b66\u4e60\u672a\u77e5\u52a8\u6001\u6548\u5e94\u5e76\u5c06\u5176\u4f5c\u4e3a\u540d\u4e49\u52a8\u529b\u5b66\u7684\u6b8b\u5dee\uff0c\u5f62\u6210\u6df7\u5408\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u8f68\u8ff9\u4f18\u5316\u5668\u4ee5\u8ba1\u7b97\u6700\u4f18\u53c2\u8003\u8f68\u8ff9\uff0c\u4ece\u800c\u8f93\u51fa\u80fd\u591f\u88ab\u7cbe\u786e\u8ddf\u8e2a\u7684\u6fc0\u8fdb\u8fd0\u52a8\u3002", "motivation": "\u5bf9\u4e8e\u73b0\u4ee3\u590d\u6742\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u73b0\u5b9e\u4e16\u754c\u7684\u7269\u7406\u5b66\u53ea\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u8fdb\u884c\u89e3\u6790\u5efa\u6a21\u3002\u56e0\u6b64\uff0c\u5728\u63a7\u5236\u5668\u5408\u6210\u8fc7\u7a0b\u4e2d\uff0c\u7531\u4e8e\u5b58\u5728\u6b8b\u7559\u7684\u7269\u7406\u7279\u6027\uff0c\u51c6\u786e\u5730\u8ddf\u8e2a\u6fc0\u8fdb\u7684\u8f68\u8ff9\u53ef\u80fd\u4f1a\u53d8\u5f97\u56f0\u96be\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u76d1\u7763\u6b8b\u5dee\u5b66\u4e60\u4e0e\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u3002", "method": "\u9996\u5148\uff0c\u5b66\u4e60\u672a\u77e5\u52a8\u6001\u6548\u5e94\u5bf9\u95ed\u73af\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u5e76\u5c06\u5176\u89c6\u4e3a\u540d\u4e49\u52a8\u529b\u5b66\u7684\u6b8b\u5dee\uff0c\u5171\u540c\u5f62\u6210\u4e00\u4e2a\u6df7\u5408\u6a21\u578b\u3002\u7136\u540e\u5c55\u793a\u4e86\u4f7f\u7528\u4ec5\u8f68\u8ff9\u7ea7\u6570\u636e\u5b9e\u73b0\u5e26\u89e3\u6790\u68af\u5ea6\u7684\u5b66\u4e60\u662f\u53ef\u80fd\u7684\uff0c\u540c\u65f6\u80fd\u4eab\u53d7\u5177\u6709\u4efb\u610f\u79ef\u5206\u6b65\u957f\u7684\u51c6\u786e\u957f\u671f\u9884\u6d4b\u3002\u63a5\u7740\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f68\u8ff9\u4f18\u5316\u5668\u6765\u8ba1\u7b97\u5e26\u6709\u6700\u5c0f\u5316\u6b8b\u5dee\u7269\u7406\u7684\u6700\u4f18\u53c2\u8003\u8f68\u8ff9\u3002", "result": "\u5229\u7528\u6df7\u5408\u52a8\u529b\u5b66\uff0c\u6240\u63d0\u51fa\u7684\u4f18\u5316\u5668\u8f93\u51fa\u53ef\u4ee5\u88ab\u7cbe\u786e\u8ddf\u8e2a\u7684\u6fc0\u8fdb\u52a8\u4f5c\u3002\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u654f\u6377\u98de\u884c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u6613\u4e8e\u540e\u7eed\u63a7\u5236\u5c42\u9762\u5904\u7406\u7684\u8f68\u8ff9\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u81ea\u76d1\u7763\u6b8b\u5dee\u5b66\u4e60\u548c\u8f68\u8ff9\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56e0\u6b8b\u4f59\u7269\u7406\u6548\u5e94\u5bfc\u81f4\u7684\u8f68\u8ff9\u8ddf\u8e2a\u96be\u9898\uff0c\u4f7f\u5f97\u590d\u6742\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6267\u884c\u6fc0\u8fdb\u7684\u52a8\u4f5c\u3002"}}
{"id": "2601.02762", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02762", "abs": "https://arxiv.org/abs/2601.02762", "authors": ["Zihan Yang", "Jindou Jia", "Meng Wang", "Yuhang Liu", "Kexin Guo", "Xiang Yu"], "title": "Unified Meta-Representation and Feedback Calibration for General Disturbance Estimation", "comment": "8 pages, 10 figures", "summary": "Precise control in modern robotic applications is always an open issue due to unknown time-varying disturbances. Existing meta-learning-based approaches require a shared representation of environmental structures, which lack flexibility for realistic non-structural disturbances. Besides, representation error and the distribution shifts can lead to heavy degradation in prediction accuracy. This work presents a generalizable disturbance estimation framework that builds on meta-learning and feedback-calibrated online adaptation. By extracting features from a finite time window of past observations, a unified representation that effectively captures general non-structural disturbances can be learned without predefined structural assumptions. The online adaptation process is subsequently calibrated by a state-feedback mechanism to attenuate the learning residual originating from the representation and generalizability limitations. Theoretical analysis shows that simultaneous convergence of both the online learning error and the disturbance estimation error can be achieved. Through the unified meta-representation, our framework effectively estimates multiple rapidly changing disturbances, as demonstrated by quadrotor flight experiments. See the project page for video, supplementary material and code: https://nonstructural-metalearn.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u5b66\u4e60\u548c\u53cd\u9988\u6821\u51c6\u5728\u7ebf\u9002\u5e94\u7684\u901a\u7528\u6270\u52a8\u4f30\u8ba1\u6846\u67b6\uff0c\u80fd\u6709\u6548\u4f30\u8ba1\u5feb\u901f\u53d8\u5316\u7684\u975e\u7ed3\u6784\u6027\u6270\u52a8\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u65b9\u6cd5\u9700\u8981\u73af\u5883\u7ed3\u6784\u7684\u5171\u4eab\u8868\u793a\uff0c\u5bf9\u4e8e\u73b0\u5b9e\u4e2d\u7684\u975e\u7ed3\u6784\u6027\u6270\u52a8\u7f3a\u4e4f\u7075\u6d3b\u6027\uff1b\u6b64\u5916\uff0c\u8868\u793a\u8bef\u5dee\u548c\u5206\u5e03\u504f\u79fb\u4f1a\u5bfc\u81f4\u9884\u6d4b\u51c6\u786e\u6027\u5927\u5e45\u4e0b\u964d\u3002", "method": "\u901a\u8fc7\u4ece\u8fc7\u53bb\u89c2\u6d4b\u503c\u7684\u6709\u9650\u65f6\u95f4\u7a97\u53e3\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u5b66\u4e60\u4e00\u4e2a\u65e0\u9700\u9884\u8bbe\u7ed3\u6784\u5047\u8bbe\u5c31\u80fd\u6709\u6548\u6355\u6349\u4e00\u822c\u975e\u7ed3\u6784\u6027\u6270\u52a8\u7684\u7edf\u4e00\u8868\u793a\u3002\u968f\u540e\uff0c\u901a\u8fc7\u72b6\u6001\u53cd\u9988\u673a\u5236\u6821\u51c6\u5728\u7ebf\u9002\u5e94\u8fc7\u7a0b\uff0c\u4ee5\u51cf\u5c11\u6765\u81ea\u8868\u793a\u548c\u6cdb\u5316\u9650\u5236\u7684\u5b66\u4e60\u6b8b\u5dee\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728\u7ebf\u5b66\u4e60\u8bef\u5dee\u548c\u6270\u52a8\u4f30\u8ba1\u8bef\u5dee\u53ef\u4ee5\u540c\u65f6\u6536\u655b\u3002\u901a\u8fc7\u7edf\u4e00\u7684\u5143\u8868\u793a\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u4f30\u8ba1\u591a\u4e2a\u5feb\u901f\u53d8\u5316\u7684\u6270\u52a8\uff0c\u56db\u65cb\u7ffc\u98de\u884c\u5b9e\u9a8c\u5bf9\u6b64\u8fdb\u884c\u4e86\u6f14\u793a\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u6846\u67b6\u5728\u5904\u7406\u672a\u77e5\u65f6\u53d8\u6270\u52a8\u65b9\u9762\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u4e0e\u6f5c\u529b\uff0c\u4e3a\u73b0\u4ee3\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u7cbe\u786e\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.02766", "categories": ["cs.RO", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.02766", "abs": "https://arxiv.org/abs/2601.02766", "authors": ["Md. Anowar Hossain", "Mohd. Ehsanul Hoque"], "title": "Advancing Assistive Robotics: Multi-Modal Navigation and Biophysical Monitoring for Next-Generation Wheelchairs", "comment": null, "summary": "Assistive electric-powered wheelchairs (EPWs) have become essential mobility aids for people with disabilities such as amyotrophic lateral sclerosis (ALS), post-stroke hemiplegia, and dementia-related mobility impairment. This work presents a novel multi-modal EPW control system designed to prioritize patient needs while allowing seamless switching between control modes. Four complementary interfaces, namely joystick, speech, hand gesture, and electrooculography (EOG), are integrated with a continuous vital sign monitoring framework measuring heart rate variability, oxygen saturation (SpO2), and skin temperature. This combination enables greater patient independence while allowing caregivers to maintain real-time supervision and early intervention capability.\n  Two-point calibration of the biophysical sensors against clinical reference devices resulted in root mean square errors of at most 2 bpm for heart rate, 0.5 degree Celsius for skin temperature, and 1 percent for SpO2. Experimental evaluation involved twenty participants with mobility impairments executing a total of 500 indoor navigation commands. The achieved command recognition accuracies were 99 percent for joystick control, 97 percent plus or minus 2 percent for speech, and 95 percent plus or minus 3 percent for hand gesture, with an average closed-loop latency of 20 plus or minus 0.5 milliseconds. Caregivers receive real-time alerts through an Android application following encrypted cloud transmission of physiological data. By integrating multi-modal mobility control with cloud-enabled health monitoring and reporting latency and energy budgets, the proposed prototype addresses key challenges in assistive robotics, contributes toward compliance with ISO 7176-31 and IEC 80601-2-78 safety standards, and establishes a foundation for future adaptive machine learning enhancements.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u6a21\u5f0f\u7535\u52a8\u8f6e\u6905\u63a7\u5236\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u56db\u79cd\u63a7\u5236\u63a5\u53e3\uff08\u64cd\u7eb5\u6746\u3001\u8bed\u97f3\u3001\u624b\u52bf\u548c\u773c\u7535\u56fe\uff09\u4e0e\u751f\u547d\u4f53\u5f81\u76d1\u6d4b\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u4e91\u7aef\u4f20\u8f93\u751f\u7406\u6570\u636e\u7ed9\u62a4\u7406\u4eba\u5458\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4e86\u9ad8\u51c6\u786e\u7387\u7684\u547d\u4ee4\u8bc6\u522b\u4ee5\u53ca\u4f4e\u5ef6\u8fdf\uff0c\u4e3a\u8f85\u52a9\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u5173\u952e\u6311\u6218\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u7b26\u5408\u5b89\u5168\u6807\u51c6\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6b8b\u75be\u4eba\u58eb\u5982ALS\u60a3\u8005\u3001\u4e2d\u98ce\u540e\u504f\u762b\u8005\u53ca\u75f4\u5446\u75c7\u60a3\u8005\u7684\u79fb\u52a8\u80fd\u529b\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u7535\u52a8\u8f6e\u6905\u63a7\u5236\u7cfb\u7edf\uff0c\u65e8\u5728\u6ee1\u8db3\u60a3\u8005\u9700\u6c42\u7684\u540c\u65f6\u5141\u8bb8\u65e0\u7f1d\u5207\u6362\u63a7\u5236\u6a21\u5f0f\uff0c\u5e76\u7ed3\u5408\u5b9e\u65f6\u751f\u547d\u4f53\u5f81\u76d1\u6d4b\u6765\u589e\u5f3a\u72ec\u7acb\u6027\u548c\u76d1\u62a4\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u96c6\u6210\u6709\u56db\u79cd\u4e92\u8865\u754c\u9762\uff08\u64cd\u7eb5\u6746\u3001\u8bed\u97f3\u3001\u624b\u90e8\u52a8\u4f5c\u548c\u773c\u7535\u56fe\uff09\u4ee5\u53ca\u8fde\u7eed\u751f\u547d\u4f53\u5f81\u76d1\u6d4b\u6846\u67b6\u7684\u591a\u6a21\u6001EPW\u63a7\u5236\u7cfb\u7edf\u3002\u901a\u8fc7\u4e0e\u4e34\u5e8a\u53c2\u8003\u8bbe\u5907\u8fdb\u884c\u4e24\u70b9\u6821\u51c6\uff0c\u8bc4\u4f30\u4e86\u751f\u7269\u7269\u7406\u4f20\u611f\u5668\u7684\u51c6\u786e\u6027\uff1b\u5e76\u901a\u8fc720\u540d\u884c\u52a8\u4e0d\u4fbf\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\u8bc4\u4ef7\u4e86\u4e0d\u540c\u63a7\u5236\u65b9\u5f0f\u4e0b\u7684\u547d\u4ee4\u8bc6\u522b\u7cbe\u5ea6\u548c\u95ed\u73af\u5ef6\u8fdf\u3002", "result": "\u5bf9\u4e8e\u64cd\u7eb5\u6746\u63a7\u5236\u8fbe\u5230\u4e8699%\u7684\u547d\u4ee4\u8bc6\u522b\u51c6\u786e\u5ea6\uff0c\u8bed\u97f3\u63a7\u5236\u4e3a97\u00b12%\uff0c\u624b\u52bf\u63a7\u5236\u4e3a95\u00b13%\uff0c\u5e73\u5747\u95ed\u73af\u5ef6\u8fdf\u4e3a20\u00b10.5\u6beb\u79d2\u3002\u540c\u65f6\uff0c\u5fc3\u7387\u3001\u76ae\u80a4\u6e29\u5ea6\u548c\u8840\u6c27\u9971\u548c\u5ea6\u7684\u6d4b\u91cf\u8bef\u5dee\u5206\u522b\u4e0d\u8d85\u8fc72\u6b21/\u5206\u949f\u30010.5\u6444\u6c0f\u5ea6\u548c1%\u3002", "conclusion": "\u63d0\u51fa\u7684\u539f\u578b\u7cfb\u7edf\u89e3\u51b3\u4e86\u8f85\u52a9\u673a\u5668\u4eba\u9886\u57df\u7684\u4e00\u4e9b\u5173\u952e\u95ee\u9898\uff0c\u7b26\u5408ISO 7176-31\u548cIEC 80601-2-78\u5b89\u5168\u6807\u51c6\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u81ea\u9002\u5e94\u673a\u5668\u5b66\u4e60\u6539\u8fdb\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.02777", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02777", "abs": "https://arxiv.org/abs/2601.02777", "authors": ["Jingcheng Cao", "Chaoran Xiong", "Jianmin Song", "Shang Yan", "Jiachen Liu", "Ling Pei"], "title": "M-SEVIQ: A Multi-band Stereo Event Visual-Inertial Quadruped-based Dataset for Perception under Rapid Motion and Challenging Illumination", "comment": "6 pages, 7 figures", "summary": "Agile locomotion in legged robots poses significant challenges for visual perception. Traditional frame-based cameras often fail in these scenarios for producing blurred images, particularly under low-light conditions. In contrast, event cameras capture changes in brightness asynchronously, offering low latency, high temporal resolution, and high dynamic range. These advantages make them suitable for robust perception during rapid motion and under challenging illumination. However, existing event camera datasets exhibit limitations in stereo configurations and multi-band sensing domains under various illumination conditions. To address this gap, we present M-SEVIQ, a multi-band stereo event visual and inertial quadruped dataset collected using a Unitree Go2 equipped with stereo event cameras, a frame-based camera, an inertial measurement unit (IMU), and joint encoders. This dataset contains more than 30 real-world sequences captured across different velocity levels, illumination wavelengths, and lighting conditions. In addition, comprehensive calibration data, including intrinsic, extrinsic, and temporal alignments, are provided to facilitate accurate sensor fusion and benchmarking. Our M-SEVIQ can be used to support research in agile robot perception, sensor fusion, semantic segmentation and multi-modal vision in challenging environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aM-SEVIQ\u7684\u6570\u636e\u96c6\uff0c\u5b83\u662f\u4e00\u4e2a\u591a\u6ce2\u6bb5\u7acb\u4f53\u4e8b\u4ef6\u89c6\u89c9\u548c\u60ef\u6027\u56db\u8db3\u673a\u5668\u4eba\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u5feb\u901f\u79fb\u52a8\u548c\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u8fdb\u884c\u9c81\u68d2\u611f\u77e5\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u817f\u5f0f\u673a\u5668\u4eba\u7684\u654f\u6377\u8fd0\u52a8\u4e2d\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u5e27\u7684\u6444\u50cf\u673a\u7531\u4e8e\u4ea7\u751f\u6a21\u7cca\u56fe\u50cf\uff08\u7279\u522b\u662f\u5728\u4f4e\u5149\u6761\u4ef6\u4e0b\uff09\u800c\u5f80\u5f80\u8868\u73b0\u4e0d\u4f73\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4e8b\u4ef6\u76f8\u673a\u80fd\u591f\u5f02\u6b65\u6355\u6349\u4eae\u5ea6\u53d8\u5316\uff0c\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u9ad8\u52a8\u6001\u8303\u56f4\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u4e8e\u5feb\u901f\u79fb\u52a8\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u7a33\u5065\u611f\u77e5\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\u5728\u5404\u79cd\u7167\u660e\u6761\u4ef6\u4e0b\u5bf9\u7acb\u4f53\u914d\u7f6e\u548c\u591a\u6ce2\u6bb5\u611f\u77e5\u9886\u57df\u7684\u652f\u6301\u6709\u9650\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4f5c\u8005\u4eec\u63d0\u51fa\u4e86M-SEVIQ\u6570\u636e\u96c6\u3002", "method": "M-SEVIQ\u6570\u636e\u96c6\u662f\u901a\u8fc7\u914d\u5907\u6709\u7acb\u4f53\u4e8b\u4ef6\u76f8\u673a\u3001\u57fa\u4e8e\u5e27\u7684\u6444\u50cf\u673a\u3001\u60ef\u6027\u6d4b\u91cf\u5355\u5143(IMU)\u4ee5\u53ca\u5173\u8282\u7f16\u7801\u5668\u7684Unitree Go2\u6536\u96c6\u800c\u6210\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e86\u8d85\u8fc730\u4e2a\u5728\u4e0d\u540c\u901f\u5ea6\u6c34\u5e73\u3001\u5149\u7167\u6ce2\u957f\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u6355\u83b7\u7684\u771f\u5b9e\u4e16\u754c\u5e8f\u5217\uff0c\u5e76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6821\u51c6\u6570\u636e\uff0c\u5305\u62ec\u5185\u90e8\u53c2\u6570\u3001\u5916\u90e8\u53c2\u6570\u53ca\u65f6\u5e8f\u5bf9\u9f50\u4fe1\u606f\uff0c\u4ee5\u4fc3\u8fdb\u51c6\u786e\u7684\u4f20\u611f\u5668\u878d\u5408\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "M-SEVIQ\u6570\u636e\u96c6\u53ef\u4ee5\u7528\u6765\u652f\u6301\u5728\u5177\u6709\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u654f\u6377\u673a\u5668\u4eba\u611f\u77e5\u3001\u4f20\u611f\u5668\u878d\u5408\u3001\u8bed\u4e49\u5206\u5272\u53ca\u591a\u6a21\u6001\u89c6\u89c9\u7814\u7a76\u3002", "conclusion": "M-SEVIQ\u6570\u636e\u96c6\u586b\u8865\u4e86\u73b0\u6709\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u96c6\u4e2d\u5173\u4e8e\u7acb\u4f53\u914d\u7f6e\u53ca\u591a\u6ce2\u6bb5\u611f\u77e5\u9886\u57df\u5728\u591a\u6837\u5316\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u7a7a\u767d\uff0c\u4e3a\u63d0\u9ad8\u5feb\u901f\u79fb\u52a8\u6216\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u673a\u5668\u4eba\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2601.02778", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02778", "abs": "https://arxiv.org/abs/2601.02778", "authors": ["Haoyu Dong", "Zhengmao He", "Yang Li", "Zhibin Li", "Xinyu Yi", "Zhe Zhao"], "title": "Closing the Reality Gap: Zero-Shot Sim-to-Real Deployment for Dexterous Force-Based Grasping and Manipulation", "comment": null, "summary": "Human-like dexterous hands with multiple fingers offer human-level manipulation capabilities, but training control policies that can directly deploy on real hardware remains difficult due to contact-rich physics and imperfect actuation. We close this gap with a practical sim-to-real reinforcement learning (RL) framework that utilizes dense tactile feedback combined with joint torque sensing to explicitly regulate physical interactions. To enable effective sim-to-real transfer, we introduce (i) a computationally fast tactile simulation that computes distances between dense virtual tactile units and the object via parallel forward kinematics, providing high-rate, high-resolution touch signals needed by RL; (ii) a current-to-torque calibration that eliminates the need for torque sensors on dexterous hands by mapping motor current to joint torque; and (iii) actuator dynamics modeling to bridge the actuation gaps with randomization of non-ideal effects such as backlash, torque-speed saturation. Using an asymmetric actor-critic PPO pipeline trained entirely in simulation, our policies deploy directly to a five-finger hand. The resulting policies demonstrated two essential skills: (1) command-based, controllable grasp force tracking, and (2) reorientation of objects in the hand, both of which were robustly executed without fine-tuning on the robot. By combining tactile and torque in the observation space with effective sensing/actuation modeling, our system provides a practical solution to achieve reliable dexterous manipulation. To our knowledge, this is the first demonstration of controllable grasping on a multi-finger dexterous hand trained entirely in simulation and transferred zero-shot on real hardware.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u5bc6\u96c6\u89e6\u89c9\u53cd\u9988\u548c\u5173\u8282\u626d\u77e9\u611f\u5e94\u6765\u8c03\u8282\u7269\u7406\u4ea4\u4e92\u3002\u901a\u8fc7\u5feb\u901f\u89e6\u89c9\u6a21\u62df\u3001\u7535\u6d41\u5230\u626d\u77e9\u6821\u51c6\u53ca\u6267\u884c\u5668\u52a8\u6001\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5728\u771f\u5b9e\u673a\u5668\u4eba\u624b\u4e0a\u76f4\u63a5\u90e8\u7f72\u7b56\u7565\uff0c\u5e76\u5c55\u793a\u4e86\u53ef\u63a7\u6293\u63e1\u529b\u8ddf\u8e2a\u548c\u7269\u4f53\u518d\u5b9a\u4f4d\u4e24\u79cd\u5173\u952e\u6280\u80fd\u3002", "motivation": "\u7531\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u7269\u7406\u73af\u5883\u548c\u4e0d\u5b8c\u7f8e\u7684\u9a71\u52a8\u95ee\u9898\uff0c\u76f4\u63a5\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u90e8\u7f72\u80fd\u591f\u63a7\u5236\u7c7b\u4eba\u7075\u5de7\u624b\u7684\u63a7\u5236\u7b56\u7565\u4ecd\u7136\u5f88\u56f0\u96be\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e0d\u5bf9\u79f0actor-critic PPO\u7ba1\u9053\u7684sim-to-real RL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5bc6\u96c6\u89e6\u89c9\u53cd\u9988\u4e0e\u5173\u8282\u626d\u77e9\u611f\u5e94\u4ee5\u663e\u5f0f\u5730\u8c03\u6574\u7269\u7406\u4ea4\u4e92\u3002\u5f15\u5165\u4e86\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u89e6\u89c9\u6a21\u62df\u3001\u4ece\u7535\u673a\u7535\u6d41\u5230\u5173\u8282\u626d\u77e9\u7684\u6620\u5c04\u6821\u51c6\u4ee5\u53ca\u6267\u884c\u5668\u52a8\u529b\u5b66\u5efa\u6a21\u7b49\u6280\u672f\u6765\u4fc3\u8fdb\u6709\u6548\u7684\u6a21\u62df\u5230\u5b9e\u9645\u8fc1\u79fb\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b56\u7565\u80fd\u591f\u5728\u6ca1\u6709\u5bf9\u673a\u5668\u4eba\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u7a33\u5065\u5730\u6267\u884c\u4e24\u79cd\u57fa\u672c\u6280\u80fd\uff1a(1) \u57fa\u4e8e\u547d\u4ee4\u7684\u53ef\u63a7\u6293\u63e1\u529b\u8ffd\u8e2a\uff1b(2) \u624b\u4e2d\u7269\u4f53\u7684\u91cd\u65b0\u5b9a\u5411\u3002", "conclusion": "\u901a\u8fc7\u5c06\u89e6\u89c9\u4e0e\u626d\u77e9\u7ed3\u5408\u5728\u89c2\u5bdf\u7a7a\u95f4\u4e2d\u5e76\u6709\u6548\u5efa\u6a21\u611f\u77e5/\u81f4\u52a8\u8fc7\u7a0b\uff0c\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u73b0\u53ef\u9760\u7075\u5de7\u64cd\u4f5c\u7684\u5b9e\u9645\u89e3\u51b3\u65b9\u6848\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u6b21\u5b8c\u5168\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u5e76\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u96f6\u6837\u672c\u8f6c\u79fb\u6210\u529f\u7684\u591a\u6307\u7075\u5de7\u624b\u53ef\u63a7\u6293\u53d6\u6f14\u793a\u3002"}}
{"id": "2601.02798", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02798", "abs": "https://arxiv.org/abs/2601.02798", "authors": ["Sicong Gao", "Chen Qian", "Laurence Xian", "Liao Wu", "Maurice Pagnucco", "Yang Song"], "title": "Reinforcement Learning for Follow-the-Leader Robotic Endoscopic Navigation via Synthetic Data", "comment": null, "summary": "Autonomous navigation is crucial for both medical and industrial endoscopic robots, enabling safe and efficient exploration of narrow tubular environments without continuous human intervention, where avoiding contact with the inner walls has been a longstanding challenge for prior approaches. We present a follow-the-leader endoscopic robot based on a flexible continuum structure designed to minimize contact between the endoscope body and intestinal walls, thereby reducing patient discomfort. To achieve this objective, we propose a vision-based deep reinforcement learning framework guided by monocular depth estimation. A realistic intestinal simulation environment was constructed in \\textit{NVIDIA Omniverse} to train and evaluate autonomous navigation strategies. Furthermore, thousands of synthetic intraluminal images were generated using NVIDIA Replicator to fine-tune the Depth Anything model, enabling dense three-dimensional perception of the intestinal environment with a single monocular camera. Subsequently, we introduce a geometry-aware reward and penalty mechanism to enable accurate lumen tracking. Compared with the original Depth Anything model, our method improves $\u03b4_{1}$ depth accuracy by 39.2% and reduces the navigation J-index by 0.67 relative to the second-best method, demonstrating the robustness and effectiveness of the proposed approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5185\u7aa5\u955c\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u5e76\u901a\u8fc7\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u51cf\u5c11\u4e0e\u80a0\u58c1\u7684\u63a5\u89e6\u3002\u8be5\u65b9\u6cd5\u5728\u80a0\u9053\u73af\u5883\u6a21\u62df\u4e2d\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u63d0\u9ad8\u4e86\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u5e76\u964d\u4f4e\u4e86\u5bfc\u822aJ\u6307\u6570\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5185\u7aa5\u955c\u673a\u5668\u4eba\u5728\u72ed\u7a84\u7ba1\u72b6\u73af\u5883\u4e2d\u63a2\u7d22\u65f6\u907f\u514d\u4e0e\u5185\u58c1\u63a5\u89e6\u8fd9\u4e00\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u51cf\u8f7b\u60a3\u8005\u4e0d\u9002\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u67d4\u6027\u8fde\u7eed\u4f53\u7ed3\u6784\u7684\u8ddf\u968f\u5f0f\u5185\u7aa5\u955c\u673a\u5668\u4eba\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7531\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5f15\u5bfc\u7684\u89c6\u89c9\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u4f7f\u7528NVIDIA Omniverse\u6784\u5efa\u4e86\u903c\u771f\u7684\u80a0\u9053\u6a21\u62df\u73af\u5883\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30\u81ea\u4e3b\u5bfc\u822a\u7b56\u7565\uff0c\u540c\u65f6\u5229\u7528NVIDIA Replicator\u751f\u6210\u5408\u6210\u56fe\u50cf\u4ee5\u5fae\u8c03Depth Anything\u6a21\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u4ec5\u7528\u5355\u4e2a\u5355\u76ee\u76f8\u673a\u5c31\u80fd\u5bf9\u80a0\u9053\u73af\u5883\u8fdb\u884c\u5bc6\u96c6\u4e09\u7ef4\u611f\u77e5\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u51e0\u4f55\u610f\u8bc6\u5956\u52b1\u548c\u60e9\u7f5a\u673a\u5236\u6765\u63d0\u9ad8\u8154\u9053\u8ddf\u8e2a\u51c6\u786e\u6027\u3002", "result": "\u76f8\u6bd4\u539f\u59cbDepth Anything\u6a21\u578b\uff0c\u672c\u65b9\u6cd5\u5c06$\u03b4_{1}$\u6df1\u5ea6\u7cbe\u5ea6\u63d0\u9ad8\u4e8639.2%\uff0c\u4e14\u76f8\u5bf9\u4e8e\u6b21\u4f18\u65b9\u6cd5\u800c\u8a00\uff0c\u5bfc\u822aJ\u6307\u6570\u51cf\u5c11\u4e860.67\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u89c6\u89c9\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5185\u7aa5\u955c\u673a\u5668\u4eba\u5728\u590d\u6742\u80a0\u9053\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\u7684\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u4e0e\u80a0\u58c1\u4e0d\u5fc5\u8981\u7684\u63a5\u89e6\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u60a3\u8005\u7684\u8212\u9002\u5ea6\u3002"}}
{"id": "2601.02857", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02857", "abs": "https://arxiv.org/abs/2601.02857", "authors": ["Chunzheng Wang", "Yiyuan Zhang", "Annan Tang", "Ziqiu Zeng", "Haoran Chen", "Quan Gao", "Zixuan Zhuang", "Boyu Li", "Zhilin Xiong", "Aoqian Zhang", "Ce Hao", "Siyuan Luo", "Tongyang Zhao", "Cecilia Laschi", "Fan Shi"], "title": "Soft Responsive Materials Enhance Humanoid Safety", "comment": "40 pages, 11 figures", "summary": "Humanoid robots are envisioned as general-purpose platforms in human-centered environments, yet their deployment is limited by vulnerability to falls and the risks posed by rigid metal-plastic structures to people and surroundings. We introduce a soft-rigid co-design framework that leverages non-Newtonian fluid-based soft responsive materials to enhance humanoid safety. The material remains compliant during normal interaction but rapidly stiffens under impact, absorbing and dissipating fall-induced forces. Physics-based simulations guide protector placement and thickness and enable learning of active fall policies. Applied to a 42 kg life-size humanoid, the protector markedly reduces peak impact and allows repeated falls without hardware damage, including drops from 3 m and tumbles down long staircases. Across diverse scenarios, the approach improves robot robustness and environmental safety. By uniting responsive materials, structural co-design, and learning-based control, this work advances interact-safe, industry-ready humanoid robots.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f-\u521a\u6027\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u975e\u725b\u987f\u6d41\u4f53\u7684\u8f6f\u54cd\u5e94\u6750\u6599\u6765\u63d0\u9ad8\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u3002\u8be5\u6750\u6599\u5728\u6b63\u5e38\u4ea4\u4e92\u4e2d\u4fdd\u6301\u67d4\u987a\uff0c\u4f46\u5728\u53d7\u5230\u51b2\u51fb\u65f6\u8fc5\u901f\u786c\u5316\uff0c\u5438\u6536\u548c\u6d88\u6563\u56e0\u8dcc\u5012\u800c\u4ea7\u751f\u7684\u529b\u3002\u901a\u8fc7\u7269\u7406\u6a21\u62df\u6307\u5bfc\u4fdd\u62a4\u5668\u7684\u4f4d\u7f6e\u548c\u539a\u5ea6\uff0c\u5e76\u5b66\u4e60\u4e3b\u52a8\u8dcc\u5012\u7b56\u7565\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u4fdd\u62a4\u5668\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u6700\u5927\u51b2\u51fb\u529b\uff0c\u5141\u8bb8\u673a\u5668\u4eba\u53cd\u590d\u8dcc\u843d\u800c\u4e0d\u635f\u574f\u786c\u4ef6\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u9c81\u68d2\u6027\u548c\u73af\u5883\u5b89\u5168\u6027\u3002", "motivation": "\u76ee\u524d\uff0c\u4eba\u5f62\u673a\u5668\u4eba\u4f5c\u4e3a\u4eba\u7c7b\u4e2d\u5fc3\u73af\u5883\u4e2d\u7684\u901a\u7528\u5e73\u53f0\uff0c\u5176\u5e94\u7528\u53d7\u9650\u4e8e\u6613\u6454\u5012\u7684\u95ee\u9898\u4ee5\u53ca\u91d1\u5c5e\u5851\u6599\u7ed3\u6784\u5bf9\u5468\u56f4\u4eba\u5458\u548c\u73af\u5883\u6784\u6210\u7684\u98ce\u9669\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u7ed3\u5408\u4e86\u975e\u725b\u987f\u6d41\u4f53\u57fa\u8f6f\u54cd\u5e94\u6750\u6599\u7684\u8f6f\u786c\u5171\u8bbe\u8ba1\u6846\u67b6\uff0c\u8fd9\u7c7b\u6750\u6599\u5728\u65e5\u5e38\u4e92\u52a8\u4e2d\u4fdd\u6301\u67d4\u8f6f\u800c\u5728\u906d\u9047\u78b0\u649e\u65f6\u5feb\u901f\u786c\u5316\u4ee5\u5438\u6536\u5e76\u5206\u6563\u7531\u8dcc\u5012\u5f15\u8d77\u7684\u529b\u3002\u540c\u65f6\uff0c\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u5b66\u7684\u4eff\u771f\u6765\u786e\u5b9a\u4fdd\u62a4\u88c5\u7f6e\u7684\u6700\u4f73\u4f4d\u7f6e\u4e0e\u539a\u5ea6\uff0c\u5e76\u652f\u6301\u5b66\u4e60\u5982\u4f55\u91c7\u53d6\u79ef\u6781\u7684\u8dcc\u5012\u5e94\u5bf9\u7b56\u7565\u3002", "result": "\u5c06\u6b64\u4fdd\u62a4\u65b9\u6848\u5e94\u7528\u4e8e\u4e00\u4e2a42\u516c\u65a4\u91cd\u7684\u771f\u5b9e\u5c3a\u5bf8\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u540e\uff0c\u53d1\u73b0\u5b83\u80fd\u591f\u663e\u8457\u51cf\u5c11\u5cf0\u503c\u649e\u51fb\u529b\uff0c\u5e76\u4e14\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u591f\u5728\u6ca1\u6709\u786c\u4ef6\u635f\u4f24\u7684\u60c5\u51b5\u4e0b\u627f\u53d7\u591a\u6b21\u8dcc\u843d\uff0c\u5305\u62ec\u4ece3\u7c73\u9ad8\u5ea6\u843d\u4e0b\u53ca\u6cbf\u7740\u957f\u697c\u68af\u6eda\u843d\u7b49\u60c5\u51b5\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u54cd\u5e94\u6027\u6750\u6599\u3001\u7ed3\u6784\u5171\u8bbe\u8ba1\u4e0e\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u8fd9\u9879\u5de5\u4f5c\u4fc3\u8fdb\u4e86\u65e2\u5b89\u5168\u53c8\u9002\u5408\u5de5\u4e1a\u5e94\u7528\u7684\u4eba\u5f62\u673a\u5668\u4eba\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.02873", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02873", "abs": "https://arxiv.org/abs/2601.02873", "authors": ["Arthur Haffemayer", "Alexandre Chapin", "Armand Jordana", "Krzysztof Wojciechowski", "Florent Lamiraux", "Nicolas Mansard", "Vladimir Petrik"], "title": "Warm-Starting Collision-Free Model Predictive Control With Object-Centric Diffusion", "comment": "An open-source implementation is provided https://cozy-fairy-0e0139.netlify.app/", "summary": "Acting in cluttered environments requires predicting and avoiding collisions while still achieving precise control. Conventional optimization-based controllers can enforce physical constraints, but they struggle to produce feasible solutions quickly when many obstacles are present. Diffusion models can generate diverse trajectories around obstacles, yet prior approaches lacked a general and efficient way to condition them on scene structure. In this paper, we show that combining diffusion-based warm-starting conditioned with a latent object-centric representation of the scene and with a collision-aware model predictive controller (MPC) yields reliable and efficient motion generation under strict time limits. Our approach conditions a diffusion transformer on the system state, task, and surroundings, using an object-centric slot attention mechanism to provide a compact obstacle representation suitable for control. The sampled trajectories are refined by an optimal control problem that enforces rigid-body dynamics and signed-distance collision constraints, producing feasible motions in real time. On benchmark tasks, this hybrid method achieved markedly higher success rates and lower latency than sampling-based planners or either component alone. Real-robot experiments with a torque-controlled Panda confirm reliable and safe execution with MPC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9884\u70ed\u542f\u52a8\u548c\u78b0\u649e\u611f\u77e5\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff08MPC\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u573a\u666f\u8868\u793a\u6765\u751f\u6210\u5728\u590d\u6742\u73af\u5883\u4e2d\u65e2\u53ef\u9760\u53c8\u9ad8\u6548\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002", "motivation": "\u5728\u590d\u6742\u7684\u73af\u5883\u4e2d\u6267\u884c\u4efb\u52a1\u9700\u8981\u80fd\u591f\u9884\u6d4b\u5e76\u907f\u514d\u78b0\u649e\u540c\u65f6\u4fdd\u6301\u7cbe\u786e\u63a7\u5236\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u63a7\u5236\u5668\u53ef\u4ee5\u5f3a\u5236\u7269\u7406\u7ea6\u675f\uff0c\u4f46\u5728\u5b58\u5728\u8bb8\u591a\u969c\u788d\u7269\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u5feb\u901f\u4ea7\u751f\u53ef\u884c\u89e3\uff1b\u800c\u6269\u6563\u6a21\u578b\u867d\u7136\u80fd\u56f4\u7ed5\u969c\u788d\u7269\u751f\u6210\u591a\u6837\u5316\u7684\u8f68\u8ff9\uff0c\u4f46\u5148\u524d\u7684\u65b9\u6cd5\u7f3a\u4e4f\u4e00\u79cd\u901a\u7528\u4e14\u6709\u6548\u7684\u65b9\u5f0f\u6765\u8ba9\u5b83\u4eec\u6839\u636e\u573a\u666f\u7ed3\u6784\u8fdb\u884c\u6761\u4ef6\u5316\u3002", "method": "\u672c\u65b9\u6cd5\u91c7\u7528\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u6280\u672f\uff0c\u8be5\u6280\u672f\u53ef\u4ee5\u6839\u636e\u7cfb\u7edf\u72b6\u6001\u3001\u4efb\u52a1\u9700\u6c42\u53ca\u5468\u56f4\u73af\u5883\u8fdb\u884c\u8c03\u6574\uff0c\u5e76\u4f7f\u7528\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u7d27\u51d1\u7684\u969c\u788d\u7269\u8868\u793a\uff0c\u9002\u7528\u4e8e\u63a7\u5236\u3002\u91c7\u6837\u7684\u8f68\u8ff9\u968f\u540e\u901a\u8fc7\u4e00\u4e2a\u6700\u4f18\u63a7\u5236\u95ee\u9898\u5f97\u5230\u6539\u8fdb\uff0c\u8be5\u95ee\u9898\u5b9e\u65bd\u4e86\u521a\u4f53\u52a8\u529b\u5b66\u4e0e\u5e26\u7b26\u53f7\u8ddd\u79bb\u7684\u78b0\u649e\u7ea6\u675f\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u65f6\u53ef\u884c\u7684\u8fd0\u52a8\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4efb\u52a1\u4e2d\uff0c\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u76f8\u6bd4\u57fa\u4e8e\u91c7\u6837\u7684\u89c4\u5212\u5668\u6216\u4efb\u4f55\u5355\u72ec\u7ec4\u4ef6\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MPC\u652f\u6301\u4e0b\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u6267\u884c\u3002", "conclusion": "\u7ed3\u5408\u57fa\u4e8e\u6269\u6563\u7684\u9884\u70ed\u542f\u52a8\u548c\u78b0\u649e\u611f\u77e5MPC\u4e3a\u5728\u4e25\u683c\u65f6\u95f4\u9650\u5236\u4e0b\u4e8e\u6742\u4e71\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5b89\u5168\u7684\u52a8\u4f5c\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02905", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02905", "abs": "https://arxiv.org/abs/2601.02905", "authors": ["Sara Micol Ferraina", "Michele Brienza", "Francesco Argenziano", "Emanuele Musumeci", "Vincenzo Suriani", "Domenico D. Bloisi", "Daniele Nardi"], "title": "LOST-3DSG: Lightweight Open-Vocabulary 3D Scene Graphs with Semantic Tracking in Dynamic Environments", "comment": null, "summary": "Tracking objects that move within dynamic environments is a core challenge in robotics. Recent research has advanced this topic significantly; however, many existing approaches remain inefficient due to their reliance on heavy foundation models. To address this limitation, we propose LOST-3DSG, a lightweight open-vocabulary 3D scene graph designed to track dynamic objects in real-world environments. Our method adopts a semantic approach to entity tracking based on word2vec and sentence embeddings, enabling an open-vocabulary representation while avoiding the necessity of storing dense CLIP visual features. As a result, LOST-3DSG achieves superior performance compared to approaches that rely on high-dimensional visual embeddings. We evaluate our method through qualitative and quantitative experiments conducted in a real 3D environment using a TIAGo robot. The results demonstrate the effectiveness and efficiency of LOST-3DSG in dynamic object tracking. Code and supplementary material are publicly available on the project website at https://lab-rococo-sapienza.github.io/lost-3dsg/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u56feLOST-3DSG\uff0c\u7528\u4e8e\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u8ddf\u8e2a\u52a8\u6001\u7269\u4f53\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8eword2vec\u548c\u53e5\u5b50\u5d4c\u5165\u91c7\u7528\u8bed\u4e49\u65b9\u5f0f\u6765\u8ddf\u8e2a\u5b9e\u4f53\uff0c\u907f\u514d\u4e86\u5b58\u50a8\u5bc6\u96c6CLIP\u89c6\u89c9\u7279\u5f81\u7684\u9700\u6c42\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLOST-3DSG\u5728\u52a8\u6001\u7269\u4f53\u8ddf\u8e2a\u65b9\u9762\u8868\u73b0\u51fa\u8272\u4e14\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u7684\u7269\u4f53\u8ddf\u8e2a\u65b9\u6cd5\u56e0\u4f9d\u8d56\u4e8e\u91cd\u578b\u57fa\u7840\u6a21\u578b\u800c\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u7814\u7a76\u8005\u4eec\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u8f7b\u4fbf\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u51cf\u5c11\u5bf9\u9ad8\u7ef4\u89c6\u89c9\u5d4c\u5165\u7684\u4f9d\u8d56\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86LOST-3DSG\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528word2vec\u4e0e\u53e5\u5b50\u5d4c\u5165\u6280\u672f\u5b9e\u73b0\u7684\u8f7b\u91cf\u7ea7\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u56fe\u65b9\u6848\u3002\u5b83\u901a\u8fc7\u8bed\u4e49\u624b\u6bb5\u8ffd\u8e2a\u5b9e\u4f53\uff0c\u5141\u8bb8\u4f7f\u7528\u5f00\u653e\u5f0f\u8bcd\u6c47\u8868\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u4fdd\u5b58\u5927\u91cf\u7684CLIP\u89c6\u89c9\u7279\u5f81\u6570\u636e\u3002", "result": "\u901a\u8fc7\u5728\u4e00\u4e2a\u771f\u5b9e\u7684\u4e09\u7ef4\u73af\u5883\u4e2d\u4f7f\u7528TIAGo\u673a\u5668\u4eba\u8fdb\u884c\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8bc4\u4f30\uff0cLOST-3DSG\u5c55\u73b0\u51fa\u4e86\u5728\u52a8\u6001\u5bf9\u8c61\u8ddf\u8e2a\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "LOST-3DSG\u4f5c\u4e3a\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u826f\u597d\u8ddf\u8e2a\u6548\u679c\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u5904\u7406\u901f\u5ea6\u548c\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u7269\u4f53\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2601.02948", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02948", "abs": "https://arxiv.org/abs/2601.02948", "authors": ["Matti Vahs", "Jaeyoun Choi", "Niklas Schmid", "Jana Tumova", "Chuchu Fan"], "title": "Parameter-Robust MPPI for Safe Online Learning of Unknown Parameters", "comment": null, "summary": "Robots deployed in dynamic environments must remain safe even when key physical parameters are uncertain or change over time. We propose Parameter-Robust Model Predictive Path Integral (PRMPPI) control, a framework that integrates online parameter learning with probabilistic safety constraints. PRMPPI maintains a particle-based belief over parameters via Stein Variational Gradient Descent, evaluates safety constraints using Conformal Prediction, and optimizes both a nominal performance-driven and a safety-focused backup trajectory in parallel. This yields a controller that is cautious at first, improves performance as parameters are learned, and ensures safety throughout. Simulation and hardware experiments demonstrate higher success rates, lower tracking error, and more accurate parameter estimates than baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPRMPPI\u63a7\u5236\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5728\u7ebf\u53c2\u6570\u5b66\u4e60\u548c\u6982\u7387\u5b89\u5168\u7ea6\u675f\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\u5c55\u793a\u4e86\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3001\u66f4\u4f4e\u7684\u8ddf\u8e2a\u8bef\u5dee\u4ee5\u53ca\u66f4\u51c6\u786e\u7684\u53c2\u6570\u4f30\u8ba1\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5373\u4f7f\u5173\u952e\u7269\u7406\u53c2\u6570\u4e0d\u786e\u5b9a\u6216\u968f\u65f6\u95f4\u53d8\u5316\u65f6\u4e5f\u80fd\u4fdd\u6301\u5b89\u5168\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u96c6\u6210\u5728\u7ebf\u53c2\u6570\u5b66\u4e60\u4e0e\u6982\u7387\u5b89\u5168\u7ea6\u675f\u7684\u65b0\u6846\u67b6\u3002", "method": "\u4f7f\u7528Stein\u53d8\u5206\u68af\u5ea6\u4e0b\u964d\u6765\u7ef4\u62a4\u57fa\u4e8e\u7c92\u5b50\u7684\u53c2\u6570\u4fe1\u5ff5\uff0c\u5229\u7528\u5171\u5f62\u9884\u6d4b\u8bc4\u4f30\u5b89\u5168\u7ea6\u675f\uff0c\u5e76\u884c\u4f18\u5316\u540d\u4e49\u6027\u80fd\u9a71\u52a8\u8f68\u8ff9\u548c\u5b89\u5168\u5bfc\u5411\u5907\u4efd\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u63a7\u5236\u5668\u76f8\u6bd4\u57fa\u7ebf\u5177\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3001\u8f83\u4f4e\u7684\u8ddf\u8e2a\u8bef\u5dee\u53ca\u66f4\u52a0\u7cbe\u786e\u7684\u53c2\u6570\u4f30\u8ba1\u3002", "conclusion": "\u63d0\u51fa\u7684PRMPPI\u63a7\u5236\u6846\u67b6\u80fd\u591f\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u540c\u65f6\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u6027\u80fd\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u53c2\u6570\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u663e\u793a\u51fa\u8c28\u614e\u6027\uff0c\u5e76\u968f\u7740\u53c2\u6570\u7684\u5b66\u4e60\u800c\u9010\u6e10\u6539\u5584\u6027\u80fd\u3002"}}
{"id": "2601.02994", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02994", "abs": "https://arxiv.org/abs/2601.02994", "authors": ["Youngjoon Jeong", "Junha Chun", "Taesup Kim"], "title": "Learning to Act Robustly with View-Invariant Latent Actions", "comment": "Website: https://joon-stack.github.io/VILA/", "summary": "Vision-based robotic policies often struggle with even minor viewpoint changes, underscoring the need for view-invariant visual representations. This challenge becomes more pronounced in real-world settings, where viewpoint variability is unavoidable and can significantly disrupt policy performance. Existing methods typically learn invariance from multi-view observations at the scene level, but such approaches rely on visual appearance and fail to incorporate the physical dynamics essential for robust generalization. We propose View-Invariant Latent Action (VILA), which models a latent action capturing transition patterns across trajectories to learn view-invariant representations grounded in physical dynamics. VILA aligns these latent actions across viewpoints using an action-guided objective based on ground-truth action sequences. Experiments in both simulation and the real world show that VILA-based policies generalize effectively to unseen viewpoints and transfer well to new tasks, establishing VILA as a strong pretraining framework that improves robustness and downstream learning performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVILA\u7684\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5efa\u6a21\u6355\u6349\u8f68\u8ff9\u95f4\u8f6c\u6362\u6a21\u5f0f\u7684\u6f5c\u5728\u52a8\u4f5c\u6765\u5b66\u4e60\u57fa\u4e8e\u7269\u7406\u52a8\u6001\u7684\u89c6\u89d2\u4e0d\u53d8\u8868\u793a\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eVILA\u7684\u7b56\u7565\u80fd\u591f\u6709\u6548\u6cdb\u5316\u5230\u672a\u89c1\u89c6\u89d2\uff0c\u5e76\u4e14\u5f88\u597d\u5730\u8fc1\u79fb\u5230\u65b0\u4efb\u52a1\u4e0a\u3002", "motivation": "\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u7b56\u7565\u5f80\u5f80\u96be\u4ee5\u5e94\u5bf9\u5373\u4f7f\u662f\u8f7b\u5fae\u7684\u89c6\u89d2\u53d8\u5316\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5c24\u5176\u6210\u95ee\u9898\uff0c\u56e0\u4e3a\u89c6\u89d2\u53d8\u5316\u662f\u4e0d\u53ef\u907f\u514d\u7684\uff0c\u4e14\u4f1a\u4e25\u91cd\u5f71\u54cd\u7b56\u7565\u7684\u8868\u73b0\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u573a\u666f\u7ea7\u522b\u7684\u591a\u89c6\u89d2\u89c2\u5bdf\u6765\u5b66\u4e60\u4e0d\u53d8\u6027\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u5916\u89c2\u800c\u5ffd\u7565\u4e86\u5bf9\u4e8e\u9c81\u68d2\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u7684\u7269\u7406\u52a8\u6001\u3002", "method": "\u63d0\u51fa\u4e86View-Invariant Latent Action (VILA) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5efa\u6a21\u4e00\u4e2a\u6355\u6349\u8f68\u8ff9\u95f4\u8fc7\u6e21\u6a21\u5f0f\u7684\u6f5c\u5728\u52a8\u4f5c\u6765\u5b66\u4e60\u57fa\u4e8e\u7269\u7406\u52a8\u529b\u5b66\u7684\u89c6\u89d2\u4e0d\u53d8\u8868\u5f81\u3002VILA\u4f7f\u7528\u57fa\u4e8e\u771f\u5b9e\u52a8\u4f5c\u5e8f\u5217\u7684\u52a8\u4f5c\u5f15\u5bfc\u76ee\u6807\u6765\u5bf9\u9f50\u4e0d\u540c\u89c6\u89d2\u4e0b\u7684\u8fd9\u4e9b\u6f5c\u5728\u52a8\u4f5c\u3002", "result": "\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eVILA\u7684\u7b56\u7565\u80fd\u6709\u6548\u5730\u63a8\u5e7f\u5230\u672a\u89c1\u8fc7\u7684\u89c6\u89d2\uff0c\u5e76\u4e14\u53ef\u4ee5\u5f88\u597d\u5730\u8f6c\u79fb\u5230\u65b0\u7684\u4efb\u52a1\u4e0a\u3002", "conclusion": "VILA\u4f5c\u4e3a\u4e00\u79cd\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u6846\u67b6\u88ab\u63d0\u51fa\u6765\uff0c\u5b83\u901a\u8fc7\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u4e0b\u6e38\u5b66\u4e60\u8868\u73b0\u6765\u89e3\u51b3\u89c6\u89d2\u53d8\u5316\u7ed9\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u7b56\u7565\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2601.03037", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.03037", "abs": "https://arxiv.org/abs/2601.03037", "authors": ["Chunhui Zhao", "Xirui Kao", "Yilin Lu", "Yang Lyu"], "title": "A Bi-directional Adaptive Framework for Agile UAV Landing", "comment": "This work has been submitted to the IEEE Robotics and Automation Letters (RA-L) for possible publication", "summary": "Autonomous landing on mobile platforms is crucial for extending quadcopter operational flexibility, yet conventional methods are often too inefficient for highly dynamic scenarios. The core limitation lies in the prevalent ``track-then-descend'' paradigm, which treats the platform as a passive target and forces the quadcopter to perform complex, sequential maneuvers. This paper challenges that paradigm by introducing a bi-directional cooperative landing framework that redefines the roles of the vehicle and the platform. The essential innovation is transforming the problem from a single-agent tracking challenge into a coupled system optimization. Our key insight is that the mobile platform is not merely a target, but an active agent in the landing process. It proactively tilts its surface to create an optimal, stable terminal attitude for the approaching quadcopter. This active cooperation fundamentally breaks the sequential model by parallelizing the alignment and descent phases. Concurrently, the quadcopter's planning pipeline focuses on generating a time-optimal and dynamically feasible trajectory that minimizes energy consumption. This bi-directional coordination allows the system to execute the recovery in an agile manner, characterized by aggressive trajectory tracking and rapid state synchronization within transient windows. The framework's effectiveness, validated in dynamic scenarios, significantly improves the efficiency, precision, and robustness of autonomous quadrotor recovery in complex and time-constrained missions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5411\u5408\u4f5c\u7740\u9646\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u79fb\u52a8\u5e73\u53f0\u89c6\u4e3a\u7740\u9646\u8fc7\u7a0b\u4e2d\u7684\u79ef\u6781\u53c2\u4e0e\u8005\u800c\u975e\u88ab\u52a8\u76ee\u6807\uff0c\u4ece\u800c\u6253\u7834\u4e86\u4f20\u7edf\u7684'\u5148\u8ddf\u8e2a\u540e\u4e0b\u964d'\u6a21\u5f0f\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u51c6\u548c\u4e0b\u964d\u9636\u6bb5\u7684\u5e76\u884c\u5316\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u56db\u65cb\u7ffc\u98de\u884c\u5668\u7684\u8f68\u8ff9\u89c4\u5212\u6765\u51cf\u5c11\u80fd\u8017\uff0c\u6700\u7ec8\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u548c\u65f6\u95f4\u53d7\u9650\u4efb\u52a1\u4e2d\u81ea\u4e3b\u6062\u590d\u7684\u6548\u7387\u3001\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u65e0\u4eba\u673a\u81ea\u52a8\u7740\u9646\u65b9\u6cd5\u5bf9\u4e8e\u9ad8\u5ea6\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5e94\u7528\u663e\u5f97\u4e0d\u591f\u9ad8\u6548\uff0c\u4e3b\u8981\u56e0\u4e3a\u5b83\u4eec\u9075\u5faa\u4e00\u79cd'\u5148\u8ddf\u8e2a\u540e\u4e0b\u964d'\u7684\u8303\u5f0f\uff0c\u8fd9\u4f7f\u5f97\u65e0\u4eba\u673a\u9700\u8981\u6267\u884c\u590d\u6742\u7684\u987a\u5e8f\u52a8\u4f5c\u3002\u8fd9\u79cd\u505a\u6cd5\u9650\u5236\u4e86\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u64cd\u4f5c\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u5411\u5408\u4f5c\u7740\u9646\u6846\u67b6\uff0c\u5176\u4e2d\u79fb\u52a8\u5e73\u53f0\u88ab\u89c6\u4f5c\u7740\u9646\u8fc7\u7a0b\u4e2d\u79ef\u6781\u4e92\u52a8\u7684\u4e00\u90e8\u5206\uff0c\u800c\u4e0d\u662f\u5355\u7eaf\u7684\u88ab\u52a8\u76ee\u6807\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4f7f\u79fb\u52a8\u5e73\u53f0\u4e3b\u52a8\u8c03\u6574\u5176\u8868\u9762\u503e\u659c\u89d2\u5ea6\u4ee5\u521b\u5efa\u6700\u4f73\u7a33\u5b9a\u7ec8\u7aef\u59ff\u6001\u7ed9\u63a5\u8fd1\u4e2d\u7684\u56db\u65cb\u7ffc\u673a\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u56db\u65cb\u7ffc\u673a\u7684\u65f6\u95f4\u6700\u4f18\u4e14\u52a8\u6001\u53ef\u884c\u8f68\u8ff9\u751f\u6210\u6d41\u7a0b\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u80fd\u91cf\u6d88\u8017\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u52a8\u6001\u573a\u666f\u4e0b\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u590d\u6742\u53ca\u65f6\u95f4\u7d27\u8feb\u4efb\u52a1\u4e2d\u81ea\u4e3b\u6062\u590d\u65f6\u7684\u6548\u7387\u3001\u7cbe\u786e\u5ea6\u4ee5\u53ca\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u5411\u534f\u4f5c\u7740\u9646\u673a\u5236\uff0c\u901a\u8fc7\u6539\u53d8\u4f20\u7edf\u7740\u9646\u7b56\u7565\uff0c\u5141\u8bb8\u79fb\u52a8\u5e73\u53f0\u4e0e\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e4b\u95f4\u8fdb\u884c\u66f4\u6709\u6548\u7684\u4ea4\u4e92\uff0c\u4ece\u800c\u6539\u5584\u4e86\u65e0\u4eba\u673a\u5728\u9ad8\u901f\u53d8\u5316\u73af\u5883\u4e2d\u7684\u7740\u9646\u6027\u80fd\u3002"}}
{"id": "2601.03038", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03038", "abs": "https://arxiv.org/abs/2601.03038", "authors": ["Changwen Li", "Rongjie Yan", "Chih-Hong Cheng", "Jian Zhang"], "title": "Validating Generalist Robots with Situation Calculus and STL Falsification", "comment": null, "summary": "Generalist robots are becoming a reality, capable of interpreting natural language instructions and executing diverse operations. However, their validation remains challenging because each task induces its own operational context and correctness specification, exceeding the assumptions of traditional validation methods. We propose a two-layer validation framework that combines abstract reasoning with concrete system falsification. At the abstract layer, situation calculus models the world and derives weakest preconditions, enabling constraint-aware combinatorial testing to systematically generate diverse, semantically valid world-task configurations with controllable coverage strength. At the concrete layer, these configurations are instantiated for simulation-based falsification with STL monitoring. Experiments on tabletop manipulation tasks show that our framework effectively uncovers failure cases in the NVIDIA GR00T controller, demonstrating its promise for validating general-purpose robot autonomy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u62bd\u8c61\u63a8\u7406\u4e0e\u5177\u4f53\u7cfb\u7edf\u9a8c\u8bc1\u7684\u4e24\u5c42\u9a8c\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u9a8c\u8bc1\u591a\u529f\u80fd\u673a\u5668\u4eba\u7684\u4efb\u52a1\u6267\u884c\u60c5\u51b5\u3002", "motivation": "\u7531\u4e8e\u6bcf\u9879\u4efb\u52a1\u90fd\u6709\u5176\u81ea\u8eab\u7684\u64cd\u4f5c\u73af\u5883\u548c\u6b63\u786e\u6027\u89c4\u8303\uff0c\u8fd9\u8d85\u51fa\u4e86\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u7684\u5047\u8bbe\u8303\u56f4\uff0c\u56e0\u6b64\u5bf9\u80fd\u591f\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5e76\u6267\u884c\u591a\u6837\u5316\u64cd\u4f5c\u7684\u901a\u7528\u673a\u5668\u4eba\u8fdb\u884c\u9a8c\u8bc1\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u5c42\u9a8c\u8bc1\u6846\u67b6\uff1a\u5728\u62bd\u8c61\u5c42\u9762\u4f7f\u7528\u60c5\u5883\u6f14\u7b97\u5efa\u6a21\u4e16\u754c\uff0c\u5e76\u63a8\u5bfc\u51fa\u6700\u5f31\u524d\u7f6e\u6761\u4ef6\uff0c\u4ece\u800c\u5b9e\u73b0\u7ea6\u675f\u611f\u77e5\u7ec4\u5408\u6d4b\u8bd5\uff1b\u5728\u5177\u4f53\u5c42\u9762\uff0c\u5219\u662f\u5c06\u8fd9\u4e9b\u914d\u7f6e\u5b9e\u4f8b\u5316\u4ee5\u8fdb\u884c\u57fa\u4e8e\u4eff\u771f\u7684\u9a8c\u8bc1\uff0c\u5e76\u5229\u7528STL\u76d1\u63a7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u6846\u67b6\u80fd\u591f\u5728NVIDIA GR00T\u63a7\u5236\u5668\u4e0a\u6709\u6548\u5730\u53d1\u73b0\u5931\u8d25\u6848\u4f8b\uff0c\u5c55\u793a\u4e86\u5b83\u5728\u9a8c\u8bc1\u901a\u7528\u76ee\u7684\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u6b64\u4e24\u5c42\u9a8c\u8bc1\u6846\u67b6\u4e3a\u89e3\u51b3\u901a\u7528\u578b\u673a\u5668\u4eba\u9762\u4e34\u7684\u9a8c\u8bc1\u96be\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u901a\u8fc7\u7ed3\u5408\u62bd\u8c61\u5206\u6790\u4e0e\u5177\u4f53\u7684\u4eff\u771f\u9a8c\u8bc1\uff0c\u80fd\u591f\u5e2e\u52a9\u8bc6\u522b\u6f5c\u5728\u7684\u95ee\u9898\u533a\u57df\u3002"}}
{"id": "2601.03044", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.03044", "abs": "https://arxiv.org/abs/2601.03044", "authors": ["Mingjie Pan", "Siyuan Feng", "Qinglin Zhang", "Xinchen Li", "Jianheng Song", "Chendi Qu", "Yi Wang", "Chuankang Li", "Ziyu Xiong", "Zhi Chen", "Yi Liu", "Jianlan Luo"], "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models", "comment": null, "summary": "Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5728\u7ebf\u540e\u8bad\u7ec3\u7cfb\u7edfSOP\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u4f7f\u901a\u7528\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u7269\u7406\u4e16\u754c\u4e2d\u8fdb\u884c\u5728\u7ebf\u3001\u5206\u5e03\u5f0f\u3001\u591a\u4efb\u52a1\u540e\u8bad\u7ec3\u3002\u901a\u8fc7\u5c06\u6267\u884c\u548c\u5b66\u4e60\u7d27\u5bc6\u7ed3\u5408\uff0c\u673a\u5668\u4eba\u961f\u5217\u6301\u7eed\u5411\u4e2d\u592e\u4e91\u7aef\u5b66\u4e60\u5668\u63d0\u4f9b\u5728\u7ebf\u7b56\u7565\u7ecf\u9a8c\u548c\u4eba\u5de5\u5e72\u9884\u4fe1\u53f7\uff0c\u5e76\u5f02\u6b65\u63a5\u6536\u66f4\u65b0\u540e\u7684\u7b56\u7565\uff0c\u4ece\u800c\u652f\u6301\u5feb\u901f\u5728\u7ebf\u4fee\u6b63\u3001\u5e76\u884c\u90e8\u7f72\u4ee5\u6269\u5927\u7ecf\u9a8c\u6536\u96c6\u8303\u56f4\uff0c\u5e76\u5728\u9002\u5e94\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u901a\u7528\u6027\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u7684\u90e8\u7f72\u8fd8\u9700\u8981\u4e13\u5bb6\u7ea7\u522b\u7684\u4efb\u52a1\u719f\u7ec3\u5ea6\u3002\u73b0\u6709\u7684VLA\u6a21\u578b\u540e\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u662f\u79bb\u7ebf\u7684\u3001\u5355\u4e2a\u673a\u5668\u4eba\u7684\u6216\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u6709\u6548\u7684\u5728\u7ebf\u7b56\u7565\u9002\u5e94\u6027\u548c\u4ece\u771f\u5b9e\u4e16\u754c\u4e92\u52a8\u4e2d\u53ef\u6269\u5c55\u5b66\u4e60\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u8005\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aScalable Online Post-training (SOP) \u7684\u7cfb\u7edf\uff0c\u5b83\u5141\u8bb8\u76f4\u63a5\u5728\u7269\u7406\u4e16\u754c\u4e2d\u5bf9\u901a\u7528VLA\u6a21\u578b\u8fdb\u884c\u5728\u7ebf\u3001\u5206\u5e03\u5f0f\u7684\u591a\u4efb\u52a1\u540e\u8bad\u7ec3\u3002SOP\u7cfb\u7edf\u901a\u8fc7\u4e00\u4e2a\u95ed\u73af\u67b6\u6784\u7d27\u5bc6\u5730\u7ed3\u5408\u4e86\u6267\u884c\u4e0e\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff0c\u4e00\u961f\u673a\u5668\u4eba\u4e0d\u65ad\u5730\u5411\u4e2d\u592e\u4e91\u7aef\u5b66\u4e60\u5668\u53d1\u9001\u5728\u7ebf\u7b56\u7565\u4f53\u9a8c\u53ca\u4eba\u7c7b\u5e72\u9884\u4fe1\u53f7\uff0c\u5e76\u5f02\u6b65\u83b7\u53d6\u66f4\u65b0\u540e\u7684\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSOP\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u9884\u8bad\u7ec3VLA\u6a21\u578b\u5728\u4e00\u7cfb\u5217\u73b0\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5982\u5e03\u6599\u6298\u53e0\u3001\u76d2\u5b50\u7ec4\u88c5\u548c\u6742\u8d27\u8865\u8d27\u7b49\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8de8\u4efb\u52a1\u5355\u4e00\u5171\u4eab\u7b56\u7565\u7684\u6709\u6548\u6027\u3002\u6709\u6548\u540e\u8bad\u7ec3\u53ef\u5728\u51e0\u5c0f\u65f6\u5185\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u5b9e\u73b0\uff0c\u4e14\u6027\u80fd\u51e0\u4e4e\u968f\u7740\u8230\u961f\u4e2d\u673a\u5668\u4eba\u6570\u91cf\u5448\u7ebf\u6027\u589e\u957f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u5728\u7ebf\u5b66\u4e60\u4e0e\u8230\u961f\u89c4\u6a21\u90e8\u7f72\u7d27\u5bc6\u7ed3\u5408\u5bf9\u4e8e\u5728\u7269\u7406\u4e16\u754c\u4e2d\u9ad8\u6548\u3001\u53ef\u9760\u5730\u6269\u5c55\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u7684\u540e\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2601.03070", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.03070", "abs": "https://arxiv.org/abs/2601.03070", "authors": ["Tamlin Love", "Ferran Gebell\u00ed", "Pradip Pramanick", "Antonio Andriella", "Guillem Aleny\u00e0", "Anais Garrell", "Raquel Ros", "Silvia Rossi"], "title": "HEXAR: a Hierarchical Explainability Architecture for Robots", "comment": "8 pages, 6 figures", "summary": "As robotic systems become increasingly complex, the need for explainable decision-making becomes critical. Existing explainability approaches in robotics typically either focus on individual modules, which can be difficult to query from the perspective of high-level behaviour, or employ monolithic approaches, which do not exploit the modularity of robotic architectures. We present HEXAR (Hierarchical EXplainability Architecture for Robots), a novel framework that provides a plug-in, hierarchical approach to generate explanations about robotic systems. HEXAR consists of specialised component explainers using diverse explanation techniques (e.g., LLM-based reasoning, causal models, feature importance, etc) tailored to specific robot modules, orchestrated by an explainer selector that chooses the most appropriate one for a given query. We implement and evaluate HEXAR on a TIAGo robot performing assistive tasks in a home environment, comparing it against end-to-end and aggregated baseline approaches across 180 scenario-query variations. We observe that HEXAR significantly outperforms baselines in root cause identification, incorrect information exclusion, and runtime, offering a promising direction for transparent autonomous systems.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u89e3\u91ca\u6846\u67b6HEXAR\uff0c\u5b83\u91c7\u7528\u5206\u5c42\u63d2\u4ef6\u5f0f\u65b9\u6cd5\u751f\u6210\u5173\u4e8e\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u89e3\u91ca\u3002\u901a\u8fc7\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u6267\u884c\u8f85\u52a9\u4efb\u52a1\u7684TIAGo\u673a\u5668\u4eba\u4e0a\u5b9e\u65bd\u548c\u8bc4\u4f30\uff0c\u4e0e\u7aef\u5230\u7aef\u548c\u805a\u5408\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cHEXAR\u5728\u6839\u672c\u539f\u56e0\u8bc6\u522b\u3001\u9519\u8bef\u4fe1\u606f\u6392\u9664\u548c\u8fd0\u884c\u65f6\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u7cfb\u7edf\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u5bf9\u53ef\u89e3\u91ca\u51b3\u7b56\u7684\u9700\u6c42\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u673a\u5668\u4eba\u89e3\u91ca\u6027\u65b9\u6cd5\u8981\u4e48\u4e13\u6ce8\u4e8e\u5355\u4e2a\u6a21\u5757\u800c\u96be\u4ee5\u4ece\u9ad8\u7ea7\u884c\u4e3a\u7684\u89d2\u5ea6\u8fdb\u884c\u67e5\u8be2\uff0c\u8981\u4e48\u91c7\u53d6\u6574\u4f53\u65b9\u6cd5\u800c\u4e0d\u5229\u7528\u673a\u5668\u4eba\u67b6\u6784\u7684\u6a21\u5757\u5316\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u63d0\u51fa\u4e86HEXAR\u6846\u67b6\u3002", "method": "HEXAR\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u5206\u5c42\u65b9\u6cd5\u6765\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u751f\u6210\u89e3\u91ca\u3002\u8be5\u6846\u67b6\u7531\u4e13\u95e8\u7684\u7ec4\u4ef6\u89e3\u91ca\u5668\u7ec4\u6210\uff0c\u4f7f\u7528\u591a\u79cd\u89e3\u91ca\u6280\u672f\uff08\u5982\u57fa\u4e8eLLM\u7684\u63a8\u7406\u3001\u56e0\u679c\u6a21\u578b\u3001\u7279\u5f81\u91cd\u8981\u6027\u7b49\uff09\u9488\u5bf9\u7279\u5b9a\u7684\u673a\u5668\u4eba\u6a21\u5757\uff0c\u5e76\u7531\u4e00\u4e2a\u89e3\u91ca\u9009\u62e9\u5668\u534f\u8c03\uff0c\u4ee5\u6839\u636e\u7ed9\u5b9a\u67e5\u8be2\u9009\u62e9\u6700\u5408\u9002\u7684\u4e00\u4e2a\u3002", "result": "\u5728180\u79cd\u573a\u666f-\u67e5\u8be2\u53d8\u4f53\u4e2d\uff0cHEXAR\u5728\u6839\u56e0\u8bc6\u522b\u3001\u9519\u8bef\u4fe1\u606f\u6392\u9664\u4ee5\u53ca\u8fd0\u884c\u65f6\u95f4\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u7aef\u5230\u7aef\u548c\u805a\u5408\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "HEXAR\u4e3a\u900f\u660e\u81ea\u4e3b\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\uff0c\u5728\u63d0\u9ad8\u673a\u5668\u4eba\u51b3\u7b56\u8fc7\u7a0b\u7684\u7406\u89e3\u6027\u548c\u900f\u660e\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u663e\u8457\u7684\u4f18\u52bf\u3002"}}
{"id": "2601.03097", "categories": ["cs.RO", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.03097", "abs": "https://arxiv.org/abs/2601.03097", "authors": ["Omayra Yago Nieto", "Alexandre Anahory Simoes", "Juan I. Giribet", "Leonardo Colombo"], "title": "Dual-quaternion learning control for autonomous vehicle trajectory tracking with safety guarantees", "comment": null, "summary": "We propose a learning-based trajectory tracking controller for autonomous robotic platforms whose motion can be described kinematically on $\\mathrm{SE}(3)$. The controller is formulated in the dual quaternion framework and operates at the velocity level, assuming direct command of angular and linear velocities, as is standard in many aerial vehicles and omnidirectional mobile robots. Gaussian Process (GP) regression is integrated into a geometric feedback law to learn and compensate online for unknown, state-dependent disturbances and modeling imperfections affecting both attitude and position, while preserving the algebraic structure and coupling properties inherent to rigid-body motion.\n  The proposed approach does not rely on explicit parametric models of the unknown effects, making it well-suited for robotic systems subject to sensor-induced disturbances, unmodeled actuation couplings, and environmental uncertainties. A Lyapunov-based analysis establishes probabilistic ultimate boundedness of the pose tracking error under bounded GP uncertainty, providing formal stability guarantees for the learning-based controller.\n  Simulation results demonstrate accurate and smooth trajectory tracking in the presence of realistic, localized disturbances, including correlated rotational and translational effects arising from magnetometer perturbations. These results illustrate the potential of combining geometric modeling and probabilistic learning to achieve robust, data-efficient pose control for autonomous robotic systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u8f68\u8ff9\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u8fd0\u52a8\u53ef\u4ee5\u88ab\u63cf\u8ff0\u4e3a$\\mathrm{SE}(3)$\u4e0a\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u5e73\u53f0\u3002\u8be5\u63a7\u5236\u5668\u5728\u53cc\u56db\u5143\u6570\u6846\u67b6\u4e2d\u6784\u5efa\uff0c\u5e76\u5728\u901f\u5ea6\u5c42\u9762\u64cd\u4f5c\uff0c\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u96c6\u6210\u5230\u51e0\u4f55\u53cd\u9988\u5f8b\u4e2d\uff0c\u4ee5\u5728\u7ebf\u5b66\u4e60\u548c\u8865\u507f\u672a\u77e5\u7684\u72b6\u6001\u4f9d\u8d56\u5e72\u6270\u548c\u5efa\u6a21\u4e0d\u5b8c\u5584\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u521a\u4f53\u8fd0\u52a8\u7684\u4ee3\u6570\u7ed3\u6784\u548c\u8026\u5408\u7279\u6027\u3002", "motivation": "\u9488\u5bf9\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u4f20\u611f\u5668\u5f15\u8d77\u7684\u5e72\u6270\u3001\u672a\u5efa\u6a21\u7684\u81f4\u52a8\u8026\u5408\u4ee5\u53ca\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7b49\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u79cd\u4e0d\u9700\u8981\u663e\u5f0f\u53c2\u6570\u6a21\u578b\u7684\u5b66\u4e60\u578b\u63a7\u5236\u5668\uff0c\u80fd\u591f\u5728\u7ebf\u5b66\u4e60\u5e76\u8865\u507f\u8fd9\u4e9b\u672a\u77e5\u5f71\u54cd\uff0c\u4ece\u800c\u5b9e\u73b0\u9c81\u68d2\u7684\u59ff\u6001\u63a7\u5236\u3002", "method": "\u4f7f\u7528\u53cc\u56db\u5143\u6570\u6846\u67b6\u8bbe\u8ba1\u63a7\u5236\u5668\uff0c\u5728\u901f\u5ea6\u5c42\u9762\u4e0a\u76f4\u63a5\u63a7\u5236\u89d2\u901f\u5ea6\u548c\u7ebf\u901f\u5ea6\uff1b\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u56de\u5f52\u6280\u672f\u6765\u5b66\u4e60\u548c\u8865\u507f\u59ff\u6001\u4e0e\u4f4d\u7f6e\u4e0a\u672a\u77e5\u4e14\u72b6\u6001\u76f8\u5173\u7684\u6270\u52a8\u53ca\u5efa\u6a21\u7f3a\u9677\uff1b\u901a\u8fc7Lyapunov\u5206\u6790\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u6709\u754cGP\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u59ff\u6001\u8ddf\u8e2a\u8bef\u5dee\u7684\u6982\u7387\u6700\u7ec8\u6709\u754c\u6027\uff0c\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u7684\u7a33\u5b9a\u6027\u4fdd\u8bc1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5b58\u5728\u771f\u5b9e\u5c40\u90e8\u6270\u52a8\u7684\u60c5\u51b5\u4e0b\uff0c\u5305\u62ec\u7531\u78c1\u529b\u8ba1\u6270\u52a8\u5f15\u8d77\u7684\u76f8\u5173\u65cb\u8f6c\u548c\u5e73\u79fb\u6548\u5e94\uff0c\u6240\u63d0\u51fa\u7684\u63a7\u5236\u5668\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u5e73\u6ed1\u7684\u8f68\u8ff9\u8ddf\u8e2a\u3002\u8fd9\u8868\u660e\u7ed3\u5408\u51e0\u4f55\u5efa\u6a21\u4e0e\u6982\u7387\u5b66\u4e60\u7684\u65b9\u6cd5\u5bf9\u4e8e\u5b9e\u73b0\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u9ad8\u7684\u59ff\u6001\u63a7\u5236\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u8f68\u8ff9\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u5b83\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u548c\u51e0\u4f55\u53cd\u9988\u5b9a\u5f8b\u6765\u5b9e\u65f6\u8865\u507f\u672a\u77e5\u6270\u52a8\u548c\u5efa\u6a21\u8bef\u5dee\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7c7b\u578b\u7684\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u3002\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u7ef4\u6301\u521a\u4f53\u8fd0\u52a8\u56fa\u6709\u7684\u4ee3\u6570\u7ed3\u6784\u548c\u8026\u5408\u5c5e\u6027\uff0c\u800c\u4e14\u8fd8\u80fd\u63d0\u4f9b\u6b63\u5f0f\u7684\u7a33\u5b9a\u6027\u4fdd\u969c\uff0c\u5e76\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2601.03200", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.03200", "abs": "https://arxiv.org/abs/2601.03200", "authors": ["Ziyang Sun", "Lingfan Bao", "Tianhu Peng", "Jingcheng Sun", "Chengxu Zhou"], "title": "A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting", "comment": "Under review of Journal of Robot Learning", "summary": "Developing high-fidelity, interactive digital twins is crucial for enabling closed-loop motion planning and reliable real-world robot execution, which are essential to advancing sim-to-real transfer. However, existing approaches often suffer from slow reconstruction, limited visual fidelity, and difficulties in converting photorealistic models into planning-ready collision geometry. We present a practical framework that constructs high-quality digital twins within minutes from sparse RGB inputs. Our system employs 3D Gaussian Splatting (3DGS) for fast, photorealistic reconstruction as a unified scene representation. We enhance 3DGS with visibility-aware semantic fusion for accurate 3D labelling and introduce an efficient, filter-based geometry conversion method to produce collision-ready models seamlessly integrated with a Unity-ROS2-MoveIt physics engine. In experiments with a Franka Emika Panda robot performing pick-and-place tasks, we demonstrate that this enhanced geometric accuracy effectively supports robust manipulation in real-world trials. These results demonstrate that 3DGS-based digital twins, enriched with semantic and geometric consistency, offer a fast, reliable, and scalable path from perception to manipulation in unstructured environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u70b9\u4e91\uff083DGS\uff09\u7684\u5b9e\u7528\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u7a00\u758fRGB\u8f93\u5165\u4e2d\u5feb\u901f\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u8fc7\u6ee4\u5668\u7684\u51e0\u4f55\u8f6c\u6362\u65b9\u6cd5\u751f\u6210\u53ef\u7528\u4e8e\u78b0\u649e\u68c0\u6d4b\u7684\u6a21\u578b\uff0c\u4ece\u800c\u652f\u6301\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u5b57\u5b6a\u751f\u5f00\u53d1\u65b9\u6cd5\u5b58\u5728\u91cd\u5efa\u901f\u5ea6\u6162\u3001\u89c6\u89c9\u903c\u771f\u5ea6\u6709\u9650\u4ee5\u53ca\u96be\u4ee5\u5c06\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u6a21\u578b\u8f6c\u6362\u4e3a\u89c4\u5212\u6240\u9700\u7684\u78b0\u649e\u51e0\u4f55\u4f53\u7b49\u95ee\u9898\u3002\u4e3a\u4e86\u4fc3\u8fdb\u6a21\u62df\u5230\u73b0\u5b9e\u4e16\u754c\u7684\u8f6c\u79fb\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u6267\u884c\u4efb\u52a1\u65f6\u7684\u53ef\u9760\u6027\u548c\u4ea4\u4e92\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u3002", "method": "\u8be5\u7814\u7a76\u4f7f\u75283D\u9ad8\u65af\u70b9\u4e91\u6280\u672f\u8fdb\u884c\u5feb\u901f\u4e14\u5177\u6709\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u7684\u91cd\u5efa\u4f5c\u4e3a\u7edf\u4e00\u573a\u666f\u8868\u793a\uff1b\u7ed3\u5408\u53ef\u89c1\u6027\u611f\u77e5\u8bed\u4e49\u878d\u5408\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u4e09\u7ef4\u6807\u6ce8\uff1b\u5e76\u901a\u8fc7\u9ad8\u6548\u8fc7\u6ee4\u5668\u57fa\u51e0\u4f55\u8f6c\u6362\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\u81f3Unity-ROS2-MoveIt\u7269\u7406\u5f15\u64ce\u4e2d\u4ea7\u751f\u53ef\u78b0\u649e\u68c0\u6d4b\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728Franka Emika Panda\u673a\u5668\u4eba\u6267\u884c\u62fe\u53d6\u4e0e\u653e\u7f6e\u4efb\u52a1\u65f6\uff0c\u589e\u5f3a\u540e\u7684\u51e0\u4f55\u7cbe\u5ea6\u6709\u6548\u652f\u6301\u4e86\u5b9e\u9645\u8bd5\u9a8c\u4e2d\u7684\u7a33\u5065\u64cd\u4f5c\u3002\u8fd9\u8bc1\u660e\u4e86\u57fa\u4e8e3DGS\u5e76\u4e30\u5bcc\u4e86\u8bed\u4e49\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u4e86\u4e00\u6761\u4ece\u611f\u77e5\u5230\u64cd\u4f5c\u7684\u5feb\u901f\u3001\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u8def\u5f84\u3002", "conclusion": "\u57fa\u4e8e3DGS\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u901a\u8fc7\u589e\u52a0\u8bed\u4e49\u548c\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u4e3a\u4ece\u611f\u77e5\u5230\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4e86\u663e\u8457\u4f18\u52bf\u3002"}}
