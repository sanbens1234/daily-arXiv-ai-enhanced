{"id": "2602.05010", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05010", "abs": "https://arxiv.org/abs/2602.05010", "authors": ["Maia Stiber", "Sameer Khan", "Russell Taylor", "Chien-Ming Huang"], "title": "Signal or 'Noise': Human Reactions to Robot Errors in the Wild", "comment": null, "summary": "In the real world, robots frequently make errors, yet little is known about people's social responses to errors outside of lab settings. Prior work has shown that social signals are reliable and useful for error management in constrained interactions, but it is unclear if this holds in the real world - especially with a non-social robot in repeated and group interactions with successive or propagated errors. To explore this, we built a coffee robot and conducted a public field deployment ($N = 49$). We found that participants consistently expressed varied social signals in response to errors and other stimuli, particularly during group interactions. Our findings suggest that social signals in the wild are rich (with participants volunteering information about the interaction), but \"noisy.\" We discuss lessons, benefits, and challenges for using social signals in real-world HRI."}
{"id": "2602.05029", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05029", "abs": "https://arxiv.org/abs/2602.05029", "authors": ["Octavio Arriaga", "Proneet Sharma", "Jichen Guo", "Marc Otto", "Siddhant Kadwe", "Rebecca Adam"], "title": "Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping", "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L) for review. This version includes the statement required by IEEE for preprints", "summary": "Operating effectively in novel real-world environments requires robotic systems to estimate and interact with previously unseen objects. Current state-of-the-art models address this challenge by using large amounts of training data and test-time samples to build black-box scene representations. In this work, we introduce a differentiable neuro-graphics model that combines neural foundation models with physics-based differentiable rendering to perform zero-shot scene reconstruction and robot grasping without relying on any additional 3D data or test-time samples. Our model solves a series of constrained optimization problems to estimate physically consistent scene parameters, such as meshes, lighting conditions, material properties, and 6D poses of previously unseen objects from a single RGBD image and bounding boxes. We evaluated our approach on standard model-free few-shot benchmarks and demonstrated that it outperforms existing algorithms for model-free few-shot pose estimation. Furthermore, we validated the accuracy of our scene reconstructions by applying our algorithm to a zero-shot grasping task. By enabling zero-shot, physically-consistent scene reconstruction and grasping without reliance on extensive datasets or test-time sampling, our approach offers a pathway towards more data efficient, interpretable and generalizable robot autonomy in novel environments."}
{"id": "2602.05079", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05079", "abs": "https://arxiv.org/abs/2602.05079", "authors": ["Vinal Asodia", "Iman Sharifi", "Saber Fallah"], "title": "Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking", "comment": "12 pages, 7 figures, 5 tables", "summary": "The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving."}
{"id": "2602.05092", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05092", "abs": "https://arxiv.org/abs/2602.05092", "authors": ["Thomas Cohn", "Lihan Tang", "Alexandre Amice", "Russ Tedrake"], "title": "A Framework for Combining Optimization-Based and Analytic Inverse Kinematics", "comment": "19 pages, 5 figures, 6 tables. Under submission", "summary": "Analytic and optimization methods for solving inverse kinematics (IK) problems have been deeply studied throughout the history of robotics. The two strategies have complementary strengths and weaknesses, but developing a unified approach to take advantage of both methods has proved challenging. A key challenge faced by optimization approaches is the complicated nonlinear relationship between the joint angles and the end-effector pose. When this must be handled concurrently with additional nonconvex constraints like collision avoidance, optimization IK algorithms may suffer high failure rates. We present a new formulation for optimization IK that uses an analytic IK solution as a change of variables, and is fundamentally easier for optimizers to solve. We test our methodology on three popular solvers, representing three different paradigms for constrained nonlinear optimization. Extensive experimental comparisons demonstrate that our new formulation achieves higher success rates than the old formulation and baseline methods across various challenging IK problems, including collision avoidance, grasp selection, and humanoid stability."}
{"id": "2602.05156", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.05156", "abs": "https://arxiv.org/abs/2602.05156", "authors": ["Dong Ho Kang", "Aaron Kim", "Mingyo Seo", "Kazuto Yokoyama", "Tetsuya Narita", "Luis Sentis"], "title": "PLATO Hand: Shaping Contact Behavior with Fingernails for Precise Manipulation", "comment": null, "summary": "We present the PLATO Hand, a dexterous robotic hand with a hybrid fingertip that embeds a rigid fingernail within a compliant pulp. This design shapes contact behavior to enable diverse interaction modes across a range of object geometries. We develop a strain-energy-based bending-indentation model to guide the fingertip design and to explain how guided contact preserves local indentation while suppressing global bending. Experimental results show that the proposed robotic hand design demonstrates improved pinching stability, enhanced force observability, and successful execution of edge-sensitive manipulation tasks, including paper singulation, card picking, and orange peeling. Together, these results show that coupling structured contact geometry with a force-motion transparent mechanism provides a principled, physically embodied approach to precise manipulation."}
{"id": "2602.05198", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05198", "abs": "https://arxiv.org/abs/2602.05198", "authors": ["Kalvik Jakkala", "Saurav Agarwal", "Jason O'Kane", "Srinivas Akella"], "title": "Informative Path Planning with Guaranteed Estimation Uncertainty", "comment": "16 pages, 11 figures, preprint", "summary": "Environmental monitoring robots often need to reconstruct spatial fields (e.g., salinity, temperature, bathymetry) under tight distance and energy constraints. Classical boustrophedon lawnmower surveys provide geometric coverage guarantees but can waste effort by oversampling predictable regions. In contrast, informative path planning (IPP) methods leverage spatial correlations to reduce oversampling, yet typically offer no guarantees on reconstruction quality. This paper bridges these approaches by addressing informative path planning with guaranteed estimation uncertainty: computing the shortest path whose measurements ensure that the Gaussian-process (GP) posterior variance -- an intrinsic uncertainty measure that lower-bounds the mean-squared prediction error under the GP model -- falls below a user-specified threshold over the monitoring region.\n  We propose a three-stage approach: (i) learn a GP model from available prior information; (ii) transform the learned GP kernel into binary coverage maps for each candidate sensing location, indicating which locations' uncertainty can be reduced below a specified target; and (iii) plan a near-shortest route whose combined coverage satisfies the global uncertainty constraint. To address heterogeneous phenomena, we incorporate a nonstationary kernel that captures spatially varying correlation structure, and we accommodate non-convex environments with obstacles. Algorithmically, we present methods with provable approximation guarantees for sensing-location selection and for the joint selection-and-routing problem under a travel budget. Experiments on real-world topographic data show that our planners meet the uncertainty target using fewer sensing locations and shorter travel distances than a recent baseline, and field experiments with bathymetry-mapping autonomous surface and underwater vehicles demonstrate real-world feasibility."}
{"id": "2602.05233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05233", "abs": "https://arxiv.org/abs/2602.05233", "authors": ["Wenbo Wang", "Fangyun Wei", "QiXiu Li", "Xi Chen", "Yaobo Liang", "Chang Xu", "Jiaolong Yang", "Baining Guo"], "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation", "comment": null, "summary": "Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments."}
{"id": "2602.05265", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05265", "abs": "https://arxiv.org/abs/2602.05265", "authors": ["Kalvik Jakkala", "Jason O'Kane"], "title": "Low-Cost Underwater In-Pipe Centering and Inspection Using a Minimal-Sensing Robot", "comment": null, "summary": "Autonomous underwater inspection of submerged pipelines is challenging due to confined geometries, turbidity, and the scarcity of reliable localization cues. This paper presents a minimal-sensing strategy that enables a free-swimming underwater robot to center itself and traverse a flooded pipe of known radius using only an IMU, a pressure sensor, and two sonars: a downward-facing single-beam sonar and a rotating 360 degree sonar. We introduce a computationally efficient method for extracting range estimates from single-beam sonar intensity data, enabling reliable wall detection in noisy and reverberant conditions. A closed-form geometric model leverages the two sonar ranges to estimate the pipe center, and an adaptive, confidence-weighted proportional-derivative (PD) controller maintains alignment during traversal. The system requires no Doppler velocity log, external tracking, or complex multi-sensor arrays. Experiments in a submerged 46 cm-diameter pipe using a Blue Robotics BlueROV2 heavy remotely operated vehicle demonstrate stable centering and successful full-pipe traversal despite ambient flow and structural deformations. These results show that reliable in-pipe navigation and inspection can be achieved with a lightweight, computationally efficient sensing and processing architecture, advancing the practicality of autonomous underwater inspection in confined environments."}
{"id": "2602.05273", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05273", "abs": "https://arxiv.org/abs/2602.05273", "authors": ["Hengxuan Xu", "Fengbo Lan", "Zhixin Zhao", "Shengjie Wang", "Mengqiao Liu", "Jieqian Sun", "Yu Cheng", "Tao Zhang"], "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions", "comment": "14 pages, 10 figures, 8 tables", "summary": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for \"I'm thirsty\") remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios."}
{"id": "2602.05310", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05310", "abs": "https://arxiv.org/abs/2602.05310", "authors": ["Jipeng Kong", "Xinzhe Liu", "Yuhang Lin", "Jinrui Han", "Sören Schwertfeger", "Chenjia Bai", "Xuelong Li"], "title": "Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework", "comment": "13 pages, 9 figures, conference", "summary": "Soccer presents a significant challenge for humanoid robots, demanding tightly integrated perception-action capabilities for tasks like perception-guided kicking and whole-body balance control. Existing approaches suffer from inter-module instability in modular pipelines or conflicting training objectives in end-to-end frameworks. We propose Perception-Action integrated Decision-making (PAiD), a progressive architecture that decomposes soccer skill acquisition into three stages: motion-skill acquisition via human motion tracking, lightweight perception-action integration for positional generalization, and physics-aware sim-to-real transfer. This staged decomposition establishes stable foundational skills, avoids reward conflicts during perception integration, and minimizes sim-to-real gaps. Experiments on the Unitree G1 demonstrate high-fidelity human-like kicking with robust performance under diverse conditions-including static or rolling balls, various positions, and disturbances-while maintaining consistent execution across indoor and outdoor scenarios. Our divide-and-conquer strategy advances robust humanoid soccer capabilities and offers a scalable framework for complex embodied skill acquisition. The project page is available at https://soccer-humanoid.github.io/."}
{"id": "2602.05325", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05325", "abs": "https://arxiv.org/abs/2602.05325", "authors": ["Jiacheng Fan", "Zhiyue Zhao", "Yiqian Zhang", "Chao Chen", "Peide Wang", "Hengdi Zhang", "Zhengxue Cheng"], "title": "RoboPaint: From Human Demonstration to Any Robot and Any View", "comment": "17 pages", "summary": "Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently \"painted\" from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation."}
{"id": "2602.05441", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05441", "abs": "https://arxiv.org/abs/2602.05441", "authors": ["Dean Fortier", "Timothy Adamson", "Tess Hellebrekers", "Teresa LaScala", "Kofi Ennin", "Michael Murray", "Andrey Kolobov", "Galen Mullins"], "title": "Benchmarking Affordance Generalization with BusyBox", "comment": null, "summary": "Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features.\n  In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $π_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox."}
{"id": "2602.05456", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.05456", "abs": "https://arxiv.org/abs/2602.05456", "authors": ["Maksym Figat", "Ryan M. Mackey", "Michel D. Ingham"], "title": "Ontology-Driven Robotic Specification Synthesis", "comment": "8 pages, 9 figures, 3 tables, journal", "summary": "This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future."}
{"id": "2602.05468", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05468", "abs": "https://arxiv.org/abs/2602.05468", "authors": ["Pranav Ponnivalavan", "Satoshi Funabashi", "Alexander Schmitz", "Tetsuya Ogata", "Shigeki Sugano"], "title": "TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation", "comment": "8 pages, 8 figures, 8 tables, ICRA2026 accepted", "summary": "Humans can achieve diverse in-hand manipulations, such as object pinching and tool use, which often involve simultaneous contact between the object and multiple fingers. This is still an open issue for robotic hands because such dexterous manipulation requires distinguishing between tactile sensations generated by their self-contact and those arising from external contact. Otherwise, object/robot breakage happens due to contacts/collisions. Indeed, most approaches ignore self-contact altogether, by constraining motion to avoid/ignore self-tactile information during contact. While this reduces complexity, it also limits generalization to real-world scenarios where self-contact is inevitable. Humans overcome this challenge through self-touch perception, using predictive mechanisms that anticipate the tactile consequences of their own motion, through a principle called sensory attenuation, where the nervous system differentiates predictable self-touch signals, allowing novel object stimuli to stand out as relevant. Deriving from this, we introduce TaSA, a two-phased deep predictive learning framework. In the first phase, TaSA explicitly learns self-touch dynamics, modeling how a robot's own actions generate tactile feedback. In the second phase, this learned model is incorporated into the motion learning phase, to emphasize object contact signals during manipulation. We evaluate TaSA on a set of insertion tasks, which demand fine tactile discrimination: inserting a pencil lead into a mechanical pencil, inserting coins into a slot, and fixing a paper clip onto a sheet of paper, with various orientations, positions, and sizes. Across all tasks, policies trained with TaSA achieve significantly higher success rates than baseline methods, demonstrating that structured tactile perception with self-touch based on sensory attenuation is critical for dexterous robotic manipulation."}
{"id": "2602.05513", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05513", "abs": "https://arxiv.org/abs/2602.05513", "authors": ["Xukun Li", "Yu Sun", "Lei Zhang", "Bosheng Huang", "Yibo Peng", "Yuan Meng", "Haojun Jiang", "Shaoxuan Xie", "Guacai Yao", "Alois Knoll", "Zhenshan Bing", "Xinlong Wang", "Zhenguo Sun"], "title": "DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter", "comment": "17 pages, 8 figures", "summary": "Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories."}
{"id": "2602.05516", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05516", "abs": "https://arxiv.org/abs/2602.05516", "authors": ["Runxiao Liu", "Pengda Mao", "Xiangli Le", "Shuang Gu", "Yapeng Chen", "Quan Quan"], "title": "Virtual-Tube-Based Cooperative Transport Control for Multi-UAV Systems in Constrained Environments", "comment": "10 pages, 8 figures", "summary": "This paper proposes a novel control framework for cooperative transportation of cable-suspended loads by multiple unmanned aerial vehicles (UAVs) operating in constrained environments. Leveraging virtual tube theory and principles from dissipative systems theory, the framework facilitates efficient multi-UAV collaboration for navigating obstacle-rich areas. The proposed framework offers several key advantages. (1) It achieves tension distribution and coordinated transportation within the UAV-cable-load system with low computational overhead, dynamically adapting UAV configurations based on obstacle layouts to facilitate efficient navigation. (2) By integrating dissipative systems theory, the framework ensures high stability and robustness, essential for complex multi-UAV operations. The effectiveness of the proposed approach is validated through extensive simulations, demonstrating its scalability for large-scale multi-UAV systems. Furthermore, the method is experimentally validated in outdoor scenarios, showcasing its practical feasibility and robustness under real-world conditions."}
{"id": "2602.05552", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05552", "abs": "https://arxiv.org/abs/2602.05552", "authors": ["Bessie Dominguez-Dager", "Sergio Suescun-Ferrandiz", "Felix Escalona", "Francisco Gomez-Donoso", "Miguel Cazorla"], "title": "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator", "comment": null, "summary": "This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments."}
{"id": "2602.05596", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05596", "abs": "https://arxiv.org/abs/2602.05596", "authors": ["Hokyun Lee", "Woo-Jeong Baek", "Junhyeok Cha", "Jaeheung Park"], "title": "TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards", "comment": "Accepted for Publication at IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "With the growing employment of learning algorithms in robotic applications, research on reinforcement learning for bipedal locomotion has become a central topic for humanoid robotics. While recently published contributions achieve high success rates in locomotion tasks, scarce attention has been devoted to the development of methods that enable to handle hardware faults that may occur during the locomotion process. However, in real-world settings, environmental disturbances or sudden occurrences of hardware faults might yield severe consequences. To address these issues, this paper presents TOLEBI (A faulT-tOlerant Learning framEwork for Bipedal locomotIon) that handles faults on the robot during operation. Specifically, joint locking, power loss and external disturbances are injected in simulation to learn fault-tolerant locomotion strategies. In addition to transferring the learned policy to the real robot via sim-to-real transfer, an online joint status module incorporated. This module enables to classify joint conditions by referring to the actual observations at runtime under real-world conditions. The validation experiments conducted both in real-world and simulation with the humanoid robot TOCABI highlight the applicability of the proposed approach. To our knowledge, this manuscript provides the first learning-based fault-tolerant framework for bipedal locomotion, thereby fostering the development of efficient learning methods in this field."}
{"id": "2602.05608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05608", "abs": "https://arxiv.org/abs/2602.05608", "authors": ["Yufei Zhu", "Shih-Min Yang", "Martin Magnusson", "Allan Wang"], "title": "HiCrowd: Hierarchical Crowd Flow Alignment for Dense Human Environments", "comment": "Accepted to the 2026 IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Navigating through dense human crowds remains a significant challenge for mobile robots. A key issue is the freezing robot problem, where the robot struggles to find safe motions and becomes stuck within the crowd. To address this, we propose HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). HiCrowd leverages surrounding pedestrian motion as guidance, enabling the robot to align with compatible crowd flows. A high-level RL policy generates a follow point to align the robot with a suitable pedestrian group, while a low-level MPC safely tracks this guidance with short horizon planning. The method combines long-term crowd aware decision making with safe short-term execution. We evaluate HiCrowd against reactive and learning-based baselines in offline setting (replaying recorded human trajectories) and online setting (human trajectories are updated to react to the robot in simulation). Experiments on a real-world dataset and a synthetic crowd dataset show that our method outperforms in navigation efficiency and safety, while reducing freezing behaviors. Our results suggest that leveraging human motion as guidance, rather than treating humans solely as dynamic obstacles, provides a powerful principle for safe and efficient robot navigation in crowds."}
{"id": "2602.05683", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.05683", "abs": "https://arxiv.org/abs/2602.05683", "authors": ["Chuwei Wang", "Eduardo Sebastián", "Amanda Prorok", "Anastasia Bizyaeva"], "title": "From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking", "comment": null, "summary": "Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform."}
{"id": "2602.05760", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05760", "abs": "https://arxiv.org/abs/2602.05760", "authors": ["Andreea Tulbure", "Carmen Scheidemann", "Elias Steiner", "Marco Hutter"], "title": "Task-Oriented Robot-Human Handovers on Legged Manipulators", "comment": "Accepted to 21st ACM/IEEE International Conference on Human-Robot Interaction (HRI) 2026", "summary": "Task-oriented handovers (TOH) are fundamental to effective human-robot collaboration, requiring robots to present objects in a way that supports the human's intended post-handover use. Existing approaches are typically based on object- or task-specific affordances, but their ability to generalize to novel scenarios is limited. To address this gap, we present AFT-Handover, a framework that integrates large language model (LLM)-driven affordance reasoning with efficient texture-based affordance transfer to achieve zero-shot, generalizable TOH. Given a novel object-task pair, the method retrieves a proxy exemplar from a database, establishes part-level correspondences via LLM reasoning, and texturizes affordances for feature-based point cloud transfer. We evaluate AFT-Handover across diverse task-object pairs, showing improved handover success rates and stronger generalization compared to baselines. In a comparative user study, our framework is significantly preferred over the current state-of-the-art, effectively reducing human regrasping before tool use. Finally, we demonstrate TOH on legged manipulators, highlighting the potential of our framework for real-world robot-human handovers."}
{"id": "2602.05791", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05791", "abs": "https://arxiv.org/abs/2602.05791", "authors": ["Yufei Xue", "YunFeng Lin", "Wentao Dong", "Yang Tang", "Jingbo Wang", "Jiangmiao Pang", "Ming Zhou", "Minghuan Liu", "Weinan Zhang"], "title": "Scalable and General Whole-Body Control for Cross-Humanoid Locomotion", "comment": null, "summary": "Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller."}
{"id": "2602.05855", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05855", "abs": "https://arxiv.org/abs/2602.05855", "authors": ["Dennis Bank", "Joost Cordes", "Thomas Seel", "Simon F. G. Ehlers"], "title": "A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion", "comment": null, "summary": "Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift."}
{"id": "2602.05895", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05895", "abs": "https://arxiv.org/abs/2602.05895", "authors": ["Qi Li", "Karsten Berns"], "title": "Residual Reinforcement Learning for Waste-Container Lifting Using Large-Scale Cranes with Underactuated Tools", "comment": "12 pages", "summary": "This paper studies the container lifting phase of a waste-container recycling task in urban environments, performed by a hydraulic loader crane equipped with an underactuated discharge unit, and proposes a residual reinforcement learning (RRL) approach that combines a nominal Cartesian controller with a learned residual policy. All experiments are conducted in simulation, where the task is characterized by tight geometric tolerances between the discharge-unit hooks and the container rings relative to the overall crane scale, making precise trajectory tracking and swing suppression essential. The nominal controller uses admittance control for trajectory tracking and pendulum-aware swing damping, followed by damped least-squares inverse kinematics with a nullspace posture term to generate joint velocity commands. A PPO-trained residual policy in Isaac Lab compensates for unmodeled dynamics and parameter variations, improving precision and robustness without requiring end-to-end learning from scratch. We further employ randomized episode initialization and domain randomization over payload properties, actuator gains, and passive joint parameters to enhance generalization. Simulation results demonstrate improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to the nominal controller alone."}
{"id": "2602.05922", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05922", "abs": "https://arxiv.org/abs/2602.05922", "authors": ["Aziz Mohamed Mili", "Louis Catar", "Paul Gérard", "Ilyass Tabiai", "David St-Onge"], "title": "From Bench to Flight: Translating Drone Impact Tests into Operational Safety Limits", "comment": null, "summary": "Indoor micro-aerial vehicles (MAVs) are increasingly used for tasks that require close proximity to people, yet practitioners lack practical methods to tune motion limits based on measured impact risk. We present an end-to-end, open toolchain that converts benchtop impact tests into deployable safety governors for drones. First, we describe a compact and replicable impact rig and protocol for capturing force-time profiles across drone classes and contact surfaces. Second, we provide data-driven models that map pre-impact speed to impulse and contact duration, enabling direct computation of speed bounds for a target force limit. Third, we release scripts and a ROS2 node that enforce these bounds online and log compliance, with support for facility-specific policies. We validate the workflow on multiple commercial off-the-shelf quadrotors and representative indoor assets, demonstrating that the derived governors preserve task throughput while meeting force constraints specified by safety stakeholders. Our contribution is a practical bridge from measured impacts to runtime limits, with shareable datasets, code, and a repeatable process that teams can adopt to certify indoor MAV operations near humans."}
{"id": "2602.06001", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06001", "abs": "https://arxiv.org/abs/2602.06001", "authors": ["Carolina Higuera", "Sergio Arnaud", "Byron Boots", "Mustafa Mukadam", "Francois Robert Hogan", "Franziska Meier"], "title": "Visuo-Tactile World Models", "comment": "Preprint", "summary": "We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations."}
{"id": "2602.06038", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.06038", "abs": "https://arxiv.org/abs/2602.06038", "authors": ["Xiaopan Zhang", "Zejin Wang", "Zhixu Li", "Jianpeng Yao", "Jiachen Li"], "title": "CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction", "comment": "IEEE International Conference on Robotics and Automation (ICRA 2026); Project Website: https://comm-cp.github.io/", "summary": "To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io."}
