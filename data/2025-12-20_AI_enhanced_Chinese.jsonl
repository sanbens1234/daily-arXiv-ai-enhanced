{"id": "2512.16861", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16861", "abs": "https://arxiv.org/abs/2512.16861", "authors": ["Zihan Zhou", "Animesh Garg", "Ajay Mandlekar", "Caelan Garrett"], "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning", "comment": null, "summary": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86ReinforceGen\u7cfb\u7edf\uff0c\u7ed3\u5408\u4efb\u52a1\u5206\u89e3\u3001\u6570\u636e\u751f\u6210\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u8fd0\u52a8\u89c4\u5212\u6765\u89e3\u51b3\u957f\u671f\u64cd\u63a7\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03\u3002\u5728Robosuite\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8680%\u7684\u6210\u529f\u7387\uff0c\u4e14\u6d88\u878d\u7814\u7a76\u8868\u660e\u5176\u5fae\u8c03\u65b9\u6cd5\u4f7f\u6027\u80fd\u5e73\u5747\u63d0\u9ad8\u4e8689%\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u9886\u57df\u4e2d\u957f\u671f\u5b58\u5728\u7684\u957f\u671f\u64cd\u63a7\u6311\u6218\u3002", "method": "\u4f7f\u7528\u4e86\u4efb\u52a1\u5206\u89e3\u3001\u6570\u636e\u751f\u6210\u3001\u6a21\u4eff\u5b66\u4e60\u4ee5\u53ca\u8fd0\u52a8\u89c4\u5212\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u5f62\u6210\u521d\u6b65\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5fae\u8c03\u6765\u6539\u8fdb\u6bcf\u4e2a\u7ec4\u4ef6\u3002", "result": "\u5728\u6700\u9ad8\u91cd\u7f6e\u8303\u56f4\u8bbe\u7f6e\u4e0b\uff0c\u5bf9\u4e8e\u6240\u6709\u5177\u6709\u89c6\u89c9-\u8fd0\u52a8\u63a7\u5236\u7684\u4efb\u52a1\uff0cReinforceGen\u8fbe\u5230\u4e8680%\u7684\u6210\u529f\u7387\u3002\u6b64\u5916\uff0c\u6d88\u878d\u7814\u7a76\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u5fae\u8c03\u65b9\u6cd5\u4f7f\u5f97\u6027\u80fd\u5e73\u5747\u63d0\u5347\u4e8689%\u3002", "conclusion": "ReinforceGen\u901a\u8fc7\u6574\u5408\u591a\u79cd\u6280\u672f\u5e76\u8fd0\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u957f\u671f\u64cd\u63a7\u96be\u9898\uff0c\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u6548\u679c\u63d0\u5347\u3002"}}
{"id": "2512.16881", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16881", "abs": "https://arxiv.org/abs/2512.16881", "authors": ["Arhan Jain", "Mingtong Zhang", "Kanav Arora", "William Chen", "Marcel Torne", "Muhammad Zubair Irshad", "Sergey Zakharov", "Yue Wang", "Sergey Levine", "Chelsea Finn", "Wei-Chiu Ma", "Dhruv Shah", "Abhishek Gupta", "Karl Pertsch"], "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies", "comment": "Website: https://polaris-evals.github.io/", "summary": "A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aPolaRiS\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u795e\u7ecf\u91cd\u5efa\u65b9\u6cd5\u5c06\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u7684\u77ed\u89c6\u9891\u626b\u63cf\u8f6c\u6362\u4e3a\u53ef\u4ea4\u4e92\u7684\u6a21\u62df\u73af\u5883\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u673a\u5668\u4eba\u7b56\u7565\u8bc4\u4f30\u3002\u901a\u8fc7\u4e0e\u73b0\u6709\u6a21\u62df\u57fa\u51c6\u76f8\u6bd4\uff0cPolaRiS\u8bc4\u4ef7\u4e0e\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u901a\u7528\u7b56\u7565\u8868\u73b0\u6709\u66f4\u5f3a\u7684\u76f8\u5173\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u5feb\u901f\u521b\u5efa\u591a\u6837\u7684\u6a21\u62df\u73af\u5883\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u7814\u7a76\u9762\u4e34\u7684\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u662f\u51c6\u786e\u6d4b\u91cf\u548c\u6bd4\u8f83\u673a\u5668\u4eba\u7b56\u7565\u7684\u8868\u73b0\u3002\u7531\u4e8e\u73b0\u5b9e\u4e16\u754c\u6d4b\u8bd5\u7684\u968f\u673a\u6027\u3001\u518d\u73b0\u6027\u548c\u8017\u65f6\u6027\u8d28\uff0c\u8fd9\u5728\u673a\u5668\u4eba\u6280\u672f\u4e2d\u5386\u6765\u662f\u4e00\u9879\u96be\u9898\u3002\u5bf9\u4e8e\u9700\u8981\u8de8\u5404\u79cd\u573a\u666f\u548c\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\u7684\u901a\u7528\u7b56\u7565\u6765\u8bf4\uff0c\u8fd9\u4e00\u95ee\u9898\u66f4\u52a0\u4e25\u91cd\u3002\u867d\u7136\u4eff\u771f\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5bf9\u73b0\u5b9e\u4e16\u754c\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u8865\u5145\uff0c\u4f46\u73b0\u6709\u4eff\u771f\u57fa\u51c6\u4e0e\u73b0\u5b9e\u4e16\u754c\u4e4b\u95f4\u7684\u89c6\u89c9\u548c\u7269\u7406\u9886\u57df\u5dee\u8ddd\u4f7f\u5176\u6210\u4e3a\u653f\u7b56\u6539\u8fdb\u4e0d\u53ef\u9760\u7684\u4fe1\u53f7\u3002\u6b64\u5916\uff0c\u6784\u5efa\u73b0\u5b9e\u4e14\u591a\u6837\u5316\u7684\u6a21\u62df\u73af\u5883\u4f20\u7edf\u4e0a\u9700\u8981\u5927\u91cf\u7684\u4eba\u529b\u548c\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u4f5c\u8005\u4eec\u63d0\u51fa\u4e86Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS)\u6846\u67b6\uff0c\u5b83\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u771f\u5b9e\u5230\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u4fdd\u771f\u7684\u6a21\u62df\u673a\u5668\u4eba\u8bc4\u4f30\u3002PolaRiS\u4f7f\u7528\u795e\u7ecf\u91cd\u5efa\u65b9\u6cd5\u5c06\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7684\u77ed\u89c6\u9891\u626b\u63cf\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u6a21\u62df\u73af\u5883\u3002\u53e6\u5916\uff0c\u4ed6\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u6a21\u62df\u6570\u636e\u5171\u540c\u8bad\u7ec3\u65b9\u6848\uff0c\u4ee5\u5f25\u5408\u5269\u4f59\u7684\u771f\u5b9e\u5230\u6a21\u62df\u5dee\u8ddd\uff0c\u5e76\u80fd\u591f\u5728\u672a\u89c1\u7684\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u914d\u5bf9\u8bc4\u4f30\uff0c\u7814\u7a76\u8868\u660ePolaRiS\u8bc4\u4f30\u4e0e\u73b0\u5b9e\u4e16\u754c\u901a\u7528\u7b56\u7565\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6bd4\u73b0\u6709\u6a21\u62df\u57fa\u51c6\u66f4\u9ad8\u7684\u76f8\u5173\u6027\u3002\u5176\u7b80\u6613\u6027\u4e5f\u4f7f\u5f97\u80fd\u591f\u8fc5\u901f\u521b\u5efa\u51fa\u591a\u79cd\u591a\u6837\u7684\u6a21\u62df\u73af\u5883\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u671d\u7740\u4e0b\u4e00\u4ee3\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u7684\u5206\u5e03\u5f0f\u548c\u6c11\u4e3b\u5316\u8bc4\u4f30\u8fc8\u51fa\u4e86\u4e00\u6b65\u3002"}}
{"id": "2512.16896", "categories": ["cs.RO", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.16896", "abs": "https://arxiv.org/abs/2512.16896", "authors": ["Jinghuan Shang", "Harsh Patel", "Ran Gong", "Karl Schmeckpeper"], "title": "Sceniris: A Fast Procedural Scene Generation Framework", "comment": "Code is available at https://github.com/rai-inst/sceniris", "summary": "Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9ad8\u6548\u76843D\u573a\u666f\u751f\u6210\u6846\u67b6Sceniris\uff0c\u5b83\u901a\u8fc7\u6279\u5904\u7406\u91c7\u6837\u548c\u66f4\u5feb\u7684\u78b0\u649e\u68c0\u6d4b\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u81f3\u5c11234\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u4e14\u652f\u6301\u591a\u6837\u5316\u7684\u573a\u666f\u9700\u6c42\u3002", "motivation": "\u5408\u62103D\u573a\u666f\u5bf9\u4e8e\u7269\u7406AI\u548c\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u8fc7\u7a0b\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u8f93\u51fa\u6548\u7387\u4f4e\u4e0b\uff0c\u8fd9\u6210\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u521b\u5efa\u4e2d\u7684\u4e00\u4e2a\u91cd\u5927\u74f6\u9888\u3002", "method": "\u4f5c\u8005\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u8fc7\u7a0b\u5316\u573a\u666f\u751f\u6210\u6846\u67b6\u2014\u2014Sceniris\uff0c\u8be5\u6846\u67b6\u9488\u5bf9\u5148\u524d\u65b9\u6cd5\u7684\u4e3b\u8981\u6027\u80fd\u9650\u5236\u8fdb\u884c\u4e86\u4f18\u5316\u8bbe\u8ba1\uff0c\u5229\u7528\u4e86\u6279\u5904\u7406\u91c7\u6837\u6280\u672f\u548ccuRobo\u4e2d\u66f4\u5feb\u901f\u7684\u78b0\u649e\u68c0\u67e5\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSceniris\u76f8\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5Scene Synthesizer\u5728\u901f\u5ea6\u4e0a\u81f3\u5c11\u63d0\u9ad8\u4e86234\u500d\uff0c\u5e76\u4e14\u80fd\u591f\u751f\u6210\u65e0\u78b0\u649e\u7684\u5927\u89c4\u6a21\u573a\u666f\u53d8\u5316\u4ee5\u53ca\u6ee1\u8db3\u673a\u5668\u4eba\u4efb\u52a1\u9700\u6c42\u7684\u64cd\u4f5c\u53ef\u884c\u573a\u666f\u3002", "conclusion": "Sceniris\u4e3a\u5feb\u901f\u751f\u6210\u5927\u89c4\u6a21\u3001\u65e0\u78b0\u649e\u4e14\u7b26\u5408\u591a\u6837\u5316\u9700\u6c42\u76843D\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u800c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
