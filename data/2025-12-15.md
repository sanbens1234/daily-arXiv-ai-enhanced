<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 18]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control](https://arxiv.org/abs/2512.11047)
*Haoran Jiang,Jin Chen,Qingwen Bu,Li Chen,Modi Shi,Yanjie Zhang,Delong Li,Chuanzhe Suo,Chuang Wang,Zhihui Peng,Hongyang Li*

Main category: cs.RO

TL;DR: 本文提出了一种统一的潜在学习框架WholeBodyVLA，旨在通过低成本的第一人称视角视频学习来获取更丰富的机器人操作知识，并且设计了一个专门针对准确和稳定核心移动操作动作（如前进、转弯和下蹲）的移动操作导向RL策略。实验表明，该方法在AgiBot X2类人机器人上的表现优于先前基线21.3%，并且在广泛的任务中展示了强大的泛化能力和高扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在具备操作意识的移动方面存在不足，这限制了机器人的工作空间，阻碍其执行大范围的操作任务。这一问题归因于：(1)由于类人机器人遥操作数据稀缺，难以获得移动操作的知识；(2)当前强化学习控制器精度有限及稳定性不足，导致难以准确可靠地执行移动命令。

Method: 提出了一个统一的潜在学习框架，使视觉-语言-动作(Vision-Language-Action, VLA)系统能够从低成本的无动作第一人称视角视频中学习。此外，还开发了一个高效的人类数据收集流程以增强数据集并扩大益处。为更精确地执行所需的移动命令，特别设计了一个面向移动操作(LMO)的强化学习策略，专注于实现诸如前进、转向和下蹲等核心移动操作动作的准确性和稳定性。

Result: 通过在AgiBot X2类人机器人上进行综合实验验证了WholeBodyVLA的有效性，与之前的基线相比性能提高了21.3%。同时，在多种任务上表现出良好的泛化能力以及高度的可扩展性。

Conclusion: WholeBodyVLA作为一种独特的方法，能够促进大范围内的类人机器人移动操作，解决了现有方法中存在的关键挑战。

Abstract: Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To more precisely execute the desired locomotion commands, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.

</details>


### [2] [Taxonomy and Modular Tool System for Versatile and Effective Non-Prehensile Manipulations](https://arxiv.org/abs/2512.11080)
*Cedric-Pascal Sommer,Robert J. Wood,Justin Werfel*

Main category: cs.RO

TL;DR: 本文提出了一种模块化工具系统，该系统基于非驱动末端执行器的关键属性分类而设计，旨在扩展标准双指夹持器在执行非抓取操作时的多功能性和有效性。


<details>
  <summary>Details</summary>
Motivation: 通用机器人末端执行器（如平行颚夹持器）虽然简单且在多种操作任务中有效，但它们并不适合按压、摩擦或刮擦等非抓握动作。这些动作对于许多常见任务是必需的。为了解决这个问题，作者提出了一个基于非驱动末端执行器关键属性分类的模块化工具系统，以增强标准两指夹持器处理非抓握操作的能力。

Method: 首先，对非驱动末端执行器的重要特性进行了分类；然后基于此分类开发了一个模块化的工具系统，该系统可以被标准的两指夹持器使用，从而增加其在执行各种非抓握和抓握操作时的功能性和效率。

Result: 通过将所提出的工具系统应用于航空航天及家庭场景中的多个需要非抓握与抓握操作的任务，证明了该系统的有效性。

Conclusion: 本文介绍了一种新的方法来提高现有简单型机器人手爪在进行复杂非抓握任务时的表现，为未来研究提供了基础。

Abstract: General-purpose robotic end-effectors of limited complexity, like the parallel-jaw gripper, are appealing for their balance of simplicity and effectiveness in a wide range of manipulation tasks. However, while many such manipulators offer versatility in grasp-like interactions, they are not optimized for non-prehensile actions like pressing, rubbing, or scraping -- manipulations needed for many common tasks. To perform such tasks, humans use a range of different body parts or tools with different rigidity, friction, etc., according to the properties most effective for a given task. Here, we discuss a taxonomy for the key properties of a non-actuated end-effector, laying the groundwork for a systematic understanding of the affordances of non-prehensile manipulators. We then present a modular tool system, based on the taxonomy, that can be used by a standard two-fingered gripper to extend its versatility and effectiveness in performing such actions. We demonstrate the application of the tool system in aerospace and household scenarios that require a range of non-prehensile and prehensile manipulations.

</details>


### [3] [Design and Experimental Validation of Closed-Form CBF-Based Safe Control for Stewart Platform Under Multiple Constraints](https://arxiv.org/abs/2512.11125)
*Benedictus C. G. Cinun,Tua A. Tamba,Immanuel R. Santjoko,Xiaofeng Wang,Michael A. Gunarso,Bin Hu*

Main category: cs.RO

TL;DR: 本文提出了一种用于Stewart机器人平台的安全约束控制的闭式解方法，该方法通过显式的闭式控制律同时处理多个位置和速度约束，无需在每个控制步骤中求解二次规划问题（QP），从而实现高效的实时实施。


<details>
  <summary>Details</summary>
Motivation: 为了解决Stewart机器人平台上的安全约束问题，并提高现有基于二次规划(QP)方法的计算效率。

Method: 开发了一种控制屏障函数(CBF)框架的闭式解方案，能够同时处理多约束问题而不必每次控制时都求解一个二次规划问题。

Result: 所提出的控制器在仿真和硬件实验中均得到验证，展示了与基于QP的方法相媲美的安全保障性能，同时将计算时间减少了超过一个数量级。

Conclusion: 该研究提供了一个可靠且计算轻量化的框架，适用于平行机器人系统的实时安全控制。

Abstract: This letter presents a closed-form solution of Control Barrier Function (CBF) framework for enforcing safety constraints on a Stewart robotic platform. The proposed method simultaneously handles multiple position and velocity constraints through an explicit closed-form control law, eliminating the need to solve a Quadratic Program (QP) at every control step and enabling efficient real-time implementation. This letter derives necessary and sufficient conditions under which the closed-form expression remains non-singular, thereby ensuring well-posedness of the CBF solution to multi-constraint problem. The controller is validated in both simulation and hardware experiments on a custom-built Stewart platform prototype, demonstrating safetyguaranteed performance that is comparable to the QP-based formulation, while reducing computation time by more than an order of magnitude. The results confirm that the proposed approach provides a reliable and computationally lightweight framework for real-time safe control of parallel robotic systems. The experimental videos are available on the project website. (https://nail-uh.github.io/StewartPlatformSafeControl.github.io/)

</details>


### [4] [Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance](https://arxiv.org/abs/2512.11173)
*Tzu-Hsien Lee,Fidan Mahmudova,Karthik Desingh*

Main category: cs.RO

TL;DR: 本文提出了一种以物体为中心的模仿学习框架，用于最后一米导航，使四足移动操纵机器人仅使用其机载摄像头的RGB观察就能达到可操作的定位。该方法通过目标图像、多视角RGB观察和指定目标物体的文字提示来调节导航策略。系统能够在不同环境条件下对未见过的物体实例进行泛化，并在边缘对齐和物体对齐两个度量标准下取得了较高的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RGB的导航系统通常只能提供粗略的位置精度，不足以支持后续精确的操作任务。这种差距导致了操控策略经常无法在其训练演示的分布范围内运行，从而造成执行失败。

Method: 提出了一种面向对象的模仿学习框架，该框架接受三个输入：目标图片、来自车载相机的多视图RGB观测以及一个指定目标对象的文本提示。此外，还利用了一个语言驱动的分割模块和一个空间得分矩阵解码器，提供了显式的物体定位和相对姿态推理。

Result: 所提出的策略在边缘对齐上达到了73.47%的成功率，在面对目标时达到了96.94%的成功率。这些结果表明，可以在没有深度信息、LiDAR或地图先验的情况下实现类别级别的精准最后阶段导航。

Conclusion: 这项工作展示了一种可行的方法，即通过简单的RGB观察实现准确的最后一米导航，为统一移动操纵开辟了一条可扩展的道路。

Abstract: Achieving precise positioning of the mobile manipulator's base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation. This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for last-meter navigation, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras. Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation. Project page: https://rpm-lab-umn.github.io/category-level-last-meter-nav/

</details>


### [5] [Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy](https://arxiv.org/abs/2512.11218)
*Kechun Xu,Zhenjie Zhu,Anzhe Chen,Shuqi Zhao,Qing Huang,Yifei Yang,Haojian Lu,Rong Xiong,Masayoshi Tomizuka,Yue Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为BayesVLA的方法，通过贝叶斯因子分解策略来解决视觉-语言-动作（VLA）模型中由于模态不平衡导致的语言遗忘问题，从而提高对未见指令、物体和环境的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在视觉-语言-动作(VLA)模型中追求分布外泛化时，经常会遇到视觉-语言模型(VLM)骨干在微调过程中发生灾难性遗忘的问题。尽管与外部推理数据联合训练有所帮助，但这需要有经验的调整和与数据相关的额外开销。此外，VLA数据集内部存在模态不平衡现象，即语言多样性远低于视觉和动作多样性，这使得模型倾向于依赖视觉捷径而忽视语言信息。

Method: 提出了BayesVLA方法，采用贝叶斯因式分解技术将策略分解为支持“看后行动”的视觉-动作先验部分以及允许“提示指定”的语言条件似然部分。该方法内在地保留了泛化能力并促进了指令跟随。另外，还引入了前后接触阶段以更好地利用预训练的基础模型，并通过信息论分析验证了其在减轻捷径学习方面的有效性。

Result: 广泛的实验表明，相比于现有方法，BayesVLA在面对未知指令、对象及环境时展现出更优的泛化性能。

Conclusion: 通过针对VLA模型中存在的模态不平衡问题开发出的新方法BayesVLA能够有效减少语言遗忘情况的发生，同时提升模型对于新情境下的适应能力。

Abstract: The pursuit of out-of-distribution generalization in Vision-Language-Action (VLA) models is often hindered by catastrophic forgetting of the Vision-Language Model (VLM) backbone during fine-tuning. While co-training with external reasoning data helps, it requires experienced tuning and data-related overhead. Beyond such external dependencies, we identify an intrinsic cause within VLA datasets: modality imbalance, where language diversity is much lower than visual and action diversity. This imbalance biases the model toward visual shortcuts and language forgetting. To address this, we introduce BayesVLA, a Bayesian factorization that decomposes the policy into a visual-action prior, supporting seeing-to-act, and a language-conditioned likelihood, enabling prompt-to-specify. This inherently preserves generalization and promotes instruction following. We further incorporate pre- and post-contact phases to better leverage pre-trained foundation models. Information-theoretic analysis formally validates our effectiveness in mitigating shortcut learning. Extensive experiments show superior generalization to unseen instructions, objects, and environments compared to existing methods. Project page is available at: https://xukechun.github.io/papers/BayesVLA.

</details>


### [6] [Optimal Control and Structurally-Informed Gradient Optimization of a Custom 4-DOF Rigid-Body Manipulator](https://arxiv.org/abs/2512.11250)
*Brock Marcinczyk,Logan E. Beaver*

Main category: cs.RO

TL;DR: 本文开发了一种针对定制4自由度刚体操作器的控制中心框架，通过将降阶庞特里亚金最大原则控制器与基于物理信息的梯度下降阶段结合。该方法提供了一个关节加速度的闭式最优控制律，并通过最小化直接从完整刚体动力学构建的成本函数来确定相应的时间范围。


<details>
  <summary>Details</summary>
Motivation: 为了提高刚体操作器的控制性能和计算效率，同时保持其物理约束和加载行为，研究提出了一种新的控制框架。

Method: 采用降阶庞特里亚金最大原则（PMP）控制器与基于物理信息的梯度下降相结合的方法。其中，PMP模型为关节加速度提供了闭式的最优控制律；而梯度下降模块则通过最小化一个根据完整刚体动力学直接建立的成本函数来决定对应的时间范围。

Result: 所提出的管道既保留了严格的控制理论结构，又以计算效率高的方式嵌入了操纵器的物理约束和加载行为。

Conclusion: 该工作展示了一种有效结合控制理论与物理约束的新方法，为刚体操作器的设计和控制提供了新颖且高效的解决方案。

Abstract: This work develops a control-centric framework for a custom 4-DOF rigid-body manipulator by coupling a reduced-order Pontryagin's Maximum Principle (PMP) controller with a physics-informed Gradient Descent stage. The reduced PMP model provides a closed-form optimal control law for the joint accelerations, while the Gradient Descent module determines the corresponding time horizons by minimizing a cost functional built directly from the full Rigid-Body Dynamics. Structural-mechanics reaction analysis is used only to initialize feasible joint velocities-most critically the azimuthal component-ensuring that the optimizer begins in a physically admissible region. The resulting kinematic trajectories and dynamically consistent time horizons are then supplied to the symbolic Euler-Lagrange model to yield closed-form inverse-dynamics inputs. This pipeline preserves a strict control-theoretic structure while embedding the physical constraints and loading behavior of the manipulator in a computationally efficient way.

</details>


### [7] [Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing](https://arxiv.org/abs/2512.11275)
*Suchang Chen,Daqiang Guo*

Main category: cs.RO

TL;DR: 该论文提出了一种以物体为中心的操作逻辑模式，作为视觉-语言模型（VLMs）在机器人操作中的知识信号，并通过特定任务验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言模型（VLMs）管道虽然重视从图像和语言中广泛语义泛化的能力，但对于制造单元内接触密集型动作所需的执行关键参数通常有所忽略。为此，作者提出了一个面向对象的操作逻辑模式，旨在暴露对象、接口、轨迹、容差以及力/阻抗信息作为一个首要的知识信号。

Method: 作者定义了一个八字段元组τ来形式化上述提到的对象中心操作逻辑模式，并建立了一个小型知识库(KB)。接着，在3D打印机线轴移除任务上实例化了τ与KB，分析了基于τ条件下的VLM规划，并使用最近的VLM/LLM规划基准调整后的计划质量指标进行评估。此外，还展示了该模式如何支持训练时的分类标签数据增强及测试时逻辑感知的检索增强提示。

Result: 研究结果表明，所提出的模式不仅能够有效提升VLM规划的质量，而且还可以作为智能制造企业助手系统构建模块的一部分，支持训练时的数据增强和测试时的逻辑意识检索增强提示。

Conclusion: 本研究提供了一种新的方法，将关键执行参数纳入到视觉-语言模型中，从而增强了这些模型对于实际制造业环境中复杂操作任务的支持能力。

Abstract: Existing pipelines for vision-language models (VLMs) in robotic manipulation prioritize broad semantic generalization from images and language, but typically omit execution-critical parameters required for contact-rich actions in manufacturing cells. We formalize an object-centric manipulation-logic schema, serialized as an eight-field tuple τ, which exposes object, interface, trajectory, tolerance, and force/impedance information as a first-class knowledge signal between human operators, VLM-based assistants, and robot controllers. We instantiate τ and a small knowledge base (KB) on a 3D-printer spool-removal task in a collaborative cell, and analyze τ-conditioned VLM planning using plan-quality metrics adapted from recent VLM/LLM planning benchmarks, while demonstrating how the same schema supports taxonomy-tagged data augmentation at training time and logic-aware retrieval-augmented prompting at test time as a building block for assistant systems in smart manufacturing enterprises.

</details>


### [8] [Incremental Validation of Automated Driving Functions using Generic Volumes in Micro- Operational Design Domains](https://arxiv.org/abs/2512.11351)
*Steffen Schäfer,Martin Cichon*

Main category: cs.RO

TL;DR: 本文提出了一种将操作设计域(ODD)细分为微ODD(mODD)的方法，并通过抽象物体表示来生成测试用例，以系统地探索障碍物检测的边缘情况并评估感知质量。


<details>
  <summary>Details</summary>
Motivation: 高度自动化的驾驶系统需要在各种现实条件下正确运行。基于场景的测试是解决这一挑战的一种重要方法，但如何从定性分类过渡到具体的测试案例仍缺乏结构化的方法。

Method: 本文介绍了一种将ODD划分为更小部分（称为mODD）的方法，并从中导出具有抽象对象表示的测试用例。使用一个一维、横向引导的操作作为示例，其中障碍物被表示为不同大小的通用立方体，以此简化方式来评估感知性能。

Result: 一系列测试在一个闭环、协同模拟的虚拟环境中进行，该环境具备照片级真实感渲染以及模拟LiDAR、GNSS和摄像头传感器。结果表明可以通过系统地探索障碍物检测中的边界情况，并基于观察到的车辆行为（以碰撞与安全停止为结果指标）来评估感知质量。

Conclusion: 这些发现支持了标准化安全论证框架的发展，并为自动驾驶功能的验证和授权提供了实际步骤。

Abstract: The validation of highly automated, perception-based driving systems must ensure that they function correctly under the full range of real-world conditions. Scenario-based testing is a prominent approach to addressing this challenge, as it involves the systematic simulation of objects and environments. Operational Design Domains (ODDs) are usually described using a taxonomy of qualitative designations for individual objects. However, the process of transitioning from taxonomy to concrete test cases remains unstructured, and completeness is theoretical. This paper introduces a structured method of subdividing the ODD into manageable sections, termed micro-ODDs (mODDs), and deriving test cases with abstract object representations. This concept is demonstrated using a one-dimensional, laterally guided manoeuvre involving a shunting locomotive within a constrained ODD. In this example, mODDs are defined and refined into narrow taxonomies that enable test case generation. Obstacles are represented as generic cubes of varying sizes, providing a simplified yet robust means of evaluating perception performance. A series of tests were conducted in a closed-loop, co-simulated virtual environment featuring photorealistic rendering and simulated LiDAR, GNSS and camera sensors. The results demonstrate how edge cases in obstacle detection can be systematically explored and how perception quality can be evaluated based on observed vehicle behaviour, using crash versus safe stop as the outcome metrics. These findings support the development of a standardised framework for safety argumentation and offer a practical step towards the validation and authorisation of automated driving functions.

</details>


### [9] [An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges](https://arxiv.org/abs/2512.11362)
*Chao Xu,Suyu Zhang,Yang Liu,Baigui Sun,Weihong Chen,Bo Xu,Qi Liu,Juncheng Wang,Shujun Wang,Shan Luo,Jan Peters,Athanasios V. Vasilakos,Stefanos Zafeiriou,Jiankang Deng*

Main category: cs.RO

TL;DR: 该论文提供了一个清晰且结构化的指南，以帮助研究者理解视觉-语言-动作（VLA）模型领域。它从基础模块开始介绍，追溯了关键里程碑，并深入探讨了当前研究前沿的五大挑战：表示、执行、泛化、安全以及数据集和评估。本文旨在为新手提供基础知识指导，同时为有经验的研究人员提供战略路线图，以加速学习过程并激发新的想法。


<details>
  <summary>Details</summary>
Motivation: 随着机器人技术中视觉-语言-动作（VLA）模型的发展，让机器能够理解指令并与物理世界互动变得越来越重要。这个领域的快速发展带来了许多新的模型和数据集，使得跟踪最新进展既令人兴奋又充满挑战。因此，需要一个全面的综述来帮助研究人员更好地理解和探索VLA领域。

Method: 本篇综述首先介绍了VLA模型的基本组成部分，接着回顾了领域内的关键历史节点，最后详细分析了当前研究面临的主要挑战，包括但不限于表示方法、执行能力、泛化性、安全性及数据集构建与评价体系等方面的问题。

Result: 通过细致地解析VLA模型所面临的五大核心难题，并对现有解决方案进行了概述，指出了未来可能的研究方向和发展机会。

Conclusion: 本文不仅为新进入该领域的研究者提供了宝贵的入门资料，也为资深学者绘制了一幅明确的战略蓝图，旨在促进知识积累的同时激发更多关于具身智能的新见解。

Abstract: Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \href{https://suyuz1.github.io/Survery/}{project page}.

</details>


### [10] [CarlaNCAP: A Framework for Quantifying the Safety of Vulnerable Road Users in Infrastructure-Assisted Collective Perception Using EuroNCAP Scenarios](https://arxiv.org/abs/2512.11551)
*Jörg Gamerdinger,Sven Teufel,Simon Roller,Oliver Bringmann*

Main category: cs.RO

TL;DR: 本文提出了一种基于基础设施的集体感知框架，用于评估对弱势道路使用者（VRUs）的安全性改进，并通过CarlaNCAP数据集展示了该技术在减少事故率方面的显著效果。


<details>
  <summary>Details</summary>
Motivation: 随着道路上用户的增加，尤其是城市环境中由于停放车辆或建筑物遮挡导致的弱势道路使用者（VRUs）风险提高，需要寻找有效的方法来降低这些风险。为了促进决策者采用新技术，需要有全面的研究和数据集来证明对于VRUs安全性提升的效果。

Method: 提出了一个特别针对VRUs的安全性改进评价框架，包括创建了一个包含安全关键EuroNCAP场景的数据集(CarlaNCAP)，并通过深入的模拟研究来展示基于基础设施的集体感知如何改善交通安全。

Result: 研究表明，在安全关键情况下，基于基础设施的集体感知可以显著减少事故发生率，与仅配备自身传感器的车辆相比，最高可达100%避免事故（后者仅为33%）。

Conclusion: 本研究通过提供一个详细的分析框架及相应数据集，为基于基础设施的集体感知技术应用于提高VRUs的道路安全提供了强有力的支持。

Abstract: The growing number of road users has significantly increased the risk of accidents in recent years. Vulnerable Road Users (VRUs) are particularly at risk, especially in urban environments where they are often occluded by parked vehicles or buildings. Autonomous Driving (AD) and Collective Perception (CP) are promising solutions to mitigate these risks. In particular, infrastructure-assisted CP, where sensor units are mounted on infrastructure elements such as traffic lights or lamp posts, can help overcome perceptual limitations by providing enhanced points of view, which significantly reduces occlusions. To encourage decision makers to adopt this technology, comprehensive studies and datasets demonstrating safety improvements for VRUs are essential. In this paper, we propose a framework for evaluating the safety improvement by infrastructure-based CP specifically targeted at VRUs including a dataset with safety-critical EuroNCAP scenarios (CarlaNCAP) with 11k frames. Using this dataset, we conduct an in-depth simulation study and demonstrate that infrastructure-assisted CP can significantly reduce accident rates in safety-critical scenarios, achieving up to 100% accident avoidance compared to a vehicle equipped with sensors with only 33%. Code is available at https://github.com/ekut-es/carla_ncap

</details>


### [11] [Cross-Entropy Optimization of Physically Grounded Task and Motion Plans](https://arxiv.org/abs/2512.11571)
*Andreu Matoses Gimenez,Nils Wilde,Chris Pek,Javier Alonso-Mora*

Main category: cs.RO

TL;DR: 本文提出了一种利用GPU并行化物理模拟器来计算包含运动控制器的计划实现的方法，通过考虑动力学和与环境接触的影响，以获得更可靠的任务执行。使用交叉熵优化采样控制器参数以找到低成本解决方案，并且由于该方法使用与真实系统相同的控制器，因此机器人可以直接执行计算出的计划。


<details>
  <summary>Details</summary>
Motivation: 现有的任务和动作规划(TAMP)算法主要集中在计算性能、完备性或最优性上，但往往忽略了低级控制器对实际系统执行计划时的影响，导致生成的计划可能无法充分考虑到完成任务所需的动力学特性或复杂接触。

Method: 采用GPU并行化的物理模拟器来计算含有运动控制器的动作计划实现，并明确考虑了动力学以及与环境的接触；运用交叉熵优化技术来抽样控制器或动作的参数，以求得成本较低的解。

Result: 该方法能够为一系列任务提供有效的解决方案，在这些任务中，机器人可以利用环境几何结构移动物体。

Conclusion: 所提出的方法不仅考虑到了任务执行中的动力学因素和环境接触，还能直接在真实系统中实施计算出来的计划，提高了机器人自主执行任务时的成功率。

Abstract: Autonomously performing tasks often requires robots to plan high-level discrete actions and continuous low-level motions to realize them. Previous TAMP algorithms have focused mainly on computational performance, completeness, or optimality by making the problem tractable through simplifications and abstractions. However, this comes at the cost of the resulting plans potentially failing to account for the dynamics or complex contacts necessary to reliably perform the task when object manipulation is required. Additionally, approaches that ignore effects of the low-level controllers may not obtain optimal or feasible plan realizations for the real system. We investigate the use of a GPU-parallelized physics simulator to compute realizations of plans with motion controllers, explicitly accounting for dynamics, and considering contacts with the environment. Using cross-entropy optimization, we sample the parameters of the controllers, or actions, to obtain low-cost solutions. Since our approach uses the same controllers as the real system, the robot can directly execute the computed plans. We demonstrate our approach for a set of tasks where the robot is able to exploit the environment's geometry to move an object. Website and code: https://andreumatoses.github.io/research/parallel-realization

</details>


### [12] [UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations](https://arxiv.org/abs/2512.11609)
*Tingyu Yuan,Biaoliang Guan,Wen Ye,Ziyan Tian,Yi Yang,Weijie Zhou,Yan Huang,Peng Wang,Chaoyang Zhao,Jinqiao Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为UniBYD的框架，该框架使用动态强化学习算法来发现与机器人物理特性相匹配的操作策略。通过引入统一形态表示(UMR)和基于混合马尔可夫的阴影引擎，使强化学习能够从模仿人类演示过渡到探索更适合不同机器人形态的策略。实验表明，该方法比现有最先进技术的成功率提高了67.90%。


<details>
  <summary>Details</summary>
Motivation: 在具身智能领域，机器人手与人手之间的体现差距给从人类示范中学习带来了重大挑战。尽管一些研究试图利用强化学习来弥合这一差距，但它们仍然局限于简单地复制人类操作，导致任务性能受限。

Method: 提出了一种称为UniBYD的统一框架，它采用动态强化学习算法来寻找与机器人物理特征相适应的操作策略；开发了统一形态表示(UMR)，以促进不同机器人手形之间的一致建模；设计了具有退火奖励计划的动态PPO，以及一种混合马尔可夫基阴影引擎，用以精细化模仿人类操作。

Result: 实验结果显示，相比当前最先进的技术，所提出的方法在成功率上提高了67.90%。

Conclusion: UniBYD提供了一个有效的解决方案，通过结合动态强化学习和对不同机器人手形适应性的考虑，成功地跨越了机器人与人类手部之间的体现鸿沟，显著提升了从人类示例中学习执行任务的能力。

Abstract: In embodied intelligence, the embodiment gap between robotic and human hands brings significant challenges for learning from human demonstrations. Although some studies have attempted to bridge this gap using reinforcement learning, they remain confined to merely reproducing human manipulation, resulting in limited task performance. In this paper, we propose UniBYD, a unified framework that uses a dynamic reinforcement learning algorithm to discover manipulation policies aligned with the robot's physical characteristics. To enable consistent modeling across diverse robotic hand morphologies, UniBYD incorporates a unified morphological representation (UMR). Building on UMR, we design a dynamic PPO with an annealed reward schedule, enabling reinforcement learning to transition from imitation of human demonstrations to explore policies adapted to diverse robotic morphologies better, thereby going beyond mere imitation of human hands. To address the frequent failures of learning human priors in the early training stage, we design a hybrid Markov-based shadow engine that enables reinforcement learning to imitate human manipulations in a fine-grained manner. To evaluate UniBYD comprehensively, we propose UniManip, the first benchmark encompassing robotic manipulation tasks spanning multiple hand morphologies. Experiments demonstrate a 67.90% improvement in success rate over the current state-of-the-art. Upon acceptance of the paper, we will release our code and benchmark at https://github.com/zhanheng-creator/UniBYD.

</details>


### [13] [Architecting Large Action Models for Human-in-the-Loop Intelligent Robots](https://arxiv.org/abs/2512.11620)
*Kanisorn Sangchai,Methasit Boonpun,Withawin Kraipetchara,Paulo Garcia*

Main category: cs.RO

TL;DR: 本研究展示了通过组合现成的基础模型来构建高效的大规模动作模型的可能性，并通过在输出上结合符号包装器和相关验证，实现了可验证的神经-符号解决方案。实验表明，通过生成PDDL代码驱动行动执行可以有效减少行动幻觉，支持了智能机器人领域中安全性的提升。


<details>
  <summary>Details</summary>
Motivation: 为了实现能够自主运行并与其他人造或人类智能体互动的智能机器人，需要整合环境感知、推理和行动。传统的基于符号的人工智能技术遇到了计算和内存成本上的扩展性瓶颈，而过去十年间大规模语言模型虽然展现了前所未有的能力，但牺牲了控制性、可解释性和可解读性。

Method: 该研究通过组合现成的基础模型来构建大规模动作模型，并且通过将符号包装器及其相关的验证融入到这些模型的输出中，以提高其可控性、可解释性和可解读性。此外，还利用Planning Domain Definition Language (PDDL)代码的生成来促进人类参与的验证阶段，从而有效缓解动作幻觉问题。

Result: 研究表明，无需进行大量的端到端训练也可以实现大规模动作模型的智能化，而是可以通过集成高效的感知模型与逻辑驱动的核心来达成目的。特别是，通过生成PDDL代码来进行动作执行的方式，能够有效地减少不切实际的动作发生情况。

Conclusion: 本研究表明，通过整合基础模型以及采用符号化的处理方式，可以在不牺牲安全性的情况下开发出功能强大的大规模动作模型。这种方法不仅为新型产业中的机器人设计提供了支持，也为确保该领域的安全性指明了方向。

Abstract: The realization of intelligent robots, operating autonomously and interacting with other intelligent agents, human or artificial, requires the integration of environment perception, reasoning, and action. Classic Artificial Intelligence techniques for this purpose, focusing on symbolic approaches, have long-ago hit the scalability wall on compute and memory costs. Advances in Large Language Models in the past decade (neural approaches) have resulted in unprecedented displays of capability, at the cost of control, explainability, and interpretability. Large Action Models aim at extending Large Language Models to encompass the full perception, reasoning, and action cycle; however, they typically require substantially more comprehensive training and suffer from the same deficiencies in reliability. Here, we show it is possible to build competent Large Action Models by composing off-the-shelf foundation models, and that their control, interpretability, and explainability can be effected by incorporating symbolic wrappers and associated verification on their outputs, achieving verifiable neuro-symbolic solutions for intelligent robots. Our experiments on a multi-modal robot demonstrate that Large Action Model intelligence does not require massive end-to-end training, but can be achieved by integrating efficient perception models with a logic-driven core. We find that driving action execution through the generation of Planning Domain Definition Language (PDDL) code enables a human-in-the-loop verification stage that effectively mitigates action hallucinations. These results can support practitioners in the design and development of robotic Large Action Models across novel industries, and shed light on the ongoing challenges that must be addressed to ensure safety in the field.

</details>


### [14] [Bench-Push: Benchmarking Pushing-based Navigation and Manipulation Tasks for Mobile Robots](https://arxiv.org/abs/2512.11736)
*Ninghan Zhong,Steven Caro,Megnath Ramesh,Rishi Bhatnagar,Avraiem Iskandar,Stephen L. Smith*

Main category: cs.RO

TL;DR: 本文提出了Bench-Push，这是首个针对基于推动的移动机器人导航和操作任务的统一基准。它包括一系列模拟环境、新颖的评估指标以及使用该基准对现有基线实现进行评估的示例。


<details>
  <summary>Details</summary>
Motivation: 随着移动机器人越来越多地被部署在有可移动物体的杂乱环境中，传统的方法面临着挑战。为了应对这一情况，并促进研究的可重复性和跨比较，作者们开发了Bench-Push，一个用于评估基于推动策略的移动机器人性能的统一平台。

Method: Bench-Push由多个部分组成：1）一系列综合性的模拟环境，捕捉到基于推动的任务中的基本挑战；2）新设计的评价指标，以衡量效率、交互努力度及部分任务完成情况；3）通过Bench-Push演示如何评估不同环境下已建立的基线实现的例子。

Result: Bench-Push作为一个开放源代码的Python库提供，具有模块化设计，支持研究人员轻松扩展并测试新的算法或方法。此外，还提供了文档和训练好的模型来辅助使用。

Conclusion: Bench-Push为基于推动策略的移动机器人研究领域引入了一个标准化的评估框架，有助于提高实验结果之间的可比性，并促进了该领域的进一步发展。

Abstract: Mobile robots are increasingly deployed in cluttered environments with movable objects, posing challenges for traditional methods that prohibit interaction. In such settings, the mobile robot must go beyond traditional obstacle avoidance, leveraging pushing or nudging strategies to accomplish its goals. While research in pushing-based robotics is growing, evaluations rely on ad hoc setups, limiting reproducibility and cross-comparison. To address this, we present Bench-Push, the first unified benchmark for pushing-based mobile robot navigation and manipulation tasks. Bench-Push includes multiple components: 1) a comprehensive range of simulated environments that capture the fundamental challenges in pushing-based tasks, including navigating a maze with movable obstacles, autonomous ship navigation in ice-covered waters, box delivery, and area clearing, each with varying levels of complexity; 2) novel evaluation metrics to capture efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-Push to evaluate example implementations of established baselines across environments. Bench-Push is open-sourced as a Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.

</details>


### [15] [The Influence of Human-like Appearance on Expected Robot Explanations](https://arxiv.org/abs/2512.11746)
*Hana Kopecka,Jose Such*

Main category: cs.RO

TL;DR: 研究了机器人的人类外观如何影响用户对其心理模型的认知以及期望从机器人那里获得的解释类型。结果表明，尽管所有条件下的大多数解释都是拟人化的，但更像人类的机器人外观与更拟人化的解释之间存在正相关关系。


<details>
  <summary>Details</summary>
Motivation: 探索机器人的人类外观是否以及在多大程度上引发拟人化认知，以及这种认知水平如何体现在人们期望从机器人那里得到的解释中。

Method: 采用被试间设计方法，展示三种具有不同程度人类外观的家庭服务机器人的视觉刺激，并要求受访者提供他们期望针对相同机器人行为收到的解释。

Result: 发现无论在哪种条件下大部分解释都是拟人化的；然而，拟人化解释与机器人的人类外观之间存在着正相关性。此外还报告了非拟人化解释及对机器人描述中的细微趋势。

Conclusion: 机器人越具人类外观，人们对它的解释就越倾向于拟人化，这表明机器人的外观确实会影响用户的心理模型和他们对机器人行为的期待解释。

Abstract: A robot's appearance is a known factor influencing user's mental model and human-robot interaction, that has not been studied in the context of its influence in expected robot explanations. In this study, we investigate whether and to what extent the human-like appearance of robots elicits anthropomorphism, which is conceptualised as an attribution of mental capacities, and how the level of anthropomorphism is revealed in explanations that people expect to receive. We designed a between-subject study comprising conditions with visual stimuli of three domestic service robots with varying human-like appearance, and we prompted respondents to provide explanations they would expect to receive from the robot for the same robot actions. We found that most explanations were anthropomorphic across all conditions. However, there is a positive correlation between the anthropomorphic explanations and human-like appearance. We also report on more nuanced trends observed in non-anthropomorphic explanations and trends in robot descriptions.

</details>


### [16] [ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics](https://arxiv.org/abs/2512.11773)
*Britton Jordan,Jordan Thompson,Jesse F. d'Almeida,Hao Li,Nithesh Kumar,Susheela Sharma Stern,Ipek Oguz,Robert J. Webster,Daniel Brown,Alan Kuntz,James Ferguson*

Main category: cs.RO

TL;DR: 本文提出了一种成本感知的主动感知框架ProbeMDE，通过结合RGB图像和稀疏的本体感觉测量来改善单目深度估计（MDE），特别是在充满挑战的环境如手术场景中。该方法利用一系列MDE模型基于RGB图像及少量已知深度测量点生成密集深度图，并通过Stein变分梯度下降法选取信息量最大的触碰位置以避免模式崩溃。实验结果表明，相比基线方法，该方法在标准深度估计指标上表现更优，同时减少了所需的本体感觉测量次数。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计（MDE）在机器人感知中有重要作用，但在诸如手术场景这样具有挑战性的环境中，由于缺乏纹理表面、镜面反射和遮挡等原因，其预测往往不确定且不准确。为了解决这个问题，提出了一个结合RGB图像与稀疏本体感觉测量的成本感知主动传感框架。

Method: 提出的ProbeMDE框架使用一组MDE模型来根据RGB图像以及通过本体感觉获得的一组稀疏已知深度测量值预测密集深度图。通过集合方差量化预测不确定性，并计算相对于候选测量位置的不确定性梯度。为了在选择最具信息量的位置进行触碰时防止模式崩溃，采用了Stein变分梯度下降法处理梯度图。

Result: 通过在中心气道阻塞手术模拟物上的仿真和物理实验验证了该方法的有效性。结果表明，所提方法在标准深度估计指标方面优于基线方法，实现了更高的准确性，同时最小化了所需本体感觉测量的数量。

Conclusion: 研究介绍了一种新的成本感知主动传感方法ProbeMDE用于改进复杂环境下的单目深度估计性能。实验证明，此方法能够有效提高深度估计精度并减少额外传感器数据的需求。

Abstract: Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements.

</details>


### [17] [Agile Flight Emerges from Multi-Agent Competitive Racing](https://arxiv.org/abs/2512.11781)
*Vineet Pasumarti,Lorenzo Bianchi,Antonio Loquercio*

Main category: cs.RO

TL;DR: 通过多智能体竞争和稀疏的高级目标（如赢得比赛），研究发现训练出的智能体能够展现出敏捷飞行能力和策略性行为。相比孤立训练，这种方法在环境复杂度增加时表现更优，并且能更可靠地将策略从模拟迁移到现实世界中，同时对未见对手也表现出一定程度的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索通过强化学习训练的智能体是否能在只有稀疏任务级别奖励的情况下，发展出高级控制技能以及策略性行为。这与传统上单独训练智能体并使用具体行为指示奖励的方法形成对比。

Method: 采用多智能体竞争框架，在仅有获胜这一稀疏奖励条件下训练智能体。实验包括了模拟环境中的测试及真实世界中的验证，特别关注了当环境中存在障碍物等复杂因素时的表现。

Result: 结果表明，基于多智能体竞争的方法不仅优于传统的单智能体进步奖励方法，尤其是在环境变得更复杂时；而且所学得的策略从模拟到现实世界的迁移更加可靠。此外，这些策略对于训练过程中未曾遇到过的对手也展现了一定程度的适应性。

Conclusion: 研究表明，即使是在物理世界中，仅依靠稀疏的任务级奖励也足以训练出具备先进低级控制能力的智能体。这种方法不仅提高了模拟到实际场景的转换效率，还增强了智能体应对未知情况的能力。

Abstract: Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.
  Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent

</details>


### [18] [AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis](https://arxiv.org/abs/2512.11797)
*Junjie Ye,Rong Xue,Basile Van Hoorick,Pavel Tokmakov,Muhammad Zubair Irshad,Yue Wang,Vitor Guizilini*

Main category: cs.RO

TL;DR: 介绍了一种名为AnchorDream的模型，该模型利用预训练的视频扩散模型来合成机器人数据，通过基于机器人运动渲染条件下的扩散过程生成大规模、多样化和高质量的数据集，从而改善了模仿学习的效果。


<details>
  <summary>Details</summary>
Motivation: 解决大规模及多样化的机器人演示收集问题，因为现实世界的数据获取成本高，而模拟器提供的多样性有限且与实际存在明显差距。现有方法要么只改变视觉外观而不创造新行为，要么由于实体一致性问题导致动作不自然。

Method: 提出了一种称为AnchorDream的具身意识世界模型，它重用了预训练的视频扩散模型来合成机器人数据。该模型通过对机器人运动渲染条件下的扩散过程进行调节，确保在合成物体和环境时保持与机器人运动学一致的同时防止产生幻觉。

Result: 实验表明，所生成的数据在下游策略学习中带来了一致性的改进，在模拟器基准测试中的相对增益为36.4%，而在真实世界研究中的性能几乎翻倍。

Conclusion: 研究表明，将生成式世界模型基于机器人运动进行定位提供了一条通向扩展模仿学习实用路径的方法。

Abstract: The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.

</details>
