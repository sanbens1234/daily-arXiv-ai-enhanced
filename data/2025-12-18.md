<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 14]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Breathe with Me: Synchronizing Biosignals for User Embodiment in Robots](https://arxiv.org/abs/2512.14952)
*Iddo Yehoshua Wald,Amber Maimon,Shiyao Zhang,Dennis Küster,Robert Porzel,Tanja Schultz,Rainer Malaka*

Main category: cs.RO

TL;DR: 本研究通过将用户的呼吸实时体现在机器人系统中，探索了增强人机交互中用户具身感的新方法。实验表明，当机器人手臂的移动与用户呼吸同步时，用户的身体所有权感显著增加。


<details>
  <summary>Details</summary>
Motivation: 为了增强用户在机器人系统中的具身感，研究者们探索了一种新的方式：通过实时体现用户的呼吸（embreathment），以期提高用户在远程存在、远程操作等场景下的体验。

Method: 采用单因素被试内设计，参与者控制一个机器人手臂，并且手臂的动作要么与参与者的呼吸同步，要么不同步。

Result: 结果显示，动作与呼吸同步条件下，参与者报告了更高的身体所有权感受，并且大多数参与者更偏好这种同步情况。

Conclusion: 生理信号的表征为人类-机器人互动提供了一个新颖的内感受途径，对远程存在、假肢、与机器人协作以及共享自主性等领域具有潜在影响。

Abstract: Embodiment of users within robotic systems has been explored in human-robot interaction, most often in telepresence and teleoperation. In these applications, synchronized visuomotor feedback can evoke a sense of body ownership and agency, contributing to the experience of embodiment. We extend this work by employing embreathment, the representation of the user's own breath in real time, as a means for enhancing user embodiment experience in robots. In a within-subjects experiment, participants controlled a robotic arm, while its movements were either synchronized or non-synchronized with their own breath. Synchrony was shown to significantly increase body ownership, and was preferred by most participants. We propose the representation of physiological signals as a novel interoceptive pathway for human-robot interaction, and discuss implications for telepresence, prosthetics, collaboration with robots, and shared autonomy.

</details>


### [2] [ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision](https://arxiv.org/abs/2512.15020)
*Wenlong Xia,Jinhao Zhang,Ce Zhang,Yaojia Wang,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: 该论文提出了一种基于3D视觉运动的扩散策略——隐式场景监督策略（ISS Policy），它通过点云观察预测连续动作序列，利用一种新的隐式场景监督模块来提高模型性能和鲁棒性。实验表明，ISS Policy在单臂操作任务和灵巧手操作上都达到了最先进的性能，并且在现实世界的测试中也表现出了强大的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉模仿学习的方法依赖于物体外观而忽略了底层3D场景结构，导致训练效率低下和泛化能力差。为了解决这些问题，本文提出了隐式场景监督策略。

Method: 提出的隐式场景监督策略是一种基于DiT的3D视觉运动扩散策略，可以从点云观测中预测连续的动作序列。作者扩展了DiT模型，加入了一个新颖的隐式场景监督模块，促使模型产生与场景几何演变一致的结果，从而提升策略的表现力和鲁棒性。

Result: ISS Policy在MetaWorld单臂操作任务和Adroit灵巧手操作任务上均取得了当前最佳的成绩。实际实验还展示了其出色的泛化能力和鲁棒性。此外，消融研究显示该方法能够有效地随着数据量和参数增加而扩展。

Conclusion: 通过引入隐式场景监督模块到DiT架构中，本研究开发出了一种改进的机器人控制策略，能够在多种操作任务中表现出色，并具有良好的泛化性和鲁棒性。

Abstract: Vision-based imitation learning has enabled impressive robotic manipulation skills, but its reliance on object appearance while ignoring the underlying 3D scene structure leads to low training efficiency and poor generalization. To address these challenges, we introduce \emph{Implicit Scene Supervision (ISS) Policy}, a 3D visuomotor DiT-based diffusion policy that predicts sequences of continuous actions from point cloud observations. We extend DiT with a novel implicit scene supervision module that encourages the model to produce outputs consistent with the scene's geometric evolution, thereby improving the performance and robustness of the policy. Notably, ISS Policy achieves state-of-the-art performance on both single-arm manipulation tasks (MetaWorld) and dexterous hand manipulation (Adroit). In real-world experiments, it also demonstrates strong generalization and robustness. Additional ablation studies show that our method scales effectively with both data and parameters. Code and videos will be released.

</details>


### [3] [HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles](https://arxiv.org/abs/2512.15047)
*Yunheng Wang,Yixiao Feng,Yuetong Fang,Shuning Zhang,Tan Jing,Jian Li,Xiangrui Jiang,Renjing Xu*

Main category: cs.RO

TL;DR: 提出了一种新的框架HERO，用于构建层次可穿越的3D场景图（3DSGs），通过将可操作障碍物建模为路径来重新定义可穿越性，从而提高了在部分和完全受阻环境中的效率和可达性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景图依赖于静态世界的假设，仅基于静态空间布局定义可穿越空间，并将交互式障碍视为不可穿越，这限制了其在现实世界中的有效性、可达性和扩展性。

Method: 提出了HERO框架，该框架通过将可操作障碍物作为路径建模，同时捕捉它们的物理互动性、功能语义以及场景的关系层次结构，以构建层次化的可穿越3D场景图。

Result: 与基线相比，在部分受阻环境中，HERO使路径长度(PL)减少了35.1%，而在完全受阻环境中，成功率(SR)增加了79.4%。

Conclusion: HERO框架通过引入对可操作障碍物的新理解，显著改善了在复杂大型环境中的长期推理和规划能力，提升了效率和可达性。

Abstract: 3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.

</details>


### [4] [BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization](https://arxiv.org/abs/2512.15111)
*Dongmyeong Lee,Jesse Quattrociocchi,Christian Ellis,Rwik Rana,Amanda Adkins,Adam Uccello,Garrett Warnell,Joydeep Biswas*

Main category: cs.RO

TL;DR: 提出了一种无GPS的顺序地理定位系统BEV-Patch-PF，该系统结合了粒子滤波器与学习到的鸟瞰图（BEV）和航拍特征图。此方法在两个现实世界的越野数据集上实现了比基于检索的基线更低的绝对轨迹误差，并且能够在NVIDIA Tesla T4上以10Hz实时运行。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种无需依赖GPS的精确地理定位系统，适用于机器人在复杂环境如密集树冠和阴影下的部署。

Method: 通过整合粒子滤波器与从RGB和深度图像构建的学习到的鸟瞰图及航拍特征图来实现定位。对于每个3自由度粒子姿态假设，从围绕大致位置查询的局部航拍图像计算出的航拍特征图中裁剪相应补丁。通过匹配BEV特征与航拍补丁特征来计算每个粒子的日志似然性。

Result: 在两个真实世界越野数据集上的实验显示，所提方法在已见路线上的绝对轨迹误差(ATE)降低了7.5倍，在未见路线上的ATE降低了7.0倍，同时保持了在密集树冠和阴影下的准确性。

Conclusion: BEV-Patch-PF展示出在没有GPS的情况下进行准确地理位置估计的能力，尤其适合于具有挑战性的户外环境。此外，该系统能够在NVIDIA Tesla T4上以10赫兹的速度实现实时运行，促进了其实用价值。

Abstract: We propose BEV-Patch-PF, a GPS-free sequential geo-localization system that integrates a particle filter with learned bird's-eye-view (BEV) and aerial feature maps. From onboard RGB and depth images, we construct a BEV feature map. For each 3-DoF particle pose hypothesis, we crop the corresponding patch from an aerial feature map computed from a local aerial image queried around the approximate location. BEV-Patch-PF computes a per-particle log-likelihood by matching the BEV feature to the aerial patch feature. On two real-world off-road datasets, our method achieves 7.5x lower absolute trajectory error (ATE) on seen routes and 7.0x lower ATE on unseen routes than a retrieval-based baseline, while maintaining accuracy under dense canopy and shadow. The system runs in real time at 10 Hz on an NVIDIA Tesla T4, enabling practical robot deployment.

</details>


### [5] [EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving](https://arxiv.org/abs/2512.15195)
*Jörg Gamerdinger,Sven Teufel,Stephan Amann,Lukas Marc Listl,Oliver Bringmann*

Main category: cs.RO

TL;DR: 本文提出了一种新的安全度量框架，用于评估自动驾驶中对象和车道检测任务的安全性能。该框架不仅考虑了整体性能，还特别关注由于感知错误可能导致的安全风险，并通过DeepAccident数据集验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的性能指标如精确度、召回率和F1分数虽然能够衡量感知系统的整体检测准确性，但未能考虑到与安全相关的方面。这导致即使在这些指标上得分高的感知系统也可能因为误检而导致严重事故。因此，有必要开发一种新的评价方法来同时评估感知系统的整体表现及其安全性。

Method: 文章介绍了一个新的安全度量框架，该框架整合了针对对象检测错误潜在风险的轻量级对象安全度量以及考虑对象与车道检测之间相互依赖关系的车道安全度量，从而为感知系统的安全性能提供了一个统一且可解释的度量标准。

Result: 利用DeepAccident数据集进行测试表明，所提出的方法能够识别出传统性能指标无法捕捉到的安全关键性感知错误，强调了以安全为中心的评估方法对于自动驾驶感知系统的重要性。

Conclusion: 研究结果证明了新提出的结合了对象和车道检测安全度量的安全评分体系的有效性，为自动驾驶车辆在复杂驾驶场景下的感知系统提供了更全面的安全性能评估手段。

Abstract: Extensive evaluation of perception systems is crucial for ensuring the safety of intelligent vehicles in complex driving scenarios. Conventional performance metrics such as precision, recall and the F1-score assess the overall detection accuracy, but they do not consider the safety-relevant aspects of perception. Consequently, perception systems that achieve high scores in these metrics may still cause misdetections that could lead to severe accidents. Therefore, it is important to evaluate not only the overall performance of perception systems, but also their safety. We therefore introduce a novel safety metric for jointly evaluating the most critical perception tasks, object and lane detection. Our proposed framework integrates a new, lightweight object safety metric that quantifies the potential risk associated with object detection errors, as well as an lane safety metric including the interdependence between both tasks that can occur in safety evaluation. The resulting combined safety score provides a unified, interpretable measure of perception safety performance. Using the DeepAccident dataset, we demonstrate that our approach identifies safety critical perception errors that conventional performance metrics fail to capture. Our findings emphasize the importance of safety-centric evaluation methods for perception systems in autonomous driving.

</details>


### [6] [Infrastructure-based Autonomous Mobile Robots for Internal Logistics -- Challenges and Future Perspectives](https://arxiv.org/abs/2512.15215)
*Erik Brorsson,Kristian Ceder,Ze Zhang,Sabino Francesco Roselli,Endre Erős,Martin Dahl,Beatrice Alenljung,Jessica Lindblom,Thanh Bui,Emmanuel Dean,Lennart Svensson,Kristofer Bengtsson,Per-Lage Götvall,Knut Åkesson*

Main category: cs.RO

TL;DR: 本文介绍了基于基础设施的自主移动机器人(AMR)系统，提出了一种结合基础设施感知、本地云计算和车载自主性的参考架构，并在重型车辆制造环境中进行了实际部署测试，旨在为复杂工业环境中的可扩展、稳健且人性化的AMR系统开发提供全面基础。


<details>
  <summary>Details</summary>
Motivation: 随着内部物流中采用自主移动机器人的趋势加快，大多数解决方案都集中在分散式的车载智能上。尽管室内环境如工厂中的AMR可以得到包括外部传感器和计算资源在内的基础设施支持，但此类系统在文献中仍较少被探讨。因此，文章旨在探索基于基础设施的AMR系统所带来的机会与挑战。

Method: 通过引入一个集成了基础设施感知、本地云计算以及车载自主性的参考架构来实现对基于基础设施的AMR系统的考察。根据该架构，作者回顾了定位、感知和规划的核心技术，并在一个重型车辆制造的实际场景中展示了这种方法的应用。

Result: 研究结果表明，在真实世界部署中，基于提出的架构方法能够有效工作；用户体验评估也提供了有价值的信息反馈。

Conclusion: 本文为未来在复杂工业环境中开发可扩展性强、稳定性高且适合人类使用的AMR系统奠定了坚实的基础。

Abstract: The adoption of Autonomous Mobile Robots (AMRs) for internal logistics is accelerating, with most solutions emphasizing decentralized, onboard intelligence. While AMRs in indoor environments like factories can be supported by infrastructure, involving external sensors and computational resources, such systems remain underexplored in the literature. This paper presents a comprehensive overview of infrastructure-based AMR systems, outlining key opportunities and challenges. To support this, we introduce a reference architecture combining infrastructure-based sensing, on-premise cloud computing, and onboard autonomy. Based on the architecture, we review core technologies for localization, perception, and planning. We demonstrate the approach in a real-world deployment in a heavy-vehicle manufacturing environment and summarize findings from a user experience (UX) evaluation. Our aim is to provide a holistic foundation for future development of scalable, robust, and human-compatible AMR systems in complex industrial environments.

</details>


### [7] [VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments](https://arxiv.org/abs/2512.15258)
*Yuze Wu,Mo Zhu,Xingxing Li,Yuheng Du,Yuxin Fan,Wenjun Li,Xin Zhou,Fei Gao*

Main category: cs.RO

TL;DR: 本文提出了一种高效的视觉-语言-动作(VLA)框架VLA-AN，旨在解决现有大型空中导航模型在复杂环境中的自主无人机导航问题。通过构建高保真数据集、引入三阶段训练框架、设计轻量级实时动作模块以及优化机载部署流程等手段，有效解决了领域差距、时间导航与推理不足、生成动作策略的安全性问题及机载部署限制等问题。实验表明，VLA-AN极大地提升了空间定位、场景理解和长距离导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型空中导航模型面临四大挑战：数据域差异、时间导航与推理能力不足、生成式动作策略带来的安全隐患以及机载部署时的资源约束。为了解决这些问题并提高无人机在复杂环境下的自主导航性能，提出了VLA-AN框架。

Method: 1. 使用3D高斯点云技术构建高保真度的数据集以缩小现实与模拟之间的领域差距。
2. 设计了一个渐进式的三阶段训练框架，逐步增强场景理解能力、基础飞行技能和复杂的导航技巧。
3. 开发了一个轻量级且支持实时操作的动作生成模块，并结合了几何安全校正机制来确保快速生成无碰撞且稳定的控制指令。
4. 通过对机载部署流程进行深度优化，在资源受限的无人机上实现了显著的推理吞吐量提升。

Result: 广泛的实验证明了VLA-AN能够显著改善空间感知、场景推理以及长航程导航的表现，单个任务的成功率最高可达98.1%。此外，该方案还展示了在轻型飞行器上实现全链条闭环自主性的高效可行途径。

Conclusion: VLA-AN作为一个创新的视觉-语言-动作框架，成功地克服了当前大型航空导航系统中存在的多个关键问题，包括但不限于数据域适应性差、缺乏有效的长时间序列导航与决策能力、潜在的安全隐患以及难以直接应用于实际设备上的局限性。它不仅大幅提高了无人机在复杂环境下执行任务的能力，而且也为未来研究提供了新的思路和技术支持。

Abstract: This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.

</details>


### [8] [A Network-Based Framework for Modeling and Analyzing Human-Robot Coordination Strategies](https://arxiv.org/abs/2512.15282)
*Martijn IJtsma,Salvatore Hargis*

Main category: cs.RO

TL;DR: 本文提出了一种新的计算框架，用于分析人机系统中的联合工作策略。通过将功能建模技术与图论表示相结合，该框架能够明确地捕捉协调需求随时间的变化，并在概念设计阶段支持对人机协调策略的早期探索。


<details>
  <summary>Details</summary>
Motivation: 随着更先进的机器人能力被部署，动态和非结构化环境中的人机交互研究表明，需要增加合作能力来支持与人类问题持有者的协作。然而，现有的框架要么侧重于实时执行的计算支持，要么依赖于静态表示来进行设计，在概念设计初期对协调动态进行推理的支持有限。

Method: 本文介绍了一个新的计算框架，它将功能建模技术与图论表示相结合，旨在分析人机系统中的联合工作策略。这个框架以系统功能之间的关系以及工作环境的物理和信息结构为特点，同时明确地捕捉了随着时间推移协调需求是如何演变的。

Result: 通过灾难机器人领域的案例研究展示了该框架如何支持早期的人机协调策略探索空间，并识别出有助于灵活管理协调开销的合作能力。

Conclusion: 这些结果表明，该框架能够使人机系统的协调需求及其时间演变变得明确，从而在实施前就支持关于合作能力和工作需求的设计时推理。

Abstract: Studies of human-robot interaction in dynamic and unstructured environments show that as more advanced robotic capabilities are deployed, the need for cooperative competencies to support collaboration with human problem-holders increases. Designing human-robot systems to meet these demands requires an explicit understanding of the work functions and constraints that shape the feasibility of alternative joint work strategies. Yet existing human-robot interaction frameworks either emphasize computational support for real-time execution or rely on static representations for design, offering limited support for reasoning about coordination dynamics during early-stage conceptual design. To address this gap, this article presents a novel computational framework for analyzing joint work strategies in human-robot systems by integrating techniques from functional modeling with graph-theoretic representations. The framework characterizes collective work in terms of the relationships among system functions and the physical and informational structure of the work environment, while explicitly capturing how coordination demands evolve over time. Its use during conceptual design is demonstrated through a case study in disaster robotics, which shows how the framework can be used to support early trade-space exploration of human-robot coordination strategies and to identify cooperative competencies that support flexible management of coordination overhead. These results show how the framework makes coordination demands and their temporal evolution explicit, supporting design-time reasoning about cooperative competency requirements and work demands prior to implementation.

</details>


### [9] [GuangMing-Explorer: A Four-Legged Robot Platform for Autonomous Exploration in General Environments](https://arxiv.org/abs/2512.15309)
*Kai Zhang,Shoubin Chen,Dong Li,Baiyang Zhang,Tao Huang,Zehao Wu,Jiasheng Chen,Bo Zhang*

Main category: cs.RO

TL;DR: 介绍了GuangMing-Explorer，一个完全集成的自主探索平台，适用于多种环境下的稳健操作，并通过实际实验展示了其在执行自主探索任务时的有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管在感知、规划、控制和运动执行等单个组件方面取得了显著进展，但对一个包含软硬件在内的完整自主探索系统的整体性且实用性的描述仍然很少。

Method: 论文提出了GuangMing-Explorer系统，详述了系统架构，包括硬件设计、软件栈、算法部署以及实验设置。

Result: 通过大量现实世界的实验证明了该平台在执行自主探索任务中的有效性和效率，表明其在复杂和非结构化环境中的应用潜力。

Conclusion: GuangMing-Explorer作为一款专为多样化环境下稳定运作而设计的全集成式自主探索平台，不仅填补了现有研究空白，还展现了其实用价值与广阔的应用前景。

Abstract: Autonomous exploration is a fundamental capability that tightly integrates perception, planning, control, and motion execution. It plays a critical role in a wide range of applications, including indoor target search, mapping of extreme environments, resource exploration, etc. Despite significant progress in individual components, a holistic and practical description of a completely autonomous exploration system, encompassing both hardware and software, remains scarce. In this paper, we present GuangMing-Explorer, a fully integrated autonomous exploration platform designed for robust operation across diverse environments. We provide a comprehensive overview of the system architecture, including hardware design, software stack, algorithm deployment, and experimental configuration. Extensive real-world experiments demonstrate the platform's effectiveness and efficiency in executing autonomous exploration tasks, highlighting its potential for practical deployment in complex and unstructured environments.

</details>


### [10] [Remotely Detectable Robot Policy Watermarking](https://arxiv.org/abs/2512.15379)
*Michael Amir,Manon Flageat,Amanda Prorok*

Main category: cs.RO

TL;DR: 本文提出了一种名为Colored Noise Coherency (CoNoCo)的新型水印策略，专为远程检测设计，能够通过机器人的动作嵌入频谱信号，并且在保持性能不降级的情况下，利用纯远程观察来验证物理策略的来源，保护机器人技术中的知识产权。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在现实世界机器人系统中的成功应用，训练好的策略成为了一种新的知识产权形式。这迫切需要新的方法来验证所有权并检测未经授权、可能不安全的滥用行为。虽然水印技术在其他领域已经成熟，但对于物理策略来说，远程检测是一个独特的挑战。现有的方法假设可以访问机器人的内部状态，但实际上审计员往往只能依赖外部观察（例如视频片段）。这种“物理观察差距”意味着必须从噪声大、不同步且受未知系统动态过滤的信号中检测出水印。

Method: 文章提出了Colored Noise Coheroency (CoNoCo)，这是首个专为解决远程检测问题而设计的水印策略。它通过利用策略内在的随机性将频谱信号嵌入到机器人的动作中。研究者还证明了CoNoCo能够保持边缘动作分布不变，从而不会降低性能。

Result: 实验显示，在包括运动捕捉和侧面/顶部视角视频在内的多种远程模式下，无论是在模拟还是真实世界机器人实验中，CoNoCo都展现了强大且稳健的检测能力。

Conclusion: 这项工作朝向保护机器人学中的知识产权迈出重要一步，提供了首个使用纯远程观察来非侵入式验证物理策略来源的方法。

Abstract: The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations.

</details>


### [11] [MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training](https://arxiv.org/abs/2512.15411)
*Zhenhan Yin,Xuanhan Wang,Jiahao Jiang,Kaiyuan Deng,Pengqi Chen,Shuangle Li,Chong Liu,Xing Xu,ingkuan Song,Lianli Gao,Heng Tao Shen*

Main category: cs.RO

TL;DR: MiVLA, a new vision-language-action model, uses human-robot mutual imitation pre-training to improve generalization in different embodiments, showing significant performance improvements over existing models.


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型（VLAs）在处理人类视频和模拟机器人数据时，由于相机视角、外观形态和实体形态的不匹配，导致其泛化能力有限。为了解决这个问题，提出了MiVLA方法。

Method: MiVLA通过利用人类手部与机械臂之间固有的行为相似性，采用双向对齐的人类与机器人动作空间的方法，并基于运动学规则及左右手坐标系统来实现。给定人类或模拟机器人的演示后，MiVLA被训练以预测一种实体的行为轨迹，并模仿另一种未出现在演示中的实体行为。

Result: 广泛的实验表明，在模拟环境和真实世界平台上使用三种不同的机器人(ARX, PiPer 和 LocoMan)进行测试时，MiVLA相比现有最先进模型提高了25%（模拟环境下）和14%（真实机器人控制任务中）的泛化性能。

Conclusion: 通过结合真实世界人类数据的行为保真度和模拟机器人数据的操作多样性到一个统一模型中，MiVLA显著增强了下游任务的泛化能力。

Abstract: While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\boldsymbolπ_{0}$, $\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.

</details>


### [12] [Load-Based Variable Transmission Mechanism for Robotic Applications](https://arxiv.org/abs/2512.15448)
*Sinan Emre,Victor Barasuol,Matteo Villa,Claudio Semini*

Main category: cs.RO

TL;DR: 本文提出了一种基于负载的可变传动(LBVT)机制，通过预紧弹簧和四连杆机构被动地改变传动比，以响应外部扭矩需求。仿真分析表明该系统在达到预定扭矩阈值时能实现高达40%的传动比提升，并且当施加力超过18N时触发扭矩放大效应，适用于需要动态扭矩适应性的机器人应用，特别是腿式机器人。


<details>
  <summary>Details</summary>
Motivation: 为了提高机器人执行器的效率与适应性，同时减少因额外执行器导致的复杂度，作者们设计了一种能够根据外部扭矩需求自动调整传动比的新机制。

Method: LBVT机制采用了预紧弹簧和四连杆机构的设计来被动地调节传动比，不需要额外的主动控制执行器。其性能通过基于仿真的分析进行了评估。

Result: 仿真结果显示，在达到特定扭矩阈值时，LBVT机制可以实现高达40%的传动比增加；并且当施加的力超过18N时，观察到了扭矩放大的效果。

Conclusion: 本研究表明，所提出的LBVT机制能够有效简化机器人关节驱动系统的复杂度，同时提供必要的动态扭矩调整能力，为轻量、高效及自适应传动系统的发展做出了贡献，特别是在对动态扭矩适应性要求较高的腿式机器人领域。

Abstract: This paper presents a Load-Based Variable Transmission (LBVT) mechanism designed to enhance robotic actuation by dynamically adjusting the transmission ratio in response to external torque demands. Unlike existing variable transmission systems that require additional actuators for active control, the proposed LBVT mechanism leverages a pre-tensioned spring and a four-bar linkage to passively modify the transmission ratio, thereby reducing the complexity of robot joint actuation systems. The effectiveness of the LBVT mechanism is evaluated through simulation-based analyses. The results confirm that the system achieves up to a 40 percent increase in transmission ratio upon reaching a predefined torque threshold, effectively amplifying joint torque when required without additional actuation. Furthermore, the simulations demonstrate a torque amplification effect triggered when the applied force exceeds 18 N, highlighting the system ability to autonomously respond to varying load conditions. This research contributes to the development of lightweight, efficient, and adaptive transmission systems for robotic applications, particularly in legged robots where dynamic torque adaptation is critical.

</details>


### [13] [OMCL: Open-vocabulary Monte Carlo Localization](https://arxiv.org/abs/2512.15557)
*Evgenii Kruzhkov,Raphael Memmesheimer,Sven Behnke*

Main category: cs.RO

TL;DR: 该论文提出了一种使用视觉-语言特征扩展蒙特卡洛定位的方法，以提高机器人定位的鲁棒性，并通过不同数据集验证了其在室内和室外场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了实现导航规划，需要确保机器人能够准确地进行自我定位。当环境地图由不同的传感器创建时，如何将机器人的测量值与地图特征稳健地关联起来就变得尤为重要。

Method: 研究者们提出了一种基于视觉-语言特征来增强蒙特卡洛定位（Monte Carlo Localization）的新方法。这种方法利用开放词汇表特征来计算给定相机姿态和3D地图条件下视觉观测的可能性。此外，还允许通过自然语言描述初始化全局定位。

Result: 实验结果表明，在Matterport3D、Replica等室内场景以及SemanticKITTI户外场景中，所提方法均能有效工作并展现出良好的泛化能力。

Conclusion: 本文介绍了一种新的机器人定位技术，它结合了视觉与语言信息，为跨模态匹配提供了可能，从而提高了机器人在复杂多变环境下定位的准确性与可靠性。

Abstract: Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes.

</details>


### [14] [An Open Toolkit for Underwater Field Robotics](https://arxiv.org/abs/2512.15597)
*Giacomo Picardi,Saverio Iacoponi,Matias Carandell,Jorge Aguirregomezcorta,Mrudul Chellapurath,Joaquin del Rio,Marcello Calisti,Iacopo Aguzzi*

Main category: cs.RO

TL;DR: 本文介绍了一种用于水下操作研究的开源硬件和软件工具包，包括一个具有早期泄漏检测功能的水下机器人关节（URJ）、紧凑的控制和电源管理电子设备以及基于ROS2的软件栈。该工具包已经过广泛的实验室测试和多次实地部署，证明了其在真实海洋环境中的鲁棒性、多功能性和可重用性。


<details>
  <summary>Details</summary>
Motivation: 水下机器人技术对于海洋科学、环境监测和水下工业作业越来越重要，但水下操作和驱动系统的发展受到高成本、专有设计和模块化研究型硬件获取有限的限制。尽管开源项目已经使车辆构造和控制软件民主化，但对于需要防水且具备反馈功能的关节驱动系统来说，仍存在明显不足。这导致许多研究团队面临漫长的开发周期、有限的可重复性和将实验室原型过渡到现场准备平台的困难。

Method: 为了填补这一空白，作者们提出了一套开放且成本效益高的硬件与软件工具包，专门用于水下操作研究。此工具包包含了一个耐压级水下机器人关节（URJ），具有早期泄漏检测能力，加上紧凑的控制及电源管理电路，还有基于ROS2的感知与多模式驱动软件堆栈。所有CAD模型、制造文件、PCB源代码、固件以及ROS2软件包都已公开发布，支持本地生产、修改以及社区驱动下的改进。

Result: 经过广泛的实验室测试和多次实地部署后，这套工具包展示了其在多样化应用中可靠运行的能力，包括3自由度水下机械手、肌腱驱动软体抓手以及欠驱动沉积物采样器等，在最大40米深度下均表现良好。这些成果验证了该工具包在实际海洋环境下的坚固性、灵活性与再利用潜力。

Conclusion: 通过提供一套完全开放且经过实地测试的平台，本研究旨在降低进入水下操作研究领域的门槛，提高实验结果的可重复性，并加速水下野外机器人技术领域的创新步伐。

Abstract: Underwater robotics is becoming increasingly important for marine science, environmental monitoring, and subsea industrial operations, yet the development of underwater manipulation and actuation systems remains restricted by high costs, proprietary designs, and limited access to modular, research-oriented hardware. While open-source initiatives have democratized vehicle construction and control software, a substantial gap persists for joint-actuated systems-particularly those requiring waterproof, feedback-enabled actuation suitable for manipulators, grippers, and bioinspired devices. As a result, many research groups face lengthy development cycles, limited reproducibility, and difficulty transitioning laboratory prototypes to field-ready platforms.
  To address this gap, we introduce an open, cost-effective hardware and software toolkit for underwater manipulation research. The toolkit includes a depth-rated Underwater Robotic Joint (URJ) with early leakage detection, compact control and power management electronics, and a ROS2-based software stack for sensing and multi-mode actuation. All CAD models, fabrication files, PCB sources, firmware, and ROS2 packages are openly released, enabling local manufacturing, modification, and community-driven improvement.
  The toolkit has undergone extensive laboratory testing and multiple field deployments, demonstrating reliable operation up to 40 m depth across diverse applications, including a 3-DoF underwater manipulator, a tendon-driven soft gripper, and an underactuated sediment sampler. These results validate the robustness, versatility, and reusability of the toolkit for real marine environments.
  By providing a fully open, field-tested platform, this work aims to lower the barrier to entry for underwater manipulation research, improve reproducibility, and accelerate innovation in underwater field robotics.

</details>
