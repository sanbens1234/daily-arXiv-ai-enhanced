<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 27]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Quick Heuristic Validation of Edges in Dynamic Roadmap Graphs](https://arxiv.org/abs/2601.20968)
*Yulie Arad,Stav Ashur,Nancy M. Amato*

Main category: cs.RO

TL;DR: 本文提出了一种名为"红-绿-灰"的范式，用于在非静态环境中调整机器人运动规划的路线图。该方法通过廉价的启发式检查快速更新路线图，并能有效标记无效、有效或未知的边。实验结果表明，与Leven和Hutchinson的方法相比，本方法提高了准确性并保持了可比的更新运行时间。


<details>
  <summary>Details</summary>
Motivation: 为了解决在非静态环境下调整机器人运动规划路线图的问题，作者们旨在开发一种能够快速且准确地更新路线图的新方法。

Method: 提出了"红-绿-灰"范式，这是对SPITE方法的一种改进，可以使用简单的计算几何技术来近似机器人的扫掠体积，并执行懒惰碰撞检测，从而将路线图中的边标记为无效（红色）、有效（绿色）或未知（灰色）。

Result: 初步实验结果显示，与Leven和Hutchinson提出的成熟技术相比，所提方法不仅提高了准确性，还能够正确地标记无效边，同时维持了相近的更新运行时间。

Conclusion: 该研究表明，"红-绿-灰"范式是一种有效提升机器人运动规划路线图适应动态环境能力的方法，具有较高的实用价值。

Abstract: In this paper we tackle the problem of adjusting roadmap graphs for robot motion planning to non-static environments. We introduce the "Red-Green-Gray" paradigm, a modification of the SPITE method, capable of classifying the validity status of nodes and edges using cheap heuristic checks, allowing fast semi-lazy roadmap updates. Given a roadmap, we use simple computational geometry methods to approximate the swept volumes of robots and perform lazy collision checks, and label a subset of the edges as invalid (red), valid (green), or unknown (gray). We present preliminary experimental results comparing our method to the well-established technique of Leven and Hutchinson, and showing increased accuracy as well as the ability to correctly label edges as invalid while maintaining comparable update runtimes.

</details>


### [2] [Meta-ROS: A Next-Generation Middleware Architecture for Adaptive and Scalable Robotic Systems](https://arxiv.org/abs/2601.21011)
*Anshul Ranjan,Anoosh Damodar,Neha Chougule,Dhruva S Nayak,Anantharaman P. N,Shylaja S S*

Main category: cs.RO

TL;DR: 本文提出了一种新的机器人中间件解决方案Meta-ROS，旨在简化开发、提高性能并确保跨平台兼容性。通过使用现代通信协议如Zenoh和ZeroMQ，Meta-ROS在不同硬件平台上实现了高效低延迟的通信，并支持多种数据类型。与ROS1和ROS2相比，测试结果表明Meta-ROS在吞吐量上提高了30%，显著降低了消息延迟，并优化了资源使用。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有机器人中间件框架（如ROS2）给新开发者带来的复杂性和互操作性问题，提出了Meta-ROS来简化集成过程、增强性能表现以及保证跨平台的一致性。

Method: Meta-ROS采用包括Zenoh和ZeroMQ在内的先进通讯协议，以实现跨异构硬件平台间的高效且低延迟的数据交换，同时能够处理音频、图像和视频等多种格式的信息。

Result: 综合评估显示，相较于ROS1及ROS2，Meta-ROS达到了高达30%的额外吞吐量，大幅减少了信息传递时延，并且更好地利用了计算资源。

Conclusion: 凭借其强大的硬件适应能力和以开发者为中心的设计理念，Meta-ROS为现代实时机器人AI应用提供了一个理想的解决方案。

Abstract: The field of robotics faces significant challenges related to the complexity and interoperability of existing middleware frameworks, like ROS2, which can be difficult for new developers to adopt. To address these issues, we propose Meta-ROS, a novel middleware solution designed to streamline robotics development by simplifying integration, enhancing performance, and ensuring cross-platform compatibility. Meta-ROS leverages modern communication protocols, such as Zenoh and ZeroMQ, to enable efficient and low-latency communication across diverse hardware platforms, while also supporting various data types like audio, images, and video. We evaluated Meta-ROS's performance through comprehensive testing, comparing it with existing middleware frameworks like ROS1 and ROS2. The results demonstrated that Meta-ROS outperforms ROS2, achieving up to 30% higher throughput, significantly reducing message latency, and optimizing resource usage. Additionally, its robust hardware support and developer-centric design facilitate seamless integration and ease of use, positioning Meta-ROS as an ideal solution for modern, real-time robotics AI applications.

</details>


### [3] [Track-centric Iterative Learning for Global Trajectory Optimization in Autonomous Racing](https://arxiv.org/abs/2601.21027)
*Youngim Nam,Jungbin Kim,Kyungtae Kang,Cheolhyeon Kwon*

Main category: cs.RO

TL;DR: 本文提出了一种针对不确定车辆动力学条件下自动驾驶赛车最小化单圈时间的全局轨迹优化框架。通过基于赛道的方法直接学习和优化全程轨迹，利用小波变换以参数空间表示轨迹，并采用贝叶斯优化进行探索。该框架在模拟和实际测试中均表现出色，相比基准提升了高达20.7%的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在跟踪级别的动力学学习上，而没有根据学到的动力学更新轨迹本身。为解决计算成本高以及因动力学不确定性导致难以保证全球最优性的问题，提出了新的方法。

Method: 首先使用小波变换将轨迹表示在一个与赛道无关的参数空间内；然后利用贝叶斯优化有效探索此空间，每条候选轨迹的时间通过运行带有学习到的动力学特性的模拟来评估；整个过程嵌入到一个迭代学习框架中，其中优化后的轨迹被部署以收集真实世界数据用于更新动力学模型，从而逐步细化轨迹。

Result: 所提框架的有效性通过仿真及实地实验得到了验证，相较于标准基线单圈时间最多可提高20.7%，并且始终优于当前最先进的方法。

Conclusion: 提出的全视野轨迹学习与优化框架能够有效应对不确定车辆动力学条件下的挑战，在提高自动驾驶赛车性能方面具有显著优势。

Abstract: This paper presents a global trajectory optimization framework for minimizing lap time in autonomous racing under uncertain vehicle dynamics. Optimizing the trajectory over the full racing horizon is computationally expensive, and tracking such a trajectory in the real world hardly assures global optimality due to uncertain dynamics. Yet, existing work mostly focuses on dynamics learning at the tracking level, without updating the trajectory itself to account for the learned dynamics. To address these challenges, we propose a track-centric approach that directly learns and optimizes the full-horizon trajectory. We first represent trajectories through a track-agnostic parametric space in light of the wavelet transform. This space is then efficiently explored using Bayesian optimization, where the lap time of each candidate is evaluated by running simulations with the learned dynamics. This optimization is embedded in an iterative learning framework, where the optimized trajectory is deployed to collect real-world data for updating the dynamics, progressively refining the trajectory over the iterations. The effectiveness of the proposed framework is validated through simulations and real-world experiments, demonstrating lap time improvement of up to 20.7% over a nominal baseline and consistently outperforming state-of-the-art methods.

</details>


### [4] [WheelArm-Sim: A Manipulation and Navigation Combined Multimodal Synthetic Data Generation Simulator for Unified Control in Assistive Robotics](https://arxiv.org/abs/2601.21129)
*Guangping Liu,Tipu Sultan,Vittorio Di Giorgio,Nick Hawkins,Flavio Esposito,Madi Babaiasl*

Main category: cs.RO

TL;DR: 本文介绍了一个名为WheelArm的集成系统，该系统结合了轮椅和机械臂的控制。为此，作者开发了一个仿真框架WheelArm-Sim用于合成数据收集，并通过一个包含13项任务的数据集展示了其潜力，初步结果表明这些数据适用于驱动机器学习模型实现综合控制。


<details>
  <summary>Details</summary>
Motivation: 尽管辅助机器人技术在轮椅安装机械臂（WMRAs）和轮椅方面取得了进展，但利用机器学习模型对这两者的集成与统一控制仍处于探索阶段。为了填补这一空白，研究者提出了WheelArm概念，旨在为上肢及移动受限人士提供更全面的支持。

Method: 研究团队创建了名为WheelArm-Sim的仿真环境，专门用于生成综合控制所需的数据集。他们通过此平台收集了一个涵盖操作与导航的多模态数据集，包括13种不同任务、232条轨迹以及总共67,783个样本点。

Result: 通过对芥末拾取任务中的动作预测建立基准模型来验证WheelArm-Sim所产生数据的有效性。实验结果显示，从WheelArm-Sim中获得的数据能够有效地支持基于数据驱动的机器学习方法来进行集成式控制。

Conclusion: WheelArm-Sim仿真框架证明了自己在为WheelArm这样的集成控制系统收集高质量训练数据方面的有效性。这标志着向实现更加智能且易于使用的辅助技术迈出了一步。

Abstract: Wheelchairs and robotic arms enhance independent living by assisting individuals with upper-body and mobility limitations in their activities of daily living (ADLs). Although recent advancements in assistive robotics have focused on Wheelchair-Mounted Robotic Arms (WMRAs) and wheelchairs separately, integrated and unified control of the combination using machine learning models remains largely underexplored. To fill this gap, we introduce the concept of WheelArm, an integrated cyber-physical system (CPS) that combines wheelchair and robotic arm controls. Data collection is the first step toward developing WheelArm models. In this paper, we present WheelArm-Sim, a simulation framework developed in Isaac Sim for synthetic data collection. We evaluate its capability by collecting a manipulation and navigation combined multimodal dataset, comprising 13 tasks, 232 trajectories, and 67,783 samples. To demonstrate the potential of the WheelArm dataset, we implement a baseline model for action prediction in the mustard-picking task. The results illustrate that data collected from WheelArm-Sim is feasible for a data-driven machine learning model for integrated control.

</details>


### [5] [InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios](https://arxiv.org/abs/2601.21173)
*Zeyi Liu,Shuang Liu,Jihai Min,Zhaoheng Zhang,Jun Cen,Pengyu Han,Songqiao Hu,Zihan Meng,Xiao He,Donghua Zhou*

Main category: cs.RO

TL;DR: InspecSafe-V1, a novel multimodal benchmark dataset for industrial inspection safety assessment, is introduced. It features real-world data from 41 inspection robots across five industrial scenarios, providing rich, multi-sensor information and detailed annotations to support advanced safety reasoning and anomaly detection.


<details>
  <summary>Details</summary>
Motivation: The motivation behind the creation of InspecSafe-V1 stems from the need to overcome current limitations in public datasets, which often lack real-world data, rely on single-modality sensing, or do not offer fine-grained object-level annotations. These limitations hinder the development of robust scene understanding and multimodal safety reasoning capabilities for AI systems in complex, dynamic industrial settings, essential for predictive maintenance and autonomous inspection.

Method: InspecSafe-V1 was developed by collecting data from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, covering five distinct industrial scenarios. The dataset includes visible-spectrum images with pixel-level segmentation annotations for key objects, along with semantic scene descriptions and safety level labels. Additionally, it incorporates seven synchronized sensing modalities (infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity) to enable comprehensive, multimodal analysis.

Result: The result is a comprehensive, multimodal benchmark dataset that supports a wide range of applications in industrial inspection, including but not limited to, multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment. This dataset fills a significant gap in the field, providing researchers and developers with a valuable resource to advance the state-of-the-art in industrial AI and robotics.

Conclusion: InspecSafe-V1 represents a critical step forward in enhancing the reliability and effectiveness of AI-driven industrial inspection. By addressing the limitations of existing datasets and offering a rich, multimodal data source, it paves the way for more sophisticated, safer, and more efficient industrial operations through improved scene understanding and safety reasoning.

Abstract: With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments.

</details>


### [6] [Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation](https://arxiv.org/abs/2601.21188)
*Hao Cheng,Feitian Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种基于移动地平线估计器（MHE）和模型预测控制器（MPC）相结合的方法，通过明确建模并补偿风引起的效应来解决轻于空气（LTA）平台缺乏扰动感知控制框架的问题。该方法利用两自由度移动质量机制生成惯性力矩和气动力矩，以提高在易受干扰环境中的飞行稳定性。实验表明，所提出的集成MHE-MPC框架在抗风飞行方面显著优于传统的PID控制。


<details>
  <summary>Details</summary>
Motivation: 由于机器人飞艇作为轻于空气的空中系统虽然具有长时间续航和本质安全操作的优点，但它们对风扰动非常敏感。为了解决LTA平台缺乏扰动感知控制框架的问题，本研究旨在通过显式建模与补偿由风引起的效应来提高这类系统的鲁棒性和稳定性。

Method: 采用移动地平线估计器(MHE)实时推断风扰动，并将这些估计提供给模型预测控制器(MPC)，从而实现在不同风条件下的鲁棒轨迹和航向调节。此外，还使用了两自由度(2-DoF)移动质量机制来产生用于姿态和航向控制的惯性力矩和气动力矩。

Result: 广泛的飞行实验表明，在顶风和侧风条件下，所提出的综合MHE-MPC框架比基线PID控制表现得更好，证明了其对于扰动感知LTA飞行的有效性。

Conclusion: 通过引入一种结合MHE与MPC的新方法以及利用2-DoF移动质量机制，能够有效增强LTA系统在存在风扰情况下的飞行稳定性和性能。

Abstract: Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight.

</details>


### [7] [HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control](https://arxiv.org/abs/2601.21346)
*Wei Zuo,Chengyang Li,Yikun Wang,Bingyang Cheng,Zeyi Ren,Shuai Wang,Derrick Wing Kwan Ng,Yik-Chung Wu*

Main category: cs.RO

TL;DR: 提出了一种层次主动调参（HPTune）框架，通过扩展评估范围至未执行动作来改善模型预测控制（MPC）运动规划器的参数更新效率。该方法结合快速与慢速调参层级，并利用Doppler LiDAR提供的障碍物速度信息增强了运动预测能力，在复杂环境中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前用于提高模型预测控制（MPC）运动规划器适应性的参数调整方法通常以一种短视的方式运作，只评估已执行的动作，这导致了由于失败事件稀少而引起的参数更新效率低下。为了解决这个问题，研究旨在开发一种新的方法，能够更有效地调整参数，特别是通过考虑那些没有被执行但可能影响系统性能的动作。

Method: 提出了一个名为HPTune的新框架，它通过将评估从仅限于已执行的操作扩展到包括未执行的操作，从而实现更加积极主动的参数调整。该框架包含两个级别的调整：快速级别使用预测接近速度和预测邻近距离的风险指标；慢速级别则基于闭环反向传播的扩展评估损失函数。此外，HPTune还整合了Doppler LiDAR技术，除了位置测量外还能提供障碍物的速度信息，进一步提升了运动预测的质量。

Result: 在高保真度模拟器上进行的广泛实验表明，HPTune能够在复杂的环境下实现高效的MPC调优，并且相比多种基线方案表现更佳。具体而言，HPTune有助于根据具体情况定制运动规划策略，形成既安全又敏捷的避碰策略。

Conclusion: HPTune通过引入对非执行动作的评估以及结合不同时间尺度上的参数调整机制，显著提高了模型预测控制中参数调整的有效性，特别是在需要频繁调整以适应环境变化的情况下。此外，整合Doppler LiDAR技术增强了系统的感知能力，使得提出的解决方案在处理动态障碍物时更具优势。

Abstract: Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.

</details>


### [8] [Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control](https://arxiv.org/abs/2601.21363)
*Weidong Huang,Zhehan Li,Hangxin Liu,Biao Hou,Yao Su,Jingwen Zhang*

Main category: cs.RO

TL;DR: 该论文提出了一种结合了大规模预训练和基于模型的微调方法，以提高类人机器人运动策略的学习效率。使用了离线策略Soft Actor-Critic (SAC)，并通过确定性策略在新环境中收集数据同时利用基于物理信息的世界模型进行随机探索，从而在保证安全的前提下提升了学习效率。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习（尤其是在线策略算法）在应用于类人机器人控制时存在样本效率低的问题，限制了其在新环境中的安全适应能力。尽管离线策略RL和基于模型的RL展示了更好的样本效率，但如何有效连接大规模预训练与高效微调之间仍存在挑战。

Method: 研究者采用了离线策略Soft Actor-Critic (SAC)算法，并通过大批次更新以及高更新与数据比(UTD比率)来支持类人机器人运动策略的大规模预训练。对于适应新环境或非分布任务的情况，则是采用基于模型的方法对预训练好的SAC策略进行微调，在新环境下执行确定性策略的同时，将随机探索限制在一个基于物理信息构建的世界模型中。

Result: 实验表明，所提出的SAC预训练政策不仅能够实现零样本部署到真实机器人上，而且还能有效地适应新的环境及处理非分布任务。此外，这种方法成功地将预训练阶段的大规模模拟带来的时间效率优势与微调过程中基于模型学习的样本效率优势结合起来。

Conclusion: 本研究展示了一种有效提升类人机器人控制策略适应性和学习效率的新方法，它通过结合SAC算法的大规模预训练能力和基于模型方法的安全高效微调特性，为解决现有强化学习技术在样本效率上的局限提供了可行方案。

Abstract: Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.

</details>


### [9] [Towards Space-Based Environmentally-Adaptive Grasping](https://arxiv.org/abs/2601.21394)
*Leonidas Askianakis,Aleksandr Artemov*

Main category: cs.RO

TL;DR: 本文通过在学习到的潜在空间中直接学习控制策略，解决了机器人在非结构化环境（如太空）中的抓取问题。相比现有方法，该方法在不同条件下表现出了更高的成功率和更快的学习效率。


<details>
  <summary>Details</summary>
Motivation: 当前先进的机器人系统在处理高维动作空间、稀疏奖励以及超出精心设计训练场景的泛化方面存在困难。本文以太空环境中的抓取任务为例，研究了如何克服这些限制。

Method: 采用了一种结合多种模态信息于单一结构表示的方法，并在此基础上使用Soft Actor-Critic (SAC)强化学习算法来学习控制策略。整个过程利用GPU加速的物理模拟进行实验。

Result: 在连续变化的抓取条件下，不到1百万次环境步骤内达到了超过95%的任务成功率。与代表性视觉基线相比，在相同的开环单次尝试条件下，本方法显示出更快的收敛速度。此外，实验证明这种方法对于新物体形状、夹具几何形状、环境杂乱程度及传感器配置具有更好的鲁棒性。

Conclusion: 研究表明，在潜在空间中明确地进行推理可以提高学习样本效率并增强对新情况的适应能力。尽管取得了一定进展，但为了实现完全自适应且可泛化的抓取能力，特别是在极端太空条件下，仍有一些挑战需要克服。

Abstract: Robotic manipulation in unstructured environments requires reliable execution under diverse conditions, yet many state-of-the-art systems still struggle with high-dimensional action spaces, sparse rewards, and slow generalization beyond carefully curated training scenarios. We study these limitations through the example of grasping in space environments. We learn control policies directly in a learned latent manifold that fuses (grammarizes) multiple modalities into a structured representation for policy decision-making. Building on GPU-accelerated physics simulation, we instantiate a set of single-shot manipulation tasks and achieve over 95% task success with Soft Actor-Critic (SAC)-based reinforcement learning in less than 1M environment steps, under continuously varying grasping conditions from step 1. This empirically shows faster convergence than representative state-of-the-art visual baselines under the same open-loop single-shot conditions. Our analysis indicates that explicitly reasoning in latent space yields more sample-efficient learning and improved robustness to novel object and gripper geometries, environmental clutter, and sensor configurations compared to standard baselines. We identify remaining limitations and outline directions toward fully adaptive and generalizable grasping in the extreme conditions of space.

</details>


### [10] [Singularity-Free Lie Group Integration and Geometrically Consistent Evaluation of Multibody System Models Described in Terms of Standard Absolute Coordinates](https://arxiv.org/abs/2601.21413)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文提出了一种将李群积分器与标准运动方程公式接口的框架，以及一种将刚体运动几何一致地纳入绝对坐标下的运动方程评估的方法。


<details>
  <summary>Details</summary>
Motivation: 在多体系统建模中，使用绝对坐标时存在一个已知问题是缺乏对空间运动无奇异性的参数化方法，通常通过单位四元数来解决。李群积分方法被提议作为一种替代方案，以实现无奇异性的时间积分，同时自然地尊重积分过程中的空间运动几何。然而，直接作用于配置空间李群上的李群积分方法与标准的运动方程公式不兼容，并且不能在现有的多体系统仿真代码中直接实现而不进行重大重构。

Method: 研究提出了两方面的贡献：（1）构建了一个框架，用于将李群积分器与标准的运动方程公式对接起来，允许用各种绝对坐标描述多体系统的同时运用李群积分方案；（2）开发了一种方法，能够在使用标准向量空间积分方案整合绝对坐标的运动方程评估过程中始终如一地结合刚体运动的几何特性。利用直积群和半直积群SO(3)×R^3及SE(3)来表示刚体运动。关键元素是局部-全局转换（LGT）映射，它有助于根据李群上的局部坐标更新（全局）绝对坐标。

Result: 通过提出的框架与方法，能够有效地将李群积分器集成到现有的多体系统模拟代码中，同时保持了对于刚体运动几何特性的正确处理。

Conclusion: 该论文提供了一种解决方案，使得李群积分技术可以应用于采用绝对坐标描述的多体系统模型之中，而无需对现有模拟软件进行大幅度改造。此外，还提出了一种新方法，确保了即使是在使用常规向量空间积分法时也能够恰当地考虑到刚体运动的几何特征。

Abstract: A classical approach to the multibody systems (MBS) modeling is to use absolute coordinates, i.e., a set of (possibly redundant) coordinates that describe the absolute position and orientation of the individual bodies with respect to an inertial frame (IFR). A well-known problem for the time integration of the equations of motion (EOM) is the lack of a singularity-free parameterization of spatial motions, which is usually tackled by using unit quaternions. Lie group integration methods were proposed as an alternative approach to the singularity-free time integration. At the same time, Lie group formulations of EOM naturally respect the geometry of spatial motions during integration. Lie group integration methods, operating directly on the configuration space Lie group, are incompatible with standard formulations of the EOM, and cannot be implemented in existing MBS simulation codes without a major restructuring. The contribution of this paper is twofold: (1) A framework for interfacing Lie group integrators to standard EOM formulations is presented. It allows describing MBS in terms of various absolute coordinates and at the same using Lie group integration schemes. (2) A method for consistently incorporating the geometry of rigid body motions into the evaluation of EOM in absolute coordinates integrated with standard vector space integration schemes. The direct product group and the semidirect product group SO(3)xR3 and the semidirect product group SE(3) are used for representing rigid body motions. The key element is the local-global transitions (LGT) transition map, which facilitates the update of (global) absolute coordinates in terms of the (local) coordinates on the Lie group. This LGT map is specific to the absolute coordinates, the local coordinates on the Lie group, and the Lie group used to represent rigid body configurations.

</details>


### [11] [Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation](https://arxiv.org/abs/2601.21416)
*Alexandre Chapin,Bruno Machado,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: 研究探讨了基于槽的对象中心表示（SBOCR）在机器人操作策略中的应用，发现相比于全局和密集特征表示，SBOCR在多种视觉条件下表现出更好的泛化能力，即使没有任务特定的预训练也是如此。


<details>
  <summary>Details</summary>
Motivation: 当前用于机器人操作策略的视觉表示方法（如全局特征和密集特征）在面对光照、纹理变化或存在干扰物等分布转移时泛化性能较差。因此，研究旨在探索一种新的中间结构化替代方案——基于槽的对象中心表示（SBOCR），以提高政策在动态真实世界环境下的泛化能力。

Method: 通过将密集特征聚合成一组有限的对象类实体来构建SBOCR，并与一系列全局及密集型表示法进行对比测试，评估其在从简单到复杂的一系列模拟和现实世界操作任务中不同视觉条件下的表现。

Result: 实验结果表明，在各种视觉条件变化下，包括照明、纹理改变以及存在干扰因素时，基于SBOCR的方法比使用密集和全局表示法的策略显示出更优的泛化能力。

Conclusion: 研究表明，采用SBOCR作为视觉系统设计的一种途径能够有效提升机器人在动态且多变的真实环境中执行任务时的泛化性能。

Abstract: The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.

</details>


### [12] [Nimbus: A Unified Embodied Synthetic Data Generation Framework](https://arxiv.org/abs/2601.21449)
*Zeyu He,Yuchang Zhang,Yuanzhen Zhou,Miao Tao,Hengjie Li,Yang Tian,Jia Zeng,Tai Wang,Wenzhe Cai,Yilun Chen,Ning Gao,Jiangmiao Pang*

Main category: cs.RO

TL;DR: Nimbus是一个统一的合成数据生成框架，通过模块化四层架构和解耦执行模型来提高资源利用率，实现2-3倍的端到端吞吐量提升，支持大规模分布式环境下的长期稳定运行。


<details>
  <summary>Details</summary>
Motivation: 随着对具身智能泛化需求的增长，现有基于物理获取的数据方式成本高昂且效率低下。尽管合成数据生成提供了一种可扩展的选择，但当前的方法存在碎片化、任务特定的问题，导致工程效率低和系统不稳定，无法满足基础模型训练所需的持续高通量数据生成要求。

Method: 提出了Nimbus，一个旨在整合异构导航与操作流程的统一合成数据生成框架。它采用模块化的四层架构设计，将轨迹规划、渲染及存储过程解耦为异步阶段，并通过动态管道调度、全局负载均衡、分布式容错机制以及针对后端优化的渲染技术最大化CPU、GPU及I/O资源的使用效率。

Result: 实验表明，相比于未经优化的基础版本，Nimbus能够实现2至3倍的端到端吞吐量增长，并确保在大规模分布式环境中长时间稳定运行。

Conclusion: Nimbus作为InternData套件的生产骨干，不仅解决了传统合成数据生成方法中存在的问题，还促进了跨领域的无缝数据合成，为高效训练具备泛化能力的智能体提供了强有力的支持。

Abstract: Scaling data volume and diversity is critical for generalizing embodied intelligence. While synthetic data generation offers a scalable alternative to expensive physical data acquisition, existing pipelines remain fragmented and task-specific. This isolation leads to significant engineering inefficiency and system instability, failing to support the sustained, high-throughput data generation required for foundation model training. To address these challenges, we present Nimbus, a unified synthetic data generation framework designed to integrate heterogeneous navigation and manipulation pipelines. Nimbus introduces a modular four-layer architecture featuring a decoupled execution model that separates trajectory planning, rendering, and storage into asynchronous stages. By implementing dynamic pipeline scheduling, global load balancing, distributed fault tolerance, and backend-specific rendering optimizations, the system maximizes resource utilization across CPU, GPU, and I/O resources. Our evaluation demonstrates that Nimbus achieves a 2-3X improvement in end-to-end throughput compared to unoptimized baselines and ensuring robust, long-term operation in large-scale distributed environments. This framework serves as the production backbone for the InternData suite, enabling seamless cross-domain data synthesis.

</details>


### [13] [4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving](https://arxiv.org/abs/2601.21454)
*Shanliang Yao,Zhuoxiao Li,Runwei Guan,Kebin Cao,Meng Xia,Fuping Hu,Sen Xu,Yong Yue,Xiaohui Zhu,Weiping Ding,Ryan Wen Liu*

Main category: cs.RO

TL;DR: 提出了一种名为4D-CAAL的统一框架，用于解决4D雷达与摄像头外参校准及自动标注问题。通过设计一种双用途校准目标并开发鲁棒的对应匹配算法，实现了准确的外参校准；同时，利用已校准传感器关系，通过几何投影和多特征优化将基于摄像头的分割标注转移到雷达点云上，大大减少了手动标注工作量。


<details>
  <summary>Details</summary>
Motivation: 4D雷达因其在高程测量和分辨率上的优势成为自动驾驶的关键传感器，但其与摄像头的有效集成需要精确的外部校准，而雷达感知算法的发展则需要大规模标注数据集。现有校准方法通常使用针对视觉或雷达模态优化的独立目标，这使得建立对应关系变得复杂。此外，手动标注稀疏雷达数据既费时又不可靠。

Method: 1. 设计了一种新颖的双用途校准目标，结合了用于相机检测的棋盘图案和位于背面中心用于雷达检测的角反射器。
2. 开发了一种鲁棒的对应匹配算法，该算法能够将棋盘中心与最强雷达反射点对齐，从而实现准确的外部校准。
3. 提出了一条自动标注流水线，利用经过校准的传感器关系，通过几何投影和多特征优化将基于相机的分割注释转换到雷达点云中。

Result: 实验结果表明，所提出的方法不仅达到了很高的校准精度，还显著减少了手动标注的工作量，促进了自动驾驶领域中鲁棒多模态感知系统的开发。

Conclusion: 4D-CAAL框架为4D雷达与摄像头之间的精准外参校准及高效自动标注提供了有效解决方案，有助于加速自动驾驶技术的发展。

Abstract: 4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving.

</details>


### [14] [IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation](https://arxiv.org/abs/2601.21506)
*Joonhee Lee,Hyunseung Shin,Jeonggil Ko*

Main category: cs.RO

TL;DR: IROS, a real-time navigation framework for indoor mobile robots, integrates efficient lightweight perceptual modules with the contextual reasoning of Vision-Language Models (VLMs) to improve decision accuracy and reduce latency, achieving more robust and human-like navigation.


<details>
  <summary>Details</summary>
Motivation: 现有的方法在室内移动机器人导航中难以同时提供快速响应和强大的语义理解。经典几何方法如SLAM虽然提供了可靠的定位，但依赖于详细的地图且无法解释对人类有用的线索（例如标志、房间号）；而视觉-语言-动作(VLA)模型虽然引入了语义基础，但仅基于可见帧做出反应，无法预测未见的交叉点或推理远处的文字线索。此外，视觉-语言模型(VLMs)虽能提供更丰富的上下文推断，但计算延迟高，不适合实时操作。

Method: 提出了IROS，一个结合了VLM级别的上下文推理与轻量级感知模块效率的实时导航框架，旨在低成本设备上运行。该框架受到双重过程理论启发，将快速反射性决策（系统一）与慢速审慎推理（系统二）分开，只有在必要时才调用VLM。此外，通过增强紧凑型VLM的空间和文本线索，IROS实现了鲁棒性高、类似人类的导航，并保持最小延迟。

Result: 在五个真实世界建筑中的测试表明，相比于连续使用VLM进行导航的方式，IROS提高了决策准确性并减少了66%的延迟。

Conclusion: IROS通过结合高效轻量级感知组件与视觉-语言模型的上下文推理能力，为室内移动机器人提供了一种更加鲁棒、低延迟的导航解决方案。

Abstract: Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues. Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms. In this work, we present IROS, a real-time navigation framework that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware. Inspired by Dual Process Theory, IROS separates fast reflexive decisions (System One) from slow deliberative reasoning (System Two), invoking the VLM only when necessary. Furthermore, by augmenting compact VLMs with spatial and textual cues, IROS delivers robust, human-like navigation with minimal latency. Across five real-world buildings, IROS improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.

</details>


### [15] [AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation](https://arxiv.org/abs/2601.21602)
*Jianli Sun,Bin Tian,Qiyao Zhang,Chengxiang Li,Zihan Song,Zhiyong Cui,Yisheng Lv,Yonglin Tian*

Main category: cs.RO

TL;DR: 提出了AIR-VLA，这是首个专为航空操作设计的视觉-语言-动作（VLA）基准。它包括一个基于物理的模拟环境和高质量的多模态数据集，并通过实验评估了主流VLA模型在无人机移动、操作器控制和高级规划方面的能力与局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型在地面机器人上取得了显著的成功，但在航空操纵系统（AMS）中的应用仍是一个未充分探索的领域。由于AMS具有浮动基座动态特性、无人机与操作器之间强耦合以及任务的多步骤长时序性质等特点，给旨在服务于静态或二维移动基座的设计模式带来了严峻挑战。

Method: 创建了一个名为AIR-VLA的新基准，专门针对空中操控进行了优化。该研究构建了一个基于物理的仿真环境，并发布了包含3000个手动遥操作演示的高质量多模态数据集，涵盖基础操作、物体与空间理解、语义推理及长远规划等内容。利用此平台对主流VLA模型及最先进的视觉语言模型进行了系统性评估。

Result: 实验不仅验证了将VLA范式转移到航空系统中的可行性，而且通过针对空中任务定制的多维度指标，揭示了当前模型在无人机机动性、操作器控制和高层次规划方面的性能边界。

Conclusion: AIR-VLA建立了一个标准化测试平台和数据基础，支持未来通用航空机器人的研究。

Abstract: While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose severe challenges to existing VLA paradigms designed for static or 2D mobile bases. To bridge this gap, we propose AIR-VLA, the first VLA benchmark specifically tailored for aerial manipulation. We construct a physics-based simulation environment and release a high-quality multimodal dataset comprising 3000 manually teleoperated demonstrations, covering base manipulation, object & spatial understanding, semantic reasoning, and long-horizon planning. Leveraging this platform, we systematically evaluate mainstream VLA models and state-of-the-art VLM models. Our experiments not only validate the feasibility of transferring VLA paradigms to aerial systems but also, through multi-dimensional metrics tailored to aerial tasks, reveal the capabilities and boundaries of current models regarding UAV mobility, manipulator control, and high-level planning. AIR-VLA establishes a standardized testbed and data foundation for future research in general-purpose aerial robotics. The resource of AIR-VLA will be available at https://anonymous.4open.science/r/AIR-VLA-dataset-B5CC/.

</details>


### [16] [CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation](https://arxiv.org/abs/2601.21712)
*Xuanran Zhai,Binkai Ou,Yemin Wang,Hui Yi Leong,Qiaojun Yu,Ce Hao,Yaohua Liu*

Main category: cs.RO

TL;DR: 本文提出了一种名为CoFreeVLA的方法，通过引入一个短期自碰撞风险估计器来增强端到端的视觉语言动作模型，以减少双臂操作中的自碰撞并提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言动作（VLA）模型虽然能够实现指令跟随操纵，但在双臂部署中由于对两臂间及与抓取物体间的自碰撞建模不足而存在安全隐患。

Method: CoFreeVLA方法通过添加一个短期自碰撞风险估计器来改进传统的端到端VLA模型，该估计器基于本体感觉、视觉嵌入以及计划的动作预测碰撞可能性。此估计器不仅能够阻止有风险的命令执行，还能够在发生潜在危险时引导机器人回到安全状态，并通过调整优化策略来促进更安全的操作流程。预训练阶段使用基于模型的碰撞标签数据进行，随后在真实机器人上进行微调以确保准确性。

Result: 实验结果表明，在PiPER机器人手臂执行五项双臂任务时，相比RDT和APEX方法，采用CoFreeVLA方法后自碰撞显著减少且任务完成率有所提高。

Conclusion: CoFreeVLA为解决双臂操作过程中遇到的安全问题提供了一个有效的解决方案，通过结合自碰撞风险评估与传统VLA模型，实现了更加安全可靠的双臂协调工作。

Abstract: Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision likelihood from proprioception, visual embeddings, and planned actions. The estimator gates risky commands, recovers to safe states via risk-guided adjustments, and shapes policy refinement for safer rollouts. It is pre-trained with model-based collision labels and posttrained on real robot rollouts for calibration. On five bimanual tasks with the PiPER robot arm, CoFreeVLA reduces selfcollisions and improves success rates versus RDT and APEX.

</details>


### [17] [Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations](https://arxiv.org/abs/2601.21713)
*Donatien Delehelle,Fei Chen,Darwin Caldwell*

Main category: cs.RO

TL;DR: 本文提出了一种高效且模块化的方法来处理布料操控问题，通过精心设计的选择，在模拟学习中显著减少了模型大小和训练时间，并展示了如何将模拟训练的模型转移到现实世界。在SoftGym基准测试上评估了这种方法，并在任务上实现了比现有基线显著的性能改进，同时使用了更小规模的模型。


<details>
  <summary>Details</summary>
Motivation: 由于布料操纵策略开发中的高维状态空间、复杂动态以及高度自遮挡倾向等难题，导致基于数据的方法通常依赖于大型模型和长时间训练，进而产生了显著的计算成本障碍。此外，考虑到从模拟到现实转换时采用端到端学习方法所带来的额外计算负担，本文旨在探索一种更加高效且模块化的强化学习方法用于布料操作，以降低模型大小与训练所需时间。

Method: 本文介绍了一种针对布料操控优化过的强化学习方法，该方法通过特定的设计选择，在保证学习效果的同时大幅度减小了模型尺寸并缩短了训练周期。此外，研究还探讨了如何有效地将这种在仿真环境中训练得到的模型应用于真实场景之中。

Result: 实验结果表明，在SoftGym标准测试集上，相较于现有的其他方法，本研究所提出的方法不仅能够达到更好的执行效果，而且所使用的模型规模也远小于传统方案。

Conclusion: 研究表明，通过精巧的设计选择，可以为布料操控创建出既高效又易于实际应用的强化学习解决方案。这种方法不仅大大降低了对计算资源的需求，同时也为从模拟环境向真实环境迁移提供了一条可行之路。

Abstract: Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.

</details>


### [18] [GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration](https://arxiv.org/abs/2601.21829)
*Bsher Karbouj,Baha Eddin Gaaloul,Jorg Kruger*

Main category: cs.RO

TL;DR: GAZELOAD是一个多模式数据集，用于工业人机协作中的心理负荷估计。它同步了眼动信号、环境实时测量和任务上下文，并在受控的任务难度和环境条件下收集。这些数据可用于开发心理负荷估计算法、特征提取以及在现实工业HRC场景中进行时间建模，并研究如照明等环境因素对基于眼睛的工作负荷标记的影响。


<details>
  <summary>Details</summary>
Motivation: 创建一个可以用来开发和评估心理负荷估计算法的数据集，特别是针对工业环境中人与机器人协作的情况。通过同时记录参与者的眼动数据、环境条件及任务详情，为研究提供全面的数据支持。

Method: 通过让26名参与者在一个实验室装配测试平台上与两台协作机器人（UR5和Franka Emika Panda）互动来收集数据，参与者佩戴Meta ARIA智能眼镜。数据集同步了眼动信号（瞳孔直径、注视点、扫视、眼动、凝视转移熵、注视分散指数）与环境实时连续测量（照度）以及任务和机器人上下文（工作台、任务块、引入的故障）。

Result: 对于每个参与者和分级任务块，提供了以250毫秒窗口聚合的眼动指标CSV文件、环境日志以及1-10李克特量表上的自我报告的心理负荷评分。

Conclusion: GAZELOAD数据集能够促进开发和基准测试心理负荷估计算法，在真实的工业HRC场景中进行特征提取及时序建模，并探索诸如照明等环境因素如何影响基于眼睛的工作负荷标志物。

Abstract: This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccades, eye gaze, gaze transition entropy, fixation dispersion index) with environmental real-time and continuous measurements (illuminance) and task and robot context (bench, task block, induced faults), under controlled manipulations of task difficulty and ambient conditions. For each participant and workload-graded task block, we provide CSV files with ocular metrics aggregated into 250 ms windows, environmental logs, and self-reported mental workload ratings on a 1-10 Likert scale, organized in participant-specific folders alongside documentation. These data can be used to develop and benchmark algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers.

</details>


### [19] [LLM-Driven Scenario-Aware Planning for Autonomous Driving](https://arxiv.org/abs/2601.21876)
*He Li,Zhaowei Chen,Rui Gao,Guoliang Li,Qi Hao,Shuai Wang,Chengzhong Xu*

Main category: cs.RO

TL;DR: 提出了一种由大型语言模型驱动的自适应规划方法LAP，旨在解决混合规划切换框架在密集交通中高效与安全驾驶难以兼顾的问题。通过场景理解和联合优化模式配置及运动规划，实现了在低复杂度场景下的高速行驶和高复杂度场景下的精确驾驶之间的切换。


<details>
  <summary>Details</summary>
Motivation: 现有的混合规划切换框架方法在密集交通环境下难以可靠地进行模式转换或维持高效的驾驶状态，这主要是由于基于启发式的场景识别以及较低频率的控制更新导致的。因此，需要一种新的方法来改善这一状况，使得自动驾驶系统能够在保证安全的同时提高驾驶效率。

Method: 本文提出了LAP方法，该方法利用大型语言模型(LLM)进行场景理解，并将这种理解整合进模式配置与运动规划的联合优化之中。通过树搜索模型预测控制与交替最小化技术解决联合优化问题。整个方案使用Python语言实现，并且是在机器人操作系统(ROS)环境中完成的。

Result: 高保真度仿真结果显示，所提出的LAP方法在驾驶时间和成功率方面均优于其他基准方法。

Conclusion: 研究结果表明，通过引入大型语言模型辅助场景理解并结合特定优化算法，能够显著提升自动驾驶系统在不同复杂度场景下的表现，从而为实现更加安全高效的自动驾驶提供了新思路。

Abstract: Hybrid planner switching framework (HPSF) for autonomous driving needs to reconcile high-speed driving efficiency with safe maneuvering in dense traffic. Existing HPSF methods often fail to make reliable mode transitions or sustain efficient driving in congested environments, owing to heuristic scene recognition and low-frequency control updates. To address the limitation, this paper proposes LAP, a large language model (LLM) driven, adaptive planning method, which switches between high-speed driving in low-complexity scenes and precise driving in high-complexity scenes, enabling high qualities of trajectory generation through confined gaps. This is achieved by leveraging LLM for scene understanding and integrating its inference into the joint optimization of mode configuration and motion planning. The joint optimization is solved using tree-search model predictive control and alternating minimization. We implement LAP by Python in Robot Operating System (ROS). High-fidelity simulation results show that the proposed LAP outperforms other benchmarks in terms of both driving time and success rate.

</details>


### [20] [Multi-Modular MANTA-RAY: A Modular Soft Surface Platform for Distributed Multi-Object Manipulation](https://arxiv.org/abs/2601.21884)
*Pratik Ingle,Jørn Lambertsen,Kasper Støy,Andres Faina*

Main category: cs.RO

TL;DR: 本文提出了一种分布式、模块化且可扩展的MANTA-RAY平台变体，通过减少执行器密度来操纵脆弱和异质物体，同时保持操纵性能。系统使用对象在模块间的传递以及基于几何变换驱动的PID控制器直接将倾斜角度控制输出映射到执行器命令，从而无需大量数据驱动或黑盒训练。实验验证了该系统能够成功操作具有不同几何形状、质量和纹理的对象，并展示了其在实际应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的密集执行器阵列虽然可以生成复杂的变形，但同时也引入了高自由度，增加了系统的复杂性并限制了可扩展性。MANTA-RAY平台旨在通过利用柔软的织物表面和减少执行器密度来解决这些问题，以操纵脆弱和异质物体。然而，先前的研究主要集中在由四个执行器支持的单模块实现上，而可扩展的多模块配置的可行性和优点尚未被探索。

Method: 研究者开发了一个分布式、模块化并且可扩展的MANTA-RAY平台版本，它通过降低执行器密度的同时保持操纵性能。该方法包括模块间物体传递机制及一个基于几何变换的PID控制器，该控制器可以直接把倾斜角度控制输出转化为执行器指令，避免了对大规模数据驱动模型或黑盒训练的需求。此外，还通过模拟不同模块配置（3x3 和 4x4）下的表现，并在2x2物理硬件原型上进行了可行性验证。

Result: 仿真结果表明，在不同模块配置下系统均能有效运行；实验结果则证明了该系统能够在物理原型上成功地处理各种几何形状、重量和材质的物品，包括易碎品如鸡蛋和苹果等。此外，该系统还允许并行操作多个物体，这表明多模块MANTA-RAY提高了可扩展性，并能在更大范围内协调操作多个物体。

Conclusion: 多模块MANTA-RAY平台通过减少执行器密度实现了更好的可扩展性，同时保持了良好的操控性能。这种设计不仅能够有效地处理多样化的物体，而且为实际应用提供了广阔的前景。

Abstract: Manipulation surfaces control objects by actively deforming their shape rather than directly grasping them. While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability. The MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation densitY) platform addresses these challenges by leveraging a soft, fabric-based surface with reduced actuator density to manipulate fragile and heterogeneous objects. Previous studies focused on single-module implementations supported by four actuators, whereas the feasibility and benefits of a scalable, multi-module configuration remain unexplored. In this work, we present a distributed, modular, and scalable variant of the MANTA-RAY platform that maintains manipulation performance with a reduced actuator density. The proposed multi-module MANTA-RAY platform and control strategy employs object passing between modules and a geometric transformation driven PID controller that directly maps tilt-angle control outputs to actuator commands, eliminating the need for extensive data-driven or black-box training. We evaluate system performance in simulation across surface configurations of varying modules (3x3 and 4x4) and validate its feasibility through experiments on a physical 2x2 hardware prototype. The system successfully manipulates objects with diverse geometries, masses, and textures including fragile items such as eggs and apples as well as enabling parallel manipulation. The results demonstrate that the multi-module MANTA-RAY improves scalability and enables coordinated manipulation of multiple objects across larger areas, highlighting its potential for practical, real-world applications.

</details>


### [21] [Information Filtering via Variational Regularization for Robot Manipulation](https://arxiv.org/abs/2601.21926)
*Jinhao Zhang,Wenlong Xia,Yaojia Wang,Zhexuan Zhou,Huizhe Li,Yichen Lai,Haoming Song,Youmin Gong,Jie Me*

Main category: cs.RO

TL;DR: 本文提出了一种名为变分正则化（Variational Regularization, VR）的轻量级模块，用于在基于3D视觉表示的扩散模型中减少任务无关噪声，从而提高机器人技能学习的表现。实验表明，在多个基准测试中相比基线方法DP3，VR能够显著提升成功率，并在实际部署中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有的基于3D视觉表示构建扩散模型的方法往往使用过大的去噪解码器，虽然增加了模型容量但也引入了冗余和噪声到中间特征块中。研究发现，通过随机屏蔽骨干特征可以改善性能，这表明存在与任务无关的噪声。因此，作者提出了一个新颖的方法来解决这个问题。

Method: 提出了变分正则化（VR），这是一种轻量级模块，通过对骨干特征施加时间步长条件下的高斯分布并应用KL散度正则化，创建了一个自适应信息瓶颈。

Result: 在三个模拟基准测试（RoboTwin2.0、Adroit和MetaWorld）上的广泛实验显示，与基线方法DP3相比，所提方法在RoboTwin2.0上提高了6.1%的成功率，在Adroit和MetaWorld上提高了4.1%的成功率，并达到了新的最先进结果。此外，现实世界中的实验也证明了该方法的有效性。

Conclusion: 变分正则化为改进基于扩散模型的机器人技能学习提供了一个有效途径，通过减少任务无关噪声提高了模型性能。

Abstract: Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills. However, most existing methods employ an oversized denoising decoder. While increasing model capacity can improve denoising, empirical evidence suggests that it also introduces redundancy and noise in intermediate feature blocks. Crucially, we find that randomly masking backbone features at inference time (without changing training) can improve performance, confirming the presence of task-irrelevant noise in intermediate features. To this end, we propose Variational Regularization (VR), a lightweight module that imposes a timestep-conditioned Gaussian over backbone features and applies a KL-divergence regularizer, forming an adaptive information bottleneck. Extensive experiments on three simulation benchmarks (RoboTwin2.0, Adroit, and MetaWorld) show that, compared to the baseline DP3, our approach improves the success rate by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld, achieving new state-of-the-art results. Real-world experiments further demonstrate that our method performs well in practical deployments. Code will released.

</details>


### [22] [MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts](https://arxiv.org/abs/2601.21971)
*Lorenzo Mazza,Ariel Rodriguez,Rayan Younis,Martin Lelis,Ortrun Hellig,Chenpan Li,Sebastian Bodenstedt,Martin Wagner,Stefanie Speidel*

Main category: cs.RO

TL;DR: 本文提出了一种监督式的混合专家(MoE)架构，用于解决手术机器人在模仿学习中的挑战。该架构可以与任何自主策略结合，并且能够从不到150次演示中通过立体内窥镜图像学习复杂的长时序操作任务。实验结果表明，相较于最新的视觉-语言-动作(VLA)模型和标准ACT基线，采用监督式MoE架构的ACT在分布内情况下的成功率更高，在分布外场景（如新的抓取位置、光照减弱和部分遮挡）中也表现出更好的鲁棒性。此外，该方法还能够在未见过的测试视角下泛化，并且无需额外训练即可零样本转移到离体猪组织上。


<details>
  <summary>Details</summary>
Motivation: 尽管模仿学习在机器人操作领域取得了显著成功，但将其应用于手术机器人仍然面临数据稀缺、工作空间受限以及需要极高水平的安全性和可预测性的挑战。

Method: 作者提出了一种监督式的混合专家(MoE)架构，专为阶段结构化的外科操作任务设计，可以叠加于任何自主策略之上。利用Action Chunking Transformer (ACT)作为轻量级动作解码器策略，仅需少于150次的演示并通过纯立体内窥镜图像即可学会复杂、长时间的操作。

Result: 评估结果显示，一般化的VLA模型完全无法掌握所研究的任务，即使是在标准分布内条件下也是如此。而标准ACT虽然在分布内条件达到了一定水平的成功率，但采用监督式MoE架构后其性能得到了显著提升，不仅提高了分布内的成功率，而且在面对新抓取点位、光线减弱及部分遮挡等分布外情境时展现出了更强的鲁棒性。值得注意的是，这种方法还能很好地适应从未见过的测试视角，并且可以在没有进一步训练的情况下直接应用于离体猪组织上。

Conclusion: 监督式MoE架构极大地增强了基于ACT的方法在手术辅助机器人执行复杂操作任务方面的能力，不仅提升了分布内任务的成功率，还展现了对分布外情形的强大适应能力。此外，它展示了向活体部署迈进的可能性。

Abstract: Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.

</details>


### [23] [Macro-Scale Electrostatic Origami Motor](https://arxiv.org/abs/2601.21976)
*Alex S. Miller,Leo McElroy,Jeffrey H. Lang*

Main category: cs.RO

TL;DR: 本文介绍了一种可折叠的宏观尺度折纸旋转电机，该电机可以平铺折叠，并在展开后运行。使用电晕放电产生扭矩，原型电机实现了2.5:1的扩展比，在-29kV驱动下达到1440转/分钟的最大速度，并且最大输出扭矩超过0.15mNm，主动组件的扭矩密度为0.04Nm/kg。


<details>
  <summary>Details</summary>
Motivation: 由于可折叠机器人具有高体积质量比、易于打包和形状适应性等优点，因此成为机器人研究的一个活跃领域。然而，先前开发的可折叠机器人要么在其结构中嵌入了线性执行器，要么附加了非折叠旋转电机。此外，这些直接嵌入到折叠介质结构中的执行器都贡献于线性或折叠运动，而不是连续旋转运动。在宏观尺度上，还没有出现过一种可以折叠的连续旋转执行器。

Method: 研究人员开发并测试了第一款可以在宏观尺度上折叠的折纸旋转电机。该电机利用电晕放电技术来产生扭矩，展示了从平面状态展开并运作的能力。

Result: 所开发的原型电机达到了2.5:1的扩张比率，在-29kV电压驱动下能够达到最高速度1440rpm，并表现出超过0.15mN m的最大输出扭矩，同时其有效组成部分的扭矩密度为0.04 Nm/kg。

Conclusion: 这项工作标志着首次成功创建了一个能够在宏观尺度上折叠的连续旋转执行器，它不仅能够以紧凑的形式存储，而且在需要时可以通过简单的展开过程转变为功能性的旋转电机。

Abstract: Foldable robots have been an active area of robotics research due to their high volume-to-mass ratio, easy packability, and shape adaptability. For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure. Further, those actuators directly embedded in the structure of the folding medium all contributed to linear or folding motion, not to continuous rotary motion. On the macro-scale there has not yet been a folding continuous rotary actuator. This paper details the development and testing of the first macro-scale origami rotary motor that can be folded flat, and then unfurled to operate. Using corona discharge for torque production, the prototype motor achieved an expansion ratio of 2.5:1, reached a top speed of 1440 rpm when driven at -29 kV, and exhibited a maximum output torque over 0.15 mN m with an active component torque density of 0.04 Nm/kg.

</details>


### [24] [PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy](https://arxiv.org/abs/2601.22018)
*Jinhao Zhang,Zhexuan Zhou,Huizhe Li,Yichen Lai,Wenlong Xia,Haoming Song,Youmin Gong,Jie Me*

Main category: cs.RO

TL;DR: 提出了一种轻量级的3D扩散策略PocketDP3，通过使用基于MLP-Mixer块的Diffusion Mixer替换了先前方法中的重型条件U-Net解码器，从而在减少模型大小的同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉基础扩散策略中存在一个常见的架构不匹配问题：高效的点云编码器通常与庞大的解码器配对。给定紧凑的场景表示后，这种配置可能导致解码器中参数的大量浪费。

Method: 提出了PocketDP3，一种口袋规模的3D扩散策略，它利用基于MLP-Mixer块的轻量级Diffusion Mixer（DiM）代替了之前方法中的重型条件U-Net解码器。该架构能够有效地跨时间和通道维度融合信息，显著减少了模型大小，并且支持两步推理过程，提高了实时部署的实用性。

Result: 在三个模拟基准测试--RoboTwin2.0、Adroit和MetaWorld--上，PocketDP3以少于前人方法1%的参数实现了最先进的性能，并加速了推理过程。此外，真实世界实验进一步证明了该方法在实际环境中的实用性和可转移性。

Conclusion: PocketDP3展示了如何通过改进解码器设计来大幅度减小模型尺寸而不牺牲性能，为3D视觉基础上的机器人操作技能学习提供了一个更加高效和实用的解决方案。

Abstract: Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet efficient point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivated by this observation, we propose PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in prior methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks. This architecture enables efficient fusion across temporal and channel dimensions, significantly reducing model size. Notably, without any additional consistency distillation techniques, our method supports two-step inference without sacrificing performance, improving practicality for real-time deployment. Across three simulation benchmarks--RoboTwin2.0, Adroit, and MetaWorld--PocketDP3 achieves state-of-the-art performance with fewer than 1% of the parameters of prior methods, while also accelerating inference. Real-world experiments further demonstrate the practicality and transferability of our method in real-world settings. Code will be released.

</details>


### [25] [mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning](https://arxiv.org/abs/2601.22074)
*Kevin Zakka,Qiayuan Liao,Brent Yi,Louis Le Lay,Koushil Sreenath,Pieter Abbeel*

Main category: cs.RO

TL;DR: 介绍了一个名为mjlab的轻量级开源机器人学习框架，该框架结合了GPU加速模拟、可组合环境，并且安装简单。


<details>
  <summary>Details</summary>
Motivation: 为了简化机器人学习的研究和开发流程，提供一个易于使用、依赖少且支持GPU加速物理模拟的学习框架。

Method: 采用Isaac Lab引入的基于管理器的API，用户可以组合模块化的构建块来处理观测、奖励和事件，并与MuJoCo Warp配合实现GPU加速的物理模拟。

Result: 开发出了一个只需一条命令即可安装的框架，它依赖最少，直接访问原生MuJoCo数据结构，并提供了速度跟踪、动作模仿和操作任务的参考实现。

Conclusion: 通过结合最新的技术如GPU加速模拟等，mjlab为研究者们提供了一个强大而灵活的工具，以促进机器人学习领域的发展。

Abstract: We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.

</details>


### [26] [ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection](https://arxiv.org/abs/2601.22090)
*Runsheng Wang,Katelyn Lee,Xinyue Zhu,Lauren Winterbottom,Dawn M. Nilsen,Joel Stein,Matei Ciocarlie*

Main category: cs.RO

TL;DR: 本文提出了一种健康到中风的适应性流程，该流程能够从大量健康个体的sEMG预训练模型初始化意图检测器，并通过少量特定于中风患者的数据进行微调。实验结果表明，这种方法在减少校准负担的同时提高了实时中风后意图识别的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 表面肌电图（sEMG）作为中风后辅助手部康复的一种有前景的控制信号，但从中风受损肌肉中检测意图往往需要长时间、特定对象的校准，并且对于变化敏感。为了解决这个问题，研究者们提出了一种新的方法来减轻校准负担并提高系统的鲁棒性。

Method: 研究者提出了一个健康至中风的适应管道，首先利用大规模健康人群的sEMG数据预训练一个意图检测模型，然后使用少量中风患者的个人数据对该模型进行微调。此外，他们还比较了不同的适应策略（仅头部调整、参数高效LoRA适配器以及完全端到端微调）。

Result: 与直接迁移学习和仅基于中风患者数据训练相比，在相同的数据预算下，采用健康预训练适应的方法一致地提升了针对中风患者的意图识别性能；最佳适应方法将平均转换准确率从0.42提升到了0.61，原始准确率从0.69提升到了0.78。

Conclusion: 研究结果显示，通过转移可重用的健康域EMG表示可以减少校准负担，同时改善了实时中风后意图检测的鲁棒性。这表明所提出的健康至中风适应性流程是一种有效的解决方案。

Abstract: Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.

</details>


### [27] [DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation](https://arxiv.org/abs/2601.22153)
*Haozhe Xie,Beichen Wen,Jiarui Zheng,Zhaoxi Chen,Fangzhou Hong,Haiwen Diao,Ziwei Liu*

Main category: cs.RO

TL;DR: 提出了DynamicVLA框架，旨在解决视觉-语言-动作模型在处理动态物体操作时遇到的挑战。该框架通过紧凑的0.4B VLA、连续推理和潜在感知动作流技术来实现快速多模态推理与及时适应物体运动，并引入了动态物体操作基准测试DOM以促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型虽然在静态操纵任务中表现良好，但在需要快速感知、时间预测及持续控制的动态场景下存在不足。为了解决这个问题，研究者们开发了一种新的框架DynamicVLA以及相应的数据集，以提高模型在动态环境下的性能。

Method: 1. 使用卷积视觉编码器创建了一个紧凑型0.4B VLA，有助于高效的空间编码。
2. 引入连续推理机制，允许推理与执行过程重叠进行，从而减少延迟并能够更快地对物体移动做出反应。
3. 采用潜在感知动作流技术，确保动作执行与感知之间的时间一致性。
此外，还构建了一个名为DOM的新基准测试，用于收集大量合成与真实世界的数据集，进一步支持动态物体操作的研究。

Result: 通过广泛的评估表明，DynamicVLA在响应速度、感知能力以及泛化性能方面都有显著提升，为不同形式的通用动态物体操作提供了一个统一的框架。

Conclusion: DynamicVLA通过其创新设计解决了现有VLA模型在动态物体操作中的局限性，不仅提高了模型的实时性和准确性，也为未来的研究提供了宝贵的资源。

Abstract: Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.

</details>
