{"id": "2602.06087", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.06087", "abs": "https://arxiv.org/abs/2602.06087", "authors": ["Kuo Chen", "Minghao Dou", "Qianqi Liu", "Yang An", "Kai Ren", "Zeming WU", "Yu Tian", "Jie Sun", "Xinping Wang", "Zhier Chen", "Jiancheng Yu"], "title": "Dynamic Modeling, Parameter Identification and Numerical Analysis of Flexible Cables in Flexibly Connected Dual-AUV Systems", "comment": null, "summary": "This research presents a dynamic modeling framework and parameter identification methods for describing the highly nonlinear behaviors of flexibly connected dual-AUV systems. The modeling framework is established based on the lumped mass method, integrating axial elasticity, bending stiffness, added mass and hydrodynamic forces, thereby accurately capturing the time-varying response of the forces and cable configurations. To address the difficulty of directly measuring material-related and hydrodynamic coefficients, this research proposes a parameter identification method that combines the physical model with experimental data. High-precision inversion of the equivalent Youngs modulus and hydrodynamic coefficients is performed through tension experiments under multiple configurations, effectively demonstrating that the identified model maintains predictive consistency in various operational conditions. Further numerical analysis indicates that the dynamic properties of flexible cable exhibit significant nonlinear characteristics, which are highly dependent on material property variations and AUV motion conditions. This nonlinear dynamic behavior results in two typical response states, slack and taut, which are jointly determined by boundary conditions and hydrodynamic effects, significantly affecting the cable configuration and endpoint loads. In this research, the dynamics of flexible cables under complex boundary conditions is revealed, providing a theoretical foundation for the design, optimization and further control research of similar systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5efa\u6a21\u6846\u67b6\u548c\u53c2\u6570\u8bc6\u522b\u65b9\u6cd5\uff0c\u7528\u4e8e\u63cf\u8ff0\u67d4\u6027\u8fde\u63a5\u7684\u53ccAUV\u7cfb\u7edf\u7684\u9ad8\u5ea6\u975e\u7ebf\u6027\u884c\u4e3a\u3002\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u6a21\u578b\u4e0e\u5b9e\u9a8c\u6570\u636e\u89e3\u51b3\u4e86\u76f4\u63a5\u6d4b\u91cf\u6750\u6599\u76f8\u5173\u548c\u6c34\u52a8\u529b\u7cfb\u6570\u7684\u96be\u9898\uff0c\u5e76\u63ed\u793a\u4e86\u67d4\u6027\u7535\u7f06\u5728\u590d\u6742\u8fb9\u754c\u6761\u4ef6\u4e0b\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u4e3a\u7c7b\u4f3c\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3001\u4f18\u5316\u53ca\u8fdb\u4e00\u6b65\u63a7\u5236\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u4e3a\u4e86\u51c6\u786e\u63cf\u8ff0\u67d4\u6027\u8fde\u63a5\u7684\u53ccAUV\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u7684\u9ad8\u5ea6\u975e\u7ebf\u6027\u884c\u4e3a\uff0c\u7279\u522b\u662f\u89e3\u51b3\u96be\u4ee5\u76f4\u63a5\u6d4b\u5b9a\u6750\u6599\u76f8\u5173\u548c\u6c34\u52a8\u529b\u7cfb\u6570\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u96c6\u4e2d\u8d28\u91cf\u6cd5\u5efa\u7acb\u4e86\u4e00\u4e2a\u7efc\u5408\u8003\u8651\u8f74\u5411\u5f39\u6027\u3001\u5f2f\u66f2\u521a\u5ea6\u3001\u9644\u52a0\u8d28\u91cf\u548c\u6c34\u52a8\u529b\u529b\u7b49\u56e0\u7d20\u7684\u5efa\u6a21\u6846\u67b6\uff1b\u63d0\u51fa\u4e86\u5c06\u7269\u7406\u6a21\u578b\u4e0e\u5b9e\u9a8c\u6570\u636e\u76f8\u7ed3\u5408\u7684\u53c2\u6570\u8bc6\u522b\u65b9\u6cd5\uff1b\u901a\u8fc7\u591a\u914d\u7f6e\u4e0b\u7684\u5f20\u529b\u5b9e\u9a8c\u5b9e\u73b0\u4e86\u7b49\u6548\u6768\u6c0f\u6a21\u91cf\u548c\u6c34\u52a8\u529b\u7cfb\u6570\u7684\u9ad8\u7cbe\u5ea6\u53cd\u6f14\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u80fd\u591f\u51c6\u786e\u6355\u6349\u529b\u7684\u65f6\u95f4\u53d8\u5316\u54cd\u5e94\u4ee5\u53ca\u7f06\u7ef3\u6784\u578b\u53d8\u5316\u7684\u6a21\u578b\uff1b\u8bc1\u5b9e\u4e86\u6240\u8bc6\u522b\u51fa\u7684\u6a21\u578b\u5728\u591a\u79cd\u64cd\u4f5c\u6761\u4ef6\u4e0b\u4fdd\u6301\u9884\u6d4b\u4e00\u81f4\u6027\uff1b\u53d1\u73b0\u67d4\u6027\u7f06\u7ef3\u7684\u52a8\u529b\u7279\u6027\u5c55\u73b0\u51fa\u663e\u8457\u7684\u975e\u7ebf\u6027\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u6781\u5927\u5730\u4f9d\u8d56\u4e8e\u6750\u6599\u5c5e\u6027\u7684\u53d8\u5316\u4ee5\u53caAUV\u7684\u8fd0\u52a8\u72b6\u51b5\u3002", "conclusion": "\u67d4\u6027\u8fde\u63a5\u7684\u53ccAUV\u7cfb\u7edf\u8868\u73b0\u51fa\u590d\u6742\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u5176\u72b6\u6001\uff08\u677e\u5f1b\u6216\u7d27\u7ef7\uff09\u7531\u8fb9\u754c\u6761\u4ef6\u548c\u6c34\u52a8\u529b\u6548\u5e94\u5171\u540c\u51b3\u5b9a\uff0c\u8fd9\u5bf9\u7f06\u7ef3\u6784\u578b\u53ca\u7aef\u70b9\u8f7d\u8377\u6709\u91cd\u5927\u5f71\u54cd\uff1b\u672c\u7814\u7a76\u63ed\u793a\u4e86\u67d4\u6027\u7f06\u7ef3\u5728\u590d\u6742\u8fb9\u754c\u6761\u4ef6\u4e0b\u7684\u52a8\u529b\u5b66\u6027\u8d28\uff0c\u4e3a\u8be5\u7c7b\u7cfb\u7edf\u7684\u8bbe\u8ba1\u4f18\u5316\u53ca\u540e\u7eed\u63a7\u5236\u7814\u7a76\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.06088", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06088", "abs": "https://arxiv.org/abs/2602.06088", "authors": ["Thomas Georges", "Adam Abdin"], "title": "Transformer-Based Reinforcement Learning for Autonomous Orbital Collision Avoidance in Partially Observable Environments", "comment": null, "summary": "We introduce a Transformer-based Reinforcement Learning framework for autonomous orbital collision avoidance that explicitly models the effects of partial observability and imperfect monitoring in space operations. The framework combines a configurable encounter simulator, a distance-dependent observation model, and a sequential state estimator to represent uncertainty in relative motion. A central contribution of this work is the use of transformer-based Partially Observable Markov Decision Process (POMDP) architecture, which leverage long-range temporal attention to interpret noisy and intermittent observations more effectively than traditional architectures. This integration provides a foundation for training collision avoidance agents that can operate more reliably under imperfect monitoring environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u8f68\u9053\u78b0\u649e\u907f\u514d\u3002\u8be5\u6846\u67b6\u8003\u8651\u4e86\u7a7a\u95f4\u64cd\u4f5c\u4e2d\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u4e0d\u5b8c\u7f8e\u76d1\u63a7\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u53ef\u914d\u7f6e\u7684\u906d\u9047\u6a21\u62df\u5668\u3001\u8ddd\u79bb\u4f9d\u8d56\u7684\u89c2\u6d4b\u6a21\u578b\u548c\u987a\u5e8f\u72b6\u6001\u4f30\u8ba1\u5668\u6765\u8868\u793a\u76f8\u5bf9\u8fd0\u52a8\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5728\u4e0d\u5b8c\u7f8e\u7684\u76d1\u63a7\u73af\u5883\u4e0b\u8fdb\u884c\u66f4\u53ef\u9760\u7684\u8f68\u9053\u78b0\u649e\u907f\u514d\u64cd\u4f5c\uff0c\u4f5c\u8005\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u79d1\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u67b6\u6784\uff0c\u4ee5\u66f4\u597d\u5730\u89e3\u91ca\u566a\u58f0\u548c\u95f4\u6b47\u6027\u7684\u89c2\u5bdf\u6570\u636e\u3002", "method": "\u7814\u7a76\u8005\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u878d\u5408\u4e86\u53ef\u914d\u7f6e\u906d\u9047\u6a21\u62df\u5668\u3001\u57fa\u4e8e\u8ddd\u79bb\u7684\u89c2\u5bdf\u6a21\u578b\u4ee5\u53ca\u5e8f\u5217\u72b6\u6001\u4f30\u8ba1\u5668\u7684\u6846\u67b6\uff0c\u5176\u4e2d\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684POMDP\u67b6\u6784\uff0c\u5229\u7528\u957f\u7a0b\u65f6\u95f4\u6ce8\u610f\u529b\u673a\u5236\u6765\u5904\u7406\u5608\u6742\u4e14\u65ad\u7eed\u7684\u89c2\u6d4b\u6570\u636e\u3002", "result": "\u8fd9\u79cd\u65b0\u65b9\u6cd5\u80fd\u591f\u6bd4\u4f20\u7edf\u67b6\u6784\u66f4\u52a0\u6709\u6548\u5730\u7406\u89e3\u548c\u5e94\u5bf9\u7531\u4e8e\u90e8\u5206\u53ef\u89c1\u6027\u53ca\u76d1\u6d4b\u4e0d\u8db3\u5e26\u6765\u7684\u6311\u6218\uff0c\u4e3a\u8bad\u7ec3\u80fd\u591f\u5728\u4e0d\u5b8c\u7f8e\u76d1\u6d4b\u6761\u4ef6\u4e0b\u5de5\u4f5c\u7684\u78b0\u649e\u89c4\u907f\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u5f0f\uff0c\u5373\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8eTransformer\u7684POMDP\u67b6\u6784\uff0c\u53ef\u4ee5\u6539\u5584\u592a\u7a7a\u64cd\u4f5c\u4e2d\u9762\u5bf9\u90e8\u5206\u53ef\u89c1\u6027\u548c\u76d1\u6d4b\u7f3a\u9677\u65f6\u7684\u81ea\u4e3b\u5bfc\u822a\u4e0e\u907f\u78b0\u80fd\u529b\u3002"}}
{"id": "2602.06191", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.06191", "abs": "https://arxiv.org/abs/2602.06191", "authors": ["Ege Yuceel", "Daniel Liberzon", "Sayan Mitra"], "title": "Active Localization of Unstable Systems with Coarse Information", "comment": "10 pages, 4 figures, Accepted by International Conference on Hybrid Systems: Computation and Control (HSCC) 2026", "summary": "We study localization and control for unstable systems under coarse, single-bit sensing. Motivated by understanding the fundamental limitations imposed by such minimal feedback, we identify sufficient conditions under which the initial state can be recovered despite instability and extremely sparse measurements. Building on these conditions, we develop an active localization algorithm that integrates a set-based estimator with a control strategy derived from Voronoi partitions, which provably estimates the initial state while ensuring the agent remains in informative regions. Under the derived conditions, the proposed approach guarantees exponential contraction of the initial-state uncertainty, and the result is further supported by numerical experiments. These findings can offer theoretical insight into localization in robotics, where sensing is often limited to coarse abstractions such as keyframes, segmentations, or line-based features.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u7c97\u7565\u5355\u6bd4\u7279\u611f\u77e5\u4e0b\u4e0d\u7a33\u5b9a\u7cfb\u7edf\u7684\u5b9a\u4f4d\u4e0e\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u5408\u4f30\u8ba1\u5668\u548cVoronoi\u5206\u533a\u63a7\u5236\u7b56\u7565\u7684\u4e3b\u52a8\u5b9a\u4f4d\u7b97\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u8bc1\u667a\u80fd\u4f53\u5904\u4e8e\u4fe1\u606f\u4e30\u5bcc\u533a\u57df\u7684\u540c\u65f6\u4f30\u8ba1\u521d\u59cb\u72b6\u6001\uff0c\u5e76\u4e14\u786e\u4fdd\u521d\u59cb\u72b6\u6001\u4e0d\u786e\u5b9a\u6027\u4ee5\u6307\u6570\u65b9\u5f0f\u51cf\u5c0f\u3002", "motivation": "\u7406\u89e3\u7531\u6781\u7b80\u53cd\u9988\uff08\u5982\u4e0d\u7a33\u5b9a\u7684\u7cfb\u7edf\u548c\u6781\u5176\u7a00\u758f\u7684\u6d4b\u91cf\uff09\u5e26\u6765\u7684\u57fa\u672c\u9650\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u57fa\u4e8e\u96c6\u5408\u7684\u4f30\u8ba1\u5668\u4e0e\u4eceVoronoi\u5206\u533a\u5bfc\u51fa\u7684\u63a7\u5236\u7b56\u7565\u7684\u4e3b\u52a8\u5b9a\u4f4d\u7b97\u6cd5\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u4fdd\u8bc1\u4e86\u521d\u59cb\u72b6\u6001\u4e0d\u786e\u5b9a\u6027\u7684\u6307\u6570\u6536\u7f29\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u5f97\u5230\u4e86\u8fdb\u4e00\u6b65\u7684\u652f\u6301\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u7406\u8bba\u89c1\u89e3\uff0c\u7279\u522b\u662f\u5728\u611f\u77e5\u901a\u5e38\u53d7\u9650\u4e8e\u8bf8\u5982\u5173\u952e\u5e27\u3001\u5206\u5272\u6216\u57fa\u4e8e\u7ebf\u7684\u7279\u5f81\u7b49\u7c97\u7cd9\u62bd\u8c61\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2602.06207", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06207", "abs": "https://arxiv.org/abs/2602.06207", "authors": ["Ruizhou Zhao", "Yichen Chu", "Shuwei Zhao", "Wenchao Yue", "Raymond Shing-Yan Tang", "Hongliang Ren"], "title": "Bioinspired Kirigami Capsule Robot for Minimally Invasive Gastrointestinal Biopsy", "comment": "8 pages, 11 figures, accepted to IEEE ICRA", "summary": "Wireless capsule endoscopy (WCE) has transformed gastrointestinal (GI) diagnostics by enabling noninvasive visualization of the digestive tract, yet its diagnostic yield remains constrained by the absence of biopsy capability, as histological analysis is still the gold standard for confirming disease. Conventional biopsy using forceps, needles, or rotating blades is invasive, limited in reach, and carries risks of perforation or mucosal trauma, while fluid- or microbiota-sampling capsules cannot provide structured tissue for pathology, leaving a critical gap in swallowable biopsy solutions. Here we present the Kiri-Capsule, a kirigami-inspired capsule robot that integrates deployable PI-film flaps actuated by a compact dual-cam mechanism to achieve minimally invasive and repeatable tissue collection. The kirigami surface remains flat during locomotion but transforms into sharp protrusions upon cam-driven stretching, enabling controlled penetration followed by rotary scraping, with specimens retained in internal fan-shaped cavities. Bench tests confirmed that PI films exhibit a Young's modulus of approximately 20 MPa and stable deployment angles (about 34$^\\circ$ at 15% strain), while ex vivo porcine studies demonstrated shallow penetration depths (median $\\sim$0.61 mm, range 0.46--0.66 mm) and biopsy yields comparable to standard forceps (mean $\\sim$10.9 mg for stomach and $\\sim$18.9 mg for intestine), with forces within safe ranges reported for GI biopsy. These findings demonstrate that the Kiri-Capsule bridges passive imaging and functional biopsy, providing a swallowable, depth-controlled, and histology-ready solution that advances capsule-based diagnostics toward safe and effective clinical application.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aKiri-Capsule\u7684\u521b\u65b0\u8bbe\u5907\uff0c\u5b83\u7ed3\u5408\u4e86\u6298\u7eb8\u7075\u611f\u7684\u8bbe\u8ba1\u548c\u7d27\u51d1\u7684\u53cc\u51f8\u8f6e\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u975e\u4fb5\u5165\u6027\u548c\u53ef\u91cd\u590d\u7684\u7ec4\u7ec7\u91c7\u96c6\u3002\u8fd9\u79cd\u80f6\u56ca\u673a\u5668\u4eba\u5728\u8fd0\u52a8\u65f6\u8868\u9762\u4fdd\u6301\u5e73\u5766\uff0c\u5728\u53d7\u5230\u51f8\u8f6e\u9a71\u52a8\u62c9\u4f38\u65f6\u8f6c\u5316\u4e3a\u5c16\u9510\u7a81\u8d77\uff0c\u80fd\u591f\u63a7\u5236\u7a7f\u900f\u5e76\u968f\u540e\u8fdb\u884c\u65cb\u8f6c\u522e\u53d6\uff0c\u6837\u672c\u88ab\u4fdd\u7559\u5728\u5185\u90e8\u6247\u5f62\u8154\u5185\u3002\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u548c\u79bb\u4f53\u732a\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u8bbe\u5907\u7684\u6709\u6548\u6027\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u80f6\u56ca\u5f0f\u8bca\u65ad\u6280\u672f\u5411\u4e34\u5e8a\u5e94\u7528\u8fc8\u8fdb\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u65e0\u7ebf\u80f6\u56ca\u5185\u955c\uff08WCE\uff09\u867d\u7136\u9769\u65b0\u4e86\u80c3\u80a0\u9053\uff08GI\uff09\u8bca\u65ad\u65b9\u5f0f\uff0c\u4f46\u7f3a\u4e4f\u6d3b\u68c0\u80fd\u529b\u9650\u5236\u4e86\u5176\u8bca\u65ad\u6548\u679c\uff1b\u4f20\u7edf\u6d3b\u68c0\u65b9\u6cd5\u5177\u6709\u4fb5\u5165\u6027\u3001\u8303\u56f4\u53d7\u9650\u53ca\u98ce\u9669\uff1b\u800c\u6d41\u4f53\u6216\u5fae\u751f\u7269\u7fa4\u91c7\u6837\u80f6\u56ca\u65e0\u6cd5\u63d0\u4f9b\u75c5\u7406\u5b66\u6240\u9700\u7684\u7ed3\u6784\u5316\u7ec4\u7ec7\u6837\u672c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u4ee5\u541e\u54bd\u4e14\u80fd\u5b89\u5168\u6709\u6548\u5730\u6536\u96c6\u7ec4\u7ec7\u6837\u672c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u53d7\u6298\u7eb8\u542f\u53d1\u7684\u80f6\u56ca\u673a\u5668\u4eba\u2014\u2014Kiri-Capsule\uff0c\u5b83\u4f7f\u7528PI\u8584\u819c\u74e3\u7247\u4e0e\u4e00\u4e2a\u5c0f\u578b\u53cc\u51f8\u8f6e\u673a\u6784\u76f8\u7ed3\u5408\u6765\u5b9e\u73b0\u6700\u5c0f\u4fb5\u5165\u6027\u548c\u53ef\u91cd\u590d\u6027\u7684\u7ec4\u7ec7\u91c7\u96c6\u3002PI\u8584\u819c\u5728\u79fb\u52a8\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u5e73\u76f4\uff0c\u4f46\u5728\u7531\u51f8\u8f6e\u9a71\u52a8\u7684\u62c9\u4f38\u4f5c\u7528\u4e0b\u4f1a\u53d8\u5f62\u4e3a\u5c16\u9510\u7a81\u8d77\uff0c\u4ece\u800c\u5141\u8bb8\u63a7\u5236\u7a7f\u900f\u7136\u540e\u8fdb\u884c\u65cb\u8f6c\u522e\u64e6\uff0c\u6536\u96c6\u5230\u7684\u6837\u672c\u4fdd\u5b58\u5728\u5185\u90e8\u6247\u5f62\u7a7a\u8154\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cPI\u8584\u819c\u8868\u73b0\u51fa\u7ea620 MPa\u7684\u6768\u6c0f\u6a21\u91cf\u4ee5\u53ca\u572815%\u5e94\u53d8\u4e0b\u7684\u7a33\u5b9a\u5c55\u5f00\u89d2\u5ea6\u7ea6\u4e3a34\u5ea6\u3002\u79bb\u4f53\u732a\u6a21\u578b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u88c5\u7f6e\u80fd\u8fbe\u5230\u6d45\u8868\u6e17\u900f\u6df1\u5ea6\uff08\u4e2d\u4f4d\u6570\u7ea60.61\u6beb\u7c73\uff09\uff0c\u5e76\u4e14\u83b7\u5f97\u7684\u6d3b\u68c0\u6837\u672c\u91cf\u4e0e\u6807\u51c6\u6d3b\u68c0\u94b3\u76f8\u5f53\uff08\u80c3\u90e8\u5e73\u5747\u7ea610.9\u6beb\u514b\uff0c\u80a0\u90e8\u7ea618.9\u6beb\u514b\uff09\u3002\u6b64\u5916\uff0c\u65bd\u52a0\u7684\u529b\u91cf\u4e5f\u5904\u4e8eGI\u6d3b\u68c0\u62a5\u544a\u7684\u5b89\u5168\u8303\u56f4\u5185\u3002", "conclusion": "Kiri-Capsule\u586b\u8865\u4e86\u88ab\u52a8\u6210\u50cf\u4e0e\u529f\u80fd\u6027\u6d3b\u68c0\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u541e\u54bd\u3001\u6df1\u5ea6\u53ef\u63a7\u4e14\u9002\u5408\u75c5\u7406\u5206\u6790\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u57fa\u4e8e\u80f6\u56ca\u7684\u8bca\u65ad\u6280\u672f\u671d\u7740\u66f4\u5b89\u5168\u6709\u6548\u7684\u4e34\u5e8a\u5e94\u7528\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2602.06265", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06265", "abs": "https://arxiv.org/abs/2602.06265", "authors": ["JaeHyung Jang", "JuYeong Seo", "Dae-Young Lee", "Jee-Hwan Ryu"], "title": "MORPH Wheel: A Passive Variable-Radius Wheel Embedding Mechanical Behavior Logic for Input-Responsive Transformation", "comment": "14 pages, 16 figures. Under review at IEEE Transactions on Robotics", "summary": "This paper introduces the Mechacnially prOgrammed Radius-adjustable PHysical (MORPH) wheel, a fully passive variable-radius wheel that embeds mechanical behavior logic for torque-responsive transformation. Unlike conventional variable transmission systems relying on actuators, sensors, and active control, the MORPH wheel achieves passive adaptation solely through its geometry and compliant structure. The design integrates a torque-response coupler and spring-loaded connecting struts to mechanically adjust the wheel radius between 80 mm and 45 mm in response to input torque, without any electrical components. The MORPH wheel provides three unique capabilities rarely achieved simultaneously in previous passive designs: (1) bidirectional operation with unlimited rotation through a symmetric coupler; (2) high torque capacity exceeding 10 N with rigid power transmission in drive mode; and (3) precise and repeatable transmission ratio control governed by deterministic kinematics. A comprehensive analytical model was developed to describe the wheel's mechanical behavior logic, establishing threshold conditions for mode switching between direct drive and radius transformation. Experimental validation confirmed that the measured torque-radius and force-displacement characteristics closely follow theoretical predictions across wheel weights of 1.8-2.8kg. Robot-level demonstrations on varying loads (0-25kg), slopes, and unstructured terrains further verified that the MORPH wheel passively adjusts its radius to provide optimal transmission ratio. The MORPH wheel exemplifies a mechanically programmed structure, embedding intelligent, context-dependent behavior directly into its physical design. This approach offers a new paradigm for passive variable transmission and mechanical intelligence in robotic mobility systems operating in unpredictable or control-limited environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aMORPH\u8f6e\u7684\u5168\u88ab\u52a8\u53ef\u53d8\u534a\u5f84\u8f6e\uff0c\u901a\u8fc7\u5176\u51e0\u4f55\u7ed3\u6784\u548c\u987a\u5e94\u6027\u7ed3\u6784\u5b9e\u73b0\u5bf9\u626d\u77e9\u54cd\u5e94\u7684\u8f6c\u6362\uff0c\u65e0\u9700\u4efb\u4f55\u7535\u5b50\u7ec4\u4ef6\u3002\u5b83\u5177\u6709\u53cc\u5411\u64cd\u4f5c\u3001\u9ad8\u626d\u77e9\u5bb9\u91cf\u4ee5\u53ca\u7cbe\u786e\u4e14\u53ef\u91cd\u590d\u7684\u4f20\u52a8\u6bd4\u63a7\u5236\u7b49\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u8d1f\u8f7d\u3001\u5761\u5ea6\u548c\u975e\u7ed3\u6784\u5316\u5730\u5f62\u4e0a\u80fd\u591f\u81ea\u52a8\u8c03\u6574\u534a\u5f84\u4ee5\u63d0\u4f9b\u6700\u4f73\u4f20\u52a8\u6bd4\u7684\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u4f20\u7edf\u53d8\u901f\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u6267\u884c\u5668\u3001\u4f20\u611f\u5668\u548c\u4e3b\u52a8\u63a7\u5236\u7684\u9650\u5236\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u8bbe\u8ba1\u601d\u8def\u2014\u2014\u5229\u7528\u673a\u68b0\u7f16\u7a0b\u903b\u8f91\u6765\u5f00\u53d1\u4e00\u79cd\u5b8c\u5168\u88ab\u52a8\u5f0f\u7684\u53ef\u53d8\u534a\u5f84\u8f6e\uff08MORPH\u8f6e\uff09\uff0c\u65e8\u5728\u4e3a\u673a\u5668\u4eba\u79fb\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e00\u79cd\u9002\u5e94\u4e0d\u53ef\u9884\u6d4b\u6216\u63a7\u5236\u53d7\u9650\u73af\u5883\u7684\u65b0\u8303\u5f0f\u3002", "method": "MORPH\u8f6e\u7684\u8bbe\u8ba1\u7ed3\u5408\u4e86\u4e00\u4e2a\u626d\u77e9\u54cd\u5e94\u8026\u5408\u5668\u4e0e\u5f39\u7c27\u52a0\u8f7d\u8fde\u63a5\u6746\uff0c\u4f7f\u5f97\u8f6e\u5b50\u53ef\u4ee5\u6839\u636e\u8f93\u5165\u626d\u77e9\u7684\u53d8\u5316\uff0c\u5728\u6ca1\u6709\u4efb\u4f55\u7535\u6c14\u5143\u4ef6\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u52a8\u5c06\u5176\u534a\u5f84\u4ece80\u6beb\u7c73\u8c03\u6574\u523045\u6beb\u7c73\u3002\u6b64\u5916\uff0c\u8fd8\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5206\u6790\u6a21\u578b\u6765\u63cf\u8ff0\u8f66\u8f6e\u7684\u673a\u68b0\u884c\u4e3a\u903b\u8f91\uff0c\u786e\u5b9a\u4e86\u76f4\u63a5\u9a71\u52a8\u6a21\u5f0f\u4e0e\u534a\u5f84\u53d8\u6362\u6a21\u5f0f\u4e4b\u95f4\u5207\u6362\u7684\u9608\u503c\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMORPH\u8f6e\u7684\u5b9e\u9645\u626d\u77e9-\u534a\u5f84\u53ca\u529b-\u4f4d\u79fb\u7279\u6027\u4e0e\u7406\u8bba\u9884\u6d4b\u9ad8\u5ea6\u543b\u5408\uff1b\u5e76\u4e14\u5728\u4e0d\u540c\u91cd\u91cf\uff080-25kg\uff09\u3001\u659c\u7387\u548c\u975e\u7ed3\u6784\u5316\u5730\u5f62\u4e0a\u7684\u673a\u5668\u4eba\u7ea7\u6f14\u793a\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86MORPH\u8f6e\u80fd\u591f\u6839\u636e\u9700\u8981\u81ea\u52a8\u8c03\u8282\u5176\u534a\u5f84\u5927\u5c0f\u4ee5\u8fbe\u5230\u6700\u4f18\u4f20\u52a8\u6bd4\u3002", "conclusion": "MORPH\u8f6e\u4ee3\u8868\u4e86\u4e00\u79cd\u673a\u68b0\u7f16\u7a0b\u7ed3\u6784\u7684\u6210\u529f\u6848\u4f8b\uff0c\u5c06\u667a\u80fd\u3001\u60c5\u5883\u76f8\u5173\u7684\u884c\u4e3a\u76f4\u63a5\u5d4c\u5165\u5230\u4e86\u7269\u7406\u8bbe\u8ba1\u4e2d\u3002\u8fd9\u79cd\u521b\u65b0\u65b9\u6cd5\u4e3a\u88ab\u52a8\u5f0f\u53ef\u53d8\u4f20\u52a8\u548c\u673a\u5668\u4eba\u79fb\u52a8\u7cfb\u7edf\u4e2d\u7684\u673a\u68b0\u667a\u80fd\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2602.06294", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06294", "abs": "https://arxiv.org/abs/2602.06294", "authors": ["Jakub F. Kowalewski", "Abdulaziz O. Alrashed", "Jacob Alpert", "Rishi Ponnapalli", "Lucas R. Meza", "Jeffrey Ian Lipton"], "title": "Robots That Generate Planarity Through Geometry", "comment": null, "summary": "Constraining motion to a flat surface is a fundamental requirement for equipment across science and engineering. Modern precision robotic motion systems, such as gantries, rely on the flatness of components, including guide rails and granite surface plates. However, translating this static flatness into motion requires precise internal alignment and tight-tolerance components that create long, error-sensitive reference chains. Here, we show that by using the geometric inversion of a sphere into a plane, we can produce robotic motion systems that derive planarity entirely from link lengths and connectivity. This allows planar motion to emerge from self-referencing geometric constraints, and without external metrology. We demonstrate these Flat-Plane Mechanisms (FPMs) from micron to meter scales and show that fabrication errors can be attenuated by an order of magnitude in the resulting flatness. Finally, we present a robotic FPM-based 3-axis positioning system that can be used for metrology surface scans ($\\pm 12$-mm) and 3D printing inside narrow containers. This work establishes an alternative geometric foundation for planar motion that can be realized across size scales and opens new possibilities in metrology, fabrication, and micro-positioning.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u901a\u8fc7\u7403\u4f53\u51e0\u4f55\u53cd\u6f14\u4e3a\u5e73\u9762\u7684\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86\u5e73\u9762\u8fd0\u52a8\u7cfb\u7edf\uff08FPMs\uff09\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5b8c\u5168\u4f9d\u9760\u94fe\u63a5\u957f\u5ea6\u548c\u8fde\u63a5\u6027\u6765\u5b9e\u73b0\u5e73\u9762\u6027\uff0c\u800c\u65e0\u9700\u5916\u90e8\u8ba1\u91cf\u3002\u8fd9\u79cd\u673a\u5236\u80fd\u591f\u5728\u5fae\u7c73\u5230\u7c73\u7ea7\u5c3a\u5ea6\u4e0a\u5de5\u4f5c\uff0c\u5e76\u4e14\u80fd\u591f\u663e\u8457\u51cf\u5c11\u5236\u9020\u8bef\u5dee\u5bf9\u5e73\u9762\u5ea6\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u4e00\u4e2a\u57fa\u4e8eFPM\u76843\u8f74\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u53ef\u7528\u4e8e\u8868\u9762\u626b\u63cf\u548c\u7a84\u5bb9\u5668\u5185\u76843D\u6253\u5370\u3002", "motivation": "\u4f20\u7edf\u7684\u7cbe\u5bc6\u673a\u5668\u4eba\u8fd0\u52a8\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u7ec4\u4ef6\uff08\u5982\u5bfc\u8f68\u548c\u82b1\u5c97\u5ca9\u5e73\u677f\uff09\u7684\u9759\u6001\u5e73\u5766\u5ea6\u8f6c\u6362\u6210\u8fd0\u52a8\u65f6\u9700\u8981\u7cbe\u786e\u5185\u90e8\u5bf9\u9f50\u53ca\u9ad8\u7cbe\u5ea6\u7ec4\u4ef6\uff0c\u8fd9\u5bfc\u81f4\u4e86\u957f\u4e14\u5bb9\u6613\u51fa\u9519\u7684\u53c2\u8003\u94fe\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5373\u5229\u7528\u7403\u4f53\u5230\u5e73\u9762\u7684\u51e0\u4f55\u53cd\u6f14\u6765\u6784\u5efa\u5e73\u9762\u8fd0\u52a8\u7cfb\u7edf\uff0c\u65e8\u5728\u7b80\u5316\u7ed3\u6784\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u8005\u4eec\u91c7\u7528\u4e86\u7403\u4f53\u81f3\u5e73\u9762\u7684\u51e0\u4f55\u53cd\u6f14\u6280\u672f\uff0c\u5f00\u53d1\u51fa\u4e86\u6240\u8c13\u7684\u5e73\u9762\u673a\u5236\uff08Flat-Plane Mechanisms, FPMs\uff09\u3002\u8fd9\u79cd\u65b9\u6cd5\u8ba9\u5e73\u9762\u8fd0\u52a8\u76f4\u63a5\u4ece\u81ea\u53c2\u7167\u51e0\u4f55\u7ea6\u675f\u4e2d\u4ea7\u751f\uff0c\u65e0\u9700\u4f9d\u8d56\u590d\u6742\u7684\u5916\u90e8\u6d4b\u91cf\u624b\u6bb5\u3002\u4ed6\u4eec\u5c55\u793a\u4e86\u8be5\u673a\u5236\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0a\u7684\u5e94\u7528\u6848\u4f8b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5bf9\u4e8e\u51cf\u5c0f\u56e0\u5236\u9020\u7f3a\u9677\u5bfc\u81f4\u7684\u5e73\u9762\u5ea6\u8bef\u5dee\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5fae\u7c73\u81f3\u7c73\u7684\u4e0d\u540c\u5c3a\u5ea6\u4e0a\uff0cFPMs\u5747\u80fd\u6709\u6548\u8fd0\u4f5c\uff0c\u5e76\u4e14\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u591f\u5c06\u7531\u4e8e\u5236\u9020\u8bef\u5dee\u5f15\u8d77\u7684\u5e73\u9762\u5ea6\u504f\u5dee\u964d\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002\u6b64\u5916\uff0c\u57fa\u4e8eFPM\u6784\u5efa\u76843\u8f74\u5b9a\u4f4d\u7cfb\u7edf\u4e0d\u4ec5\u9002\u7528\u4e8e\u8868\u9762\u626b\u63cf\u4efb\u52a1\uff0c\u8fd8\u80fd\u7528\u4e8e\u72ed\u5c0f\u7a7a\u95f4\u5185\u76843D\u6253\u5370\u4f5c\u4e1a\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5e73\u9762\u8fd0\u52a8\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u51e0\u4f55\u57fa\u7840\uff0c\u5b83\u53ef\u4ee5\u5728\u591a\u79cd\u5c3a\u5bf8\u8303\u56f4\u5185\u5b9e\u73b0\uff0c\u5e76\u4e3a\u8ba1\u91cf\u5b66\u3001\u5236\u9020\u4ee5\u53ca\u5fae\u5b9a\u4f4d\u7b49\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u673a\u9047\u3002"}}
{"id": "2602.06296", "categories": ["cs.RO", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.06296", "abs": "https://arxiv.org/abs/2602.06296", "authors": ["Takeshi Ishida"], "title": "Internalized Morphogenesis: A Self-Organizing Model for Growth, Replication, and Regeneration via Local Token Exchange in Modular Systems", "comment": null, "summary": "This study presents an internalized morphogenesis model for autonomous systems, such as swarm robotics and micro-nanomachines, that eliminates the need for external spatial computation. Traditional self-organizing models often require calculations across the entire coordinate space, including empty areas, which is impractical for resource-constrained physical modules. Our proposed model achieves complex morphogenesis through strictly local interactions between adjacent modules within the \"body.\" By extending the \"Ishida token model,\" modules exchange integer values using an RD-inspired discrete analogue without solving differential equations. The internal potential, derived from token accumulation and aging, guides autonomous growth, shrinkage, and replication. Simulations on a hexagonal grid demonstrated the emergence of limb-like extensions, self-division, and robust regeneration capabilities following structural amputation. A key feature is the use of the body boundary as a natural sink for information entropy (tokens) to maintain a dynamic equilibrium. These results indicate that sophisticated morphological behaviors can emerge from minimal, internal-only rules. This framework offers a computationally efficient and biologically plausible approach to developing self-repairing, adaptive, and autonomous hardware.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5185\u5316\u5f62\u6001\u53d1\u751f\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u7fa4\u673a\u5668\u4eba\u548c\u5fae\u7eb3\u7c73\u673a\u5668\u7b49\u81ea\u4e3b\u7cfb\u7edf\uff0c\u901a\u8fc7\u76f8\u90bb\u6a21\u5757\u95f4\u7684\u5c40\u90e8\u4ea4\u4e92\u5b9e\u73b0\u590d\u6742\u5f62\u6001\u7684\u53d1\u751f\uff0c\u65e0\u9700\u5916\u90e8\u7a7a\u95f4\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u81ea\u7ec4\u7ec7\u6a21\u578b\u901a\u5e38\u9700\u8981\u5728\u6574\u4e2a\u5750\u6807\u7a7a\u95f4\uff08\u5305\u62ec\u7a7a\u767d\u533a\u57df\uff09\u8fdb\u884c\u8ba1\u7b97\uff0c\u8fd9\u5bf9\u8d44\u6e90\u53d7\u9650\u7684\u7269\u7406\u6a21\u5757\u6765\u8bf4\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u4ec5\u4f9d\u8d56\u5185\u90e8\u89c4\u5219\u3001\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u81ea\u4fee\u590d\u3001\u9002\u5e94\u6027\u548c\u81ea\u4e3b\u786c\u4ef6\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u201cIshida\u4ee4\u724c\u6a21\u578b\u201d\uff0c\u6a21\u5757\u95f4\u4f7f\u7528\u53d7RD\u542f\u53d1\u7684\u79bb\u6563\u6a21\u62df\u65b9\u6cd5\u4ea4\u6362\u6574\u6570\u503c\uff0c\u4e0d\u9700\u8981\u89e3\u5fae\u5206\u65b9\u7a0b\u3002\u4f53\u5185\u79ef\u7d2f\u4e0e\u8001\u5316\u4ea7\u751f\u7684\u4ee4\u724c\u51b3\u5b9a\u4e86\u81ea\u4e3b\u751f\u957f\u3001\u6536\u7f29\u53ca\u590d\u5236\u7684\u884c\u4e3a\u3002", "result": "\u5728\u516d\u89d2\u5f62\u7f51\u683c\u4e0a\u7684\u6a21\u62df\u663e\u793a\u4e86\u7c7b\u4f3c\u80a2\u4f53\u7684\u5ef6\u4f38\u3001\u81ea\u6211\u5206\u88c2\u4ee5\u53ca\u7ed3\u6784\u622a\u80a2\u540e\u7684\u5f3a\u5927\u518d\u751f\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8eab\u4f53\u8fb9\u754c\u4f5c\u4e3a\u4fe1\u606f\u71b5\uff08\u4ee4\u724c\uff09\u7684\u81ea\u7136\u6c47\u6765\u7ef4\u6301\u52a8\u6001\u5e73\u8861\u88ab\u89c6\u4e3a\u4e00\u4e2a\u91cd\u8981\u7279\u5f81\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u590d\u6742\u7684\u5f62\u6001\u884c\u4e3a\u53ef\u4ee5\u4ece\u6700\u5c11\u7684\u3001\u4ec5\u5185\u90e8\u7684\u89c4\u5219\u4e2d\u6d8c\u73b0\u51fa\u6765\u3002\u8fd9\u79cd\u6846\u67b6\u4e3a\u5f00\u53d1\u81ea\u4fee\u590d\u3001\u9002\u5e94\u6027\u53ca\u81ea\u4e3b\u786c\u4ef6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u9014\u5f84\u3002"}}
{"id": "2602.06339", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06339", "abs": "https://arxiv.org/abs/2602.06339", "authors": ["Harold Soh", "Eugene Lim"], "title": "Action Hallucination in Generative Visual-Language-Action Models", "comment": "22 pages", "summary": "Robot Foundation Models such as Vision-Language-Action models are rapidly reshaping how robot policies are trained and deployed, replacing hand-designed planners with end-to-end generative action models. While these systems demonstrate impressive generalization, it remains unclear whether they fundamentally resolve the long-standing challenges of robotics. We address this question by analyzing action hallucinations that violate physical constraints and their extension to plan-level failures. Focusing on latent-variable generative policies, we show that hallucinations often arise from structural mismatches between feasible robot behavior and common model architectures. We study three such barriers -- topological, precision, and horizon -- and show how they impose unavoidable tradeoffs. Our analysis provides mechanistic explanations for reported empirical failures of generative robot policies and suggests principled directions for improving reliability and trustworthiness, without abandoning their expressive power.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\uff08\u5982\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff09\u5728\u8bad\u7ec3\u548c\u90e8\u7f72\u673a\u5668\u4eba\u7b56\u7565\u65b9\u9762\u7684\u5f71\u54cd\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u7cfb\u7edf\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u662f\u5426\u771f\u6b63\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u957f\u671f\u6311\u6218\u4ecd\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u805a\u7126\u4e8e\u751f\u6210\u6027\u653f\u7b56\u4e2d\u51fa\u73b0\u7684\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\u7684\u884c\u4e3a\u5e7b\u89c9\u53ca\u5176\u5bf9\u8ba1\u5212\u5c42\u9762\u5931\u8d25\u7684\u5f71\u54cd\uff0c\u5e76\u63ed\u793a\u4e86\u53ef\u884c\u673a\u5668\u4eba\u884c\u4e3a\u4e0e\u5e38\u89c1\u6a21\u578b\u67b6\u6784\u4e4b\u95f4\u5b58\u5728\u7684\u7ed3\u6784\u4e0d\u5339\u914d\u95ee\u9898\u3002\u901a\u8fc7\u5206\u6790\u4e09\u79cd\u969c\u788d\u2014\u2014\u62d3\u6251\u969c\u788d\u3001\u7cbe\u5ea6\u969c\u788d\u548c\u89c6\u91ce\u969c\u788d\u2014\u2014\u6307\u51fa\u5b83\u4eec\u7ed9\u6a21\u578b\u5e26\u6765\u7684\u4e0d\u53ef\u907f\u514d\u7684\u6743\u8861\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7b49\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u90e8\u7f72\u673a\u5668\u4eba\u7b56\u7565\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u80fd\u591f\u6839\u672c\u89e3\u51b3\u673a\u5668\u4eba\u9886\u57df\u5185\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5173\u4e8e\u8fdd\u53cd\u7269\u7406\u9650\u5236\u7684\u52a8\u4f5c\u5e7b\u89c9\u4ee5\u53ca\u7531\u6b64\u5f15\u53d1\u7684\u4efb\u52a1\u7ea7\u5931\u8d25\u95ee\u9898\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5bf9\u6f5c\u53d8\u91cf\u751f\u6210\u7b56\u7565\u7684\u7814\u7a76\uff0c\u7279\u522b\u5173\u6ce8\u5bfc\u81f4\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\u884c\u4e3a\u5e7b\u89c9\u53d1\u751f\u7684\u7ed3\u6784\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5e76\u4ece\u62d3\u6251\u969c\u788d\u3001\u7cbe\u5ea6\u969c\u788d\u53ca\u89c6\u91ce\u969c\u788d\u4e09\u4e2a\u65b9\u9762\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u53ef\u884c\u7684\u673a\u5668\u4eba\u884c\u4e3a\u4e0e\u5f53\u524d\u666e\u904d\u4f7f\u7528\u7684\u6a21\u578b\u67b6\u6784\u95f4\u5b58\u5728\u7ed3\u6784\u6027\u4e0d\u5339\u914d\uff0c\u8fd9\u4e09\u5927\u969c\u788d\u5bfc\u81f4\u4e86\u4e0d\u53ef\u907f\u514d\u7684\u6298\u8877\u9009\u62e9\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5df2\u62a5\u544a\u7684\u751f\u6210\u5f0f\u673a\u5668\u4eba\u7b56\u7565\u7684\u7ecf\u9a8c\u6027\u5931\u8d25\u63d0\u4f9b\u4e86\u673a\u5236\u4e0a\u7684\u89e3\u91ca\uff0c\u5e76\u6307\u51fa\u4e86\u5728\u4fdd\u6301\u5176\u8868\u73b0\u529b\u7684\u540c\u65f6\u63d0\u9ad8\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u7684\u539f\u5219\u6027\u65b9\u5411\u3002"}}
{"id": "2602.06341", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06341", "abs": "https://arxiv.org/abs/2602.06341", "authors": ["Zhanxiang Cao", "Liyun Yan", "Yang Zhang", "Sirui Chen", "Jianming Ma", "Tianyue Zhan", "Shengcheng Fu", "Yufei Jia", "Cewu Lu", "Yue Gao"], "title": "HiWET: Hierarchical World-Frame End-Effector Tracking for Long-Horizon Humanoid Loco-Manipulation", "comment": null, "summary": "Humanoid loco-manipulation requires executing precise manipulation tasks while maintaining dynamic stability amid base motion and impacts. Existing approaches typically formulate commands in body-centric frames, fail to inherently correct cumulative world-frame drift induced by legged locomotion. We reformulate the problem as world-frame end-effector tracking and propose HiWET, a hierarchical reinforcement learning framework that decouples global reasoning from dynamic execution. The high-level policy generates subgoals that jointly optimize end-effector accuracy and base positioning in the world frame, while the low-level policy executes these commands under stability constraints. We introduce a Kinematic Manifold Prior (KMP) that embeds the manipulation manifold into the action space via residual learning, reducing exploration dimensionality and mitigating kinematically invalid behaviors. Extensive simulation and ablation studies demonstrate that HiWET achieves precise and stable end-effector tracking in long-horizon world-frame tasks. We validate zero-shot sim-to-real transfer of the low-level policy on a physical humanoid, demonstrating stable locomotion under diverse manipulation commands. These results indicate that explicit world-frame reasoning combined with hierarchical control provides an effective and scalable solution for long-horizon humanoid loco-manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6HiWET\uff0c\u65e8\u5728\u89e3\u51b3\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u6267\u884c\u7cbe\u786e\u64cd\u4f5c\u4efb\u52a1\u65f6\u4fdd\u6301\u52a8\u6001\u7a33\u5b9a\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5c06\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e16\u754c\u5750\u6807\u7cfb\u4e0b\u7684\u672b\u7aef\u6267\u884c\u5668\u8ddf\u8e2a\uff0c\u5e76\u91c7\u7528\u9ad8\u3001\u4f4e\u7ea7\u7b56\u7565\u5206\u522b\u8fdb\u884c\u5168\u5c40\u63a8\u7406\u548c\u52a8\u6001\u6267\u884c\uff0c\u7ed3\u5408\u52a8\u529b\u5b66\u7ea6\u675f\u548c\u51cf\u5c11\u63a2\u7d22\u7ef4\u5ea6\u7684\u8fd0\u52a8\u5b66\u6d41\u5f62\u5148\u9a8c(KMP)\uff0c\u5b9e\u73b0\u4e86\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u548c\u7a33\u5b9a\u7684\u672b\u7aef\u6267\u884c\u5668\u8ddf\u8e2a\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u4ece\u6a21\u62df\u5230\u771f\u5b9e\u73af\u5883\u96f6\u6837\u672c\u8fc1\u79fb\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u673a\u4f53\u4e2d\u5fc3\u5750\u6807\u7cfb\u6765\u5236\u5b9a\u547d\u4ee4\uff0c\u96be\u4ee5\u81ea\u52a8\u7ea0\u6b63\u7531\u817f\u90e8\u79fb\u52a8\u5f15\u8d77\u7684\u7d2f\u79ef\u7684\u4e16\u754c\u5750\u6807\u7cfb\u6f02\u79fb\u95ee\u9898\u3002\u4e3a\u4e86\u63d0\u9ad8\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u6267\u884c\u7cbe\u786e\u64cd\u4f5c\u4efb\u52a1\u65f6\u7684\u6574\u4f53\u7a33\u5b9a\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u8003\u8651\u57fa\u5ea7\u79fb\u52a8\u548c\u51b2\u51fb\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHiWET\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u5c06\u5168\u5c40\u63a8\u7406\u4e0e\u52a8\u6001\u6267\u884c\u5206\u5f00\u5904\u7406\u3002\u9ad8\u5c42\u7b56\u7565\u751f\u6210\u5b50\u76ee\u6807\uff0c\u8fd9\u4e9b\u5b50\u76ee\u6807\u540c\u65f6\u4f18\u5316\u4e86\u4e16\u754c\u5750\u6807\u7cfb\u4e2d\u672b\u7aef\u6267\u884c\u5668\u7684\u7cbe\u5ea6\u548c\u57fa\u5ea7\u4f4d\u7f6e\uff1b\u800c\u4f4e\u5c42\u7b56\u7565\u5219\u5728\u6ee1\u8db3\u7a33\u5b9a\u6027\u7ea6\u675f\u6761\u4ef6\u4e0b\u6267\u884c\u8fd9\u4e9b\u547d\u4ee4\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u8fd0\u52a8\u5b66\u6d41\u5f62\u5148\u9a8c(Kinematic Manifold Prior, KMP)\uff0c\u901a\u8fc7\u6b8b\u5dee\u5b66\u4e60\u5c06\u64cd\u4f5c\u6d41\u5f62\u5d4c\u5165\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u63a2\u7d22\u7ef4\u5ea6\u5e76\u907f\u514d\u4e86\u65e0\u6548\u7684\u52a8\u529b\u5b66\u884c\u4e3a\u3002", "result": "\u5e7f\u6cdb\u7684\u4eff\u771f\u7814\u7a76\u548c\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\uff0cHiWET\u80fd\u591f\u5728\u957f\u65f6\u57df\u4e16\u754c\u5750\u6807\u7cfb\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7cbe\u786e\u4e14\u7a33\u5b9a\u7684\u672b\u7aef\u6267\u884c\u5668\u8ddf\u8e2a\u3002\u7269\u7406\u7c7b\u4eba\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u4f4e\u5c42\u7b56\u7565\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u73af\u5883\u96f6\u6837\u672c\u8fc1\u79fb\u7684\u6210\u529f\uff0c\u8bc1\u660e\u4e86\u5728\u591a\u79cd\u64cd\u4f5c\u547d\u4ee4\u4e0b\u80fd\u591f\u4fdd\u6301\u7a33\u5b9a\u7684\u884c\u8d70\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u660e\u786e\u5730\u5728\u4e16\u754c\u5750\u6807\u7cfb\u4e2d\u8fdb\u884c\u63a8\u7406\u52a0\u4e0a\u5206\u5c42\u63a7\u5236\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4e3a\u957f\u65f6\u95f4\u7c7b\u4eba\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e00\u4e2a\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06366", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06366", "abs": "https://arxiv.org/abs/2602.06366", "authors": ["Teresa Yeo", "Dulaj Weerakoon", "Dulanga Weerakoon", "Archan Misra"], "title": "Towards Adaptive Environment Generation for Training Embodied Agents", "comment": "Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification", "summary": "Embodied agents struggle to generalize to new environments, even when those environments share similar underlying structures to their training settings. Most current approaches to generating these training environments follow an open-loop paradigm, without considering the agent's current performance. While procedural generation methods can produce diverse scenes, diversity without feedback from the agent is inefficient. The generated environments may be trivially easy, providing limited learning signal. To address this, we present a proof-of-concept for closed-loop environment generation that adapts difficulty to the agent's current capabilities. Our system employs a controllable environment representation, extracts fine-grained performance feedback beyond binary success or failure, and implements a closed-loop adaptation mechanism that translates this feedback into environment modifications. This feedback-driven approach generates training environments that more challenging in the ways the agent needs to improve, enabling more efficient learning and better generalization to novel settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u95ed\u73af\u73af\u5883\u751f\u6210\u7684\u6982\u5ff5\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u667a\u80fd\u4f53\u5f53\u524d\u7684\u80fd\u529b\u8c03\u6574\u96be\u5ea6\uff0c\u901a\u8fc7\u4ece\u667a\u80fd\u4f53\u83b7\u53d6\u7ec6\u7c92\u5ea6\u7684\u6027\u80fd\u53cd\u9988\uff0c\u5e76\u5c06\u8fd9\u4e9b\u53cd\u9988\u8f6c\u5316\u4e3a\u73af\u5883\u4fee\u6539\uff0c\u4ece\u800c\u751f\u6210\u66f4\u5177\u6311\u6218\u6027\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u4fc3\u8fdb\u66f4\u6709\u6548\u7684\u5b66\u4e60\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u751f\u6210\u8bad\u7ec3\u73af\u5883\u7684\u65b9\u6cd5\u90fd\u662f\u5f00\u73af\u7684\uff0c\u6ca1\u6709\u8003\u8651\u5230\u667a\u80fd\u4f53\u5f53\u524d\u7684\u8868\u73b0\u3002\u867d\u7136\u7a0b\u5e8f\u751f\u6210\u65b9\u6cd5\u53ef\u4ee5\u4ea7\u751f\u591a\u6837\u5316\u7684\u573a\u666f\uff0c\u4f46\u5982\u679c\u7f3a\u4e4f\u6765\u81ea\u667a\u80fd\u4f53\u7684\u53cd\u9988\uff0c\u8fd9\u79cd\u591a\u6837\u6027\u662f\u4f4e\u6548\u7684\u3002\u751f\u6210\u7684\u73af\u5883\u53ef\u80fd\u8fc7\u4e8e\u7b80\u5355\uff0c\u63d0\u4f9b\u7684\u5b66\u4e60\u4fe1\u53f7\u6709\u9650\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u95ed\u73af\u73af\u5883\u751f\u6210\u7684\u6982\u5ff5\u9a8c\u8bc1\uff0c\u65e8\u5728\u4f7f\u73af\u5883\u96be\u5ea6\u9002\u5e94\u4e8e\u667a\u80fd\u4f53\u5f53\u524d\u7684\u80fd\u529b\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u53ef\u63a7\u73af\u5883\u8868\u793a\u6cd5\uff0c\u63d0\u53d6\u4e86\u8d85\u8d8a\u4e8c\u5143\u6210\u529f\u6216\u5931\u8d25\u4e4b\u5916\u7684\u7ec6\u7c92\u5ea6\u6027\u80fd\u53cd\u9988\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u95ed\u73af\u9002\u5e94\u673a\u5236\uff0c\u8be5\u673a\u5236\u80fd\u591f\u5c06\u8fd9\u4e9b\u53cd\u9988\u8f6c\u5316\u4e3a\u5bf9\u73af\u5883\u7684\u5177\u4f53\u4fee\u6539\u3002", "result": "\u901a\u8fc7\u8fd9\u79cd\u53cd\u9988\u9a71\u52a8\u7684\u65b9\u6cd5\u6240\u751f\u6210\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u5728\u667a\u80fd\u4f53\u9700\u8981\u6539\u8fdb\u7684\u65b9\u5411\u4e0a\u53d8\u5f97\u66f4\u52a0\u5177\u6709\u6311\u6218\u6027\uff0c\u4fc3\u8fdb\u4e86\u66f4\u52a0\u9ad8\u6548\u7684\u5b66\u4e60\u8fc7\u7a0b\u4ee5\u53ca\u5bf9\u4e8e\u65b0\u73af\u5883\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u95ed\u73af\u73af\u5883\u751f\u6210\u7b56\u7565\uff0c\u5b83\u80fd\u591f\u6839\u636e\u667a\u80fd\u4f53\u7684\u5b9e\u9645\u8868\u73b0\u81ea\u52a8\u8c03\u6574\u73af\u5883\u7684\u96be\u5ea6\uff0c\u8fdb\u800c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u5e76\u589e\u5f3a\u667a\u80fd\u4f53\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.06380", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.06380", "abs": "https://arxiv.org/abs/2602.06380", "authors": ["Xinran Li", "Shuaikang Zheng", "Pengcheng Zheng", "Xinyang Wang", "Jiacheng Li", "Zhitian Li", "Xudong Zou"], "title": "A Consistency-Improved LiDAR-Inertial Bundle Adjustment", "comment": null, "summary": "Simultaneous Localization and Mapping (SLAM) using 3D LiDAR has emerged as a cornerstone for autonomous navigation in robotics. While feature-based SLAM systems have achieved impressive results by leveraging edge and planar structures, they often suffer from the inconsistent estimator associated with feature parameterization and estimated covariance. In this work, we present a consistency-improved LiDAR-inertial bundle adjustment (BA) with tailored parameterization and estimator. First, we propose a stereographic-projection representation parameterizing the planar and edge features, and conduct a comprehensive observability analysis to support its integrability with consistent estimator. Second, we implement a LiDAR-inertial BA with Maximum a Posteriori (MAP) formulation and First-Estimate Jacobians (FEJ) to preserve the accurate estimated covariance and observability properties of the system. Last, we apply our proposed BA method to a LiDAR-inertial odometry.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u4e00\u81f4\u6027\u7684\u6fc0\u5149\u96f7\u8fbe-\u60ef\u6027\u675f\u8c03\u6574\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u7acb\u4f53\u6295\u5f71\u8868\u793a\u53c2\u6570\u5316\u5e73\u9762\u548c\u8fb9\u7f18\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u516c\u5f0f\u4e0e\u9996\u6b21\u4f30\u8ba1\u96c5\u53ef\u6bd4\uff08FEJ\uff09\uff0c\u4ee5\u4fdd\u6301\u7cfb\u7edf\u4f30\u8ba1\u534f\u65b9\u5dee\u7684\u51c6\u786e\u6027\u53ca\u53ef\u89c2\u6d4b\u6027\u5c5e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7279\u5f81\u7684SLAM\u7cfb\u7edf\u867d\u7136\u5229\u7528\u4e86\u8fb9\u7f18\u548c\u5e73\u9762\u7ed3\u6784\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4f46\u5f80\u5f80\u7531\u4e8e\u7279\u5f81\u53c2\u6570\u5316\u548c\u4f30\u8ba1\u534f\u65b9\u5dee\u5bfc\u81f4\u4f30\u8ba1\u5668\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7acb\u4f53\u6295\u5f71\u8868\u793a\u6cd5\u6765\u53c2\u6570\u5316\u5e73\u9762\u548c\u8fb9\u7f18\u7279\u5f81\uff0c\u5e76\u8fdb\u884c\u4e86\u5168\u9762\u7684\u53ef\u89c2\u6d4b\u6027\u5206\u6790\u4ee5\u652f\u6301\u5176\u4e0e\u4e00\u81f4\u6027\u4f30\u8ba1\u5668\u7684\u96c6\u6210\uff1b\u5176\u6b21\uff0c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5e26\u6709\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u516c\u5f0f\u548c\u9996\u6b21\u4f30\u8ba1\u96c5\u53ef\u6bd4\uff08FEJ\uff09\u7684\u6fc0\u5149\u96f7\u8fbe-\u60ef\u6027\u675f\u8c03\u6574\uff08BA\uff09\uff0c\u4ee5\u7ef4\u6301\u4f30\u8ba1\u534f\u65b9\u5dee\u7684\u51c6\u786e\u6027\u548c\u7cfb\u7edf\u7684\u53ef\u89c2\u6d4b\u6027\u8d28\uff1b\u6700\u540e\uff0c\u5c06\u6240\u63d0\u51fa\u7684BA\u65b9\u6cd5\u5e94\u7528\u4e8e\u6fc0\u5149\u96f7\u8fbe-\u60ef\u6027\u91cc\u7a0b\u8ba1\u4e2d\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u7cfb\u7edf\u4f30\u8ba1\u534f\u65b9\u5dee\u51c6\u786e\u6027\u548c\u53ef\u89c2\u6d4b\u6027\u65b9\u9762\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u7ed9\u51fa\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u53c2\u6570\u5316\u65b9\u6cd5\u548c\u4f30\u8ba1\u6280\u672f\uff0c\u672c\u5de5\u4f5c\u4e3a\u89e3\u51b3\u57fa\u4e8e3D LiDAR SLAM\u4e2d\u7684\u4f30\u8ba1\u4e00\u81f4\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.06382", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06382", "abs": "https://arxiv.org/abs/2602.06382", "authors": ["Wandong Sun", "Yongbo Su", "Leoric Huang", "Alex Zhang", "Dwyane Wei", "Mu San", "Daniel Tian", "Ellie Cao", "Finn Yan", "Ethan Xie", "Zongwu Xie"], "title": "Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels", "comment": null, "summary": "Achieving robust vision-based humanoid locomotion remains challenging due to two fundamental issues: the sim-to-real gap introduces significant perception noise that degrades performance on fine-grained tasks, and training a unified policy across diverse terrains is hindered by conflicting learning objectives. To address these challenges, we present an end-to-end framework for vision-driven humanoid locomotion. For robust sim-to-real transfer, we develop a high-fidelity depth sensor simulation that captures stereo matching artifacts and calibration uncertainties inherent in real-world sensing. We further propose a vision-aware behavior distillation approach that combines latent space alignment with noise-invariant auxiliary tasks, enabling effective knowledge transfer from privileged height maps to noisy depth observations. For versatile terrain adaptation, we introduce terrain-specific reward shaping integrated with multi-critic and multi-discriminator learning, where dedicated networks capture the distinct dynamics and motion priors of each terrain type. We validate our approach on two humanoid platforms equipped with different stereo depth cameras. The resulting policy demonstrates robust performance across diverse environments, seamlessly handling extreme challenges such as high platforms and wide gaps, as well as fine-grained tasks including bidirectional long-term staircase traversal.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u9a71\u52a8\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u6df1\u5ea6\u4f20\u611f\u5668\u6a21\u62df\u548c\u89c6\u89c9\u611f\u77e5\u7684\u884c\u4e3a\u84b8\u998f\u65b9\u6cd5\u89e3\u51b3\u4e86\u4ece\u6a21\u62df\u5230\u5b9e\u9645\u73af\u5883\u7684\u8f6c\u6362\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u7279\u5b9a\u5730\u5f62\u5956\u52b1\u5851\u9020\u7ed3\u5408\u591a\u8bc4\u5224\u8005\u548c\u591a\u5224\u522b\u5668\u5b66\u4e60\u4ee5\u9002\u5e94\u591a\u79cd\u5730\u5f62\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7b56\u7565\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u9a71\u52a8\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u4e2d\u7684\u4e24\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u6a21\u62df\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\u5bfc\u81f4\u7cbe\u7ec6\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\uff0c\u4ee5\u53ca\u8de8\u4e0d\u540c\u5730\u5f62\u8bad\u7ec3\u7edf\u4e00\u7b56\u7565\u65f6\u9047\u5230\u7684\u5b66\u4e60\u76ee\u6807\u51b2\u7a81\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u4fdd\u771f\u5ea6\u7684\u6df1\u5ea6\u4f20\u611f\u5668\u6a21\u62df\u6765\u6355\u6349\u7acb\u4f53\u5339\u914d\u4f2a\u5f71\u548c\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\uff1b\u63d0\u51fa\u4e86\u4e00\u4e2a\u89c6\u89c9\u611f\u77e5\u7684\u884c\u4e3a\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u4e0e\u566a\u58f0\u4e0d\u53d8\u7684\u8f85\u52a9\u4efb\u52a1\u76f8\u7ed3\u5408\uff1b\u5f15\u5165\u4e86\u57fa\u4e8e\u7279\u5b9a\u5730\u5f62\u7684\u5956\u52b1\u5851\u9020\u96c6\u6210\u591a\u8bc4\u5224\u8005\u548c\u591a\u5224\u522b\u5668\u5b66\u4e60\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b56\u7565\u5728\u4e24\u79cd\u914d\u5907\u4e86\u4e0d\u540c\u7acb\u4f53\u6df1\u5ea6\u76f8\u673a\u7684\u4eba\u5f62\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u5305\u62ec\u6781\u7aef\u6311\u6218\uff08\u5982\u9ad8\u5e73\u53f0\u548c\u5bbd\u95f4\u9699\uff09\u53ca\u7ec6\u7c92\u5ea6\u4efb\u52a1\uff08\u5982\u53cc\u5411\u957f\u671f\u697c\u68af\u7a7f\u8d8a\uff09\u4e0b\u7684\u9c81\u68d2\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u7aef\u5230\u7aef\u6846\u67b6\u6709\u6548\u5730\u4fc3\u8fdb\u4e86\u89c6\u89c9\u9a71\u52a8\u4e0b\u4eba\u5f62\u673a\u5668\u4eba\u5728\u4ece\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u8f6c\u79fb\u8fc7\u7a0b\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u9002\u5e94\u5404\u79cd\u7c7b\u578b\u7684\u5730\u5f62\u6761\u4ef6\uff0c\u4e3a\u5b9e\u73b0\u66f4\u7075\u6d3b\u3001\u9002\u5e94\u6027\u5f3a\u7684\u4eba\u5f62\u673a\u5668\u4eba\u79fb\u52a8\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06445", "abs": "https://arxiv.org/abs/2602.06445", "authors": ["Weidong Huang", "Jingwen Zhang", "Jiongye Li", "Shibowen Zhang", "Jiayang Wu", "Jiayi Wang", "Hangxin Liu", "Yaodong Yang", "Yao Su"], "title": "ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking", "comment": "IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING. PREPRINT VERSION. ACCEPTED FEB, 2026", "summary": "Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aECO\uff08\u80fd\u91cf\u7ea6\u675f\u4f18\u5316\uff09\u7684\u53d7\u9650\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u80fd\u91cf\u76f8\u5173\u6307\u6807\u4ece\u5956\u52b1\u4e2d\u5206\u79bb\u51fa\u6765\uff0c\u5e76\u5c06\u5176\u91cd\u65b0\u5b9a\u4e49\u4e3a\u663e\u5f0f\u7684\u4e0d\u7b49\u5f0f\u7ea6\u675f\u3002\u5b9e\u9a8c\u8868\u660e\uff0cECO\u76f8\u6bd4\u5176\u4ed6\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u884c\u8d70\u65f6\u7684\u80fd\u91cf\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7a33\u5065\u7684\u884c\u8d70\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u7684\u65b9\u6cd5\u5728\u4f18\u5316\u4eba\u5f62\u673a\u5668\u4eba\u884c\u8d70\u6548\u7387\u65f6\uff0c\u5f80\u5f80\u9700\u8981\u5927\u91cf\u7684\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u5e76\u4e14\u96be\u4ee5\u8fbe\u5230\u6700\u4f18\u7b56\u7565\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u53d7\u9650RL\u6846\u67b6\u2014\u2014ECO\uff0c\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5b9a\u3001\u5bf9\u79f0\u4e14\u80fd\u6548\u66f4\u9ad8\u7684\u884c\u8d70\u6a21\u5f0f\u3002", "method": "ECO\u6846\u67b6\u901a\u8fc7\u5c06\u4e0e\u80fd\u91cf\u76f8\u5173\u7684\u5ea6\u91cf\u6807\u51c6\u4ece\u5956\u52b1\u51fd\u6570\u4e2d\u5206\u79bb\u5e76\u91cd\u6784\u6210\u663e\u5f0f\u7684\u4e0d\u7b49\u5f0f\u7ea6\u675f\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6e05\u6670\u4e14\u53ef\u89e3\u91ca\u7684\u80fd\u91cf\u6210\u672c\u7269\u7406\u8868\u793a\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u91c7\u7528\u4e86\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u6765\u65bd\u52a0\u4e13\u95e8\u9488\u5bf9\u80fd\u91cf\u6d88\u8017\u53ca\u53c2\u8003\u52a8\u4f5c\u7684\u7ea6\u675f\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728BRUCE\u5c0f\u578b\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u7684\u6a21\u62df\u5230\u6a21\u62df\u4ee5\u53ca\u6a21\u62df\u5230\u771f\u5b9e\u73af\u5883\u8fc1\u79fb\u6d4b\u8bd5\u4e2d\uff0cECO\u76f8\u8f83\u4e8eMPC\u3001\u5e26\u6709\u5956\u52b1\u5851\u5f62\u7684\u6807\u51c6RL\u4ee5\u53ca\u5176\u4ed6\u56db\u79cd\u6700\u5148\u8fdb\u7684\u53d7\u9650RL\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u80fd\u91cf\u6d88\u8017\u7684\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u884c\u8d70\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165ECO\u6846\u67b6\uff0c\u7814\u7a76\u4eba\u5458\u6210\u529f\u5730\u63d0\u9ad8\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u884c\u8d70\u65f6\u7684\u80fd\u6e90\u6548\u7387\uff0c\u8fd9\u6807\u5fd7\u7740\u5728\u5f00\u53d1\u66f4\u52a0\u9ad8\u6548\u8282\u80fd\u7684\u4eba\u5f62\u673a\u5668\u4eba\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2602.06504", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.06504", "abs": "https://arxiv.org/abs/2602.06504", "authors": ["Stephany Ortuno-Chanelo", "Paolo Rabino", "Enrico Civitelli", "Tatiana Tommasi", "Raffaello Camoriano"], "title": "MultiGraspNet: A Multitask 3D Vision Model for Multi-gripper Robotic Grasping", "comment": null, "summary": "Vision-based models for robotic grasping automate critical, repetitive, and draining industrial tasks. Existing approaches are typically limited in two ways: they either target a single gripper and are potentially applied on costly dual-arm setups, or rely on custom hybrid grippers that require ad-hoc learning procedures with logic that cannot be transferred across tasks, restricting their general applicability. In this work, we present MultiGraspNet, a novel multitask 3D deep learning method that predicts feasible poses simultaneously for parallel and vacuum grippers within a unified framework, enabling a single robot to handle multiple end effectors. The model is trained on the richly annotated GraspNet-1Billion and SuctionNet-1Billion datasets, which have been aligned for the purpose, and generates graspability masks quantifying the suitability of each scene point for successful grasps. By sharing early-stage features while maintaining gripper-specific refiners, MultiGraspNet effectively leverages complementary information across grasping modalities, enhancing robustness and adaptability in cluttered scenes. We characterize MultiGraspNet's performance with an extensive experimental analysis, demonstrating its competitiveness with single-task models on relevant benchmarks. We run real-world experiments on a single-arm multi-gripper robotic setup showing that our approach outperforms the vacuum baseline, grasping 16% percent more seen objects and 32% more of the novel ones, while obtaining competitive results for the parallel task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMultiGraspNet\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u4e3a\u5e76\u884c\u5939\u722a\u548c\u5e73\u9762\u5438\u76d8\u9884\u6d4b\u53ef\u884c\u7684\u59ff\u6001\uff0c\u5e76\u5728\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u4e0b\u5de5\u4f5c\u3002\u901a\u8fc7\u5728GraspNet-1Billion\u548cSuctionNet-1Billion\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u8be5\u6a21\u578b\u751f\u6210\u4e86\u6293\u53d6\u6027\u63a9\u7801\u6765\u91cf\u5316\u6bcf\u4e2a\u573a\u666f\u70b9\u7684\u6210\u529f\u6293\u53d6\u53ef\u80fd\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cMultiGraspNet\u5728\u5904\u7406\u65b0\u65e7\u7269\u4f53\u65f6\u5747\u4f18\u4e8e\u5355\u4efb\u52a1\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u6293\u53d6\u6a21\u578b\u901a\u5e38\u53d7\u9650\u4e8e\u53ea\u9488\u5bf9\u5355\u4e00\u7c7b\u578b\u7684\u5939\u5177\u8bbe\u8ba1\u6216\u9700\u8981\u5b9a\u5236\u5316\u7684\u6df7\u5408\u5939\u5177\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u901a\u7528\u6027\u548c\u6210\u672c\u6548\u76ca\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u66f4\u52a0\u7075\u6d3b\u4e14\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u540c\u4e00\u673a\u5668\u4eba\u80fd\u5229\u7528\u591a\u79cd\u672b\u7aef\u6267\u884c\u5668\u5b8c\u6210\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86MultiGraspNet\uff0c\u4e00\u79cd\u65b0\u7684\u591a\u4efb\u52a13D\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u80fd\u591f\u5728\u7edf\u4e00\u6846\u67b6\u5185\u540c\u65f6\u4e3a\u5e76\u8054\u5939\u5177\u548c\u771f\u7a7a\u5438\u76d8\u9884\u6d4b\u53ef\u884c\u4f4d\u7f6e\u3002\u6a21\u578b\u4f7f\u7528\u5bf9\u9f50\u540e\u7684GraspNet-1Billion\u548cSuctionNet-1Billion\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ea7\u751f\u6293\u53d6\u6027\u63a9\u6a21\u4ee5\u8bc4\u4f30\u6bcf\u4e2a\u573a\u666f\u70b9\u6210\u529f\u6293\u53d6\u7684\u53ef\u80fd\u6027\u3002\u901a\u8fc7\u5171\u4eab\u65e9\u671f\u7279\u5f81\u540c\u65f6\u4fdd\u6301\u5939\u5177\u7279\u5b9a\u7ec6\u5316\u5668\u7684\u65b9\u5f0f\uff0c\u6709\u6548\u5229\u7528\u8de8\u6293\u53d6\u6a21\u5f0f\u95f4\u7684\u4e92\u8865\u4fe1\u606f\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86MultiGraspNet\u4e0e\u5355\u4efb\u52a1\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u5728\u5355\u81c2\u591a\u5939\u5177\u673a\u5668\u4eba\u8bbe\u7f6e\u4e0a\u6d4b\u8bd5\u663e\u793a\uff0c\u5bf9\u4e8e\u5df2\u89c1\u7269\u4f53\u548c\u65b0\u578b\u7269\u4f53\uff0c\u5176\u6293\u53d6\u6210\u529f\u7387\u5206\u522b\u63d0\u9ad8\u4e8616%\u548c32%\uff0c\u540c\u65f6\u5728\u5e73\u884c\u4efb\u52a1\u4e0a\u4e5f\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "conclusion": "MultiGraspNet\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u7684\u6293\u53d6\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7269\u4f53\u65f6\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.06508", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06508", "abs": "https://arxiv.org/abs/2602.06508", "authors": ["Xiaokang Liu", "Zechen Bai", "Hai Ci", "Kevin Yuchen Ma", "Mike Zheng Shou"], "title": "World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy", "comment": "14 pages, 8 figures", "summary": "Recent progress in robotic world models has leveraged video diffusion transformers to predict future observations conditioned on historical states and actions. While these models can simulate realistic visual outcomes, they often exhibit poor action-following precision, hindering their utility for downstream robotic learning. In this work, we introduce World-VLA-Loop, a closed-loop framework for the joint refinement of world models and Vision-Language-Action (VLA) policies. We propose a state-aware video world model that functions as a high-fidelity interactive simulator by jointly predicting future observations and reward signals. To enhance reliability, we introduce the SANS dataset, which incorporates near-success trajectories to improve action-outcome alignment within the world model. This framework enables a closed-loop for reinforcement learning (RL) post-training of VLA policies entirely within a virtual environment. Crucially, our approach facilitates a co-evolving cycle: failure rollouts generated by the VLA policy are iteratively fed back to refine the world model precision, which in turn enhances subsequent RL optimization. Evaluations across simulation and real-world tasks demonstrate that our framework significantly boosts VLA performance with minimal physical interaction, establishing a mutually beneficial relationship between world modeling and policy learning for general-purpose robotics. Project page: https://showlab.github.io/World-VLA-Loop/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWorld-VLA-Loop\u7684\u95ed\u73af\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4e16\u754c\u6a21\u578b\u548c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u7b56\u7565\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u884c\u52a8\u8ddf\u968f\u7684\u7cbe\u786e\u5ea6\u3002\u8be5\u6846\u67b6\u5229\u7528\u4e00\u4e2a\u72b6\u6001\u611f\u77e5\u89c6\u9891\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u4ea4\u4e92\u6a21\u62df\u5668\uff0c\u5e76\u5f15\u5165\u4e86SANS\u6570\u636e\u96c6\u4ee5\u6539\u5584\u52a8\u4f5c-\u7ed3\u679c\u5bf9\u9f50\u3002\u6b64\u65b9\u6cd5\u5141\u8bb8\u5728\u865a\u62df\u73af\u5883\u4e2d\u5b8c\u5168\u8fdb\u884cVLA\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60(RL)\u540e\u8bad\u7ec3\uff0c\u5f62\u6210\u4e00\u79cd\u5171\u8fdb\u5316\u5faa\u73af\uff0c\u4ece\u800c\u5728\u6700\u5c0f\u7269\u7406\u4ea4\u4e92\u4e0b\u663e\u8457\u63d0\u5347VLA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u7684\u4e16\u754c\u6a21\u578b\u867d\u7136\u80fd\u591f\u9884\u6d4b\u672a\u6765\u89c2\u5bdf\u60c5\u51b5\uff0c\u4f46\u5728\u884c\u52a8\u8ddf\u968f\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0b\u6e38\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u5e0c\u671b\u5f00\u53d1\u51fa\u65e2\u80fd\u63d0\u4f9b\u903c\u771f\u89c6\u89c9\u6548\u679c\u53c8\u80fd\u4fdd\u8bc1\u884c\u52a8\u6267\u884c\u51c6\u786e\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86World-VLA-Loop\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u4e00\u4e2a\u72b6\u6001\u611f\u77e5\u89c6\u9891\u4e16\u754c\u6a21\u578b\u4e0e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u653f\u7b56\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002\u8be5\u4e16\u754c\u6a21\u578b\u4e0d\u4ec5\u80fd\u9884\u6d4b\u672a\u6765\u7684\u89c2\u6d4b\u7ed3\u679c\uff0c\u8fd8\u80fd\u751f\u6210\u5956\u52b1\u4fe1\u53f7\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u521b\u5efa\u4e86SANS\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e86\u63a5\u8fd1\u6210\u529f\u7684\u8f68\u8ff9\u6765\u52a0\u5f3a\u4e16\u754c\u6a21\u578b\u5185\u7684\u884c\u52a8-\u7ed3\u679c\u4e00\u81f4\u6027\u3002\u6574\u4e2a\u7cfb\u7edf\u652f\u6301\u5728\u4e00\u4e2a\u865a\u62df\u73af\u5883\u5185\u5b8c\u6210VLA\u653f\u7b56\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u7eed\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5171\u540c\u6f14\u5316\u7684\u5faa\u73af\u673a\u5236\uff1a\u7531VLA\u653f\u7b56\u4ea7\u751f\u7684\u5931\u8d25\u5c1d\u8bd5\u88ab\u53cd\u590d\u53cd\u9988\u7528\u4e8e\u7cbe\u70bc\u4e16\u754c\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u8fdb\u800c\u4fc3\u8fdb\u63a5\u4e0b\u6765\u7684RL\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u65e0\u8bba\u662f\u5728\u4eff\u771f\u8fd8\u662f\u5b9e\u9645\u4efb\u52a1\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u90fd\u80fd\u4ee5\u6781\u5c11\u7684\u5b9e\u9645\u64cd\u4f5c\u5927\u5e45\u63d0\u5347VLA\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u4e16\u754c\u5efa\u6a21\u4e0e\u7b56\u7565\u5b66\u4e60\u4e4b\u95f4\u5b58\u5728\u4e92\u5229\u5173\u7cfb\u3002", "conclusion": "World-VLA-Loop\u6846\u67b6\u901a\u8fc7\u5efa\u7acb\u4e16\u754c\u6a21\u578b\u4e0eVLA\u653f\u7b56\u4e4b\u95f4\u7684\u95ed\u73af\u4e92\u52a8\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u884c\u52a8\u8ddf\u968f\u7cbe\u5ea6\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5bf9\u7269\u7406\u4ea4\u4e92\u7684\u9700\u6c42\uff0c\u4e3a\u901a\u7528\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\u5f00\u8f9f\u4e86\u65b0\u7684\u8def\u5f84\u3002"}}
{"id": "2602.06512", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06512", "abs": "https://arxiv.org/abs/2602.06512", "authors": ["Junhong Zhu", "Ji Zhang", "Jingkuan Song", "Lianli Gao", "Heng Tao Shen"], "title": "Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation", "comment": "accept by IEEE International Conference on Robotics and Automation (ICRA 2026), 8 pages, 6 figures,", "summary": "While generalist robot policies hold significant promise for learning diverse manipulation skills through imitation, their performance is often hindered by the long-tail distribution of training demonstrations. Policies learned on such data, which is heavily skewed towards a few data-rich head tasks, frequently exhibit poor generalization when confronted with the vast number of data-scarce tail tasks. In this work, we conduct a comprehensive analysis of the pervasive long-tail challenge inherent in policy learning. Our analysis begins by demonstrating the inefficacy of conventional long-tail learning strategies (e.g., re-sampling) for improving the policy's performance on tail tasks. We then uncover the underlying mechanism for this failure, revealing that data scarcity on tail tasks directly impairs the policy's spatial reasoning capability. To overcome this, we introduce Approaching-Phase Augmentation (APA), a simple yet effective scheme that transfers knowledge from data-rich head tasks to data-scarce tail tasks without requiring external demonstrations. Extensive experiments in both simulation and real-world manipulation tasks demonstrate the effectiveness of APA. Our code and demos are publicly available at: https://mldxy.github.io/Project-VLA-long-tail/.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u7b56\u7565\u5b66\u4e60\u4e2d\u666e\u904d\u5b58\u5728\u7684\u957f\u5c3e\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u63a5\u8fd1\u9636\u6bb5\u589e\u5f3a\uff08APA\uff09\u7684\u6709\u6548\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u6570\u636e\u7a00\u7f3a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u867d\u7136\u53ef\u4ee5\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u591a\u79cd\u64cd\u4f5c\u6280\u80fd\uff0c\u4f46\u5176\u6027\u80fd\u5f80\u5f80\u53d7\u5230\u8bad\u7ec3\u6f14\u793a\u7684\u957f\u5c3e\u5206\u5e03\u7684\u5f71\u54cd\u3002\u5bf9\u4e8e\u6570\u636e\u4e30\u5bcc\u7684\u5934\u90e8\u4efb\u52a1\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9762\u5bf9\u4f17\u591a\u6570\u636e\u7a00\u7f3a\u7684\u5c3e\u90e8\u4efb\u52a1\u65f6\uff0c\u8fd9\u4e9b\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002", "method": "\u9996\u5148\u5c55\u793a\u4e86\u4f20\u7edf\u957f\u5c3e\u5b66\u4e60\u7b56\u7565\uff08\u5982\u91cd\u91c7\u6837\uff09\u5728\u6539\u5584\u5c3e\u90e8\u4efb\u52a1\u4e0a\u7b56\u7565\u6027\u80fd\u65b9\u9762\u7684\u65e0\u6548\u6027\uff0c\u7136\u540e\u63ed\u793a\u4e86\u5c3e\u90e8\u4efb\u52a1\u6570\u636e\u7a00\u7f3a\u76f4\u63a5\u635f\u5bb3\u7b56\u7565\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u6839\u672c\u673a\u5236\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u63a5\u8fd1\u9636\u6bb5\u589e\u5f3a\uff08APA\uff09\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u65e0\u9700\u5916\u90e8\u6f14\u793a\u5373\u53ef\u5c06\u77e5\u8bc6\u4ece\u6570\u636e\u4e30\u5bcc\u7684\u5934\u90e8\u4efb\u52a1\u8f6c\u79fb\u5230\u6570\u636e\u7a00\u7f3a\u7684\u5c3e\u90e8\u4efb\u52a1\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAPA\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5904\u7406\u6570\u636e\u7a00\u7f3a\u4efb\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u6df1\u5165\u63a2\u8ba8\u4e86\u653f\u7b56\u5b66\u4e60\u4e2d\u9762\u4e34\u7684\u957f\u5c3e\u6311\u6218\uff0c\u8fd8\u901a\u8fc7APA\u63d0\u4f9b\u4e86\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.06541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06541", "abs": "https://arxiv.org/abs/2602.06541", "authors": ["Seifeddine Sellemi", "Abdelbadia Chaker", "Tanguy Vendeuvre", "Terence Essomba", "Med Amine Laribi"], "title": "Primary Experimental Feedback on a Co-manipulated Robotic System for Assisted Cervical Surgery", "comment": null, "summary": "Robotic-assisted surgery has emerged as a promising approach to improve surgical ergonomics, precision, and workflow efficiency, particularly in complex procedures such as cervical spine surgery. In this study, we evaluate the performance of a collaborative robotic system designed to assist surgeons in drilling tasks by assessing its accuracy in executing predefined trajectories. A total of 14 drillings were performed by eight experienced cervical surgeons, utilizing a robotic-assisted setup aimed at ensuring stability and alignment. The primary objective of this study is to quantify the deviations in the position and orientation of the drilling tool relative to the planned trajectory, providing insights into the system's reliability and potential impact on clinical outcomes. While the primary function of robotic assistance in surgery is to enhance surgeon comfort and procedural guidance rather than solely optimizing precision, understanding the system's accuracy remains crucial for its effective integration into surgical practices part of this primary experimental feedback, the study offers an in-depth analysis of the co-manipulated robotic system's performance, focusing on the experimental setup and error evaluation methods. The findings of this study will contribute to the ongoing development of robotic-assisted cervical surgery, highlighting both its advantages and areas for improvement in achieving safer and more efficient surgical workflows", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u534f\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u9888\u690e\u624b\u672f\u94bb\u5b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u6d4b\u91cf\u94bb\u5b54\u5de5\u5177\u76f8\u5bf9\u4e8e\u9884\u5b9a\u8f68\u8ff9\u7684\u4f4d\u7f6e\u548c\u65b9\u5411\u504f\u5dee\u6765\u91cf\u5316\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3002\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u4e86\u89e3\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u7684\u53ef\u9760\u6027\u53ca\u5176\u5bf9\u4e34\u5e8a\u7ed3\u679c\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u540c\u65f6\u4e5f\u4e3a\u673a\u5668\u4eba\u8f85\u52a9\u9888\u690e\u624b\u672f\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6d1e\u89c1\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u8bc4\u4f30\u4e00\u4e2a\u4e13\u4e3a\u534f\u52a9\u5916\u79d1\u533b\u751f\u8fdb\u884c\u94bb\u5b54\u8bbe\u8ba1\u7684\u534f\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5176\u6267\u884c\u9884\u8bbe\u8f68\u8ff9\u7684\u51c6\u786e\u6027\uff0c\u6765\u6539\u8fdb\u624b\u672f\u4eba\u4f53\u5de5\u7a0b\u5b66\u3001\u7cbe\u5ea6\u548c\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u3002", "method": "\u7814\u7a76\u4e2d\uff0c8\u4f4d\u7ecf\u9a8c\u4e30\u5bcc\u7684\u9888\u690e\u5916\u79d1\u533b\u751f\u4f7f\u7528\u673a\u5668\u4eba\u8f85\u52a9\u88c5\u7f6e\u8fdb\u884c\u4e86\u603b\u517114\u6b21\u94bb\u5b54\u64cd\u4f5c\uff0c\u91cd\u70b9\u5728\u4e8e\u786e\u4fdd\u7a33\u5b9a\u6027\u548c\u5bf9\u9f50\u5ea6\u3002\u4e3b\u8981\u76ee\u6807\u662f\u91cf\u5316\u94bb\u5b54\u5de5\u5177\u76f8\u5bf9\u4e8e\u8ba1\u5212\u8def\u5f84\u7684\u4f4d\u7f6e\u4e0e\u65b9\u5411\u4e0a\u7684\u504f\u5dee\uff0c\u4ee5\u6b64\u63d0\u4f9b\u5173\u4e8e\u8be5\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u89c1\u89e3\u4ee5\u53ca\u5b83\u53ef\u80fd\u5bf9\u4e34\u5e8a\u7ed3\u679c\u4ea7\u751f\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u6df1\u5165\u5206\u6790\u4e86\u5171\u64cd\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u96c6\u4e2d\u4e8e\u5b9e\u9a8c\u8bbe\u7f6e\u548c\u8bef\u5dee\u8bc4\u4f30\u65b9\u6cd5\u4e0a\u3002\u5c3d\u7ba1\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u7684\u4e3b\u8981\u529f\u80fd\u5728\u4e8e\u63d0\u9ad8\u5916\u79d1\u533b\u751f\u8212\u9002\u5ea6\u548c\u7a0b\u5e8f\u6307\u5bfc\u800c\u975e\u5355\u7eaf\u4f18\u5316\u7cbe\u5ea6\uff0c\u4f46\u7406\u89e3\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u5bf9\u4e8e\u5176\u6709\u6548\u878d\u5165\u624b\u672f\u5b9e\u8df5\u4ecd\u7136\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u7684\u7ed3\u679c\u5c06\u4fc3\u8fdb\u673a\u5668\u4eba\u8f85\u52a9\u9888\u690e\u624b\u672f\u9886\u57df\u7684\u6301\u7eed\u53d1\u5c55\uff0c\u5f3a\u8c03\u4e86\u5b83\u5728\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u7684\u624b\u672f\u6d41\u7a0b\u65b9\u9762\u7684\u4f18\u52bf\u53ca\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2602.06572", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06572", "abs": "https://arxiv.org/abs/2602.06572", "authors": ["Malte Huerkamp", "Jonas Dech", "Michael Beetz"], "title": "The Law of Task-Achieving Body Motion: Axiomatizing Success of Robot Manipulation Actions", "comment": "9 pages, 3 figures, submitted to the 2026 International Joint Conference on Artificial Intelligence (IJCAI)", "summary": "Autonomous agents that perform everyday manipulation actions need to ensure that their body motions are semantically correct with respect to a task request, causally effective within their environment, and feasible for their embodiment. In order to enable robots to verify these properties, we introduce the Law of Task-Achieving Body Motion as an axiomatic correctness specification for body motions. To that end we introduce scoped Task-Environment-Embodiment (TEE) classes that represent world states as Semantic Digital Twins (SDTs) and define applicable physics models to decompose task achievement into three predicates: SatisfiesRequest for semantic request satisfaction over SDT state evolution; Causes for causal sufficiency under the scoped physics model; and CanPerform for safety and feasibility verification at the embodiment level. This decomposition yields a reusable, implementation-independent interface that supports motion synthesis and the verification of given body motions. It also supports typed failure diagnosis (semantic, causal, embodiment and out-of-scope), feasibility across robots and environments, and counterfactual reasoning about robot body motions. We demonstrate the usability of the law in practice by instantiating it for articulated container manipulation in kitchen environments on three contrasting mobile manipulation platforms", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u5b8c\u6210\u8eab\u4f53\u8fd0\u52a8\u6cd5\u5219\uff0c\u7528\u4e8e\u9a8c\u8bc1\u673a\u5668\u4eba\u5728\u6267\u884c\u65e5\u5e38\u64cd\u4f5c\u52a8\u4f5c\u65f6\u5176\u8eab\u4f53\u8fd0\u52a8\u662f\u5426\u7b26\u5408\u4efb\u52a1\u8bf7\u6c42\u3001\u73af\u5883\u56e0\u679c\u6548\u5e94\u53ca\u5176\u5b9e\u4f53\u53ef\u884c\u6027\u3002\u901a\u8fc7\u5f15\u5165\u4efb\u52a1-\u73af\u5883-\u5b9e\u4f53\uff08TEE\uff09\u7c7b\u548c\u8bed\u4e49\u6570\u5b57\u5b6a\u751f\uff08SDT\uff09\u6765\u8868\u793a\u4e16\u754c\u72b6\u6001\uff0c\u5e76\u5b9a\u4e49\u9002\u7528\u7684\u7269\u7406\u6a21\u578b\u5c06\u4efb\u52a1\u5b8c\u6210\u5206\u89e3\u4e3a\u4e09\u4e2a\u8c13\u8bcd\uff1a\u6ee1\u8db3\u8bf7\u6c42\u3001\u56e0\u679c\u5145\u5206\u6027\u4ee5\u53ca\u5b89\u5168\u548c\u53ef\u884c\u6027\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u9a8c\u8bc1\u5176\u8eab\u4f53\u8fd0\u52a8\u5bf9\u4e8e\u4efb\u52a1\u8bf7\u6c42\u662f\u8bed\u4e49\u6b63\u786e\u7684\uff0c\u5728\u73af\u5883\u4e2d\u5177\u6709\u56e0\u679c\u6548\u5e94\uff0c\u5e76\u4e14\u5728\u5176\u5b9e\u4f53\u4e0a\u53ef\u884c\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4efb\u52a1\u5b8c\u6210\u8eab\u4f53\u8fd0\u52a8\u6cd5\u5219\u4f5c\u4e3a\u8eab\u4f53\u8fd0\u52a8\u7684\u516c\u7406\u5316\u6b63\u786e\u6027\u89c4\u8303\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4efb\u52a1-\u73af\u5883-\u5b9e\u4f53(TEE)\u7c7b\u6765\u8868\u793a\u4e16\u754c\u72b6\u6001\u4e3a\u8bed\u4e49\u6570\u5b57\u5b6a\u751f(SDT)\uff0c\u5e76\u5b9a\u4e49\u4e86\u9002\u7528\u7684\u7269\u7406\u6a21\u578b\u4ee5\u5c06\u4efb\u52a1\u5b8c\u6210\u5206\u89e3\u6210\u4e09\u4e2a\u8c13\u8bcd\uff1aSatisfiesRequest\uff08\u6ee1\u8db3\u8bf7\u6c42\uff09\u3001Causes\uff08\u56e0\u679c\u5173\u7cfb\uff09\u548cCanPerform\uff08\u53ef\u6267\u884c\uff09\u3002\u8fd9\u79cd\u5206\u89e3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u91cd\u7528\u4e14\u72ec\u7acb\u4e8e\u5b9e\u73b0\u7684\u63a5\u53e3\uff0c\u652f\u6301\u8fd0\u52a8\u5408\u6210\u4e0e\u7ed9\u5b9a\u8eab\u4f53\u8fd0\u52a8\u7684\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u652f\u6301\u8de8\u4e0d\u540c\u673a\u5668\u4eba\u548c\u73af\u5883\u7684\u53ef\u884c\u6027\u5206\u6790\uff0c\u5bf9\u673a\u5668\u4eba\u8eab\u4f53\u8fd0\u52a8\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u4ee5\u53ca\u7c7b\u578b\u5316\u7684\u6545\u969c\u8bca\u65ad\uff08\u8bed\u4e49\u3001\u56e0\u679c\u3001\u5b9e\u4f53\u548c\u8d85\u51fa\u8303\u56f4\uff09\u3002\u7814\u7a76\u8fd8\u901a\u8fc7\u5728\u4e09\u79cd\u4e0d\u540c\u7684\u79fb\u52a8\u64cd\u7eb5\u5e73\u53f0\u4e0a\u9488\u5bf9\u53a8\u623f\u73af\u5883\u4e2d\u7684\u5173\u8282\u5bb9\u5668\u64cd\u7eb5\u5b9e\u4f8b\u5316\u8be5\u6cd5\u5219\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u4efb\u52a1\u5b8c\u6210\u8eab\u4f53\u8fd0\u52a8\u6cd5\u5219\uff0c\u672c\u6587\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u9a8c\u8bc1\u5176\u6267\u884c\u65e5\u5e38\u64cd\u4f5c\u52a8\u4f5c\u65f6\u8eab\u4f53\u8fd0\u52a8\u6027\u8d28\u7684\u4e00\u79cd\u65b0\u65b9\u5f0f\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u652f\u6301\u8fd0\u52a8\u5408\u6210\u4e0e\u9a8c\u8bc1\uff0c\u8fd8\u4fc3\u8fdb\u4e86\u8de8\u5e73\u53f0\u548c\u73af\u5883\u7684\u53ef\u884c\u6027\u5206\u6790\u53ca\u53cd\u4e8b\u5b9e\u63a8\u7406\u3002"}}
{"id": "2602.06575", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.06575", "abs": "https://arxiv.org/abs/2602.06575", "authors": ["Fangyuan Wang", "Peng Zhou", "Jiaming Qi", "Shipeng Lyu", "David Navarro-Alarcon", "Guodong Guo"], "title": "Think Proprioceptively: Embodied Visual Reasoning for VLA Manipulation", "comment": null, "summary": "Vision-language-action (VLA) models typically inject proprioception only as a late conditioning signal, which prevents robot state from shaping instruction understanding and from influencing which visual tokens are attended throughout the policy. We introduce ThinkProprio, which converts proprioception into a sequence of text tokens in the VLM embedding space and fuses them with the task instruction at the input. This early fusion lets embodied state participate in subsequent visual reasoning and token selection, biasing computation toward action-critical evidence while suppressing redundant visual tokens. In a systematic ablation over proprioception encoding, state entry point, and action-head conditioning, we find that text tokenization is more effective than learned projectors, and that retaining roughly 15% of visual tokens can match the performance of using the full token set. Across CALVIN, LIBERO, and real-world manipulation, ThinkProprio matches or improves over strong baselines while reducing end-to-end inference latency over 50%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aThinkProprio\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u672c\u4f53\u611f\u89c9\u8f6c\u6362\u4e3a\u6587\u672c\u6807\u8bb0\u5e8f\u5217\uff0c\u5e76\u4e0e\u4efb\u52a1\u6307\u4ee4\u5728\u8f93\u5165\u7aef\u878d\u5408\uff0c\u4ee5\u6539\u5584\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u901a\u5e38\u4ec5\u5728\u540e\u671f\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\u5f15\u5165\u672c\u4f53\u611f\u77e5\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u4eba\u72b6\u6001\u5bf9\u6307\u4ee4\u7406\u89e3\u548c\u89c6\u89c9\u6807\u8bb0\u9009\u62e9\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5c06\u672c\u4f53\u611f\u89c9\u8f6c\u6362\u6210VLM\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u6587\u672c\u6807\u8bb0\u5e8f\u5217\u5e76\u4e0e\u4efb\u52a1\u6307\u4ee4\u5728\u8f93\u5165\u5904\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u65e9\u671f\u878d\u5408\uff0c\u4f7f\u5b9e\u4f53\u72b6\u6001\u80fd\u591f\u53c2\u4e0e\u540e\u7eed\u7684\u89c6\u89c9\u63a8\u7406\u548c\u6807\u8bb0\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6587\u672c\u6807\u8bb0\u5316\u6bd4\u5b66\u4e60\u6295\u5f71\u66f4\u6709\u6548\uff0c\u4fdd\u7559\u7ea615%\u7684\u89c6\u89c9\u6807\u8bb0\u5373\u53ef\u8fbe\u5230\u4f7f\u7528\u5b8c\u6574\u6807\u8bb0\u96c6\u7684\u6027\u80fd\u3002\u5728CALVIN\u3001LIBERO\u4ee5\u53ca\u73b0\u5b9e\u4e16\u754c\u64cd\u4f5c\u4e2d\uff0cThinkProprio\u76f8\u6bd4\u5f3a\u5927\u7684\u57fa\u7ebf\u6709\u6240\u6539\u8fdb\u6216\u6301\u5e73\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8d85\u8fc750%\u7684\u7aef\u5230\u7aef\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "ThinkProprio\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u5f0f\u6765\u5229\u7528\u672c\u4f53\u611f\u89c9\u4fe1\u606f\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86VLA\u6a21\u578b\u7684\u8868\u73b0\uff0c\u8fd8\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002"}}
{"id": "2602.06620", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.06620", "abs": "https://arxiv.org/abs/2602.06620", "authors": ["Hiroshi Sato", "Sho Sakaino", "Toshiaki Tsuji"], "title": "Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique", "comment": "Accepted for IEEE Access", "summary": "In contact-rich tasks, while position trajectories are often easy to obtain, appropriate force commands are typically unknown. Although it is conceivable to generate force commands using a pretrained foundation model such as Vision-Language-Action (VLA) models, force control is highly dependent on the specific hardware of the robot, which makes the application of such models challenging. To bridge this gap, we propose a force generative model that estimates force commands from given position trajectories. However, when dealing with unseen position trajectories, the model struggles to generate accurate force commands. To address this, we introduce a feedback control mechanism. Our experiments reveal that feedback control does not converge when the force generative model has memory. We therefore adopt a model without memory, enabling stable feedback control. This approach allows the system to generate force commands effectively, even for unseen position trajectories, improving generalization for real-world robot writing tasks.", "AI": {"tldr": "Researchers developed a force generative model to estimate force commands from position trajectories for robots in contact-rich tasks, incorporating a feedback control mechanism that required a memoryless model for stable operation, thereby enhancing the system's ability to handle unseen position trajectories and improving performance in real-world applications like robot writing.", "motivation": "\u5728\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\uff0c\u5c3d\u7ba1\u4f4d\u7f6e\u8f68\u8ff9\u5bb9\u6613\u83b7\u53d6\uff0c\u4f46\u9002\u5f53\u7684\u529b\u547d\u4ee4\u901a\u5e38\u662f\u672a\u77e5\u7684\u3002\u5c3d\u7ba1\u53ef\u4ee5\u8bbe\u60f3\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff08\u5982\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff09\u6765\u751f\u6210\u529b\u547d\u4ee4\uff0c\u4f46\u529b\u63a7\u5236\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u673a\u5668\u4eba\u7684\u7279\u5b9a\u786c\u4ef6\uff0c\u8fd9\u4f7f\u5f97\u8fd9\u7c7b\u6a21\u578b\u7684\u5e94\u7528\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u529b\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u4ece\u7ed9\u5b9a\u7684\u4f4d\u7f6e\u8f68\u8ff9\u4f30\u8ba1\u529b\u547d\u4ee4\u3002\u4e3a\u4e86\u89e3\u51b3\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\u4f4d\u7f6e\u8f68\u8ff9\u65f6\u6a21\u578b\u96be\u4ee5\u751f\u6210\u51c6\u786e\u529b\u547d\u4ee4\u7684\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u53cd\u9988\u63a7\u5236\u673a\u5236\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u529b\u751f\u6210\u6a21\u578b\u5177\u6709\u8bb0\u5fc6\u65f6\uff0c\u53cd\u9988\u63a7\u5236\u4e0d\u4f1a\u6536\u655b\u3002\u56e0\u6b64\u91c7\u7528\u4e86\u65e0\u8bb0\u5fc6\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u53cd\u9988\u63a7\u5236\u3002", "result": "\u901a\u8fc7\u91c7\u7528\u65e0\u8bb0\u5fc6\u6a21\u578b\uff0c\u7cfb\u7edf\u5373\u4f7f\u5bf9\u4e8e\u672a\u89c1\u8fc7\u7684\u4f4d\u7f6e\u8f68\u8ff9\u4e5f\u80fd\u591f\u6709\u6548\u5730\u751f\u6210\u529b\u547d\u4ee4\uff0c\u63d0\u9ad8\u4e86\u5bf9\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4e66\u5199\u4efb\u52a1\u7b49\u5e94\u7528\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u529b\u751f\u6210\u6a21\u578b\u7ed3\u5408\u53cd\u9988\u63a7\u5236\u673a\u5236\uff0c\u7279\u522b\u662f\u4f7f\u7528\u65e0\u8bb0\u5fc6\u6a21\u578b\u540e\uff0c\u663e\u8457\u6539\u5584\u4e86\u673a\u5668\u4eba\u5904\u7406\u65b0\u4f4d\u7f6e\u8f68\u8ff9\u7684\u80fd\u529b\uff0c\u5e76\u589e\u5f3a\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2602.06643", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06643", "abs": "https://arxiv.org/abs/2602.06643", "authors": ["Ruiqian Nai", "Boyuan Zheng", "Junming Zhao", "Haodong Zhu", "Sicong Dai", "Zunhao Chen", "Yihang Hu", "Yingdong Hu", "Tong Zhang", "Chuan Wen", "Yang Gao"], "title": "Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations", "comment": "Website: https://humanoid-manipulation-interface.github.io", "summary": "Current approaches for humanoid whole-body manipulation, primarily relying on teleoperation or visual sim-to-real reinforcement learning, are hindered by hardware logistics and complex reward engineering. Consequently, demonstrated autonomous skills remain limited and are typically restricted to controlled environments. In this paper, we present the Humanoid Manipulation Interface (HuMI), a portable and efficient framework for learning diverse whole-body manipulation tasks across various environments. HuMI enables robot-free data collection by capturing rich whole-body motion using portable hardware. This data drives a hierarchical learning pipeline that translates human motions into dexterous and feasible humanoid skills. Extensive experiments across five whole-body tasks--including kneeling, squatting, tossing, walking, and bimanual manipulation--demonstrate that HuMI achieves a 3x increase in data collection efficiency compared to teleoperation and attains a 70% success rate in unseen environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86HuMI\uff0c\u4e00\u79cd\u7528\u4e8e\u5b66\u4e60\u5404\u79cd\u73af\u5883\u4e2d\u5168\u8eab\u64cd\u4f5c\u4efb\u52a1\u7684\u4fbf\u643a\u9ad8\u6548\u6846\u67b6\u3002\u901a\u8fc7\u4f7f\u7528\u4fbf\u643a\u5f0f\u786c\u4ef6\u6355\u6349\u4e30\u5bcc\u7684\u5168\u8eab\u8fd0\u52a8\uff0cHuMI\u80fd\u591f\u65e0\u9700\u673a\u5668\u4eba\u53c2\u4e0e\u5373\u53ef\u6536\u96c6\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u5b66\u4e60\u6d41\u7a0b\u5c06\u4eba\u7c7b\u52a8\u4f5c\u8f6c\u5316\u4e3a\u7075\u5de7\u4e14\u53ef\u884c\u7684\u4eba\u5f62\u6280\u80fd\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0e\u8fdc\u7a0b\u64cd\u4f5c\u76f8\u6bd4\uff0cHuMI\u7684\u6570\u636e\u6536\u96c6\u6548\u7387\u63d0\u9ad8\u4e863\u500d\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u73af\u5883\u4e2d\u6210\u529f\u7387\u8fbe\u523070%\u3002", "motivation": "\u5f53\u524d\u4eba\u5f62\u5168\u8eab\u64cd\u4f5c\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u8fdc\u7a0b\u64cd\u4f5c\u6216\u89c6\u89c9\u4eff\u771f\u5230\u73b0\u5b9e\u5f3a\u5316\u5b66\u4e60\uff0c\u4f46\u53d7\u5230\u786c\u4ef6\u540e\u52e4\u548c\u590d\u6742\u5956\u52b1\u5de5\u7a0b\u7684\u9650\u5236\uff0c\u5bfc\u81f4\u5c55\u793a\u51fa\u7684\u81ea\u4e3b\u6280\u80fd\u6709\u9650\u4e14\u901a\u5e38\u5c40\u9650\u4e8e\u53d7\u63a7\u73af\u5883\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86Humanoid Manipulation Interface (HuMI)\uff0c\u4e00\u4e2a\u652f\u6301\u5728\u591a\u79cd\u73af\u5883\u4e0b\u5b66\u4e60\u591a\u6837\u5316\u5168\u8eab\u64cd\u4f5c\u4efb\u52a1\u7684\u4fbf\u643a\u9ad8\u6548\u6846\u67b6\u3002\u5b83\u5141\u8bb8\u901a\u8fc7\u4fbf\u643a\u5f0f\u786c\u4ef6\u6355\u83b7\u4e30\u5bcc\u7684\u5168\u8eab\u8fd0\u52a8\u6765\u8fdb\u884c\u65e0\u673a\u5668\u4eba\u53c2\u4e0e\u7684\u6570\u636e\u6536\u96c6\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u9a71\u52a8\u4e00\u4e2a\u5206\u5c42\u5b66\u4e60\u6d41\u7a0b\uff0c\u5c06\u4eba\u7c7b\u52a8\u4f5c\u8f6c\u6362\u6210\u4eba\u5f62\u673a\u5668\u4eba\u7684\u7075\u5de7\u53ca\u53ef\u884c\u6280\u80fd\u3002", "result": "\u8de8\u4e94\u4e2a\u5168\u8eab\u4efb\u52a1\uff08\u5305\u62ec\u8dea\u4e0b\u3001\u8e72\u4e0b\u3001\u6295\u63b7\u3001\u884c\u8d70\u548c\u53cc\u624b\u64cd\u4f5c\uff09\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u8fdc\u7a0b\u64cd\u4f5c\u76f8\u6bd4\uff0cHuMI\u4f7f\u6570\u636e\u6536\u96c6\u6548\u7387\u63d0\u9ad8\u4e863\u500d\uff0c\u5e76\u5728\u672a\u66fe\u89c1\u8fc7\u7684\u73af\u5883\u4e2d\u8fbe\u5230\u4e8670%\u7684\u6210\u529f\u7387\u3002", "conclusion": "HuMI\u4e3a\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9014\u5f84\uff0c\u901a\u8fc7\u66f4\u9ad8\u6548\u5730\u6536\u96c6\u6570\u636e\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u7684\u4eba\u5f62\u673a\u5668\u4eba\u6280\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u6267\u884c\u590d\u6742\u5168\u8eab\u64cd\u4f5c\u7684\u80fd\u529b\u3002"}}
{"id": "2602.06653", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06653", "abs": "https://arxiv.org/abs/2602.06653", "authors": ["Zi Yin", "Fanhong Li", "Shurui Zheng", "Jia Liu"], "title": "RAPID: Reconfigurable, Adaptive Platform for Iterative Design", "comment": null, "summary": "Developing robotic manipulation policies is iterative and hypothesis-driven: researchers test tactile sensing, gripper geometries, and sensor placements through real-world data collection and training. Yet even minor end-effector changes often require mechanical refitting and system re-integration, slowing iteration. We present RAPID, a full-stack reconfigurable platform designed to reduce this friction. RAPID is built around a tool-free, modular hardware architecture that unifies handheld data collection and robot deployment, and a matching software stack that maintains real-time awareness of the underlying hardware configuration through a driver-level Physical Mask derived from USB events. This modular hardware architecture reduces reconfiguration to seconds and makes systematic multi-modal ablation studies practical, allowing researchers to sweep diverse gripper and sensing configurations without repeated system bring-up. The Physical Mask exposes modality presence as an explicit runtime signal, enabling auto-configuration and graceful degradation under sensor hot-plug events, so policies can continue executing when sensors are physically added or removed. System-centric experiments show that RAPID reduces the setup time for multi-modal configurations by two orders of magnitude compared to traditional workflows and preserves policy execution under runtime sensor hot-unplug events. The hardware designs, drivers, and software stack are open-sourced at https://rapid-kit.github.io/ .", "AI": {"tldr": "\u4ecb\u7ecd\u4e86RAPID\uff0c\u4e00\u4e2a\u53ef\u91cd\u65b0\u914d\u7f6e\u7684\u5168\u6808\u5e73\u53f0\uff0c\u65e8\u5728\u901a\u8fc7\u6a21\u5757\u5316\u7684\u786c\u4ef6\u67b6\u6784\u548c\u5339\u914d\u7684\u8f6f\u4ef6\u5806\u6808\u6765\u7b80\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u5f00\u53d1\u8fc7\u7a0b\uff0c\u4ece\u800c\u51cf\u5c11\u56e0\u7ec8\u7aef\u6267\u884c\u5668\u53d8\u5316\u800c\u9700\u8981\u8fdb\u884c\u673a\u68b0\u91cd\u88c5\u548c\u7cfb\u7edf\u91cd\u65b0\u96c6\u6210\u7684\u65f6\u95f4\u3002", "motivation": "\u5728\u5f00\u53d1\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u65f6\uff0c\u5373\u4f7f\u662f\u5c0f\u7684\u7ec8\u7aef\u6267\u884c\u5668\u66f4\u6539\u4e5f\u7ecf\u5e38\u9700\u8981\u673a\u68b0\u91cd\u88c5\u548c\u7cfb\u7edf\u91cd\u65b0\u96c6\u6210\uff0c\u8fd9\u4f1a\u51cf\u6162\u8fed\u4ee3\u901f\u5ea6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86RAPID\u5e73\u53f0\uff0c\u5b83\u80fd\u591f\u5feb\u901f\u91cd\u65b0\u914d\u7f6e\u786c\u4ef6\uff0c\u5e76\u4e14\u652f\u6301\u591a\u6a21\u6001\u6d88\u878d\u7814\u7a76\uff0c\u4f7f\u5f97\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u65b9\u4fbf\u5730\u6d4b\u8bd5\u4e0d\u540c\u7684\u5939\u5177\u548c\u4f20\u611f\u914d\u7f6e\u800c\u4e0d\u5fc5\u53cd\u590d\u542f\u52a8\u7cfb\u7edf\u3002", "method": "RAPID\u57fa\u4e8e\u65e0\u9700\u5de5\u5177\u7684\u6a21\u5757\u5316\u786c\u4ef6\u67b6\u6784\u8bbe\u8ba1\uff0c\u8be5\u67b6\u6784\u5c06\u624b\u6301\u6570\u636e\u6536\u96c6\u4e0e\u673a\u5668\u4eba\u90e8\u7f72\u7edf\u4e00\u8d77\u6765\uff1b\u540c\u65f6\u914d\u5907\u4e86\u4e00\u4e2a\u914d\u5957\u7684\u8f6f\u4ef6\u5806\u6808\uff0c\u8be5\u8f6f\u4ef6\u5806\u6808\u901a\u8fc7\u4eceUSB\u4e8b\u4ef6\u6d3e\u751f\u7684\u9a71\u52a8\u7ea7\u7269\u7406\u63a9\u7801\u4fdd\u6301\u5bf9\u5e95\u5c42\u786c\u4ef6\u914d\u7f6e\u7684\u5b9e\u65f6\u611f\u77e5\u3002\u7269\u7406\u63a9\u7801\u516c\u5f00\u4e86\u6a21\u5f0f\u5b58\u5728\u6027\u4f5c\u4e3a\u663e\u5f0f\u7684\u8fd0\u884c\u65f6\u4fe1\u53f7\uff0c\u5141\u8bb8\u81ea\u52a8\u914d\u7f6e\u4ee5\u53ca\u4f20\u611f\u5668\u70ed\u63d2\u62d4\u4e8b\u4ef6\u4e0b\u7684\u4f18\u96c5\u964d\u7ea7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edf\u5de5\u4f5c\u6d41\u7a0b\uff0cRAPID\u5c06\u591a\u6a21\u6001\u914d\u7f6e\u7684\u8bbe\u7f6e\u65f6\u95f4\u51cf\u5c11\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u8fd0\u884c\u65f6\u4f20\u611f\u5668\u70ed\u62d4\u63d2\u4e8b\u4ef6\u4e0b\u4fdd\u6301\u7b56\u7565\u6267\u884c\u3002", "conclusion": "RAPID\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u52a0\u901f\u673a\u5668\u4eba\u64cd\u7eb5\u7b56\u7565\u7684\u7814\u7a76\u4e0e\u53d1\u5c55\uff0c\u901a\u8fc7\u5176\u72ec\u7279\u7684\u786c\u4ef6\u548c\u8f6f\u4ef6\u89e3\u51b3\u65b9\u6848\u663e\u8457\u63d0\u9ad8\u4e86\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2602.06749", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06749", "abs": "https://arxiv.org/abs/2602.06749", "authors": ["Robert Wilbrandt", "R\u00fcdiger Dillmann"], "title": "Constraint Manifold Exploration for Efficient Continuous Coverage Estimation", "comment": "8 pages, 7 figures", "summary": "Many automated manufacturing processes rely on industrial robot arms to move process-specific tools along workpiece surfaces. In applications like grinding, sanding, spray painting, or inspection, they need to cover a workpiece fully while keeping their tools perpendicular to its surface. While there are approaches to generate trajectories for these applications, there are no sufficient methods for analyzing the feasibility of full surface coverage. This work proposes a sampling-based approach for continuous coverage estimation that explores reachable surface regions in the configuration space. We define an extended ambient configuration space that allows for the representation of tool position and orientation constraints. A continuation-based approach is used to explore it using two different sampling strategies. A thorough evaluation across different kinematics and environments analyzes their runtime and efficiency. This validates our ability to accurately and efficiently calculate surface coverage for complex surfaces in complicated environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u8fde\u7eed\u8986\u76d6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a2\u7d22\u914d\u7f6e\u7a7a\u95f4\u4e2d\u53ef\u5230\u8fbe\u7684\u8868\u9762\u533a\u57df\uff0c\u4ece\u800c\u5206\u6790\u5168\u8868\u9762\u8986\u76d6\u7684\u53ef\u884c\u6027\u3002\u901a\u8fc7\u5b9a\u4e49\u4e00\u4e2a\u6269\u5c55\u7684\u73af\u5883\u914d\u7f6e\u7a7a\u95f4\uff0c\u5e76\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u7684\u91c7\u6837\u7b56\u7565\u8fdb\u884c\u63a2\u7d22\uff0c\u6700\u7ec8\u5728\u4e0d\u540c\u8fd0\u52a8\u5b66\u548c\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u9ad8\u6548\u5730\u8ba1\u7b97\u590d\u6742\u73af\u5883\u4e0b\u7684\u8868\u9762\u8986\u76d6\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5de5\u4e1a\u673a\u5668\u4eba\u81c2\u5728\u8bf8\u5982\u7814\u78e8\u3001\u7802\u5149\u3001\u55b7\u6f06\u6216\u68c0\u67e5\u7b49\u5e94\u7528\u4e2d\u9700\u8981\u5b8c\u5168\u8986\u76d6\u5de5\u4ef6\u540c\u65f6\u4fdd\u6301\u5de5\u5177\u5782\u76f4\u4e8e\u5176\u8868\u9762\uff0c\u4f46\u7f3a\u4e4f\u5145\u5206\u7684\u65b9\u6cd5\u6765\u5206\u6790\u5168\u8868\u9762\u8986\u76d6\u7684\u53ef\u884c\u6027\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\u6765\u8fdb\u884c\u8fde\u7eed\u8986\u76d6\u4f30\u8ba1\uff0c\u901a\u8fc7\u5b9a\u4e49\u4e00\u4e2a\u6269\u5c55\u7684\u73af\u5883\u914d\u7f6e\u7a7a\u95f4\u6765\u8868\u793a\u5de5\u5177\u7684\u4f4d\u7f6e\u548c\u65b9\u5411\u7ea6\u675f\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u5ef6\u7eed\u7684\u65b9\u6cd5\u7ed3\u5408\u4e24\u79cd\u4e0d\u540c\u7684\u91c7\u6837\u7b56\u7565\u5bf9\u8be5\u7a7a\u95f4\u8fdb\u884c\u63a2\u7d22\u3002", "result": "\u7ecf\u8fc7\u5bf9\u4e0d\u540c\u8fd0\u52a8\u5b66\u7ed3\u6784\u548c\u73af\u5883\u7684\u7efc\u5408\u8bc4\u4ef7\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u8fd0\u884c\u65f6\u95f4\u548c\u6548\u7387\u65b9\u9762\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u80fd\u591f\u9488\u5bf9\u590d\u6742\u7684\u8868\u9762\u53ca\u73af\u5883\u51c6\u786e\u4e14\u9ad8\u6548\u5730\u8ba1\u7b97\u8868\u9762\u8986\u76d6\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u91c7\u6837\u7684\u8fde\u7eed\u8986\u76d6\u4f30\u8ba1\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u673a\u5668\u4eba\u81c2\u5728\u7279\u5b9a\u5e94\u7528\u573a\u666f\u4e0b\u5168\u8868\u9762\u8986\u76d6\u53ef\u884c\u6027\u5206\u6790\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06811", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06811", "abs": "https://arxiv.org/abs/2602.06811", "authors": ["Weibin Gu", "Chenrui Feng", "Lian Liu", "Chen Yang", "Xingchi Jiao", "Yuhe Ding", "Xiaofei Shi", "Chao Gao", "Alessandro Rizzo", "Guyue Zhou"], "title": "A 26-Gram Butterfly-Inspired Robot Achieving Autonomous Tailless Flight", "comment": null, "summary": "Flapping-wing micro air vehicles (FWMAVs) have demonstrated remarkable bio-inspired agility, yet tailless two-winged configurations remain largely unexplored due to their complex fluid-structure and wing-body coupling. Here we present \\textit{AirPulse}, a 26-gram butterfly-inspired FWMAV that achieves fully onboard, closed-loop, untethered flight without auxiliary control surfaces. The AirPulse robot replicates key biomechanical traits of butterfly flight, including low wing aspect ratio, compliant carbon-fiber-reinforced wings, and low-frequency, high-amplitude flapping that induces cyclic variations in the center of gravity and moment of inertia, producing characteristic body undulation. We establish a quantitative mapping between flapping modulation parameters and force-torque generation, and introduce the Stroke Timing Asymmetry Rhythm (STAR) generator, enabling smooth, stable, and linearly parameterized wingstroke asymmetry for flapping control. Integrating these with an attitude controller, the AirPulse robot maintains pitch and yaw stability despite strong oscillatory dynamics. Free-flight experiments demonstrate stable climbing and turning maneuvers via either angle offset or stroke timing modulation, marking the first onboard controlled flight of the lightest two-winged, tailless butterfly-inspired FWMAV reported in peer-reviewed literature. This work corroborates a foundational platform for lightweight, collision-proof FWMAVs, bridging biological inspiration with practical aerial robotics. Their non-invasive maneuverability is ideally suited for real-world applications, such as confined-space inspection and ecological monitoring, inaccessible to traditional drones, while their biomechanical fidelity provides a physical model to decode the principles underlying the erratic yet efficient flight of real butterflies.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aAirPulse\u768426\u514b\u4eff\u8774\u8776\u6251\u7ffc\u5fae\u578b\u98de\u884c\u5668\uff0c\u5b83\u5b9e\u73b0\u4e86\u5b8c\u5168\u81ea\u4e3b\u3001\u95ed\u73af\u4e14\u65e0\u9700\u8f85\u52a9\u63a7\u5236\u9762\u7684\u98de\u884c\u3002\u901a\u8fc7\u6a21\u4eff\u8774\u8776\u98de\u884c\u7684\u5173\u952e\u751f\u7269\u529b\u5b66\u7279\u6027\uff0c\u5e76\u7ed3\u5408\u4e13\u95e8\u8bbe\u8ba1\u7684\u59ff\u6001\u63a7\u5236\u5668\uff0cAirPulse\u80fd\u591f\u5728\u5f3a\u632f\u8361\u52a8\u529b\u5b66\u4e0b\u4fdd\u6301\u4fef\u4ef0\u548c\u504f\u822a\u7a33\u5b9a\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u98de\u884c\u5668\u80fd\u591f\u6267\u884c\u7a33\u5b9a\u7684\u722c\u5347\u548c\u8f6c\u5f2f\u52a8\u4f5c\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u78b0\u649e\u9632\u62a4\u6251\u7ffc\u98de\u884c\u5668\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u5e73\u53f0\uff0c\u5e76\u9002\u7528\u4e8e\u53d7\u9650\u7a7a\u95f4\u68c0\u67e5\u4e0e\u751f\u6001\u76d1\u6d4b\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u5c3d\u7ba1\u6251\u7ffc\u5fae\u578b\u98de\u884c\u5668\uff08FWMAVs\uff09\u5c55\u73b0\u4e86\u51fa\u8272\u7684\u751f\u7269\u542f\u53d1\u5f0f\u654f\u6377\u6027\uff0c\u4f46\u65e0\u5c3e\u53cc\u7ffc\u914d\u7f6e\u7531\u4e8e\u5176\u590d\u6742\u7684\u6d41\u4f53\u7ed3\u6784\u548c\u7ffc\u8eab\u8026\u5408\u95ee\u9898\u800c\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u53d7\u8774\u8776\u542f\u53d1\u7684\u8f7b\u578b\u6251\u7ffc\u5fae\u578b\u98de\u884c\u5668\uff0c\u4ee5\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u4e14\u65e0\u9700\u8f85\u52a9\u63a7\u5236\u9762\u7684\u98de\u884c\uff0c\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u4e86AirPulse\uff0c\u4e00\u6b3e\u91cd\u91cf\u4ec5\u4e3a26\u514b\u7684\u4eff\u8774\u8776\u6251\u7ffc\u5fae\u578b\u98de\u884c\u5668\u3002AirPulse\u91c7\u7528\u4e86\u4f4e\u5c55\u5f26\u6bd4\u7684\u67d4\u6027\u78b3\u7ea4\u7ef4\u589e\u5f3a\u7ffc\uff0c\u4ee5\u53ca\u4f4e\u9891\u9ad8\u5e45\u632f\u52a8\u6765\u6a21\u62df\u8774\u8776\u98de\u884c\u4e2d\u7684\u91cd\u5fc3\u548c\u60ef\u6027\u77e9\u5468\u671f\u53d8\u5316\uff0c\u4ece\u800c\u4ea7\u751f\u7279\u5f81\u6027\u7684\u8eab\u4f53\u6ce2\u52a8\u3002\u6b64\u5916\uff0c\u8fd8\u5efa\u7acb\u4e86\u62cd\u52a8\u8c03\u8282\u53c2\u6570\u4e0e\u529b\u77e9\u751f\u6210\u4e4b\u95f4\u7684\u5b9a\u91cf\u6620\u5c04\uff0c\u5e76\u5f15\u5165\u4e86Stroke Timing Asymmetry Rhythm (STAR) \u751f\u6210\u5668\uff0c\u4ee5\u652f\u6301\u5e73\u6ed1\u3001\u7a33\u5b9a\u4e14\u7ebf\u6027\u53c2\u6570\u5316\u7684\u62cd\u52a8\u4e0d\u5bf9\u79f0\u6027\u63a7\u5236\u3002", "result": "AirPulse\u6210\u529f\u5c55\u793a\u4e86\u5728\u5f3a\u632f\u8361\u52a8\u529b\u5b66\u6761\u4ef6\u4e0b\u7ef4\u6301\u4fef\u4ef0\u548c\u504f\u822a\u7a33\u5b9a\u6027\u7684\u80fd\u529b\u3002\u81ea\u7531\u98de\u884c\u5b9e\u9a8c\u663e\u793a\uff0c\u901a\u8fc7\u89d2\u5ea6\u504f\u79fb\u6216\u62cd\u6253\u65f6\u673a\u8c03\u8282\uff0c\u5b83\u53ef\u4ee5\u6267\u884c\u7a33\u5b9a\u7684\u722c\u5347\u548c\u8f6c\u5411\u52a8\u4f5c\u3002\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u6587\u732e\u62a5\u9053\u4e2d\u6700\u8f7b\u7684\u4e24\u7ffc\u65e0\u5c3e\u4eff\u751f\u6251\u7ffc\u5fae\u578b\u98de\u884c\u5668\u9996\u6b21\u5b9e\u73b0\u673a\u8f7d\u63a7\u5236\u98de\u884c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u5b9e\u4e86AirPulse\u4f5c\u4e3a\u8f7b\u91cf\u5316\u3001\u9632\u649e\u6251\u7ffc\u5fae\u578b\u98de\u884c\u5668\u7684\u57fa\u7840\u5e73\u53f0\u5730\u4f4d\uff0c\u4e0d\u4ec5\u8fde\u63a5\u4e86\u751f\u7269\u5b66\u7075\u611f\u4e0e\u5b9e\u7528\u7a7a\u4e2d\u673a\u5668\u4eba\u6280\u672f\uff0c\u800c\u4e14\u5176\u975e\u4fb5\u5165\u5f0f\u7684\u673a\u52a8\u6027\u80fd\u975e\u5e38\u9002\u5408\u4e8e\u53d7\u9650\u7a7a\u95f4\u68c0\u6d4b\u548c\u751f\u6001\u76d1\u63a7\u7b49\u771f\u5b9e\u4e16\u754c\u5e94\u7528\uff0c\u540c\u65f6\u4e3a\u7406\u89e3\u771f\u5b9e\u8774\u8776\u9ad8\u6548\u98de\u884c\u80cc\u540e\u7684\u539f\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7269\u7406\u6a21\u578b\u3002"}}
{"id": "2602.06834", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06834", "abs": "https://arxiv.org/abs/2602.06834", "authors": ["Allen Tao", "Jun Yang", "Stanko Oparnica", "Wenjie Xue"], "title": "Perception-Control Coupled Visual Servoing for Textureless Objects Using Keypoint-Based EKF", "comment": null, "summary": "Visual servoing is fundamental to robotic applications, enabling precise positioning and control. However, applying it to textureless objects remains a challenge due to the absence of reliable visual features. Moreover, adverse visual conditions, such as occlusions, often corrupt visual feedback, leading to reduced accuracy and instability in visual servoing. In this work, we build upon learning-based keypoint detection for textureless objects and propose a method that enhances robustness by tightly integrating perception and control in a closed loop. Specifically, we employ an Extended Kalman Filter (EKF) that integrates per-frame keypoint measurements to estimate 6D object pose, which drives pose-based visual servoing (PBVS) for control. The resulting camera motion, in turn, enhances the tracking of subsequent keypoints, effectively closing the perception-control loop. Additionally, unlike standard PBVS, we propose a probabilistic control law that computes both camera velocity and its associated uncertainty, enabling uncertainty-aware control for safe and reliable operation. We validate our approach on real-world robotic platforms using quantitative metrics and grasping experiments, demonstrating that our method outperforms traditional visual servoing techniques in both accuracy and practical application.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b66\u4e60\u578b\u5173\u952e\u70b9\u68c0\u6d4b\u4e0e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668(EKF)\u7684\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u65e0\u7eb9\u7406\u7269\u4f53\u89c6\u89c9\u4f3a\u670d\u7684\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u7d27\u5bc6\u96c6\u6210\u611f\u77e5\u4e0e\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u59ff\u6001\u7684\u89c6\u89c9\u4f3a\u670d(PBVS)\uff0c\u5e76\u5f15\u5165\u4e86\u6982\u7387\u63a7\u5236\u5f8b\u6765\u8ba1\u7b97\u76f8\u673a\u901f\u5ea6\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u8fd0\u884c\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u4f18\u4e8e\u4f20\u7edf\u89c6\u89c9\u4f3a\u670d\u6280\u672f\u3002", "motivation": "\u89c6\u89c9\u4f3a\u670d\u662f\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u57fa\u672c\u6280\u672f\uff0c\u4f46\u5728\u65e0\u7eb9\u7406\u7269\u4f53\u4ee5\u53ca\u4e0d\u5229\u89c6\u89c9\u6761\u4ef6\u4e0b\uff08\u5982\u906e\u6321\uff09\u7684\u5e94\u7528\u9762\u4e34\u6311\u6218\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u63d0\u9ad8\u89c6\u89c9\u4f3a\u670d\u7cfb\u7edf\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u91c7\u7528\u5b66\u4e60\u578b\u5173\u952e\u70b9\u68c0\u6d4b\u6280\u672f\u8bc6\u522b\u65e0\u7eb9\u7406\u7269\u4f53\u7684\u5173\u952e\u70b9\uff0c\u5e76\u5229\u7528EKF\u6574\u5408\u6bcf\u5e27\u7684\u5173\u952e\u70b9\u6d4b\u91cf\u6570\u636e\u4f30\u8ba16D\u7269\u4f53\u59ff\u6001\uff0c\u8fdb\u800c\u9a71\u52a8PBVS\u8fdb\u884c\u63a7\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8ba1\u7b97\u76f8\u673a\u901f\u5ea6\u53ca\u4e0d\u786e\u5b9a\u6027\u7684\u6982\u7387\u63a7\u5236\u5f8b\uff0c\u4ee5\u6b64\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u610f\u8bc6\u4e0b\u7684\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u4e0d\u4ec5\u5728\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u89c6\u89c9\u4f3a\u670d\u6280\u672f\uff0c\u5728\u6293\u53d6\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u4e5f\u5c55\u73b0\u4e86\u66f4\u4f18\u7684\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7d27\u5bc6\u96c6\u6210\u611f\u77e5\u4e0e\u63a7\u5236\u673a\u5236\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u9488\u5bf9\u65e0\u7eb9\u7406\u7269\u4f53\u7684\u89c6\u89c9\u4f3a\u670d\u6027\u80fd\u3002\u63d0\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u610f\u8bc6\u63a7\u5236\u7b56\u7565\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2602.06864", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06864", "abs": "https://arxiv.org/abs/2602.06864", "authors": ["Zhuocheng Zhang", "Haizhou Zhao", "Xudong Sun", "Aaron M. Johnson", "Majid Khadiv"], "title": "SURE: Safe Uncertainty-Aware Robot-Environment Interaction using Trajectory Optimization", "comment": null, "summary": "Robotic tasks involving contact interactions pose significant challenges for trajectory optimization due to discontinuous dynamics. Conventional formulations typically assume deterministic contact events, which limit robustness and adaptability in real-world settings. In this work, we propose SURE, a robust trajectory optimization framework that explicitly accounts for contact timing uncertainty. By allowing multiple trajectories to branch from possible pre-impact states and later rejoin a shared trajectory, SURE achieves both robustness and computational efficiency within a unified optimization framework. We evaluate SURE on two representative tasks with unknown impact times. In a cart-pole balancing task involving uncertain wall location, SURE achieves an average improvement of 21.6% in success rate when branch switching is enabled during control. In an egg-catching experiment using a robotic manipulator, SURE improves the success rate by 40%. These results demonstrate that SURE substantially enhances robustness compared to conventional nominal formulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f68\u8ff9\u4f18\u5316\u6846\u67b6SURE\uff0c\u901a\u8fc7\u8003\u8651\u63a5\u89e6\u65f6\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u63a5\u89e6\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4e0d\u786e\u5b9a\u5899\u58c1\u4f4d\u7f6e\u7684\u6446\u6746\u5e73\u8861\u4efb\u52a1\u4e2d\u548c\u4f7f\u7528\u673a\u68b0\u81c2\u7684\u63a5\u86cb\u5b9e\u9a8c\u4e2d\uff0cSURE\u5206\u522b\u63d0\u9ad8\u4e8621.6%\u548c40%\u7684\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u76f8\u5bf9\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "motivation": "\u7531\u4e8e\u63a5\u89e6\u4e92\u52a8\u4e2d\u7684\u4e0d\u8fde\u7eed\u52a8\u529b\u5b66\u7279\u6027\uff0c\u4f20\u7edf\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u786e\u5b9a\u6027\u7684\u63a5\u89e6\u4e8b\u4ef6\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u663e\u5f0f\u8003\u8651\u63a5\u89e6\u65f6\u95f4\u4e0d\u786e\u5b9a\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5904\u7406\u63a5\u89e6\u4efb\u52a1\u65f6\u7684\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aSURE\u7684\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5141\u8bb8\u4ece\u53ef\u80fd\u7684\u9884\u51b2\u51fb\u72b6\u6001\u5206\u652f\u51fa\u591a\u6761\u8f68\u8ff9\uff0c\u5e76\u968f\u540e\u91cd\u65b0\u52a0\u5165\u5171\u4eab\u8f68\u8ff9\uff0c\u4ece\u800c\u5728\u4e00\u4e2a\u7edf\u4e00\u7684\u4f18\u5316\u6846\u67b6\u5185\u540c\u65f6\u5b9e\u73b0\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "SURE\u5728\u4e24\u4e2a\u5177\u6709\u672a\u77e5\u51b2\u51fb\u65f6\u95f4\u7684\u4ee3\u8868\u6027\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff1a\u4e00\u662f\u5728\u4e0d\u786e\u5b9a\u5899\u58c1\u4f4d\u7f6e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6446\u6746\u5e73\u8861\u4efb\u52a1\uff1b\u4e8c\u662f\u7528\u673a\u68b0\u81c2\u8fdb\u884c\u63a5\u86cb\u5b9e\u9a8c\u3002\u7ed3\u679c\u663e\u793a\uff0cSURE\u5728\u63a7\u5236\u8fc7\u7a0b\u4e2d\u542f\u7528\u5206\u652f\u5207\u6362\u65f6\uff0c\u6210\u529f\u7387\u8fbe\u5230\u5e73\u574721.6%\u7684\u63d0\u5347\uff1b\u800c\u5728\u63a5\u86cb\u5b9e\u9a8c\u4e2d\uff0c\u6210\u529f\u7387\u63d0\u5347\u4e8640%\u3002", "conclusion": "SURE\u901a\u8fc7\u660e\u786e\u8003\u8651\u63a5\u89e6\u65f6\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u8f68\u8ff9\u4f18\u5316\u5bf9\u4e8e\u63a5\u89e6\u4ea4\u4e92\u4efb\u52a1\u7684\u9c81\u68d2\u6027\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684\u540d\u4e49\u914d\u5236\u65b9\u6cd5\u5c55\u73b0\u51fa\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2602.06868", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.06868", "abs": "https://arxiv.org/abs/2602.06868", "authors": ["Xudong Sun", "Armand Jordana", "Massimo Fornasier", "Jalal Etesami", "Majid Khadiv"], "title": "Consensus-based optimization (CBO): Towards Global Optimality in Robotics", "comment": null, "summary": "Zero-order optimization has recently received significant attention for designing optimal trajectories and policies for robotic systems. However, most existing methods (e.g., MPPI, CEM, and CMA-ES) are local in nature, as they rely on gradient estimation. In this paper, we introduce consensus-based optimization (CBO) to robotics, which is guaranteed to converge to a global optimum under mild assumptions. We provide theoretical analysis and illustrative examples that give intuition into the fundamental differences between CBO and existing methods. To demonstrate the scalability of CBO for robotics problems, we consider three challenging trajectory optimization scenarios: (1) a long-horizon problem for a simple system, (2) a dynamic balance problem for a highly underactuated system, and (3) a high-dimensional problem with only a terminal cost. Our results show that CBO is able to achieve lower costs with respect to existing methods on all three challenging settings. This opens a new framework to study global trajectory optimization in robotics.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u5171\u8bc6\u4f18\u5316(CBO)\u7684\u65b9\u6cd5\u5230\u673a\u5668\u4eba\u9886\u57df\uff0c\u8be5\u65b9\u6cd5\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u4fdd\u8bc1\u6536\u655b\u81f3\u5168\u5c40\u6700\u4f18\u89e3\u3002\u901a\u8fc7\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u8f68\u8ff9\u4f18\u5316\u573a\u666f\u5bf9\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08\u5982MPPI\u3001CEM\u548cCMA-ES\uff09\uff0c\u7ed3\u679c\u663e\u793aCBO\u80fd\u591f\u5728\u6240\u6709\u4e09\u79cd\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u4f4e\u7684\u6210\u672c\u3002", "motivation": "\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u8fd1\u6765\u53d7\u5230\u5173\u6ce8\uff0c\u7528\u4e8e\u8bbe\u8ba1\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6700\u4f18\u8f68\u8ff9\u4e0e\u7b56\u7565\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u65b9\u6cd5\u672c\u8d28\u4e0a\u662f\u5c40\u90e8\u7684\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u68af\u5ea6\u4f30\u8ba1\u3002\u672c\u6587\u65e8\u5728\u4e3a\u673a\u5668\u4eba\u95ee\u9898\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u80fd\u591f\u627e\u5230\u5168\u5c40\u6700\u4f18\u89e3\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u4ecb\u7ecd\u5e76\u91c7\u7528\u4e86\u5171\u8bc6\u4f18\u5316(CBO)\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u53ca\u793a\u4f8b\u4ee5\u9610\u660eCBO\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e4b\u95f4\u7684\u57fa\u672c\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4e09\u4e2a\u5177\u4f53\u6848\u4f8b\u7814\u7a76\u4e86CBO\u5728\u5904\u7406\u957f\u671f\u89c4\u5212\u3001\u9ad8\u5ea6\u6b20\u9a71\u52a8\u7cfb\u7edf\u52a8\u6001\u5e73\u8861\u4ee5\u53ca\u4ec5\u542b\u7ec8\u7aef\u6210\u672c\u7684\u9ad8\u7ef4\u95ee\u9898\u65f6\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u6240\u6709\u4e09\u4e2a\u6d4b\u8bd5\u573a\u666f\u4e2d\uff0c\u76f8\u6bd4\u4e8e\u73b0\u6709\u6280\u672f\uff0cCBO\u80fd\u591f\u8fbe\u5230\u66f4\u4f4e\u7684\u6210\u672c\uff0c\u663e\u793a\u4e86\u5176\u5728\u89e3\u51b3\u590d\u6742\u673a\u5668\u4eba\u95ee\u9898\u4e0a\u7684\u6f5c\u529b\u3002", "conclusion": "CBO\u4e3a\u673a\u5668\u4eba\u9886\u57df\u7684\u5168\u5c40\u8f68\u8ff9\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5176\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u4e00\u7cfb\u5217\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.06925", "categories": ["cs.RO", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.06925", "abs": "https://arxiv.org/abs/2602.06925", "authors": ["Andrei-Carlo Papuc", "Lasse Peters", "Sihao Sun", "Laura Ferranti", "Javier Alonso-Mora"], "title": "Strategizing at Speed: A Learned Model Predictive Game for Multi-Agent Drone Racing", "comment": null, "summary": "Autonomous drone racing pushes the boundaries of high-speed motion planning and multi-agent strategic decision-making. Success in this domain requires drones not only to navigate at their limits but also to anticipate and counteract competitors' actions. In this paper, we study a fundamental question that arises in this domain: how deeply should an agent strategize before taking an action? To this end, we compare two planning paradigms: the Model Predictive Game (MPG), which finds interaction-aware strategies at the expense of longer computation times, and contouring Model Predictive Control (MPC), which computes strategies rapidly but does not reason about interactions. We perform extensive experiments to study this trade-off, revealing that MPG outperforms MPC at moderate velocities but loses its advantage at higher speeds due to latency. To address this shortcoming, we propose a Learned Model Predictive Game (LMPG) approach that amortizes model predictive gameplay to reduce latency. In both simulation and hardware experiments, we benchmark our approach against MPG and MPC in head-to-head races, finding that LMPG outperforms both baselines.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u81ea\u4e3b\u65e0\u4eba\u673a\u7ade\u901f\u4e2d\uff0c\u7b56\u7565\u89c4\u5212\u6df1\u5ea6\u5bf9\u8868\u73b0\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5bf9\u6bd4\u6a21\u578b\u9884\u6d4b\u535a\u5f08\uff08MPG\uff09\u548c\u8f6e\u5ed3\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u4e24\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u867d\u7136MPG\u5728\u4e2d\u7b49\u901f\u5ea6\u4e0b\u4f18\u4e8eMPC\uff0c\u4f46\u5728\u9ad8\u901f\u65f6\u7531\u4e8e\u5ef6\u8fdf\u800c\u5931\u53bb\u4f18\u52bf\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u578b\u6a21\u578b\u9884\u6d4b\u535a\u5f08\uff08LMPG\uff09\u65b9\u6cd5\u6765\u51cf\u5c11\u5ef6\u8fdf\uff0c\u5e76\u5728\u6a21\u62df\u548c\u5b9e\u9645\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86LMPG\u4f18\u4e8eMPG\u548cMPC\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u5728\u65e0\u4eba\u673a\u7ade\u901f\u9886\u57df\u5185\uff0c\u667a\u80fd\u4f53\u5728\u91c7\u53d6\u884c\u52a8\u524d\u5e94\u8fbe\u5230\u7684\u7b56\u7565\u89c4\u5212\u6df1\u5ea6\uff0c\u4ee5\u89e3\u51b3\u5982\u4f55\u5e73\u8861\u8003\u8651\u5bf9\u624b\u4e92\u52a8\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u95ee\u9898\u3002", "method": "\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e24\u79cd\u89c4\u5212\u8303\u5f0f\uff1a\u6a21\u578b\u9884\u6d4b\u535a\u5f08(MPG) \u548c \u8f6e\u5ed3\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408\u4e24\u8005\u4f18\u70b9\u7684\u5b66\u4e60\u578b\u6a21\u578b\u9884\u6d4b\u535a\u5f08(LMPG)\u65b9\u6cd5\u3002\u901a\u8fc7\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u4e0d\u540c\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e2d\u7b49\u901f\u5ea6\u6761\u4ef6\u4e0b\uff0cMPG\u7684\u8868\u73b0\u4f18\u4e8eMPC\uff1b\u7136\u800c\u968f\u7740\u901f\u5ea6\u589e\u52a0\uff0c\u56e0\u5ef6\u8fdf\u95ee\u9898\u5bfc\u81f4MPG\u4e0d\u518d\u5177\u6709\u4f18\u52bf\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cLMPG\u80fd\u591f\u5728\u51cf\u5c11\u5ef6\u8fdf\u7684\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u91c7\u7528LMPG\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5e73\u8861\u7b56\u7565\u89c4\u5212\u4e2d\u7684\u4ea4\u4e92\u610f\u8bc6\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e3a\u65e0\u4eba\u673a\u7ade\u901f\u63d0\u4f9b\u66f4\u4f18\u89e3\u3002"}}
