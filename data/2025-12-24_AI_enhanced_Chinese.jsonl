{"id": "2512.19855", "categories": ["cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.19855", "abs": "https://arxiv.org/abs/2512.19855", "authors": ["Andrew Stirling", "Mykola Lukashchuk", "Dmitry Bagaev", "Wouter Kouw", "James R. Forbes"], "title": "Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study", "comment": null, "summary": "This letter extends the exactly sparse Gaussian variational inference (ESGVI) algorithm for state estimation in two complementary directions. First, ESGVI is generalized to operate on matrix Lie groups, enabling the estimation of states with orientation components while respecting the underlying group structure. Second, factors are introduced to accommodate heavy-tailed and skewed noise distributions, as commonly encountered in ultra-wideband (UWB) localization due to non-line-of-sight (NLOS) and multipath effects. Both extensions are shown to integrate naturally within the ESGVI framework while preserving its sparse and derivative-free structure. The proposed approach is validated in a UWB localization experiment with NLOS-rich measurements, demonstrating improved accuracy and comparable consistency. Finally, a Python implementation within a factor-graph-based estimation framework is made open-source (https://github.com/decargroup/gvi_ws) to support broader research use.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86ESGVI\u7b97\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u77e9\u9635\u674e\u7fa4\u4e0a\u8fd0\u884c\uff0c\u5e76\u5f15\u5165\u4e86\u5904\u7406\u91cd\u5c3e\u548c\u504f\u659c\u566a\u58f0\u5206\u5e03\u7684\u56e0\u7d20\uff0c\u7279\u522b\u9002\u7528\u4e8eUWB\u5b9a\u4f4d\u4e2d\u7684NLOS\u548c\u591a\u8def\u5f84\u6548\u5e94\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u5e76\u4fdd\u6301\u4e86\u4e00\u81f4\u6027\u3002", "motivation": "\u4f5c\u8005\u5e0c\u671b\u5c06ESGVI\u7b97\u6cd5\u63a8\u5e7f\u5230\u77e9\u9635\u674e\u7fa4\u4e0a\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\uff0c\u5e76\u89e3\u51b3UWB\u5b9a\u4f4d\u4e2d\u7531\u4e8eNLOS\u53ca\u591a\u8def\u5f84\u6548\u5e94\u5bfc\u81f4\u7684\u91cd\u5c3e\u548c\u504f\u659c\u566a\u58f0\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06ESGVI\u7b97\u6cd5\u6cdb\u5316\u81f3\u77e9\u9635\u674e\u7fa4\u4ee5\u6b63\u786e\u5904\u7406\u5305\u542b\u65b9\u5411\u5206\u91cf\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u5e76\u5f15\u5165\u65b0\u56e0\u7d20\u6765\u9002\u5e94\u91cd\u5c3e\u548c\u504f\u659c\u566a\u58f0\u5206\u5e03\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5bcc\u542bNLOS\u6d4b\u91cf\u503c\u7684UWB\u5b9a\u4f4d\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u6709\u6548\uff0c\u663e\u793a\u51fa\u4e86\u66f4\u9ad8\u7684\u7cbe\u5ea6\u4ee5\u53ca\u76f8\u5f53\u7684\u4e00\u81f4\u6027\u8868\u73b0\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6210\u529f\u5730\u62d3\u5c55\u4e86ESGVI\u6846\u67b6\u7684\u529f\u80fd\u8303\u56f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5176\u7a00\u758f\u6027\u548c\u65e0\u5bfc\u6570\u7ed3\u6784\u7684\u7279\u70b9\uff0c\u5e76\u4e14\u901a\u8fc7\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u4e86\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u5e94\u7528\u3002"}}
{"id": "2512.19914", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.19914", "abs": "https://arxiv.org/abs/2512.19914", "authors": ["Sujan Warnakulasooriya", "Andreas Willig", "Xiaobing Wu"], "title": "A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones", "comment": "35 pages", "summary": "Drone applications continue to expand across various domains, with flocking offering enhanced cooperative capabilities but introducing significant challenges during initial formation. Existing flocking algorithms often struggle with efficiency and scalability, particularly when potential collisions force drones into suboptimal trajectories. This paper presents a time-efficient prioritised scheduling algorithm that improves the initial formation process of drone flocks. The method assigns each drone a priority based on its number of potential collisions and its likelihood of reaching its target position without permanently obstructing other drones. Using this hierarchy, each drone computes an appropriate delay to ensure a collision-free path. Simulation results show that the proposed algorithm successfully generates collision-free trajectories for flocks of up to 5000 drones and outperforms the coupling-degree-based heuristic prioritised planning method (CDH-PP) in both performance and computational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5148\u7ea7\u8c03\u5ea6\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u65e0\u4eba\u673a\u7fa4\u5728\u521d\u59cb\u5f62\u6210\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\u3002\u8be5\u7b97\u6cd5\u6839\u636e\u6bcf\u67b6\u65e0\u4eba\u673a\u6f5c\u5728\u78b0\u649e\u6b21\u6570\u53ca\u5176\u65e0\u969c\u788d\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u7684\u53ef\u80fd\u6027\u6765\u5206\u914d\u4f18\u5148\u7ea7\uff0c\u5e76\u8ba1\u7b97\u9002\u5f53\u7684\u5ef6\u8fdf\u4ee5\u786e\u4fdd\u65e0\u78b0\u649e\u8def\u5f84\u3002\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u4e3a\u591a\u8fbe5000\u67b6\u65e0\u4eba\u673a\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u57fa\u4e8e\u8026\u5408\u5ea6\u7684\u542f\u53d1\u5f0f\u4f18\u5148\u89c4\u5212\u65b9\u6cd5\uff08CDH-PP\uff09\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5e94\u7528\u9886\u57df\u7684\u4e0d\u65ad\u6269\u5c55\uff0c\u96c6\u7fa4\u6280\u672f\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u5408\u4f5c\u80fd\u529b\u4f46\u4e5f\u5e26\u6765\u4e86\u521d\u59cb\u5316\u5f62\u6210\u9636\u6bb5\u7684\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u7fa4\u96c6\u7b97\u6cd5\u5f80\u5f80\u96be\u4ee5\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u5730\u89e3\u51b3\u5f53\u6f5c\u5728\u78b0\u649e\u8feb\u4f7f\u65e0\u4eba\u673a\u91c7\u53d6\u6b21\u4f18\u8f68\u8ff9\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u9ad8\u6548\u7684\u4f18\u5148\u7ea7\u8c03\u5ea6\u7b97\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u6bcf\u67b6\u65e0\u4eba\u673a\u53ef\u80fd\u9047\u5230\u7684\u78b0\u649e\u6570\u91cf\u4ee5\u53ca\u5176\u65e0\u969c\u788d\u8fbe\u5230\u6307\u5b9a\u4f4d\u7f6e\u7684\u6982\u7387\u6765\u4e3a\u5176\u5206\u914d\u4f18\u5148\u7ea7\uff1b\u7136\u540e\uff0c\u4f9d\u636e\u6b64\u4f18\u5148\u7ea7\u987a\u5e8f\u8ba9\u6bcf\u67b6\u65e0\u4eba\u673a\u8ba1\u7b97\u51fa\u4e00\u4e2a\u5408\u9002\u7684\u5ef6\u65f6\uff0c\u4ece\u800c\u4fdd\u8bc1\u6240\u6709\u65e0\u4eba\u673a\u90fd\u80fd\u6cbf\u7740\u65e0\u78b0\u649e\u8def\u5f84\u79fb\u52a8\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u4e3a\u6700\u591a\u5305\u542b5000\u67b6\u65e0\u4eba\u673a\u7684\u7fa4\u4f53\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u90fd\u4f18\u4e8e\u57fa\u4e8e\u8026\u5408\u5ea6\u7684\u542f\u53d1\u5f0f\u4f18\u5148\u89c4\u5212\u65b9\u6cd5\uff08CDH-PP\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u53d1\u7684\u65f6\u95f4\u9ad8\u6548\u4f18\u5148\u8c03\u5ea6\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u7fa4\u5728\u521d\u59cb\u5f62\u6210\u8fc7\u7a0b\u4e2d\u907f\u514d\u78b0\u649e\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u7f16\u961f\u64cd\u4f5c\u3002"}}
{"id": "2512.20014", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20014", "abs": "https://arxiv.org/abs/2512.20014", "authors": ["Sangoh Lee", "Sangwoo Mo", "Wook-Shin Han"], "title": "Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting", "comment": null, "summary": "While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as \"bring my cup\", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u89c6\u89c9\u6ce8\u610f\u63d0\u793a\uff08VAP\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u5728\u5904\u7406\u4e2a\u6027\u5316\u6307\u4ee4\u65f6\u8bc6\u522b\u548c\u64cd\u4f5c\u7279\u5b9a\u7528\u6237\u5bf9\u8c61\u7684\u96be\u9898\u3002\u901a\u8fc7\u5c06\u53c2\u8003\u56fe\u50cf\u4f5c\u4e3a\u975e\u53c2\u6570\u89c6\u89c9\u8bb0\u5fc6\uff0c\u5e76\u5229\u7528\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u4e0e\u5d4c\u5165\u5f0f\u5339\u914d\u6765\u5b9a\u4f4d\u4e2a\u4eba\u7269\u54c1\uff0c\u7136\u540e\u5c06\u6b64\u5b9a\u4f4d\u4f5c\u4e3a\u89c6\u89c9\u63d0\u793a\u6ce8\u5165\uff0c\u4ee5\u7a81\u51fa\u663e\u793a\u5bf9\u8c61\u5e76\u91cd\u5199\u6307\u4ee4\u3002\u5b9e\u9a8c\u8868\u660e\uff0cVAP\u5728\u6210\u529f\u7387\u548c\u6b63\u786e\u7269\u4f53\u64cd\u4f5c\u65b9\u9762\u4f18\u4e8e\u901a\u7528\u7b56\u7565\u548c\u5176\u4ed6\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u867d\u7136\u80fd\u591f\u5f88\u597d\u5730\u6267\u884c\u901a\u7528\u6307\u4ee4\uff0c\u4f46\u5728\u9762\u5bf9\u4e2a\u6027\u5316\u547d\u4ee4\u5982\u201c\u62ff\u6211\u7684\u676f\u5b50\u201d\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u7c7b\u4efb\u52a1\u8981\u6c42\u673a\u5668\u4eba\u80fd\u591f\u5728\u89c6\u89c9\u4e0a\u76f8\u4f3c\u7684\u5bf9\u8c61\u4e2d\u8bc6\u522b\u5e76\u4f5c\u7528\u4e8e\u67d0\u4e00\u7279\u5b9a\u5b9e\u4f8b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u63a2\u7d22\u4e86\u5982\u4f55\u8ba9VLA\u6a21\u578b\u80fd\u591f\u57fa\u4e8e\u5c11\u91cf\u53c2\u8003\u56fe\u7247\u8bc6\u522b\u5e76\u63a7\u5236\u672a\u66fe\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u89c1\u8fc7\u7684\u7528\u6237\u7279\u5b9a\u5bf9\u8c61\u3002", "method": "\u63d0\u51fa\u4e86\u89c6\u89c9\u6ce8\u610f\u63d0\u793a(VAP)\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u611f\u77e5\u9002\u914d\u5668\uff0c\u5b83\u4f7f\u51bb\u7ed3\u72b6\u6001\u4e0b\u7684VLA\u6a21\u578b\u5177\u5907\u81ea\u9876\u5411\u4e0b\u7684\u9009\u62e9\u6027\u6ce8\u610f\u529b\u80fd\u529b\u3002VAP\u5c06\u63d0\u4f9b\u7684\u53c2\u8003\u56fe\u7247\u89c6\u4e3a\u4e00\u79cd\u975e\u53c2\u6570\u5316\u7684\u89c6\u89c9\u8bb0\u5fc6\u5e93\uff0c\u901a\u8fc7\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u53ca\u57fa\u4e8e\u5d4c\u5165\u7684\u5339\u914d\u6280\u672f\u6765\u573a\u666f\u4e2d\u5b9a\u4f4d\u4e2a\u4eba\u7269\u54c1\uff0c\u5e76\u901a\u8fc7\u5f3a\u8c03\u8be5\u7269\u54c1\u548c\u6539\u5199\u539f\u59cb\u6307\u4ee4\u7684\u65b9\u5f0f\u5c06\u5176\u4f5c\u4e3a\u89c6\u89c9\u63d0\u793a\u8fdb\u884c\u6ce8\u5165\u3002", "result": "\u901a\u8fc7\u6784\u5efa\u4e24\u4e2a\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u73af\u5883Personalized-SIMPLER\u548c\u4e2a\u4eba\u5316VLABench\u4ee5\u53ca\u4e00\u4e2a\u73b0\u5b9e\u4e16\u754c\u7684\u684c\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u591a\u4e2a\u673a\u5668\u4eba\u548c\u4efb\u52a1\u4e0a\u7684\u4e2a\u6027\u5316\u64cd\u63a7\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u6210\u529f\u7387\u548c\u6b63\u786e\u5bf9\u8c61\u64cd\u63a7\u65b9\u9762\uff0cVAP\u6301\u7eed\u4f18\u4e8e\u901a\u7528\u7b56\u7565\u4ee5\u53ca\u5176\u4ed6\u57fa\u4e8e\u5b66\u4e60\u4ee4\u724c\u7684\u57fa\u7840\u65b9\u6cd5\u3002", "conclusion": "\u89c6\u89c9\u6ce8\u610f\u63d0\u793a(VAP)\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\u6765\u589e\u5f3aVLA\u6a21\u578b\u5904\u7406\u4e2a\u6027\u5316\u547d\u4ee4\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5f53\u9700\u8981\u4ece\u89c6\u89c9\u4e0a\u7c7b\u4f3c\u4f46\u5177\u6709\u4e0d\u540c\u610f\u4e49\u7684\u5bf9\u8c61\u4e2d\u51c6\u786e\u8bc6\u522b\u51fa\u7279\u5b9a\u4e2a\u4f53\u65f6\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u52a9\u4e8e\u7f29\u5c0f\u8bed\u4e49\u7406\u89e3\u548c\u5b9e\u4f8b\u7ea7\u63a7\u5236\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2512.20166", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20166", "abs": "https://arxiv.org/abs/2512.20166", "authors": ["Xiaofan Wang", "Xingyu Gao", "Jianlong Fu", "Zuolei Li", "Dean Fortier", "Galen Mullins", "Andrey Kolobov", "Baining Guo"], "title": "LoLA: Long Horizon Latent Action Learning for General Robot Manipulation", "comment": null, "summary": "The capability of performing long-horizon, language-guided robotic manipulation tasks critically relies on leveraging historical information and generating coherent action sequences. However, such capabilities are often overlooked by existing Vision-Language-Action (VLA) models. To solve this challenge, we propose LoLA (Long Horizon Latent Action Learning), a framework designed for robot manipulation that integrates long-term multi-view observations and robot proprioception to enable multi-step reasoning and action generation. We first employ Vision-Language Models to encode rich contextual features from historical sequences and multi-view observations. We further introduces a key module, State-Aware Latent Re-representation, which transforms visual inputs and language commands into actionable robot motion space. Unlike existing VLA approaches that merely concatenate robot proprioception (e.g., joint angles) with VL embeddings, this module leverages such robot states to explicitly ground VL representations in physical scale through a learnable \"embodiment-anchored\" latent space. We trained LoLA on diverse robotic pre-training datasets and conducted extensive evaluations on simulation benchmarks (SIMPLER and LIBERO), as well as two real-world tasks on Franka and Bi-Manual Aloha robots. Results show that LoLA significantly outperforms prior state-of-the-art methods (e.g., pi0), particularly in long-horizon manipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86LoLA\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u957f\u671f\u591a\u89c6\u89d2\u89c2\u5bdf\u548c\u673a\u5668\u4eba\u672c\u4f53\u611f\u77e5\u6765\u5b9e\u73b0\u591a\u6b65\u9aa4\u63a8\u7406\u548c\u52a8\u4f5c\u751f\u6210\uff0c\u4ece\u800c\u5728\u957f\u65f6\u5e8f\u3001\u8bed\u8a00\u5f15\u5bfc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709Vision-Language-Action\uff08VLA\uff09\u6a21\u578b\u901a\u5e38\u5ffd\u7565\u4e86\u5229\u7528\u5386\u53f2\u4fe1\u606f\u4ee5\u53ca\u751f\u6210\u8fde\u8d2f\u52a8\u4f5c\u5e8f\u5217\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u5bf9\u4e8e\u6267\u884c\u957f\u65f6\u5e8f\u3001\u8bed\u8a00\u6307\u5bfc\u4e0b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u540d\u4e3aLoLA\uff08Long Horizon Latent Action Learning\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u9996\u5148\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4ece\u5386\u53f2\u5e8f\u5217\u548c\u591a\u89c6\u89d2\u89c2\u5bdf\u4e2d\u7f16\u7801\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\uff1b\u63a5\u7740\u5f15\u5165\u4e86\u4e00\u4e2a\u5173\u952e\u6a21\u5757\u2014\u2014\u72b6\u6001\u611f\u77e5\u6f5c\u8868\u793a\u91cd\u6784\uff0c\u5c06\u89c6\u89c9\u8f93\u5165\u548c\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u53ef\u64cd\u4f5c\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u201c\u4f53\u73b0\u951a\u5b9a\u201d\u6f5c\u5728\u7a7a\u95f4\u660e\u786e\u5730\u5c06VL\u8868\u5f81\u4e0e\u7269\u7406\u5c3a\u5ea6\u8054\u7cfb\u8d77\u6765\u3002", "result": "LoLA\u5728\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u5e76\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\uff08SIMPLER\u548cLIBERO\uff09\u53ca\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u957f\u65f6\u5e8f\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff08\u5982pi0\uff09\u3002", "conclusion": "LoLA\u901a\u8fc7\u521b\u65b0\u5730\u7ed3\u5408\u957f\u671f\u89c2\u6d4b\u6570\u636e\u4e0e\u673a\u5668\u4eba\u81ea\u8eab\u611f\u77e5\u4fe1\u606f\uff0c\u5728\u5904\u7406\u9700\u8981\u957f\u65f6\u95f4\u89c4\u5212\u7684\u8bed\u8a00\u5f15\u5bfc\u578b\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u65b9\u9762\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20188", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20188", "abs": "https://arxiv.org/abs/2512.20188", "authors": ["Teqiang Zou", "Hongliang Zeng", "Yuxuan Nong", "Yifan Li", "Kehui Liu", "Haotian Yang", "Xinyang Ling", "Xin Li", "Lianyang Ma"], "title": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation", "comment": null, "summary": "Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6b65\u7684Fast-Slow VLA\u6846\u67b6\uff08DuoCore-FS\uff09\uff0c\u901a\u8fc7\u533a\u5206\u9ad8\u9891\u52a8\u4f5c\u751f\u6210\u548c\u4f4e\u9891\u4e30\u5bcc\u7684VLM\u63a8\u7406\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7cfb\u7edf\u4e2d\u56e0\u5927\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\u800c\u5bfc\u81f4\u7684\u63a7\u5236\u7a33\u5b9a\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u7cfb\u7edf\u7531\u4e8e\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u63a8\u7406\u901f\u5ea6\u8f83\u4f4e\uff0c\u5bfc\u81f4\u7b56\u7565\u8868\u73b0\u53d7\u9650\uff0c\u5e76\u4e14\u5728\u6d89\u53ca\u66f4\u591a\u5173\u8282\u3001\u66f4\u5927\u8fd0\u52a8\u7a7a\u95f4\u53ca\u52a8\u6001\u89c6\u89d2\u53d8\u5316\u7684\u5168\u8eab\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\uff0c\u540c\u6b65\u6267\u884c\u4e25\u91cd\u9650\u5236\u4e86\u63a7\u5236\u7684\u7a33\u5b9a\u6027\u548c\u5b9e\u65f6\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aDuoCore-FS\u7684\u771f\u6b63\u5f02\u6b65Fast-Slow VLA\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u7531\u4e00\u4e2a\u7528\u4e8e\u9ad8\u9891\u52a8\u4f5c\u751f\u6210\u7684\u5feb\u901f\u8def\u5f84\u548c\u4e00\u4e2a\u8fdb\u884c\u4e30\u5bccVLM\u63a8\u7406\u7684\u6162\u901f\u8def\u5f84\u7ec4\u6210\u3002\u5229\u7528\u6f5c\u53d8\u91cf\u8868\u793a\u7f13\u5b58\u8fde\u63a5\u5feb\u6162\u4e24\u7cfb\u7edf\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5168\u8eab\u4f53\u52a8\u4f5ctokenizer\u63d0\u4f9b\u7d27\u51d1\u7edf\u4e00\u7684\u52a8\u4f5c\u8868\u793a\u3002\u5c3d\u7ba1\u5982\u6b64\uff0cVLM\u4e0e\u52a8\u4f5c\u4e13\u5bb6\u4ecd\u53ef\u7aef\u5230\u7aef\u8054\u5408\u8bad\u7ec3\uff0c\u4fdd\u6301\u4e86\u7edf\u4e00\u7684\u7b56\u7565\u5b66\u4e60\u540c\u65f6\u5b9e\u73b0\u4e86\u5f02\u6b65\u6267\u884c\u3002", "result": "DuoCore-FS\u652f\u630130\u4ebf\u53c2\u6570\u7ea7\u522b\u7684VLM\u7684\u540c\u65f6\u8fbe\u5230\u4e86\u7ea630Hz\u7684\u5168\u8eab\u4f53\u52a8\u4f5c\u5757\u751f\u6210\u901f\u7387\uff0c\u6bd4\u540c\u7c7b\u5927\u5c0f\u7684\u73b0\u6709VLA\u6a21\u578b\u5feb\u5927\u7ea6\u4e09\u500d\u3002\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u54cd\u5e94\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DuoCore-FS\u4e3a\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7cfb\u7edf\u4e2d\u7684\u5b9e\u65f6\u6027\u80fd\u4e0e\u63a7\u5236\u7a33\u5b9a\u6027\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5f02\u6b65\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u5168\u8eab\u4f53\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u8868\u73b0\u3002"}}
{"id": "2512.20224", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20224", "abs": "https://arxiv.org/abs/2512.20224", "authors": ["Qijun Qin", "Ziqi Zhang", "Yihan Zhong", "Feng Huang", "Xikun Liu", "Runzhi Hu", "Hang Chen", "Wei Hu", "Dongzhe Su", "Jun Zhang", "Hoi-Fung Ng", "Weisong Wen"], "title": "UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas", "comment": "8 pages, 9 figures, IEEE ITSC 2025", "summary": "Due to the limitations of a single autonomous vehicle, Cellular Vehicle-to-Everything (C-V2X) technology opens a new window for achieving fully autonomous driving through sensor information sharing. However, real-world datasets supporting vehicle-infrastructure cooperative navigation in complex urban environments remain rare. To address this gap, we present UrbanV2X, a comprehensive multisensory dataset collected from vehicles and roadside infrastructure in the Hong Kong C-V2X testbed, designed to support research on smart mobility applications in dense urban areas. Our onboard platform provides synchronized data from multiple industrial cameras, LiDARs, 4D radar, ultra-wideband (UWB), IMU, and high-precision GNSS-RTK/INS navigation systems. Meanwhile, our roadside infrastructure provides LiDAR, GNSS, and UWB measurements. The entire vehicle-infrastructure platform is synchronized using the Precision Time Protocol (PTP), with sensor calibration data provided. We also benchmark various navigation algorithms to evaluate the collected cooperative data. The dataset is publicly available at https://polyu-taslab.github.io/UrbanV2X/.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86UrbanV2X\uff0c\u4e00\u4e2a\u4e3a\u652f\u6301\u5bc6\u96c6\u57ce\u5e02\u533a\u57df\u667a\u80fd\u79fb\u52a8\u5e94\u7528\u7814\u7a76\u800c\u8bbe\u8ba1\u7684\u591a\u4f20\u611f\u5668\u6570\u636e\u96c6\uff0c\u5b83\u901a\u8fc7\u8f66\u8f86\u548c\u8def\u4fa7\u57fa\u7840\u8bbe\u65bd\u6536\u96c6\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u5404\u79cd\u5bfc\u822a\u7b97\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u7531\u4e8e\u5355\u4e00\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5b58\u5728\u5c40\u9650\u6027\uff0cC-V2X\u6280\u672f\u901a\u8fc7\u4f20\u611f\u5668\u4fe1\u606f\u5171\u4eab\u4e3a\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u3002\u7136\u800c\uff0c\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u652f\u6301\u8f66-\u57fa\u7840\u8bbe\u65bd\u534f\u540c\u5bfc\u822a\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4ecd\u7136\u7a00\u7f3a\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86UrbanV2X\u6570\u636e\u96c6\u3002", "method": "\u672c\u7814\u7a76\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u5229\u7528\u9999\u6e2fC-V2X\u8bd5\u9a8c\u573a\u5185\u7684\u8f66\u8f86\u548c\u8def\u8fb9\u57fa\u7840\u8bbe\u65bd\u6536\u96c6\u4e86\u4e00\u5957\u5168\u9762\u7684\u6570\u636e\u96c6UrbanV2X\u3002\u8f66\u8f7d\u5e73\u53f0\u63d0\u4f9b\u6765\u81ea\u591a\u4e2a\u5de5\u4e1a\u76f8\u673a\u3001\u6fc0\u5149\u96f7\u8fbe\u30014D\u96f7\u8fbe\u3001\u8d85\u5bbd\u5e26(UWB)\u3001\u60ef\u6027\u6d4b\u91cf\u5355\u5143(IMU)\u4ee5\u53ca\u9ad8\u7cbe\u5ea6GNSS-RTK/INS\u5bfc\u822a\u7cfb\u7edf\u7684\u540c\u6b65\u6570\u636e\uff1b\u540c\u65f6\uff0c\u8def\u8fb9\u8bbe\u65bd\u4e5f\u63d0\u4f9b\u4e86\u6fc0\u5149\u96f7\u8fbe\u3001GNSS\u548cUWB\u6d4b\u91cf\u6570\u636e\u3002\u6574\u4e2a\u8f66-\u57fa\u7840\u8bbe\u65bd\u5e73\u53f0\u4f7f\u7528\u7cbe\u786e\u65f6\u95f4\u534f\u8bae(PTP)\u8fdb\u884c\u540c\u6b65\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f20\u611f\u5668\u6821\u51c6\u6570\u636e\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aUrbanV2X\u7684\u7efc\u5408\u591a\u611f\u5b98\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u4e13\u4e3a\u652f\u6301\u5728\u5bc6\u96c6\u57ce\u533a\u5185\u8fdb\u884c\u667a\u6167\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u7684\u7814\u7a76\u800c\u8bbe\u8ba1\u3002\u6b64\u5916\uff0c\u8fd8\u5bf9\u591a\u79cd\u5bfc\u822a\u7b97\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u8bc4\u4f30\u6240\u6536\u96c6\u7684\u5408\u4f5c\u6570\u636e\u7684\u8d28\u91cf\u4e0e\u9002\u7528\u6027\u3002", "conclusion": "UrbanV2X\u6570\u636e\u96c6\u65e8\u5728\u4fc3\u8fdb\u5173\u4e8e\u5982\u4f55\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e0b\u5229\u7528C-V2X\u6280\u672f\u6539\u5584\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\u7684\u7814\u7a76\u5de5\u4f5c\u3002\u5b83\u4e0d\u4ec5\u5305\u62ec\u4e86\u4ece\u4e0d\u540c\u7c7b\u578b\u7684\u4f20\u611f\u5668\u83b7\u53d6\u7684\u4fe1\u606f\uff0c\u800c\u4e14\u8fd8\u5b9e\u73b0\u4e86\u8de8\u8bbe\u5907\u7684\u65f6\u95f4\u540c\u6b65\uff0c\u8fd9\u4f7f\u5f97\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u6765\u63a2\u7d22\u672a\u6765\u7684\u667a\u80fd\u4ea4\u901a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20299", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20299", "abs": "https://arxiv.org/abs/2512.20299", "authors": ["Zhongyu Xia", "Wenhao Chen", "Yongtao Wang", "Ming-Hsuan Yang"], "title": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System", "comment": null, "summary": "Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.", "AI": {"tldr": "\u63d0\u51fa\u4e86KnowVal\uff0c\u4e00\u4e2a\u901a\u8fc7\u6574\u5408\u5f00\u653e\u4e16\u754c\u611f\u77e5\u548c\u77e5\u8bc6\u68c0\u7d22\u6765\u5b9e\u73b0\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u6784\u5efa\u9a7e\u9a76\u77e5\u8bc6\u56fe\u8c31\u548c\u4ef7\u503c\u6a21\u578b\u6765\u6539\u8fdb\u51b3\u7b56\u6027\u80fd\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u5728nuScenes\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f4e\u78b0\u649e\u7387\uff0c\u5728Bench2Drive\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u7684\u5b66\u4e60\uff0c\u96be\u4ee5\u901a\u8fc7\u6a21\u4eff\u6216\u6709\u9650\u7684\u5f3a\u5316\u5956\u52b1\u6355\u6349\u5230\u51b3\u7b56\u80cc\u540e\u7684\u590d\u6742\u903b\u8f91\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u5f0f\u6765\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u7406\u89e3\u548c\u51b3\u7b56\u80fd\u529b\u3002", "method": "KnowVal\u7ed3\u5408\u4e86\u5f00\u653e\u4e16\u754c\u7684\u611f\u77e5\u80fd\u529b\u548c\u57fa\u4e8e\u9ad8\u6548LLM\u7684\u77e5\u8bc6\u68c0\u7d22\u673a\u5236\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u9a7e\u9a76\u77e5\u8bc6\u56fe\u8c31\uff0c\u7f16\u7801\u4e86\u4ea4\u901a\u6cd5\u89c4\u3001\u9632\u5fa1\u6027\u9a7e\u9a76\u539f\u5219\u548c\u4f26\u7406\u89c4\u8303\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u4ef7\u503c\u6a21\u578b\uff0c\u7528\u4e8e\u6307\u5bfc\u53ef\u89e3\u91ca\u7684\u3001\u7b26\u5408\u4ef7\u503c\u89c2\u7684\u8f68\u8ff9\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u89c4\u5212\u6027\u80fd\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u67b6\u6784\u517c\u5bb9\u3002\u7279\u522b\u5730\uff0cKnowVal\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u8fbe\u6210\u4e86\u6700\u4f4e\u78b0\u649e\u7387\u7684\u6210\u7ee9\uff0c\u5728Bench2Drive\u8bc4\u6d4b\u4e2d\u4e5f\u53d6\u5f97\u4e86\u9886\u5148\u7684\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u4ee5\u53ca\u5bf9\u9a7e\u9a76\u77e5\u8bc6\u548c\u4ef7\u503c\u4e00\u81f4\u6027\u7684\u91cd\u89c6\uff0cKnowVal\u4e3a\u9ad8\u7ea7\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u63d0\u5347\u5b89\u5168\u6027\u548c\u6027\u80fd\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.20322", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20322", "abs": "https://arxiv.org/abs/2512.20322", "authors": ["Katsu Uchiyama", "Ryuma Niiyama"], "title": "Pneumatic bladder links with wide range of motion joints for articulated inflatable robots", "comment": "Accepted at IROS2024 (IEEE/RSJ International Conference on Intelligent Robots and Systems)", "summary": "Exploration of various applications is the frontier of research on inflatable robots. We proposed an articulated robots consisting of multiple pneumatic bladder links connected by rolling contact joints called Hillberry joints. The bladder link is made of a double-layered structure of tarpaulin sheet and polyurethane sheet, which is both airtight and flexible in shape. The integration of the Hilberry joint into an inflatable robot is also a new approach. The rolling contact joint allows wide range of motion of $\\pm 150 ^{\\circ}$, the largest among the conventional inflatable joints. Using the proposed mechanism for inflatable robots, we demonstrated moving a 500 g payload with a 3-DoF arm and lifting 3.4 kg and 5 kg payloads with 2-DoF and 1-DoF arms, respectively. We also experimented with a single 3-DoF inflatable leg attached to a dolly to show that the proposed structure worked for legged locomotion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u591a\u4e2a\u6c14\u52a8\u56ca\u5f0f\u8fde\u6746\u901a\u8fc7\u6eda\u52a8\u63a5\u89e6\u5173\u8282\uff08\u79f0\u4e3aHillberry\u5173\u8282\uff09\u8fde\u63a5\u800c\u6210\u7684\u5173\u8282\u673a\u5668\u4eba\u3002\u8fd9\u79cd\u56ca\u5f0f\u8fde\u6746\u91c7\u7528\u53cc\u5c42\u7ed3\u6784\uff0c\u65e2\u4fdd\u8bc1\u4e86\u5bc6\u5c01\u6027\u4e5f\u63d0\u4f9b\u4e86\u5f62\u72b6\u4e0a\u7684\u7075\u6d3b\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u673a\u5236\u53ef\u7528\u4e8e\u79fb\u52a8500\u514b\u7684\u6709\u6548\u8f7d\u8377\uff0c\u5e76\u4e14\u80fd\u591f\u5206\u522b\u75282\u81ea\u7531\u5ea6\u548c1\u81ea\u7531\u5ea6\u7684\u624b\u81c2\u4e3e\u8d773.4\u5343\u514b\u548c5\u5343\u514b\u7684\u6709\u6548\u8f7d\u8377\u3002\u6b64\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u5355\u4e2a3\u81ea\u7531\u5ea6\u5145\u6c14\u817f\u4e0e\u5c0f\u8f66\u7ed3\u5408\u7528\u4e8e\u817f\u90e8\u8fd0\u52a8\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u63a2\u7d22\u5145\u6c14\u673a\u5668\u4eba\u7684\u5404\u79cd\u5e94\u7528\u662f\u5f53\u524d\u7814\u7a76\u7684\u4e00\u4e2a\u524d\u6cbf\u9886\u57df\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u578b\u7684\u3001\u5177\u6709\u66f4\u5e7f\u6cdb\u6d3b\u52a8\u8303\u56f4\u7684\u5145\u6c14\u673a\u5668\u4eba\u5173\u8282\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u5e76\u5236\u9020\u4e86\u4e00\u79cd\u7531\u591a\u6bb5\u6c14\u52a8\u56ca\u5f0f\u8fde\u6746\u7ec4\u6210\u7684\u5173\u8282\u673a\u5668\u4eba\uff0c\u8fd9\u4e9b\u8fde\u6746\u901a\u8fc7\u540d\u4e3aHillberry\u5173\u8282\u7684\u7279\u6b8a\u6eda\u52a8\u63a5\u89e6\u5173\u8282\u76f8\u8fde\u3002\u6bcf\u6bb5\u8fde\u6746\u4f7f\u7528\u4e86\u53cc\u5c42\u6750\u6599\u6765\u786e\u4fdd\u5176\u5bc6\u5c01\u6027\u548c\u67d4\u97e7\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u673a\u6784\u80fd\u591f\u57283-DoF\u624b\u81c2\u4e0a\u79fb\u52a8500\u514b\u6709\u6548\u8f7d\u8377\uff0c\u5e76\u4e14\u57282-DoF\u548c1-DoF\u914d\u7f6e\u4e0b\u5206\u522b\u63d0\u53473.4\u5343\u514b\u548c5\u5343\u514b\u91cd\u91cf\u3002\u6b64\u5916\uff0c\u5355\u4e2a3-DoF\u5145\u6c14\u817f\u88ab\u8bc1\u660e\u9002\u5408\u4e8e\u817f\u90e8\u884c\u8d70\u8fd0\u52a8\u3002", "conclusion": "\u672c\u7814\u7a76\u6240\u63d0\u51fa\u7684\u57fa\u4e8eHillberry\u5173\u8282\u7684\u5145\u6c14\u673a\u5668\u4eba\u5c55\u793a\u51fa\u4e86\u5728\u627f\u8f7d\u80fd\u529b\u548c\u7075\u6d3b\u5ea6\u65b9\u9762\u7684\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u5145\u6c14\u673a\u5668\u4eba\u5728\u66f4\u591a\u5e94\u7528\u573a\u666f\u4e2d\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.20355", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20355", "abs": "https://arxiv.org/abs/2512.20355", "authors": ["Hao Wei", "Peiji Wang", "Qianhao Wang", "Tong Qin", "Fei Gao", "Yulin Si"], "title": "FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration", "comment": null, "summary": "Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FAR-AVIO\uff0c\u4e00\u79cd\u57fa\u4e8eSchur\u8865\u7684\u7d27\u5bc6\u8026\u5408\u58f0\u5b66-\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u4e13\u4e3a\u6c34\u4e0b\u673a\u5668\u4eba\u8bbe\u8ba1\u3002\u901a\u8fc7\u5d4c\u5165\u81ea\u9002\u5e94\u6743\u91cd\u8c03\u6574\u548c\u53ef\u9760\u6027\u8bc4\u4f30\u6a21\u5757\u4ee5\u53ca\u9ad8\u6548\u7684\u5728\u7ebf\u6821\u51c6\u65b9\u6848\uff0cFAR-AVIO\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u9002\u5408\u4f4e\u529f\u8017\u5d4c\u5165\u5f0f\u5e73\u53f0\u4f7f\u7528\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u5bf9\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u6311\u6218\uff0c\u5305\u62ec\u5f3a\u5149\u8870\u51cf\u3001\u6d77\u6d0b\u96ea\u548c\u6d51\u6d4a\u5ea6\u7b49\u95ee\u9898\uff0c\u52a0\u4e0a\u5f31\u6fc0\u52b1\u8fd0\u52a8\uff0c\u964d\u4f4e\u4e86\u60ef\u6027\u53ef\u89c2\u6d4b\u6027\u5e76\u5bfc\u81f4\u957f\u671f\u64cd\u4f5c\u4e2d\u9891\u7e41\u8ddf\u8e2a\u5931\u8d25\u3002\u867d\u7136\u7d27\u5bc6\u8026\u5408\u7684\u58f0\u5b66-\u89c6\u89c9-\u60ef\u6027\u878d\u5408\uff08\u901a\u5e38\u901a\u8fc7\u58f0\u5b66\u591a\u666e\u52d2\u901f\u5ea6\u8bb0\u5f55\u4eeaDVL\u4e0e\u89c6\u89c9-\u60ef\u6027\u6d4b\u91cf\u96c6\u6210\uff09\u53ef\u4ee5\u63d0\u4f9b\u51c6\u786e\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u4f46\u76f8\u5173\u7684\u56fe\u4f18\u5316\u65b9\u6cd5\u5f80\u5f80\u56e0\u8ba1\u7b97\u6210\u672c\u9ad8\u800c\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u90e8\u7f72\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5e73\u53f0\u4e0a\u3002", "method": "\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFAR-AVIO\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u5c06Schur\u8865\u516c\u5f0f\u5d4c\u5165\u5230\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668(EKF)\u4e2d\uff0c\u4f7f\u5f97\u80fd\u591f\u5728\u4fdd\u6301\u6052\u5b9a\u65f6\u95f4\u66f4\u65b0\u7684\u540c\u65f6\u8fdb\u884c\u59ff\u6001-\u5730\u6807\u8054\u5408\u4f18\u5316\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u6743\u91cd\u8c03\u6574\u4e0e\u53ef\u9760\u6027\u8bc4\u4f30(AWARE)\u5728\u7ebf\u4f20\u611f\u5668\u5065\u5eb7\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u6301\u7eed\u8bc4\u4f30\u89c6\u89c9\u3001\u60ef\u6027\u548cDVL\u6d4b\u91cf\u7684\u53ef\u9760\u6027\uff0c\u5e76\u81ea\u9002\u5e94\u5730\u8c03\u8282\u5b83\u4eec\u7684sigma\u6743\u91cd\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4e00\u4e2a\u65e0\u9700\u4e13\u95e8\u6821\u51c6\u52a8\u4f5c\u5c31\u80fd\u5171\u540c\u4f30\u7b97DVL-IMU\u5916\u90e8\u53c2\u6570\u7684\u6709\u6548\u5728\u7ebf\u6821\u51c6\u65b9\u6848\u3002", "result": "\u6570\u503c\u6a21\u62df\u548c\u5b9e\u9645\u6c34\u4e0b\u5b9e\u9a8c\u4e00\u81f4\u8868\u660e\uff0cFAR-AVIO\u5728\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6c34\u4e0bSLAM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u4f4e\u529f\u8017\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u7a33\u5065\u8fd0\u884c\u3002", "conclusion": "FAR-AVIO\u6210\u529f\u5730\u89e3\u51b3\u4e86\u4f20\u7edf\u6c34\u4e0b\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\u9762\u4e34\u7684\u96be\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u6027\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u8db3\u591f\u7684\u8ba1\u7b97\u6548\u7387\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2512.20475", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20475", "abs": "https://arxiv.org/abs/2512.20475", "authors": ["Maulana Bisyir Azhari", "Donghun Han", "Je In You", "Sungjun Park", "David Hyunchul Shim"], "title": "Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing", "comment": null, "summary": "The Abu Dhabi Autonomous Racing League(A2RL) x Drone Champions League competition(DCL) requires teams to perform high-speed autonomous drone racing using only a single camera and a low-quality inertial measurement unit -- a minimal sensor set that mirrors expert human drone racing pilots. This sensor limitation makes the system susceptible to drift from Visual-Inertial Odometry (VIO), particularly during long and fast flights with aggressive maneuvers. This paper presents the system developed for the championship, which achieved a competitive performance. Our approach corrected VIO drift by fusing its output with global position measurements derived from a YOLO-based gate detector using a Kalman filter. A perception-aware planner generated trajectories that balance speed with the need to keep gates visible for the perception system. The system demonstrated high performance, securing podium finishes across multiple categories: third place in the AI Grand Challenge with top speed of 43.2 km/h, second place in the AI Drag Race with over 59 km/h, and second place in the AI Multi-Drone Race. We detail the complete architecture and present a performance analysis based on experimental data from the competition, contributing our insights on building a successful system for monocular vision-based autonomous drone flight.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e3a\u963f\u5e03\u624e\u6bd4\u81ea\u4e3b\u8d5b\u8f66\u8054\u76dfx\u65e0\u4eba\u673a\u51a0\u519b\u8054\u8d5b\u8bbe\u8ba1\u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u5c06\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1(VIO)\u8f93\u51fa\u4e0e\u57fa\u4e8eYOLO\u7684\u95e8\u68c0\u6d4b\u5668\u5f97\u5230\u7684\u5168\u5c40\u4f4d\u7f6e\u6d4b\u91cf\u503c\u878d\u5408\uff0c\u6765\u7ea0\u6b63VIO\u6f02\u79fb\uff0c\u5e76\u4f7f\u7528\u611f\u77e5\u610f\u8bc6\u89c4\u5212\u5668\u751f\u6210\u5e73\u8861\u901f\u5ea6\u548c\u611f\u77e5\u7cfb\u7edf\u9700\u6c42\u7684\u8f68\u8ff9\u3002\u7cfb\u7edf\u5728\u591a\u9879\u6bd4\u8d5b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u53d6\u5f97\u4e86\u9886\u5956\u53f0\u7684\u6210\u7ee9\u3002", "motivation": "\u9488\u5bf9\u963f\u5e03\u624e\u6bd4\u81ea\u4e3b\u8d5b\u8f66\u8054\u76dfx\u65e0\u4eba\u673a\u51a0\u519b\u8054\u8d5b\u4e2d\u4ec5\u4f7f\u7528\u5355\u4e00\u6444\u50cf\u5934\u548c\u4f4e\u8d28\u91cf\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u8fdb\u884c\u9ad8\u901f\u81ea\u4e3b\u65e0\u4eba\u673a\u7ade\u8d5b\u7684\u9700\u6c42\uff0c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u56e0\u4f20\u611f\u5668\u9650\u5236\u5bfc\u81f4\u7684\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u6f02\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6\u95f4\u3001\u9ad8\u901f\u5ea6\u4ee5\u53ca\u6fc0\u70c8\u64cd\u4f5c\u6761\u4ef6\u4e0b\u3002", "method": "\u91c7\u7528\u7684\u65b9\u6cd5\u5305\u62ec\uff1a1. \u901a\u8fc7\u7ed3\u5408\u57fa\u4e8eYOLO\u7684\u95e8\u68c0\u6d4b\u5668\u63d0\u4f9b\u7684\u5168\u5c40\u4f4d\u7f6e\u6d4b\u91cf\u503c\u4e0eVIO\u8f93\u51fa\uff0c\u5229\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4fee\u6b63VIO\u6f02\u79fb\uff1b2. \u5229\u7528\u611f\u77e5\u610f\u8bc6\u89c4\u5212\u5668\u751f\u6210\u8003\u8651\u901f\u5ea6\u540c\u65f6\u786e\u4fdd\u95e8\u5bf9\u611f\u77e5\u7cfb\u7edf\u53ef\u89c1\u6027\u7684\u8f68\u8ff9\u3002", "result": "\u6240\u5f00\u53d1\u7684\u7cfb\u7edf\u5728\u591a\u4e2a\u7c7b\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ecAI\u5927\u5956\u8d5b\u7b2c\u4e09\u540d\uff08\u6700\u9ad8\u901f\u5ea643.2\u516c\u91cc/\u5c0f\u65f6\uff09\u3001AI\u76f4\u7ebf\u7ade\u901f\u8d5b\u7b2c\u4e8c\u540d\uff08\u8d85\u8fc759\u516c\u91cc/\u5c0f\u65f6\uff09\u53caAI\u591a\u65e0\u4eba\u673a\u7ade\u8d5b\u7b2c\u4e8c\u540d\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u6539\u5584\u5355\u76ee\u89c6\u89c9\u57fa\u7840\u4e0b\u7684\u81ea\u4e3b\u65e0\u4eba\u673a\u98de\u884c\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7ade\u8d5b\u73af\u5883\u4e2d\u3002\u901a\u8fc7\u5b9e\u9a8c\u6570\u636e\u5206\u6790\uff0c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u76f8\u5173\u5de5\u4f5c\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2512.20591", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.20591", "abs": "https://arxiv.org/abs/2512.20591", "authors": ["Changyi Lin", "Boda Huo", "Mingyang Yu", "Emily Ruppel", "Bingqing Chen", "Jonathan Francis", "Ding Zhao"], "title": "LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing", "comment": null, "summary": "Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.", "AI": {"tldr": "LightTact, a novel visual-tactile sensor, uses an optics-based principle to detect light contact without relying on deformation, enabling robust detection of interactions with liquids and ultra-soft materials. It generates high-contrast images for accurate contact segmentation and can be used in robotic manipulation tasks, including water and thin-film interaction, as well as reasoning for robotic sorting.", "motivation": "The motivation behind this paper is to develop a tactile sensor capable of detecting light contact with surfaces that do not deform macroscopically, such as liquids or ultra-soft materials, which current tactile sensors struggle with due to their reliance on deformation for contact inference.", "method": "The method involves the creation of LightTact, a visual-tactile fingertip sensor that employs an ambient-blocking optical configuration. This setup allows only diffuse light from true contacts to pass through, producing high-contrast images where non-contact areas remain dark and contact areas appear as they naturally do, facilitating pixel-level contact segmentation.", "result": "LightTact successfully achieves accurate and robust contact detection across various material properties, forces, appearances, and lighting conditions. The sensor was integrated into a robotic arm and demonstrated effective use in tasks requiring extremely light touch, like handling water and facial cream, and also showed potential for value reasoning in sorting tasks by using vision-language models.", "conclusion": "LightTact represents a significant advancement in tactile sensing, offering a solution for perceiving light contacts that are deformation-independent, thus expanding the capabilities of robots in delicate and precise manipulation tasks."}}
