<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 18]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Wake Vectoring for Efficient Morphing Flight](https://arxiv.org/abs/2512.05211)
*Ioannis Mandralis,Severin Schumacher,Morteza Gharib*

Main category: cs.RO

TL;DR: 本文介绍了一种创新的被动尾流矢量机制，集成于新型机器人系统ATMO中，能够恢复飞行变形过程中损失的垂直推力，从而提高悬停和机动能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决飞行机器人在空中变形时由于推进器倾斜导致的垂直推力减少问题，这影响了机器人的稳定性与操控性。

Method: 通过设计一种无电子元件解决方案——内部偏转器拦截并重新定向旋翼尾流至下方，以被动方式引导原本会被浪费的气流动量。

Result: 该方法能够在没有有效推力产生的配置下恢复高达40%的垂直推力，显著提升了变形过程中的悬停时间和机动性能。

Conclusion: 本研究为变形飞行机器人的设计开辟了新方向，证明了被动空气动力学结构可以实现高效敏捷飞行，而无需增加机械复杂性。

Abstract: Morphing aerial robots have the potential to transform autonomous flight, enabling navigation through cluttered environments, perching, and seamless transitions between aerial and terrestrial locomotion. Yet mid-flight reconfiguration presents a critical aerodynamic challenge: tilting propulsors to achieve shape change reduces vertical thrust, undermining stability and control authority. Here, we introduce a passive wake vectoring mechanism that recovers lost thrust during morphing. Integrated into a novel robotic system, Aerially Transforming Morphobot (ATMO), internal deflectors intercept and redirect rotor wake downward, passively steering airflow momentum that would otherwise be wasted. This electronics-free solution achieves up to a 40% recovery of vertical thrust in configurations where no useful thrust would otherwise be produced, substantially extending hover and maneuvering capabilities during transformation. Our findings highlight a new direction for morphing aerial robot design, where passive aerodynamic structures, inspired by thrust vectoring in rockets and aircraft, enable efficient, agile flight without added mechanical complexity.

</details>


### [2] [Search at Scale: Improving Numerical Conditioning of Ergodic Coverage Optimization for Multi-Scale Domains](https://arxiv.org/abs/2512.05229)
*Yanis Lahrach,Christian Hughes,Ian Abraham*

Main category: cs.RO

TL;DR: 本文提出了一种基于最大均值差异度量(MMD)的无尺度自适应遍历覆盖优化方法，解决了现有方法对问题空间数值缩放高度敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的遍历覆盖规划方法虽然能够适应广泛的几何覆盖问题，但对问题空间的数值缩放非常敏感，导致优化公式在改变尺度时变得脆弱且数值不稳定，特别是在非线性约束下。

Method: 通过开发一种基于最大均值差异度量（MMD）的无尺度和自适应遍历覆盖优化方法来解决这个问题。该方法允许优化器在确保物理一致性的同时求解微分约束的比例，并调整超参数以最适合问题域。此外，还在对数空间中推导了遍历度量的一个变体，提供了额外的数值条件而不损失性能。

Result: 与现有覆盖规划方法相比，所提出的方法在广泛覆盖问题上展示了其效用。

Conclusion: 新提出的无尺度自适应遍历覆盖优化方法有效地提升了对于不同规模问题的适应能力，同时保持了良好的性能。

Abstract: Recent methods in ergodic coverage planning have shown promise as tools that can adapt to a wide range of geometric coverage problems with general constraints, but are highly sensitive to the numerical scaling of the problem space. The underlying challenge is that the optimization formulation becomes brittle and numerically unstable with changing scales, especially under potentially nonlinear constraints that impose dynamic restrictions, due to the kernel-based formulation. This paper proposes to address this problem via the development of a scale-agnostic and adaptive ergodic coverage optimization method based on the maximum mean discrepancy metric (MMD). Our approach allows the optimizer to solve for the scale of differential constraints while annealing the hyperparameters to best suit the problem domain and ensure physical consistency. We also derive a variation of the ergodic metric in the log space, providing additional numerical conditioning without loss of performance. We compare our approach with existing coverage planning methods and demonstrate the utility of our approach on a wide range of coverage problems.

</details>


### [3] [Invariance Co-training for Robot Visual Generalization](https://arxiv.org/abs/2512.05230)
*Jonathan Yang,Chelsea Finn,Dorsa Sadigh*

Main category: cs.RO

TL;DR: 本文提出通过引入两种辅助任务（状态相似性和对观测扰动的不变性）来提高机器人政策在不同环境下的泛化能力，利用更昂贵的机器人演示数据和较便宜的、视觉丰富的合成图像共同训练可以显著提升机器人在未见过的相机视角、光照配置以及干扰条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前许多大规模的机器人策略在面对诸如相机视角变化、光照变化及存在干扰物体等观察变化时仍表现出敏感性，这主要是由于这些模型缺乏足够的多样性来覆盖这些准静态轴，并且目前缺少展示丰富变化的大规模机器人数据集。

Method: 提出了两种关键的辅助任务：状态相似性和对观测扰动的不变性，并将这两种方法应用于演示数据和静态视觉数据中。同时，采用成本较高的机器人演示数据与成本较低但视觉信息丰富的非基于物理模拟生成的合成图像（如使用虚幻引擎创建）相结合的方式进行共同训练。

Result: 通过上述方法，在应对未见过的相机视角、光照设置和干扰条件下，机器人的性能相比现有的生成增强方法提高了18%。

Conclusion: 研究结果表明，通过结合多样化的数据来源并应用所提出的辅助任务，能够有效提高机器人策略对于不同观测变化情况下的适应性和泛化能力。

Abstract: Reasoning from diverse observations is a fundamental capability for generalist robot policies to operate in a wide range of environments. Despite recent advancements, many large-scale robotic policies still remain sensitive to key sources of observational variation such as changes in camera perspective, lighting, and the presence of distractor objects. We posit that the limited generalizability of these models arises from the substantial diversity required to robustly cover these quasistatic axes, coupled with the current scarcity of large-scale robotic datasets that exhibit rich variation across them. In this work, we propose to systematically examine what robots need to generalize across these challenging axes by introducing two key auxiliary tasks, state similarity and invariance to observational perturbations, applied to both demonstration data and static visual data. We then show that via these auxiliary tasks, leveraging both more-expensive robotic demonstration data and less-expensive, visually rich synthetic images generated from non-physics-based simulation (for example, Unreal Engine) can lead to substantial increases in generalization to unseen camera viewpoints, lighting configurations, and distractor conditions. Our results demonstrate that co-training on this diverse data improves performance by 18 percent over existing generative augmentation methods. For more information and videos, please visit https://invariance-cotraining.github.io

</details>


### [4] [Disturbance Compensation for Safe Kinematic Control of Robotic Systems with Closed Architecture](https://arxiv.org/abs/2512.05292)
*Fan Zhang,Jinfeng Chen,Joseph J. B. Mvogo Ahanda,Hanz Richter,Ge Lv,Bin Hu,Qin Lin*

Main category: cs.RO

TL;DR: 该论文提出了一种结合扰动抑制控制与鲁棒控制屏障函数的外环层附加组件，以实现工业机械臂高性能跟踪和安全控制。


<details>
  <summary>Details</summary>
Motivation: 在商业机器人系统中，用户通常无法修改内部环路的扭矩控制器，但可以访问发送运动学命令给内部环路控制器跟踪的外部环路控制器。本文旨在开发一种易于集成到外部环路层的附加组件，尤其适用于内部环路控制器不完美、不可更改且不确定以及动态模型存在显著不确定性的情况。

Method: 通过结合扰动抑制控制技术和鲁棒控制屏障函数技术来开发一种外环层的附加组件。

Result: 稳定性分析、正式的安全保证证明以及使用PUMA机器人操纵器进行的硬件实验表明了所提方案的有效性。

Conclusion: 提出的解决方案在实现简单性、鲁棒性、跟踪精度及安全性方面优于现有技术水平。

Abstract: In commercial robotic systems, it is common to encounter a closed inner-loop torque controller that is not user-modifiable. However, the outer-loop controller, which sends kinematic commands such as position or velocity for the inner-loop controller to track, is typically exposed to users. In this work, we focus on the development of an easily integrated add-on at the outer-loop layer by combining disturbance rejection control and robust control barrier function for high-performance tracking and safe control of the whole dynamic system of an industrial manipulator. This is particularly beneficial when 1) the inner-loop controller is imperfect, unmodifiable, and uncertain; and 2) the dynamic model exhibits significant uncertainty. Stability analysis, formal safety guarantee proof, and hardware experiments with a PUMA robotic manipulator are presented. Our solution demonstrates superior performance in terms of simplicity of implementation, robustness, tracking precision, and safety compared to the state of the art. Video: https://youtu.be/zw1tanvrV8Q

</details>


### [5] [Seabed-to-Sky Mapping of Maritime Environments with a Dual Orthogonal SONAR and LiDAR Sensor Suite](https://arxiv.org/abs/2512.05303)
*Christian Westerdahl,Jonas Poulsen,Daniel Holmelund,Peter Nicholas Hansen,Fletcher Thompson,Roberto Galeazzi*

Main category: cs.RO

TL;DR: 本文提出了一种不依赖GNSS的统一映射系统，该系统结合了LiDAR-IMU与双正交前视声纳（FLS），能够从自主水面航行器生成连贯的从海床到天空的地图。通过扩展声学处理和改进LIO-SAM算法以整合3D声纳点及线条扫描，实现在真实世界数据上的实时操作，并产生跨越空气-水领域的统一3D模型。


<details>
  <summary>Details</summary>
Motivation: 关键的海上基础设施日益需要对海面之上和之下的情况有全面了解，但现有的‘海床到天空’测绘流程要么依赖于容易受到遮挡或欺骗攻击的GNSS，要么依赖昂贵的测深声纳。为解决这一问题，提出了一个既独立于GNSS又成本效益更高的解决方案。

Method: 开发了一个融合LiDAR-IMU与一对正交安装的前视声纳(FLS)的系统，实现了从自主表面载具生成一致的海床至天空地图的能力。在声学方面，扩大了正交宽孔径融合技术的应用范围来处理任意声纳间的位移；在绘图方面，则修改了LIO-SAM以同时吸收立体衍生的3D声纳点和领先边缘线扫描，使稀疏声学更新可以连续贡献给单一因子图地图。

Result: 系统在哥本哈根Belvederekanalen的真实世界数据上进行了验证，展示了大约2.65赫兹的地图更新率以及约2.85赫兹的里程计更新率，同时生成了一个横跨空气-水界面的统一3D模型。

Conclusion: 所提出的GNSS独立映射系统成功地提供了一种经济高效且可靠的方法来实现从海床到天空的一致性地图绘制，适用于关键海上基础设施的情境感知需求。

Abstract: Critical maritime infrastructure increasingly demands situational awareness both above and below the surface, yet existing ''seabed-to-sky'' mapping pipelines either rely on GNSS (vulnerable to shadowing/spoofing) or expensive bathymetric sonars. We present a unified, GNSS-independent mapping system that fuses LiDAR-IMU with a dual, orthogonally mounted Forward Looking Sonars (FLS) to generate consistent seabed-to-sky maps from an Autonomous Surface Vehicle. On the acoustic side, we extend orthogonal wide-aperture fusion to handle arbitrary inter-sonar translations (enabling heterogeneous, non-co-located models) and extract a leading edge from each FLS to form line-scans. On the mapping side, we modify LIO-SAM to ingest both stereo-derived 3D sonar points and leading-edge line-scans at and between keyframes via motion-interpolated poses, allowing sparse acoustic updates to contribute continuously to a single factor-graph map. We validate the system on real-world data from Belvederekanalen (Copenhagen), demonstrating real-time operation with approx. 2.65 Hz map updates and approx. 2.85 Hz odometry while producing a unified 3D model that spans air-water domains.

</details>


### [6] [State-Conditional Adversarial Learning: An Off-Policy Visual Domain Transfer Method for End-to-End Imitation Learning](https://arxiv.org/abs/2512.05335)
*Yuxiang Liu,Shengfan Cao*

Main category: cs.RO

TL;DR: 本文研究了在目标域数据完全离策略、无专家且稀缺的情况下，端到端模仿学习中的视觉领域迁移问题。提出了一种新的方法——状态条件对抗学习（SCAL），该方法通过基于判别器的条件KL散度估计对齐源域和目标域观测模型之间的潜在分布。实验表明，SCAL能够实现稳健的迁移并具有良好的样本效率。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决一个现实且具挑战性的问题：当目标领域的数据严格地是非策略性的、缺乏专家指导并且数量稀少时，如何进行有效的视觉领域迁移以支持端到端模仿学习。

Method: 提出了状态条件对抗学习（State-Conditional Adversarial Learning, SCAL），一种非策略对抗框架，它利用基于判别器的方法来估计条件KL散度项，并据此调整源领域与目标领域观察模型之间在给定系统状态下潜在分布的一致性。

Result: 实验结果表明，在BARC-CARLA模拟器构建的不同视觉环境下，所提方法SCAL能够有效完成领域迁移任务，并展现出良好的样本效率及鲁棒性。

Conclusion: 本研究为在极端条件下执行视觉领域迁移提供了一个创新解决方案，证明了状态条件对抗学习方法对于提高端到端模仿学习系统的泛化能力和减少对大量标记数据依赖的有效性。

Abstract: We study visual domain transfer for end-to-end imitation learning in a realistic and challenging setting where target-domain data are strictly off-policy, expert-free, and scarce. We first provide a theoretical analysis showing that the target-domain imitation loss can be upper bounded by the source-domain loss plus a state-conditional latent KL divergence between source and target observation models. Guided by this result, we propose State- Conditional Adversarial Learning, an off-policy adversarial framework that aligns latent distributions conditioned on system state using a discriminator-based estimator of the conditional KL term. Experiments on visually diverse autonomous driving environments built on the BARC-CARLA simulator demonstrate that SCAL achieves robust transfer and strong sample efficiency.

</details>


### [7] [Spatiotemporal Tubes for Differential Drive Robots with Model Uncertainty](https://arxiv.org/abs/2512.05495)
*Ratnangshu Das,Ahan Basu,Christos Verginis,Pushpak Jagtap*

Main category: cs.RO

TL;DR: 本文提出了一种基于时空管(STT)的控制框架，用于具有动态不确定性和外部干扰的差分驱动移动机器人，确保满足时间到达-避免-停留(T-RAS)规范。通过圆形STT定义动态安全走廊，并开发了一种采样合成算法来构造满足预定时间和安全约束的可行STT。设计了无需近似的闭式控制律以确保机器人保持在该管道内。仿真研究表明，该方法在鲁棒性、准确性和计算效率方面优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为存在动态不确定性和外部干扰情况下的差分驱动移动机器人提供一种能够保证满足时间到达-避免-停留（T-RAS）要求的控制策略。

Method: 提出了一个基于时空管(STT)的控制架构，其中STT由平滑变化中心点和半径定义，形成动态的安全路径引导机器人从起点到终点同时避开障碍物。首先，开发了一种基于抽样的合成算法来构建符合规定的时间与安全约束条件的可行STT；其次，设计了一种无需任何模型简化或在线优化的解析形式控制法则，以确保机器人始终位于所设定的STT之内。

Result: 提出的控制框架不仅计算效率高，而且对干扰和模型不确定性具有很强的鲁棒性。通过在一个差分驱动机器人上的仿真测试验证了本方法的有效性，并将其与最先进方法进行了对比，显示出了更好的鲁棒性、精确度以及计算效率。

Conclusion: 本文介绍了一种新颖的基于时空管(STT)的方法，用于解决差分驱动移动机器人在面对动态不确定性和外界干扰时如何有效地完成任务的问题。实验结果表明，该方法能够有效提高系统性能。

Abstract: This paper presents a Spatiotemporal Tube (STT)-based control framework for differential-drive mobile robots with dynamic uncertainties and external disturbances, guaranteeing the satisfaction of Temporal Reach-Avoid-Stay (T-RAS) specifications. The approach employs circular STT, characterized by smoothly time-varying center and radius, to define dynamic safe corridors that guide the robot from the start region to the goal while avoiding obstacles. In particular, we first develop a sampling-based synthesis algorithm to construct a feasible STT that satisfies the prescribed timing and safety constraints with formal guarantees. To ensure that the robot remains confined within this tube, we then design analytically a closed-form, approximation-free control law. The resulting controller is computationally efficient, robust to disturbances and {model uncertainties}, and requires no model approximations or online optimization. The proposed framework is validated through simulation studies on a differential-drive robot and benchmarked against state-of-the-art methods, demonstrating superior robustness, accuracy, and computational efficiency.

</details>


### [8] [A Comprehensive Framework for Automated Quality Control in the Automotive Industry](https://arxiv.org/abs/2512.05579)
*Panagiota Moraiti,Panagiotis Giannikos,Athanasios Mastrogeorgiou,Panagiotis Mavridis,Linghao Zhou,Panagiotis Chatzakos*

Main category: cs.RO

TL;DR: 本文介绍了一种先进的机器人检测解决方案，通过使用协作机器人、高分辨率视觉系统和YOLOv3深度学习模型来自动化汽车制造中的质量控制过程。实验结果表明该系统在各种缺陷上实现了实时高性能，并且具有高度的可扩展性和适应性。


<details>
  <summary>Details</summary>
Motivation: 为了提高汽车制造业中铝高压压铸（HPDC）汽车部件的质量控制效率与准确性，减少表面及螺纹缺陷的人工检测误差，同时提升检测系统的灵活性以适应不同的生产环境。

Method: 采用一对装有高分辨率摄像机视觉系统的协作机器人进行检测；利用专门镜头和优化照明配置确保图像采集质量；基于YOLOv3深度学习模型，并结合图像分割、集成学习和边界框合并等技术改进性能；应用图像处理技术评估缺陷程度。

Result: 实验结果显示，该方案能够针对多种类型的缺陷实现高精度实时检测，同时显著降低了误检率。

Conclusion: 提出的机器人检测解决方案不仅有效提高了汽车零部件缺陷检测的速度与准确度，而且具备良好的可扩展性，可以灵活应对不同生产场景的需求变化。

Abstract: This paper presents a cutting-edge robotic inspection solution designed to automate quality control in automotive manufacturing. The system integrates a pair of collaborative robots, each equipped with a high-resolution camera-based vision system to accurately detect and localize surface and thread defects in aluminum high-pressure die casting (HPDC) automotive components. In addition, specialized lenses and optimized lighting configurations are employed to ensure consistent and high-quality image acquisition. The YOLO11n deep learning model is utilized, incorporating additional enhancements such as image slicing, ensemble learning, and bounding-box merging to significantly improve performance and minimize false detections. Furthermore, image processing techniques are applied to estimate the extent of the detected defects. Experimental results demonstrate real-time performance with high accuracy across a wide variety of defects, while minimizing false detections. The proposed solution is promising and highly scalable, providing the flexibility to adapt to various production environments and meet the evolving demands of the automotive industry.

</details>


### [9] [An Integrated System for WEEE Sorting Employing X-ray Imaging, AI-based Object Detection and Segmentation, and Delta Robot Manipulation](https://arxiv.org/abs/2512.05599)
*Panagiotis Giannikos,Lampis Papakostas,Evangelos Katralis,Panagiotis Mavridis,George Chryssinas,Myrto Inglezou,Nikolaos Panagopoulos,Antonis Porichis,Athanasios Mastrogeorgiou,Panagiotis Chatzakos*

Main category: cs.RO

TL;DR: 本研究提出了一种新的方法，通过集成专门的X射线透射双能成像子系统和先进的预处理算法，结合YOLO和U-Net模型进行电池检测与分割，并利用智能跟踪与定位算法指导Delta机器人提取并妥善处置含电池物品。该方法在NVIDIA Isaac Sim开发的逼真模拟环境及实际设置中得到验证。


<details>
  <summary>Details</summary>
Motivation: 由于电池使用量快速增长以及自然资源有限，电池回收变得越来越重要。同时，随着电池能量密度不断提高，在回收过程中不当处理会带来重大安全隐患，包括回收设施可能发生火灾。尽管在优化检测技术和模型修改方面取得了进展，但尚未实现一种能够准确识别和分类不同废弃电子电气设备（WEEE）中电池的全自动化解决方案。

Method: 本研究的方法是整合一个专用于X射线透射双能成像的子系统与先进的预处理算法，以生成高对比度图像，从而有效区分WEEE中的密集材料和薄材料。装置沿着输送带通过高分辨率X射线成像系统时，采用YOLO和U-Net模型精确检测并分割含有电池的物品。之后，通过智能跟踪与位置估计算法指导配备吸盘夹持器的Delta机器人选择性地提取目标设备并正确处理。

Result: 该方法在NVIDIA Isaac Sim开发的逼真模拟环境中进行了验证，并且也在实际设置中得到了测试。

Conclusion: 本研究提出的新型方法能够有效解决当前电池回收过程中存在的问题，通过集成先进的成像技术、AI模型及机器人技术，实现了对不同WEEE类型中电池的精准识别与自动分离。

Abstract: Battery recycling is becoming increasingly critical due to the rapid growth in battery usage and the limited availability of natural resources. Moreover, as battery energy densities continue to rise, improper handling during recycling poses significant safety hazards, including potential fires at recycling facilities. Numerous systems have been proposed for battery detection and removal from WEEE recycling lines, including X-ray and RGB-based visual inspection methods, typically driven by AI-powered object detection models (e.g., Mask R-CNN, YOLO, ResNets). Despite advances in optimizing detection techniques and model modifications, a fully autonomous solution capable of accurately identifying and sorting batteries across diverse WEEEs types has yet to be realized. In response to these challenges, we present our novel approach which integrates a specialized X-ray transmission dual energy imaging subsystem with advanced pre-processing algorithms, enabling high-contrast image reconstruction for effective differentiation of dense and thin materials in WEEE. Devices move along a conveyor belt through a high-resolution X-ray imaging system, where YOLO and U-Net models precisely detect and segment battery-containing items. An intelligent tracking and position estimation algorithm then guides a Delta robot equipped with a suction gripper to selectively extract and properly discard the targeted devices. The approach is validated in a photorealistic simulation environment developed in NVIDIA Isaac Sim and on the real setup.

</details>


### [10] [Scenario-aware Uncertainty Quantification for Trajectory Prediction with Statistical Guarantees](https://arxiv.org/abs/2512.05682)
*Yiming Shu,Jiahui Xu,Linghuan Kong,Fangni Zhang,Guodong Yin,Chen Sun*

Main category: cs.RO

TL;DR: 提出了一种新的场景感知不确定性量化框架，用于为预测轨迹提供预测区间和可靠性评估。该方法利用CopulaCPTS作为一致性校准方法生成不同场景的时间预测区间，并通过轨迹可靠性判别器（TRD）分析平均误差与校准置信区间来建立不同场景的可靠性模型。此外，基于Frenet坐标系下的纵向和横向预测区间联合风险模型，识别关键点以区分可靠与不可靠的轨迹段落，从而为下游规划模块提供可操作的可靠性结果。使用nuPlan数据集对该框架进行了评估，在多样化的驾驶场景中展示了其在场景感知不确定性量化和可靠性评估方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习预测器缺乏适应异构现实世界场景的不确定性感知框架，而对安全至关重要的自动驾驶系统需要可靠的轨迹预测不确定性量化。

Method: 1. 首先将训练好的预测器得到的预测轨迹及其真实值投影到Frenet坐标系下的地图参考路线上。
2. 采用CopulaCPTS作为一致性校准方法，为不同场景生成时间预测区间作为不确定性的度量。
3. 在提出的轨迹可靠性判别器(TRD)中，协同分析平均误差和校准后的置信区间，以针对不同场景建立可靠性模型。
4. 风险感知判别器利用一个结合了Frenet坐标下纵向和横向预测区间的联合风险模型来确定关键点，使得能够将轨迹分割成可靠与不可靠部分。

Result: 所提框架在实际nuPlan数据集上进行了测试，证明了其能够在各种驾驶环境中有效执行场景感知不确定性量化及可靠性评估任务。

Conclusion: 这项工作介绍了一个新的场景感知不确定性量化框架，旨在提高自动驾驶系统中轨迹预测的安全性和可靠性。通过实验验证表明，该框架对于不同驾驶场景下的不确定性估计和可靠性评估具有显著效果。

Abstract: Reliable uncertainty quantification in trajectory prediction is crucial for safety-critical autonomous driving systems, yet existing deep learning predictors lack uncertainty-aware frameworks adaptable to heterogeneous real-world scenarios. To bridge this gap, we propose a novel scenario-aware uncertainty quantification framework to provide the predicted trajectories with prediction intervals and reliability assessment. To begin with, predicted trajectories from the trained predictor and their ground truth are projected onto the map-derived reference routes within the Frenet coordinate system. We then employ CopulaCPTS as the conformal calibration method to generate temporal prediction intervals for distinct scenarios as the uncertainty measure. Building upon this, within the proposed trajectory reliability discriminator (TRD), mean error and calibrated confidence intervals are synergistically analyzed to establish reliability models for different scenarios. Subsequently, the risk-aware discriminator leverages a joint risk model that integrates longitudinal and lateral prediction intervals within the Frenet coordinate to identify critical points. This enables segmentation of trajectories into reliable and unreliable segments, holding the advantage of informing downstream planning modules with actionable reliability results. We evaluated our framework using the real-world nuPlan dataset, demonstrating its effectiveness in scenario-aware uncertainty quantification and reliability assessment across diverse driving contexts.

</details>


### [11] [HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies](https://arxiv.org/abs/2512.05693)
*Zhiying Du,Bei Liu,Yaobo Liang,Yichao Shen,Haidong Cao,Xiangyu Zheng,Zhiyuan Feng,Zuxuan Wu,Jiaolong Yang,Yu-Gang Jiang*

Main category: cs.RO

TL;DR: 本文提出了一种名为HiMoE-VLA的新框架，旨在有效处理具有异质性的多样化机器人数据。通过引入层次化的专家混合架构来适应性地处理不同层次的异质性，并将其逐渐抽象为共享的知识表示，从而在多样化的机器人和动作空间中实现了更高的准确性和鲁棒的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于机器人演示数据在实体、动作空间以及其他显著变量如传感器配置和动作控制频率等方面存在极大的异质性，现有的方法难以有效地整合这些多样性因素，导致其泛化能力受限且在新环境下的表现不佳。

Method: 提出了HiMoE-VLA框架，特别是针对动作模块采用了层次化专家混合（HiMoE）架构，该架构能够自适应地处理跨层的多种异质性来源，并逐步将它们抽象成共享知识表示。

Result: 经过广泛的实验验证，包括模拟基准测试和真实世界中的机器人平台应用，HiMoE-VLA相比现有VLA基线模型展示出一致性的性能提升，在多样化的机器人与动作空间上达到了更高准确率及更强大的泛化能力。

Conclusion: HiMoE-VLA框架证明了其在处理具有高度异质性的机器人数据方面的有效性，为提高基于基础模型的具身智能系统的泛化能力和实际应用场景提供了新的解决方案。

Abstract: The development of foundation models for embodied intelligence critically depends on access to large-scale, high-quality robot demonstration data. Recent approaches have sought to address this challenge by training on large collections of heterogeneous robotic datasets. However, unlike vision or language data, robotic demonstrations exhibit substantial heterogeneity across embodiments and action spaces as well as other prominent variations such as senor configurations and action control frequencies. The lack of explicit designs for handling such heterogeneity causes existing methods to struggle with integrating diverse factors, thereby limiting their generalization and leading to degraded performance when transferred to new settings. In this paper, we present HiMoE-VLA, a novel vision-language-action (VLA) framework tailored to effectively handle diverse robotic data with heterogeneity. Specifically, we introduce a Hierarchical Mixture-of-Experts (HiMoE) architecture for the action module which adaptively handles multiple sources of heterogeneity across layers and gradually abstracts them into shared knowledge representations. Through extensive experimentation with simulation benchmarks and real-world robotic platforms, HiMoE-VLA demonstrates a consistent performance boost over existing VLA baselines, achieving higher accuracy and robust generalization across diverse robots and action spaces. The code and models are publicly available at https://github.com/ZhiyingDu/HiMoE-VLA.

</details>


### [12] [Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning](https://arxiv.org/abs/2512.05711)
*Ali Krayani,Seyedeh Fatemeh Sadati,Lucio Marcenaro,Carlo Regazzoni*

Main category: cs.RO

TL;DR: 本文提出了一种在对抗性干扰条件下无人机使用的分层轨迹规划框架，结合了专家生成的演示和概率生成建模。通过在线推断来预测干扰、定位干扰源并相应调整其轨迹。仿真结果表明，该方法接近专家性能，显著减少了通信干扰和任务成本，同时在动态环境中保持了强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决无人机在对抗性干扰条件下执行任务时面临的挑战，特别是当无人机没有预先知道干扰源位置的情况下，需要一种能够有效减少通信干扰和任务成本的方法。

Method: 使用贝叶斯主动推理，结合专家生成的示例与概率生成模型，对高层次符号规划、低层次运动策略及无线信号反馈进行编码。部署期间，无人机进行在线推理以预测干扰、定位干扰源，并据此调整飞行路径。

Result: 仿真结果显示，所提方法几乎达到了专家级别的表现，在减少通信干扰和任务成本方面显著优于无模型强化学习基线方法，同时在动态环境中仍能保持良好的鲁棒性和泛化能力。

Conclusion: 通过采用贝叶斯主动推理以及结合专家知识和概率建模的方式，本研究为无人机在对抗性干扰环境下的高效运行提供了一个有效的解决方案。

Abstract: This paper proposes a hierarchical trajectory planning framework for UAVs operating under adversarial jamming conditions. Leveraging Bayesian Active Inference, the approach combines expert-generated demonstrations with probabilistic generative modeling to encode high-level symbolic planning, low-level motion policies, and wireless signal feedback. During deployment, the UAV performs online inference to anticipate interference, localize jammers, and adapt its trajectory accordingly, without prior knowledge of jammer locations. Simulation results demonstrate that the proposed method achieves near-expert performance, significantly reducing communication interference and mission cost compared to model-free reinforcement learning baselines, while maintaining robust generalization in dynamic environments.

</details>


### [13] [Global stability of vehicle-with-driver dynamics via Sum-of-Squares programming](https://arxiv.org/abs/2512.05806)
*Martino Gulisano,Marco Gabiccini*

Main category: cs.RO

TL;DR: 本文提出了一种新的基于迭代平方和(SOS)过程优化Lyapunov函数的方法，用于估计车辆-驾驶员系统的安全不变子集。该方法首先在一个两状态基准系统上得到了验证，然后被应用于欠操纵和过操纵场景中，并与详尽模拟得到的参考边界进行了比较。结果表明SOS技术能够有效提供由Lyapunov定义的安全区域，支持其作为主动车辆控制监督层在实时安全性评估中的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为一个包含七个状态变量的车辆-驾驶员系统估计出既保证渐近稳定性又考虑了沿系统轨迹状态安全界限影响的安全不变子集。通过这样做，希望能够开发一种可以应用于实时安全评估的有效方法，比如作为主动车辆控制的监管层级。

Method: 采用了一种原创性的迭代平方和（SOS）程序来优化Lyapunov函数，进而计算出安全集合。首先在两状态基准模型上验证了该方法的有效性；接着，对于所研究的具体车辆-驾驶员系统，使用了延迟预览跟踪模型来模仿人类驾驶行为，并且通过适当参数选择也能模拟数字控制器的工作方式。为了使SOS优化成为可能，还推导出了非线性车辆模型的多项式逼近及其操作包络约束条件。

Result: 结果显示，在欠操纵和过操纵两种情况下，利用SOS技术估计出的安全集合与从详尽仿真中获得的参考边界非常接近。这证明了SOS技术能够在保持高效的同时准确地确定Lyapunov定义下的安全区域。

Conclusion: 研究表明，提出的SOS优化方法能够有效地为复杂车辆-驾驶员系统生成精确的安全不变子集，这对于实现实时安全评估具有重要意义，特别是在作为主动车辆控制系统中的监督层方面展示出了巨大的潜力。

Abstract: This work estimates safe invariant subsets of the Region of Attraction (ROA) for a seven-state vehicle-with-driver system, capturing both asymptotic stability and the influence of state-safety bounds along the system trajectory. Safe sets are computed by optimizing Lyapunov functions through an original iterative Sum-of-Squares (SOS) procedure. The method is first demonstrated on a two-state benchmark, where it accurately recovers a prescribed safe region as the 1-level set of a polynomial Lyapunov function. We then describe the distinguishing characteristics of the studied vehicle-with-driver system: the control dynamics mimic human driver behavior through a delayed preview-tracking model that, with suitable parameter choices, can also emulate digital controllers. To enable SOS optimization, a polynomial approximation of the nonlinear vehicle model is derived, together with its operating-envelope constraints. The framework is then applied to understeering and oversteering scenarios, and the estimated safe sets are compared with reference boundaries obtained from exhaustive simulations. The results show that SOS techniques can efficiently deliver Lyapunov-defined safe regions, supporting their potential use for real-time safety assessment, for example as a supervisory layer for active vehicle control.

</details>


### [14] [Real-time Remote Tracking and Autonomous Planning for Whale Rendezvous using Robots](https://arxiv.org/abs/2512.05808)
*Sushmita Bhattacharya,Ninad Jadhav,Hammad Izhar,Karen Li,Kevin George,Robert Wood,Stephanie Gil*

Main category: cs.RO

TL;DR: 本文介绍了一种使用自主无人驾驶飞行器实现实时抹香鲸海上会合的系统。该系统结合了现场传感器数据和经验鲸鱼潜水模型，采用基于模型的强化学习来指导导航决策，并通过实地实验、硬件测试和模拟仿真进行了评估。


<details>
  <summary>Details</summary>
Motivation: 为了实现实时追踪并会合海洋中的抹香鲸，研究提出了一种利用自主无人飞行器的新方法，旨在解决实时声学跟踪、分布式通信与决策制定以及机载信号处理等关键挑战。

Method: 研究采用了基于模型的强化学习方法，结合现场传感器收集的数据和一个经验性的鲸鱼潜水行为模型来进行导航决策；同时，通过在多米尼加海域对抹香鲸进行实际会合测试、陆地上的硬件实验及根据海洋生物学家观察到的表面轨迹插值得出的鲸鱼运动路径进行模拟来评估系统的有效性。

Result: 研究表明，所提出的系统能够在复杂环境中有效定位并接近目标抹香鲸，表明了其在实时追踪海洋哺乳动物方面的潜力。

Conclusion: 本研究成功开发了一种能够实时追踪并与海中抹香鲸会合的无人机系统，为未来海洋哺乳动物的研究提供了新的工具和技术途径。

Abstract: We introduce a system for real-time sperm whale rendezvous at sea using an autonomous uncrewed aerial vehicle. Our system employs model-based reinforcement learning that combines in situ sensor data with an empirical whale dive model to guide navigation decisions. Key challenges include (i) real-time acoustic tracking in the presence of multiple whales, (ii) distributed communication and decision-making for robot deployments, and (iii) on-board signal processing and long-range detection from fish-trackers. We evaluate our system by conducting rendezvous with sperm whales at sea in Dominica, performing hardware experiments on land, and running simulations using whale trajectories interpolated from marine biologists' surface observations.

</details>


### [15] [Physically-Based Simulation of Automotive LiDAR](https://arxiv.org/abs/2512.05932)
*L. Dudzik,M. Roschani,A. Sielemann,K. Trampert,J. Ziehn,J. Beyerer,C. Neumann*

Main category: cs.RO

TL;DR: 本文提出了一种用于模拟汽车飞行时间（ToF）LiDAR的分析模型，该模型考虑了散射光、回声脉冲宽度和环境光线等因素，并通过光学实验室测量系统地确定模型参数。


<details>
  <summary>Details</summary>
Motivation: 为了更精确地模拟汽车ToF LiDAR的工作情况，特别是考虑到诸如散射光、回声脉冲宽度以及环境光线等影响因素，从而提高仿真准确性。

Method: 基于近红外域内的物理基础渲染(PBR)，假设单次反射和逆反射在栅格化渲染图像上进行，同时考虑传感器发出的光以及其他非相关源（如阳光）产生的杂散光。采用灵活的光束转向模式建模激光雷达发射光束及接收二极管灵敏度。

Result: 成功提取适用于两个不同特性的汽车LiDAR系统（Valeo Scala Gen. 2 和 Blickfeld Cube 1）的相关模型参数，表明该方法的有效性。

Conclusion: 本研究提出的模型能够较好地模拟包括散射光、回声脉冲宽度和环境光线在内的多种因素对汽车ToF LiDAR性能的影响，为相关领域的进一步研究提供了有力支持。

Abstract: We present an analytic model for simulating automotive time-of-flight (ToF) LiDAR that includes blooming, echo pulse width, and ambient light, along with steps to determine model parameters systematically through optical laboratory measurements. The model uses physically based rendering (PBR) in the near-infrared domain. It assumes single-bounce reflections and retroreflections over rasterized rendered images from shading or ray tracing, including light emitted from the sensor as well as stray light from other, non-correlated sources such as sunlight. Beams from the sensor and sensitivity of the receiving diodes are modeled with flexible beam steering patterns and with non-vanishing diameter.
  Different (all non-real time) computational approaches can be chosen based on system properties, computing capabilities, and desired output properties.
  Model parameters include system-specific properties, namely the physical spread of the LiDAR beam, combined with the sensitivity of the receiving diode; the intensity of the emitted light; the conversion between the intensity of reflected light and the echo pulse width; and scenario parameters such as environment lighting, positioning, and surface properties of the target(s) in the relevant infrared domain. System-specific properties of the model are determined from laboratory measurements of the photometric luminance on different target surfaces aligned with a goniometer at 0.01° resolution, which marks the best available resolution for measuring the beam pattern.
  The approach is calibrated for and tested on two automotive LiDAR systems, the Valeo Scala Gen. 2 and the Blickfeld Cube 1. Both systems differ notably in their properties and available interfaces, but the relevant model parameters could be extracted successfully.

</details>


### [16] [Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning](https://arxiv.org/abs/2512.05953)
*Yunhao Cao,Zubin Bhaumik,Jessie Jia,Xingyi He,Kuan Fang*

Main category: cs.RO

TL;DR: 提出了对应导向模仿学习(COIL)，一种用于3D视觉运动控制的条件策略学习框架，通过自动生成的对应标签进行自我监督训练，支持可变的空间和时间粒度的任务描述，并在真实世界操纵任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了实现更加灵活的任务表示方法，在3D视觉运动控制中引入了一种新的模仿学习框架，该框架能够适应不同的用户意图和任务需求，同时支持不同空间和时间粒度的任务定义。

Method: COIL利用选定物体上关键点的预期运动来定义每个任务，采用时空注意力机制设计了一个条件策略，以有效融合多模态输入信息。该策略通过一个可扩展的自我监督流程进行训练，使用了在模拟环境中收集并事后自动生成对应标签的演示数据。

Result: COIL能够在不同任务、物体及运动模式之间泛化，并且在真实世界的操纵任务中，无论是稀疏还是密集的任务描述下，都展现出了比先前方法更优的表现。

Conclusion: COIL为3D视觉运动控制提供了一种强大的新工具，它不仅能够处理变化的任务规格还能够在多样化的场景下保持高效性能。

Abstract: We introduce Correspondence-Oriented Imitation Learning (COIL), a conditional policy learning framework for visuomotor control with a flexible task representation in 3D. At the core of our approach, each task is defined by the intended motion of keypoints selected on objects in the scene. Instead of assuming a fixed number of keypoints or uniformly spaced time intervals, COIL supports task specifications with variable spatial and temporal granularity, adapting to different user intents and task requirements. To robustly ground this correspondence-oriented task representation into actions, we design a conditional policy with a spatio-temporal attention mechanism that effectively fuses information across multiple input modalities. The policy is trained via a scalable self-supervised pipeline using demonstrations collected in simulation, with correspondence labels automatically generated in hindsight. COIL generalizes across tasks, objects, and motion patterns, achieving superior performance compared to prior methods on real-world manipulation tasks under both sparse and dense specifications.

</details>


### [17] [SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models](https://arxiv.org/abs/2512.05955)
*Haowen Liu,Shaoxiong Yao,Haonan Chen,Jiawei Gao,Jiayuan Mao,Jia-Bin Huang,Yilun Du*

Main category: cs.RO

TL;DR: 本文提出了一种名为SIMPACT的测试时仿真支持的动作规划框架，该框架通过循环中的仿真世界建模为视觉-语言模型（VLMs）提供了物理推理能力，而无需额外训练。此方法在需要细致物理推理的真实世界刚体和可变形物体操作任务上展示了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型(VLMs)虽然展现了出色的常识与语义推理能力，但缺乏对物理动态的理解。这种限制源于它们是基于静态互联网规模的视觉-语言数据进行训练的，这些数据不包含因果交互或动作条件变化的信息。因此，利用VLMs执行需要物理理解、推理及相应行动计划的精细机器人操作任务变得极具挑战性。

Method: 研究者们提出了SIMPACT，一个测试时仿真支持的动作规划框架，它通过模拟环境中的循环世界建模来赋予VLMs物理推理的能力，且不需要额外训练。从单次RGB-D观测出发，SIMPACT能够高效地构建物理模拟，使VLM能够提出有根据的动作建议，观察模拟运行，并迭代地改进其推理过程。通过将语言推理与物理预测相结合，这种仿真支持下的VLM能够在物理基础上理解接触动力学和行动结果。

Result: SIMPACT方法在五个具有挑战性的、需要细粒度物理推理的真实世界刚体和可变形物体操作任务上表现出了最先进的性能，超过了现有的通用机器人操作模型。

Conclusion: 实验结果表明，在测试阶段通过高效模拟将物理理解嵌入到VLM推理中，为实现可泛化的具身智能提供了一条有希望的道路。

Abstract: Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io

</details>


### [18] [Training-Time Action Conditioning for Efficient Real-Time Chunking](https://arxiv.org/abs/2512.05964)
*Kevin Black,Allen Z. Ren,Michael Equi,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出了一种在训练时模拟推理延迟并直接以前缀动作为条件的方法，以替代实时分块中的推理时修复方法。这种方法不需要改变模型架构或机器人运行时间，并且可以减少计算开销。实验表明，该方法在较高推理延迟下优于推理时修复，并保持了任务性能和速度的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前的实时分块（RTC）技术通过异步预测动作片段并在推理时基于先前确认的动作进行修复来生成平滑响应的机器人轨迹，但这种方法引入了额外的计算负担，导致推理延迟增加。为了解决这一问题，文章提出了一种简单有效的替代方案：在训练阶段模拟推理延迟，并直接以前缀动作为条件，从而消除任何推理时的额外开销。

Method: 研究者们提出了一种无需修改模型结构或机器人运行时设置的新方法，只需少量代码调整即可实现。核心思想是在训练过程中模拟实际应用中可能遇到的推理延迟情况，并直接利用动作序列的前缀作为条件输入，以此来替代原有的推理时执行的数据修复过程。

Result: 仿真测试显示，在较高推理延迟条件下，训练时RTC表现优于推理时RTC。此外，通过在盒子构建和制作浓缩咖啡等真实世界任务上使用$π_{0.6}$ VLA进行实验验证，证明了训练时RTC不仅能够维持与推理时RTC相当的任务完成度和执行速度，同时还能显著降低计算成本。

Conclusion: 研究结果表明，对于实时机器人控制而言，训练时动作条件设定是一种实用且易于实施的解决方案，它可以有效地取代现有方法中的推理时数据修复步骤，同时保持甚至提高系统的整体性能。

Abstract: Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $π_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.

</details>
