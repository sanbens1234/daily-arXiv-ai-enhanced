<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Gimballed Rotor Mechanism for Omnidirectional Quadrotors](https://arxiv.org/abs/2511.15909)
*J. Cristobal,A. Z. Zain Aldeen,M. Izadi,R. Faieghi*

Main category: cs.RO

TL;DR: 本文介绍了一种新型的万向四旋翼无人机设计，通过在旋翼平台上集成伺服电机实现每个旋翼独立倾斜，从而达到全驱动，使飞行器能够独立地在六个自由度上运动。此外，还为这种设计开发了新的控制分配方案，并通过成功的飞行测试验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的四旋翼无人机由于其结构限制，只能实现欠驱动，这意味着它们不能独立地在所有六个自由度上移动。为了解决这个问题，提出了一个基于万向节转子机制的设计，旨在提供一种模块化且高效的解决方案来构建全方向四旋翼无人机。

Method: 研究者们提出了一种新颖的万向节转子系统，该系统通过在每个转子平台中加入伺服电机来允许各个转子独立倾斜，同时保持轻量化和易于集成到传统四旋翼无人机结构中的特点。为了支持这一创新设计，他们还在PX4自动驾驶仪软件中开发了专门的控制分配算法。

Result: 实验结果表明，所提出的万向节转子机制以及配套的控制策略能够有效地工作，使得四旋翼无人机能够在六个自由度上进行独立操控。

Conclusion: 这项研究表明，通过采用万向节转子机制并结合适当的控制策略，可以成功地将常规四旋翼无人机改造成为全方向、全驱动的飞行平台，为未来更复杂多样的飞行任务提供了可能。

Abstract: This paper presents the design of a gimballed rotor mechanism as a modular and efficient solution for constructing omnidirectional quadrotors. Unlike conventional quadrotors, which are underactuated, this class of quadrotors achieves full actuation, enabling independent motion in all six degrees of freedom. While existing omnidirectional quadrotor designs often require significant structural modifications, the proposed gimballed rotor system maintains a lightweight and easy-to-integrate design by incorporating servo motors within the rotor platforms, allowing independent tilting of each rotor without major alterations to the central structure of a quadrotor. To accommodate this unconventional design, we develop a new control allocation scheme in PX4 Autopilot and present successful flight tests, validating the effectiveness of the proposed approach.

</details>


### [2] [I've Changed My Mind: Robots Adapting to Changing Human Goals during Collaboration](https://arxiv.org/abs/2511.15914)
*Debasmita Ghose,Oz Gitelson,Ryan Jin,Grace Abawe,Marynel Vazquez,Brian Scassellati*

Main category: cs.RO

TL;DR: 提出了一种检测人类目标变化的方法，通过跟踪多个候选动作序列并验证其合理性来实现。该方法在协作烹饪环境中得到了评估，并显示出优于其他基线算法的性能，在任务完成时间和协作效率上都有所提高。


<details>
  <summary>Details</summary>
Motivation: 为了有效的人机协作，机器人需要与人类的目标保持一致，即使这些目标在任务中途发生变化。现有方法通常假设目标是固定的，将目标预测简化为一次性推理。然而，在现实场景中，人们经常改变目标，这对机器人来说是一个挑战，因为它们没有明确的沟通就很难适应。

Method: 本研究提出了一方法，通过追踪多条候选动作序列并对照策略库验证它们的可信度来检测目标变更。一旦检测到变更，机器人会更新对过去相关行动的信任，并构建回溯规划（RHP）树以主动选择能够协助人类同时促进区分性行动的动作，从而揭示更新后的目标。

Result: 此方法在一个包含多达30种独特食谱的合作烹饪环境中进行了评估，并且与三种可比较的人类目标预测算法进行了对比。结果表明，该方法在所有基线上都表现得更好，能够在切换后迅速收敛至正确的目标，减少任务完成时间，并提高了合作效率。

Conclusion: 这项研究表明，通过监测和适应人类伙伴不断变化的目标，可以显著提升人机协作过程中的效率和效果。提出的基于多动作序列跟踪及策略库验证的方法，不仅能够快速识别出目标的变化，还能有效地调整自身行为以更好地支持人类伙伴。

Abstract: For effective human-robot collaboration, a robot must align its actions with human goals, even as they change mid-task. Prior approaches often assume fixed goals, reducing goal prediction to a one-time inference. However, in real-world scenarios, humans frequently shift goals, making it challenging for robots to adapt without explicit communication. We propose a method for detecting goal changes by tracking multiple candidate action sequences and verifying their plausibility against a policy bank. Upon detecting a change, the robot refines its belief in relevant past actions and constructs Receding Horizon Planning (RHP) trees to actively select actions that assist the human while encouraging Differentiating Actions to reveal their updated goal. We evaluate our approach in a collaborative cooking environment with up to 30 unique recipes and compare it to three comparable human goal prediction algorithms. Our method outperforms all baselines, quickly converging to the correct goal after a switch, reducing task completion time, and improving collaboration efficiency.

</details>


### [3] [The Role of Consequential and Functional Sound in Human-Robot Interaction: Toward Audio Augmented Reality Interfaces](https://arxiv.org/abs/2511.15956)
*Aliyah Smith,Monroe Kennedy*

Main category: cs.RO

TL;DR: 本研究探讨了机器人通过声音与人类互动的效果，发现操作声音对人的感知没有负面影响，侧向声源定位准确而前方声源定位准确性下降，空间声音能够有效传达任务相关信息并提升亲切感、减少不适。


<details>
  <summary>Details</summary>
Motivation: 随着机器人越来越多地融入日常生活环境，理解它们如何与人类沟通变得至关重要。声音作为强大的交互渠道，不仅包括操作噪音，还涵盖了专门设计的声音信号。

Method: 本研究检查了结果性声音和功能性声音对人类感知和行为的影响，并首次探索了通过定位和交接任务中的空间声音效果。

Result: 结果表明，Kinova Gen3操纵器的结果性声音并未对感知产生负面影响；对于侧向提示音的空间定位非常准确，但对于前方提示音的准确性有所下降；空间声音可以在同时传达任务相关信息的同时促进温暖感并减少不适。

Conclusion: 这些发现强调了功能性和变革性听觉设计在增强人机协作方面的潜力，并为未来基于声音的交互策略提供了信息。

Abstract: As robots become increasingly integrated into everyday environments, understanding how they communicate with humans is critical. Sound offers a powerful channel for interaction, encompassing both operational noises and intentionally designed auditory cues. In this study, we examined the effects of consequential and functional sounds on human perception and behavior, including a novel exploration of spatial sound through localization and handover tasks. Results show that consequential sounds of the Kinova Gen3 manipulator did not negatively affect perceptions, spatial localization is highly accurate for lateral cues but declines for frontal cues, and spatial sounds can simultaneously convey task-relevant information while promoting warmth and reducing discomfort. These findings highlight the potential of functional and transformative auditory design to enhance human-robot collaboration and inform future sound-based interaction strategies.

</details>


### [4] [PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization](https://arxiv.org/abs/2511.15995)
*Zili Tang,Ying Zhang,Meng Guo*

Main category: cs.RO

TL;DR: 本文提出了一种基于组合混合优化的方法，用于控制机器人团队协作推动各种形状的物体到达指定位置。该方法包括三个主要部分：推子任务的分解、排序和滚动分配；关键帧引导的混合搜索以优化每个子任务的参数化推动模式序列；以及执行这些模式并进行切换的混合控制。此外，还采用了一种基于扩散的加速器来预测应优先考虑的关键帧和推动模式，从而提高规划效率。


<details>
  <summary>Details</summary>
Motivation: 许多机器人没有配备操作臂，而且很多物体不适合抓取操作（如大型箱子和圆柱体）。在这种情况下，推动是一种简单但有效的非抓取技能，可以让机器人与环境互动并对环境做出改变。现有工作通常假定一组预定义的推动模式和固定形状的物体。本研究旨在解决控制机器人团队协同推动任意形状的大量物体到各自目的地的一般问题，在充满障碍物且可移动的复杂环境中完成此任务。

Method: 提出的方法基于动态任务分配和通过一系列推动模式及关联力实现的混合执行的组合混合优化。它由三个主要组成部分构成：(I) 将推动子任务分解、排序，并滚动分配给机器人小组；(II) 通过关键帧引导的混合搜索优化每项子任务中参数化推动模式的顺序；(III) 利用混合控制执行这些模式及其之间的转换。另外，采用了基于扩散的加速器来预测在混合搜索期间应该优先处理的关键帧和推动模式，以此进一步提高计划效率。

Result: 该框架在温和假设下是完整的。通过模拟实验和硬件实验验证了不同数量的机器人和一般形状物体下的效率与有效性，并展示了其向异构机器人、平面装配和6D推动任务扩展的能力。

Conclusion: 所提出的控制多机器人系统协同推动任意形状物体至目标地点的方法，不仅有效解决了存在不确定性和受限接触力条件下在线任务协调的问题，同时也提高了规划过程中的效率。

Abstract: Many robots are not equipped with a manipulator and many objects are not suitable for prehensile manipulation (such as large boxes and cylinders). In these cases, pushing is a simple yet effective non-prehensile skill for robots to interact with and further change the environment. Existing work often assumes a set of predefined pushing modes and fixed-shape objects. This work tackles the general problem of controlling a robotic fleet to push collaboratively numerous arbitrary objects to respective destinations, within complex environments of cluttered and movable obstacles. It incorporates several characteristic challenges for multi-robot systems such as online task coordination under large uncertainties of cost and duration, and for contact-rich tasks such as hybrid switching among different contact modes, and under-actuation due to constrained contact forces. The proposed method is based on combinatorial hybrid optimization over dynamic task assignments and hybrid execution via sequences of pushing modes and associated forces. It consists of three main components: (I) the decomposition, ordering and rolling assignment of pushing subtasks to robot subgroups; (II) the keyframe guided hybrid search to optimize the sequence of parameterized pushing modes for each subtask; (III) the hybrid control to execute these modes and transit among them. Last but not least, a diffusion-based accelerator is adopted to predict the keyframes and pushing modes that should be prioritized during hybrid search; and further improve planning efficiency. The framework is complete under mild assumptions. Its efficiency and effectiveness under different numbers of robots and general-shaped objects are validated extensively in simulations and hardware experiments, as well as generalizations to heterogeneous robots, planar assembly and 6D pushing.

</details>


### [5] [Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud](https://arxiv.org/abs/2511.16048)
*Qing Zhang,Jing Huang,Mingyang Xu,Jun Rekimoto*

Main category: cs.RO

TL;DR: 本文探讨了一种故意采用“低精度”方法的创意潜力，介绍了一种名为“语义故障”的软飞行机器人艺术装置。该装置通过多模态大语言模型的定性、语义理解来导航，并通过自然语言提示为机器人创作了生物启发的人格。实验表明，这种框架能够成功地创建具有独特人格但不完美的伴侣，其成功标准在于角色魅力而非效率。


<details>
  <summary>Details</summary>
Motivation: 探索一种故意采用‘低精度’方法在机器人领域中的创意潜力，旨在创造一个基于语义理解和生物启发人格的软飞行机器人艺术装置，以展示这种非传统方法在产生独特且吸引人的机器人物格方面的可能性。

Method: 设计并实现了一个名为‘语义故障’的艺术装置，它依赖于多模态大语言模型对环境的理解进行导航，而不是使用传统的传感器如LiDAR和SLAM系统。通过自然语言提示给机器人赋予特定性格特征，从而形成一种‘叙事思维’与其实验性质的身体相匹配。

Result: 通过13分钟自主飞行日志及后续研究统计验证了所提出框架的有效性，能够用于创作出可量化区分的不同人格。分析揭示了从基于地标导航到令人信服的‘计划到执行’差距等新兴行为，以及由于缺乏精确本体感觉而产生的不可预测但合理的行为。

Conclusion: 研究表明，即使是在基于低精度技术和有限感知能力的框架也能够创造出富有魅力的角色型伴侣机器人，强调了在评价此类作品时应更加关注其角色吸引力而非操作效率。

Abstract: While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately "lo-fi" approach. We present the "Semantic Glitch," a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a "physical glitch" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a "narrative mind" that complements the "weak," historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling "plan to execution" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.

</details>


### [6] [Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers](https://arxiv.org/abs/2511.16050)
*Takeru Tsunoori,Masato Kobayashi,Yuki Uranishi*

Main category: cs.RO

TL;DR: 本文提出了Bi-AQUA，这是一个基于双边控制的模仿学习框架，特别针对水下机器人操作设计，通过整合光照感知视觉处理来应对极端光照变化、色彩失真和可见度降低等问题。实验表明，该方法在各种静态和动态光照条件下执行水下取放任务时表现稳健，并显著优于无光照建模的双边控制基线方法。


<details>
  <summary>Details</summary>
Motivation: 水下机器人操作面临的主要挑战包括极端光照变化、色彩失真及能见度降低。为了解决这些问题，研究者们开发了Bi-AQUA，这是首个结合光照意识视觉处理技术的水下双边控制模仿学习框架，旨在改善水下机械臂的操作性能。

Method: Bi-AQUA采用了一种三层级光照适应机制：一是从RGB图像中提取光照表示而无需手动标注的光照编码器（Lighting Encoder），其训练受到模仿目标的隐式监督；二是利用FiLM调制视觉骨干特征以实现自适应、光照敏感的特征提取；三是向Transformer编码器输入添加明确的光照标记以进行任务相关的条件设置。

Result: 真实世界中的水下取放任务实验显示，在多种静态与动态光照条件下，Bi-AQUA能够保持稳定的表现，并且相比没有考虑光照因素的传统双边控制方法有显著优势。消融研究表明，所有三个光照敏感组件对于整体性能都至关重要。

Conclusion: 这项工作将陆地上的基于双边控制的模仿学习与水下操控相结合，使得能够在具有挑战性的海洋环境中实现对力敏感的自主操作。

Abstract: Underwater robotic manipulation is fundamentally challenged by extreme lighting variations, color distortion, and reduced visibility. We introduce Bi-AQUA, the first underwater bilateral control-based imitation learning framework that integrates lighting-aware visual processing for underwater robot arms. Bi-AQUA employs a hierarchical three-level lighting adaptation mechanism: a Lighting Encoder that extracts lighting representations from RGB images without manual annotation and is implicitly supervised by the imitation objective, FiLM modulation of visual backbone features for adaptive, lighting-aware feature extraction, and an explicit lighting token added to the transformer encoder input for task-aware conditioning. Experiments on a real-world underwater pick-and-place task under diverse static and dynamic lighting conditions show that Bi-AQUA achieves robust performance and substantially outperforms a bilateral baseline without lighting modeling. Ablation studies further confirm that all three lighting-aware components are critical. This work bridges terrestrial bilateral control-based imitation learning and underwater manipulation, enabling force-sensitive autonomous operation in challenging marine environments. For additional material, please check: https://mertcookimg.github.io/bi-aqua

</details>


### [7] [MagBotSim: Physics-Based Simulation and Reinforcement Learning Environments for Magnetic Robotics](https://arxiv.org/abs/2511.16158)
*Lara Bergmann,Cedric Grothues,Klaus Neumann*

Main category: cs.RO

TL;DR: 本文介绍了一种基于磁悬浮技术的机器人系统（MagBots），旨在通过结合运输和操作功能来提高制造系统的效率、适应性和紧凑性，并为此开发了仿真工具MagBotSim。


<details>
  <summary>Details</summary>
Motivation: 为了提高工业自动化中材料流动的灵活性和效率，文章提出将磁悬浮系统的运输与操作功能结合起来，形成一个协调的磁性机器人集群(MagBots)，以实现更高的生产效率、适应性和空间利用率。

Method: 作者们创建了一个名为MagBotSim的物理基础模拟器，专门用于磁悬浮系统的研究。该模拟器支持智能算法的发展，为下一代由磁性机器人驱动的制造系统奠定了基础。

Result: 通过将磁悬浮系统视为机器人集群并提供专用仿真工具，研究为未来制造业提供了新的可能性。此外，还公开了MagBotSim的相关文档、视频、实验及代码等资源。

Conclusion: 本研究通过引入MagBots概念及配套的MagBotSim仿真软件，为开发更加高效灵活的新一代制造系统开辟了道路。

Abstract: Magnetic levitation is about to revolutionize in-machine material flow in industrial automation. Such systems are flexibly configurable and can include a large number of independently actuated shuttles (movers) that dynamically rebalance production capacity. Beyond their capabilities for dynamic transportation, these systems possess the inherent yet unexploited potential to perform manipulation. By merging the fields of transportation and manipulation into a coordinated swarm of magnetic robots (MagBots), we enable manufacturing systems to achieve significantly higher efficiency, adaptability, and compactness. To support the development of intelligent algorithms for magnetic levitation systems, we introduce MagBotSim (Magnetic Robotics Simulation): a physics-based simulation for magnetic levitation systems. By framing magnetic levitation systems as robot swarms and providing a dedicated simulation, this work lays the foundation for next generation manufacturing systems powered by Magnetic Robotics. MagBotSim's documentation, videos, experiments, and code are available at: https://ubi-coro.github.io/MagBotSim/

</details>


### [8] [DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks](https://arxiv.org/abs/2511.16223)
*Vincenzo Pomponi,Paolo Franceschi,Stefano Baraldo,Loris Roveda,Oliver Avram,Luca Maria Gambardella,Anna Valente*

Main category: cs.RO

TL;DR: DynaMimicGen (D-MG) is a scalable dataset generation framework that allows for the creation of robust manipulation policies with minimal human supervision, supporting dynamic task environments. It segments human demonstrations into sub-tasks and uses Dynamic Movement Primitives to adapt these behaviors to new and changing scenarios. This method enhances the ability of robots to perform complex tasks, such as cube stacking or placing mugs in drawers, even when faced with unpredictable changes in the environment.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to overcome the limitations associated with learning robust manipulation policies, which usually demand large, diverse datasets. Collecting such data is often time-consuming and impractical, especially in dynamic settings. The aim is to develop a solution that can effectively train policies from very limited human input while also being adaptable to dynamic environments.

Method: The introduced method, DynaMimicGen (D-MG), operates by first segmenting a small number of human demonstrations into meaningful sub-tasks. It then utilizes Dynamic Movement Primitives (DMPs) to generalize and adapt these demonstrated behaviors to various and dynamically changing situations. D-MG generates smooth, realistic, and consistent Cartesian trajectories that can adjust in real-time to changes in object positions, robot states, or scene layout during task execution.

Result: Robot agents trained using imitation learning on data generated by D-MG were able to demonstrate strong performance across a range of benchmarks, including long-horizon and contact-rich tasks. These tasks involved activities like stacking cubes and placing mugs inside drawers, even under conditions where the environment was subject to unexpected changes.

Conclusion: D-MG presents a significant advancement in the field of autonomous robot learning by offering a more efficient and scalable alternative to traditional manual data collection methods. It successfully reduces the dependency on extensive human demonstrations and supports generalization in dynamic environments, thereby facilitating the development of versatile and adaptive robotic systems.

Abstract: Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning.

</details>


### [9] [FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models](https://arxiv.org/abs/2511.16233)
*Kewei Chen,Yayu Long,Shuai Li,Mingsheng Shang*

Main category: cs.RO

TL;DR: 本文提出了一种新的数据为中心的生成式数据蒸馏框架FT-NCFM，用于解决视觉-语言-动作（VLA）模型对大量冗余且价值不均的数据集过度依赖的问题。通过结合因果归因和程序化对比验证的自包含事实追踪引擎评估样本内在价值，并指导对抗性NCFM过程合成与模型无关、信息密集且可重用的数据资产。实验结果表明，在多个主流VLA基准上使用仅5%经蒸馏的核心集训练的模型达到了85-90%的成功率，同时减少了超过80%的训练时间。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作（VLA）模型强大的泛化能力受限于其对大规模但冗余且价值分布不均的数据集的高度依赖，这阻碍了这些模型的广泛应用。现有的以模型为中心的优化路径，如模型压缩（通常导致性能下降）或策略提炼（产物特定于模型且缺乏通用性），未能从根本上解决这一数据层面的挑战。

Method: 提出了FT-NCFM框架，该框架采用一种自我包含的事实追踪(FT)引擎来结合因果归属分析和程序化的对比验证方法来评价样本的价值。基于这些评价，利用对抗性的NCFM过程来合成一个与模型无关、信息密集并且可以重复使用的数据资产。

Result: 在几个主流VLA基准测试中，仅使用我们精简后核心数据集的5%，训练出的模型就能达到85-90%的成功率，相较于使用完整数据集训练的情况；同时，这种方法还能够将训练时间减少超过80%。

Conclusion: 研究表明，智能的数据蒸馏为构建高效高性能的VLA模型开辟了一条极具前景的新途径。

Abstract: The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.

</details>


### [10] [How Robot Dogs See the Unseeable](https://arxiv.org/abs/2511.16262)
*Oliver Bimber,Karl Dietrich von Ellenrieder,Michael Haller,Rakesh John Amala Arokia Nathan,Gianni Lunardi,Marco Camurri,Mohamed Youssef,Santos Miguel Orozco Soto,Jeremy E. Niven*

Main category: cs.RO

TL;DR: 该研究通过模仿动物的侧向运动（Peering），利用合成孔径（SA）感应技术来克服机器人视觉中的部分遮挡问题。通过让机器人执行侧向运动，其相机可以捕获一系列图像，这些图像经过计算整合后，能够生成一个具有极浅景深的图像，从而模糊掉前景障碍物并使背景清晰可见。这种方法不仅恢复了基本场景的理解能力，还增强了大型多模态模型中的高级视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人摄像头由于小光圈和大景深的特点，导致前景障碍物和背景物体都处于清晰对焦状态，这使得遮挡物会遮挡关键的场景信息。为了解决这一问题，研究人员受到了动物通过侧向运动估计距离的启发，提出了使用合成孔径感应技术的方法。

Method: 通过让机器人执行类似动物的侧向运动，其搭载的相机在移动过程中记录下一系列图像。之后，通过计算手段将这些图像进行整合，以合成一个具有极浅景深的新图像。这样做的结果是能够有效地模糊掉遮挡元素，同时保持背景对象清晰可见。

Result: 实验表明，所提出的方法不仅能恢复被传统方法遮挡影像的基本场景理解能力，而且还能增强大规模多模态模型中对于复杂环境下的高级视觉推理性能。相比依赖特征的多视角3D视觉方法或像LiDAR这样的主动传感器，基于侧向运动的合成孔径感应技术更加鲁棒于遮挡情况、计算效率更高，并且可以立即部署到任何移动机器人上。

Conclusion: 本研究表明，模仿动物行为的侧向运动结合合成孔径感应技术为机器人在复杂拥挤环境中实现高级别的场景理解提供了新的途径。

Abstract: Peering, a side-to-side motion used by animals to estimate distance through motion parallax, offers a powerful bio-inspired strategy to overcome a fundamental limitation in robotic vision: partial occlusion. Conventional robot cameras, with their small apertures and large depth of field, render both foreground obstacles and background objects in sharp focus, causing occluders to obscure critical scene information. This work establishes a formal connection between animal peering and synthetic aperture (SA) sensing from optical imaging. By having a robot execute a peering motion, its camera describes a wide synthetic aperture. Computational integration of the captured images synthesizes an image with an extremely shallow depth of field, effectively blurring out occluding elements while bringing the background into sharp focus. This efficient, wavelength-independent technique enables real-time, high-resolution perception across various spectral bands. We demonstrate that this approach not only restores basic scene understanding but also empowers advanced visual reasoning in large multimodal models, which fail with conventionally occluded imagery. Unlike feature-dependent multi-view 3D vision methods or active sensors like LiDAR, SA sensing via peering is robust to occlusion, computationally efficient, and immediately deployable on any mobile robot. This research bridges animal behavior and robotics, suggesting that peering motions for synthetic aperture sensing are a key to advanced scene understanding in complex, cluttered environments.

</details>


### [11] [Funabot-Upper: McKibben Actuated Haptic Suit Inducing Kinesthetic Perceptions in Trunk, Shoulder, Elbow, and Wrist](https://arxiv.org/abs/2511.16265)
*Haru Fukatsu,Ryoji Yasuda,Yuki Funabora,Shinji Doki*

Main category: cs.RO

TL;DR: This paper introduces Funabot-Upper, a haptic suit designed to induce the perception of 14 upper-body motions. It improves upon previous designs by reducing perceptual mixing and increasing recognition accuracy from 68.8% to 94.6%, making it a promising tool for future haptic applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a haptic suit that can effectively induce kinesthetic perceptions across multiple joints of the upper body, addressing the issue of perceptual mixing observed in earlier designs, and thereby improving the overall recognition accuracy and user experience with wearable haptic devices.

Method: A new, simplified design policy was established, leading to the development of Funabot-Upper, which stimulates joints and muscles independently to induce kinesthetic perceptions in the trunk, shoulder, elbow, and wrist. The effectiveness of this approach was verified through experiments that examined the relationship between the stimulation provided and the perceived kinesthetic sensations.

Result: Experiments confirmed that Funabot-Upper successfully induces kinesthetic perception across multiple joints while significantly reducing the perceptual mixing that occurred in previous designs. The recognition accuracy was improved from 68.8% to 94.6%.

Conclusion: Funabot-Upper represents an advancement in wearable haptic technology, offering a solution to the challenge of inducing clear kinesthetic perceptions across multiple upper body parts. Its success in minimizing perceptual mixing and enhancing recognition accuracy highlights its potential for broad application in the field of haptics.

Abstract: This paper presents Funabot-Upper, a wearable haptic suit that enables users to perceive 14 upper-body motions, including those of the trunk, shoulder, elbow, and wrist. Inducing kinesthetic perception through wearable haptic devices has attracted attention, and various devices have been developed in the past. However, these have been limited to verifications on single body parts, and few have applied the same method to multiple body parts as well. In our previous study, we developed a technology that uses the contraction of artificial muscles to deform clothing in three dimensions. Using this technology, we developed a haptic suit that induces kinesthetic perception of 7 motions in multiple upper body. However, perceptual mixing caused by stimulating multiple human muscles has occurred between the shoulder and the elbow. In this paper, we established a new, simplified design policy and developed a novel haptic suit that induces kinesthetic perceptions in the trunk, shoulder, elbow, and wrist by stimulating joints and muscles independently. We experimentally demonstrated the induced kinesthetic perception and examined the relationship between stimulation and perceived kinesthetic perception under the new design policy. Experiments confirmed that Funabot-Upper successfully induces kinesthetic perception across multiple joints while reducing perceptual mixing observed in previous designs. The new suit improved recognition accuracy from 68.8% to 94.6% compared to the previous Funabot-Suit, demonstrating its superiority and potential for future haptic applications.

</details>


### [12] [Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning](https://arxiv.org/abs/2511.16330)
*Shreyas Kumar,Ravi Prakash*

Main category: cs.RO

TL;DR: 本文提出了一种新的强化学习框架C-GMS，它结合了动态运动基元(DMP)和可变阻抗控制(VIC)，同时保证了Lyapunov稳定性以及致动器的可行性。通过从一个数学定义的稳定增益调度流形中采样来重新定义策略探索，确保每个策略展开都是稳定的并且物理上可行。此外，该方法在存在有界模型误差和部署时不确定性的情况下也能确保有界的跟踪误差。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习方法虽然能够帮助机器人学习复杂的协作技能，但其无模型特性往往导致不稳定性和不安全的探索行为，特别是在阻抗增益随时间变化的情况下。为了解决这个问题，并确保学习过程中的稳定性和安全性，提出了Certified Gaussian Manifold Sampling (C-GMS) 方法。

Method: C-GMS是一种以轨迹为中心的强化学习框架，它将DMP与VIC策略相结合，通过构造保证了Lyapunov稳定性和致动器的可行性。该方法的核心思想是将策略探索视为从一个由数学定义的稳定增益调度流形中进行采样的过程，从而确保所有生成的策略既稳定又实际可行。

Result: 研究结果表明，C-GMS不仅在模拟环境中有效，在真实机器人上的测试也验证了其有效性。此外，理论分析还证明了即使面对有界模型错误和执行时的不确定性，C-GMS依然能够保证有限的跟踪误差。

Conclusion: 通过引入C-GMS框架，这项工作为机器人在复杂环境下实现可靠自主交互铺平了道路。它解决了传统RL方法中存在的不稳定性和安全隐患问题，提供了一个既能保证学习效率又能保障安全性的新途径。

Abstract: Reinforcement learning (RL) offers a powerful approach for robots to learn complex, collaborative skills by combining Dynamic Movement Primitives (DMPs) for motion and Variable Impedance Control (VIC) for compliant interaction. However, this model-free paradigm often risks instability and unsafe exploration due to the time-varying nature of impedance gains. This work introduces Certified Gaussian Manifold Sampling (C-GMS), a novel trajectory-centric RL framework that learns combined DMP and VIC policies while guaranteeing Lyapunov stability and actuator feasibility by construction. Our approach reframes policy exploration as sampling from a mathematically defined manifold of stable gain schedules. This ensures every policy rollout is guaranteed to be stable and physically realizable, thereby eliminating the need for reward penalties or post-hoc validation. Furthermore, we provide a theoretical guarantee that our approach ensures bounded tracking error even in the presence of bounded model errors and deployment-time uncertainties. We demonstrate the effectiveness of C-GMS in simulation and verify its efficacy on a real robot, paving the way for reliable autonomous interaction in complex environments.

</details>


### [13] [Flow-Aided Flight Through Dynamic Clutters From Point To Motion](https://arxiv.org/abs/2511.16372)
*Bowen Xu,Zexuan Yan,Minghao Lu,Xiyu Fan,Yi Luo,Youshen Lin,Zhiqiang Chen,Yeke Chen,Qiyuan Qiao,Peng Lu*

Main category: cs.RO

TL;DR: 本研究提出了一种基于单激光雷达感知的自主飞行系统，通过深度距离图和环境变化点流来表示复杂动态环境，并采用强化学习隐式地驱动无人机避开动态障碍物。该系统展示了优越的成功率和适应性，且从模拟器中得出的策略能够使真实世界的四旋翼无人机安全地执行操作。


<details>
  <summary>Details</summary>
Motivation: 在穿越动态杂乱环境中，主要挑战在于有效感知环境动态并考虑障碍物运动产生避让行为。现有解决方案虽然在避免动态障碍物方面取得进展，但在高度动态场景下，特别是在存在遮挡的情况下，决策的关键依赖——明确建模动态障碍物运动耗时且不可靠。

Method: 1. 从原始点云编码得到固定形状、低分辨率但保留细节的深度距离图。
2. 从多帧观测中提取出作为运动特征的环境变化点流。
3. 将上述两种方法整合成一种轻量级易于学习的复杂动态环境表示方法。
4. 利用提出的感知表征方式隐式驱动避让行为生成，其中策略优化由相对运动调节的距离场指示。
5. 使用部署友好的感知模拟与无需动力学模型的加速度控制。

Result: 所提出的系统相对于其他方法表现出更高的成功率和更好的适应性；从模拟器中训练出的策略可以直接应用于现实世界中的四旋翼无人机上，实现安全导航。

Conclusion: 这项工作证明了仅依靠单激光雷达感知即可支持复杂动态环境下的自主飞行任务，为开发更加灵活可靠的无人机提供了新思路。

Abstract: Challenges in traversing dynamic clutters lie mainly in the efficient perception of the environmental dynamics and the generation of evasive behaviors considering obstacle movement. Previous solutions have made progress in explicitly modeling the dynamic obstacle motion for avoidance, but this key dependency of decision-making is time-consuming and unreliable in highly dynamic scenarios with occlusions. On the contrary, without introducing object detection, tracking, and prediction, we empower the reinforcement learning (RL) with single LiDAR sensing to realize an autonomous flight system directly from point to motion. For exteroception, a depth sensing distance map achieving fixed-shape, low-resolution, and detail-safe is encoded from raw point clouds, and an environment change sensing point flow is adopted as motion features extracted from multi-frame observations. These two are integrated into a lightweight and easy-to-learn representation of complex dynamic environments. For action generation, the behavior of avoiding dynamic threats in advance is implicitly driven by the proposed change-aware sensing representation, where the policy optimization is indicated by the relative motion modulated distance field. With the deployment-friendly sensing simulation and dynamics model-free acceleration control, the proposed system shows a superior success rate and adaptability to alternatives, and the policy derived from the simulator can drive a real-world quadrotor with safe maneuvers.

</details>


### [14] [Robot Metacognition: Decision Making with Confidence for Tool Invention](https://arxiv.org/abs/2511.16390)
*Ajith Anil Meera,Poppy Collis,Polina Arbuzova,Abián Torres,Paul F Kinghorn,Ricardo Sanz,Pablo Lanillos*

Main category: cs.RO

TL;DR: 本文提出了一种受神经科学启发的机器人元认知架构，该架构以信心为中心，并通过自主工具发明的案例展示了这种架构。信心作为元认知度量被用于机器人的决策方案中，使机器人能够评估其决策的可靠性，从而提高在实际物理部署中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的机器人通常缺乏真正智能行为的关键要素：反思自身认知过程和决策的能力。这种自我监控或元认知对学习、决策和问题解决至关重要。受神经科学的启发，作者旨在开发一种基于信心（对决策的二阶判断）的机器人元认知架构，以提升机器人决策的可靠性和现实世界应用中的鲁棒性。

Method: 研究者提出并实现了一个基于信心度量的机器人元认知架构，该架构让机器人能够在进行决策时评估自己对于完成任务的信心水平，进而调节自身行为和资源分配。这一方法是通过具体的实验场景——自主工具发明来展示的。

Result: 研究表明，利用信心作为元认知度量纳入到机器人决策过程中，可以有效帮助机器人评估其决策的可靠性，这有助于改善机器人在面对真实世界物理环境时的行为表现和适应能力。

Conclusion: 本文介绍了一种新的机器人元认知架构，强调了身体动作监控作为达成更明智决策手段的重要性，并探讨了机器人元认知领域未来可能的应用方向与研究趋势。

Abstract: Robots today often miss a key ingredient of truly intelligent behavior: the ability to reflect on their own cognitive processes and decisions. In humans, this self-monitoring or metacognition is crucial for learning, decision making and problem solving. For instance, they can evaluate how confident they are in performing a task, thus regulating their own behavior and allocating proper resources. Taking inspiration from neuroscience, we propose a robot metacognition architecture centered on confidence (a second-order judgment on decisions) and we demonstrate it on the use case of autonomous tool invention. We propose the use of confidence as a metacognitive measure within the robot decision making scheme. Confidence-informed robots can evaluate the reliability of their decisions, improving their robustness during real-world physical deployment. This form of robotic metacognition emphasizes embodied action monitoring as a means to achieve better informed decisions. We also highlight potential applications and research directions for robot metacognition.

</details>


### [15] [Homogeneous Proportional-Integral-Derivative Controller in Mobile Robotic Manipulators](https://arxiv.org/abs/2511.16406)
*Luis Luna,Isaac Chairez,Andrey Polyakov*

Main category: cs.RO

TL;DR: 本文提出了一种新的同质比例-积分-微分（hPID）控制策略，专为移动机器人操作器设计，以实现鲁棒和协调的运动控制。通过采用同质控制理论的数学框架，该控制器能够系统地增强闭环系统的稳定性和收敛性属性，即使在存在动态不确定性和外部干扰的情况下也是如此。实验结果表明，与传统的线性PID控制器相比，hPID控制器在响应时间、稳态误差以及对模型不确定性的鲁棒性方面表现更优，适用于下一代移动操作系统的自主性和可靠性提升。


<details>
  <summary>Details</summary>
Motivation: 移动机器人操作器（MRMs）结合了移动性和操纵能力，但由于其非线性动力学、欠驱动特性以及基座和操作臂子系统之间的耦合，给控制带来了重大挑战。为了克服这些挑战，并提高MRMs在面对动态不确定性及外部干扰时的性能，提出了本研究。

Method: 研究中引入了一种基于同质控制理论的新式hPID控制策略。此方法通过将传统PID增益推广到非线性的状态依赖函数来改善跟踪误差的收敛性。利用Lyapunov方法进行了稳定性分析，证明了在温和条件下hPID控制器可以保证全局渐近稳定性和有限时间收敛。

Result: 实验结果显示，在代表性的MRM模型上测试时，所提出的hPID控制器比传统的线性PID控制器在响应时间、稳态误差以及对抗模型不确定性的鲁棒性方面表现得更好。这证实了hPID控制器在实现高精度轨迹跟踪方面的有效性。

Conclusion: 这项研究为下一代移动操作系统提供了一个可扩展且有理论基础的控制框架，有助于提高这些系统在结构化和非结构化环境中的自主性和可靠性。

Abstract: Mobile robotic manipulators (MRMs), which integrate mobility and manipulation capabilities, present significant control challenges due to their nonlinear dynamics, underactuation, and coupling between the base and manipulator subsystems. This paper proposes a novel homogeneous Proportional-Integral-Derivative (hPID) control strategy tailored for MRMs to achieve robust and coordinated motion control. Unlike classical PID controllers, the hPID controller leverages the mathematical framework of homogeneous control theory to systematically enhance the stability and convergence properties of the closed-loop system, even in the presence of dynamic uncertainties and external disturbances involved into a system in a homogeneous way. A homogeneous PID structure is designed, ensuring improved convergence of tracking errors through a graded homogeneity approach that generalizes traditional PID gains to nonlinear, state-dependent functions. Stability analysis is conducted using Lyapunov-based methods, demonstrating that the hPID controller guarantees global asymptotic stability and finite-time convergence under mild assumptions. Experimental results on a representative MRM model validate the effectiveness of the hPID controller in achieving high-precision trajectory tracking for both the mobile base and manipulator arm, outperforming conventional linear PID controllers in terms of response time, steady-state error, and robustness to model uncertainties. This research contributes a scalable and analytically grounded control framework for enhancing the autonomy and reliability of next-generation mobile manipulation systems in structured and unstructured environments.

</details>


### [16] [From Prompts to Printable Models: Support-Effective 3D Generation via Offset Direct Preference Optimization](https://arxiv.org/abs/2511.16434)
*Chenming Wu,Xiaofan Li,Chengkai Dai*

Main category: cs.RO

TL;DR: 本文提出了一种新的框架SEG，该框架在3D模型生成过程中直接优化最小支撑材料的使用。通过将支撑结构模拟整合到训练过程中，SEG促进了生成本质上需要较少支撑的几何形状，从而减少了材料浪费和生产时间。实验结果表明，与基线模型相比，SEG在减少支撑体积和可打印性方面表现更优，并且保持了对输入提示的高度保真度。


<details>
  <summary>Details</summary>
Motivation: 当前的切片技术虽然提供了先进的支撑策略，但主要集中在后处理优化上，而不是解决在模型生成阶段就需要高效支撑设计的问题。为了解决这一问题，研究者提出了SEG框架，旨在直接优化模型以减少支撑材料的使用，从而提高3D打印过程中的可持续性和效率。

Method: SEG框架采用了直接偏好优化加偏移量（ODPO）的方法，并将其集成到了3D生成管道中。通过在训练过程中引入支撑结构仿真，使得生成的几何体天然地要求较少的支撑物，进而降低了材料浪费和生产时间。

Result: 通过对两个基准数据集Thingi10k-Val和GPT-3DP-Val进行广泛实验，发现SEG显著优于TRELLIS、DPO和DRO等基线模型，在支撑体积减少和支持结构需求最小化的同时保证了高保真度。

Conclusion: 研究结果强调了SEG能够通过在生成过程中直接优化模型来改变3D打印领域，为更加可持续和高效的数字制造实践铺平了道路。

Abstract: The transition from digital 3D models to physical objects via 3D printing often requires support structures to prevent overhanging features from collapsing during the fabrication process. While current slicing technologies offer advanced support strategies, they focus on post-processing optimizations rather than addressing the underlying need for support-efficient design during the model generation phase. This paper introduces SEG (\textit{\underline{S}upport-\underline{E}ffective \underline{G}eneration}), a novel framework that integrates Direct Preference Optimization with an Offset (ODPO) into the 3D generation pipeline to directly optimize models for minimal support material usage. By incorporating support structure simulation into the training process, SEG encourages the generation of geometries that inherently require fewer supports, thus reducing material waste and production time. We demonstrate SEG's effectiveness through extensive experiments on two benchmark datasets, Thingi10k-Val and GPT-3DP-Val, showing that SEG significantly outperforms baseline models such as TRELLIS, DPO, and DRO in terms of support volume reduction and printability. Qualitative results further reveal that SEG maintains high fidelity to input prompts while minimizing the need for support structures. Our findings highlight the potential of SEG to transform 3D printing by directly optimizing models during the generative process, paving the way for more sustainable and efficient digital fabrication practices.

</details>


### [17] [MiMo-Embodied: X-Embodied Foundation Model Technical Report](https://arxiv.org/abs/2511.16518)
*Xiaoshuai Hao,Lei Zhou,Zhijian Huang,Zhiwen Hou,Yingbo Tang,Lingfeng Zhang,Guang Li,Zheng Lu,Shuhuai Ren,Xianhui Meng,Yuchen Zhang,Jing Wu,Jinghui Lu,Chenxu Dang,Jiayi Guan,Jianhua Wu,Zhiyi Hou,Hanbing Li,Shumeng Xia,Mingliang Zhou,Yinan Zheng,Zihao Yue,Shuhao Gu,Hao Tian,Yuannan Shen,Jianwei Cui,Wen Zhang,Shaoqing Xu,Bing Wang,Haiyang Sun,Zeyu Zhu,Yuncheng Jiang,Zibin Guo,Chuhong Gong,Chaofan Zhang,Wenbo Ding,Kun Ma,Guang Chen,Rui Cai,Diyun Xiang,Heng Qu,Fuli Luo,Hangjun Ye,Long Chen*

Main category: cs.RO

TL;DR: 本文介绍了一个名为MiMo-Embodied的跨实体基础模型，该模型在自动驾驶和实体AI两个领域都达到了最先进性能，并且在这两个领域之间展现出显著的正向迁移效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于开发一个能够同时在自动驾驶与实体人工智能两大领域内均表现出色的基础模型，以证明通过精心设计的数据构建、多阶段学习以及CoT/RL微调等方法，可以在不同但相关的任务间实现积极的知识迁移。

Method: 采用了多阶段学习、精选数据集构造以及链式思维/强化学习（CoT/RL）微调技术来训练MiMo-Embodied模型。

Result: MiMo-Embodied模型在17个实体AI基准测试（包括任务规划、可操作性预测和空间理解）及12个自动驾驶基准测试（涵盖环境感知、状态预测和驾驶规划）中创下了新纪录，表现优于现有开源、闭源及专门化基线解决方案。

Conclusion: 研究表明，通过采用特定的学习策略和技术手段，在自动驾驶与实体AI这两个看似不同的领域之间可以观察到强烈的正面迁移效应，从而为未来的研究提供了有价值的见解。

Abstract: We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.

</details>


### [18] [InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy](https://arxiv.org/abs/2511.16651)
*Yang Tian,Yuyin Yang,Yiman Xie,Zetao Cai,Xu Shi,Ning Gao,Hangxu Liu,Xuekun Jiang,Zherui Qiu,Feng Yuan,Yaping Li,Ping Wang,Junhao Cai,Jia Zeng,Hao Dong,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 该论文首次证明了仅使用合成数据（InternData-A1）可以与最强的现实机器人数据集在预训练VLA模型时达到相同的性能，显示出大规模模拟的重要价值。此外，该模型还表现出惊人的零样本模拟到现实迁移能力。


<details>
  <summary>Details</summary>
Motivation: 虽然当前的VLA模型已经展示了大规模真实机器人预训练的强大有效性，但合成数据在此之前尚未在规模上展示出可比的能力。本文旨在探索合成数据是否能够单独匹配最强大的π-数据集在预训练VLA模型中的表现，并探讨其对挑战性任务中零样本sim-to-real转移的影响。

Method: 研究人员创建了一个名为InternData-A1的大规模合成数据集，包含超过630,000个轨迹和7,433小时的数据，涵盖了多种操作场景。该数据集通过一个高度自主、完全解耦且组合式的仿真管道生成，支持长时间技能组合、灵活的任务组装以及异构体现形式。使用与π_0相同架构，研究者完全基于InternData-A1进行了模型预训练。

Result: 实验结果表明，完全基于InternData-A1预训练的模型，在49项模拟任务、5项现实世界任务以及4项长时灵巧任务上与官方π_0的表现相匹配。

Conclusion: 这项工作提供了首个证据，证明仅依靠合成数据即可为VLA模型提供与实际机器人数据相当甚至更好的预训练效果，强调了大规模模拟对于增强模型泛化能力和实现零样本sim-to-real转移的重要性。

Abstract: Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $π$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $π_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $π_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.

</details>


### [19] [Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations](https://arxiv.org/abs/2511.16661)
*Irmak Guzey,Haozhi Qi,Julen Urain,Changhao Wang,Jessica Yin,Krishna Bodduluri,Mike Lambeta,Lerrel Pinto,Akshara Rai,Jitendra Malik,Tingfan Wu,Akash Sharma,Homanga Bharadhwaj*

Main category: cs.RO

TL;DR: The paper introduces AINA, a framework that uses Aria Gen 2 glasses to learn multi-fingered robot policies from human daily task videos, overcoming the embodiment gap and enabling robust policy learning without needing robot-specific data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce the reliance on labor-intensive robot data collection by learning multi-fingered robot manipulation policies directly from human demonstrations in natural environments, thus advancing toward more generalizable robot use in human settings.

Method: The method involves using Aria Gen 2 glasses, which are equipped with a high-resolution RGB camera, accurate 3D head and hand pose estimation, and a wide stereo view for depth estimation. This hardware, combined with the AINA framework, allows for the extraction of relevant contextual and motion cues necessary to learn 3D point-based policies for multi-fingered hands.

Result: The results show that the AINA framework can learn policies that are robust to background changes and can be deployed across nine everyday manipulation tasks without the need for any robot data, including online corrections, reinforcement learning, or simulation.

Conclusion: The conclusion is that with the AINA framework and the use of Aria Gen 2 glasses, the research represents a significant step towards achieving the goal of learning multi-fingered robot policies from human demonstrations, thereby making robot manipulation more adaptable to human environments.

Abstract: Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been bottle-necked by the embodiment gap between humans and robots, as well as by difficulties in extracting relevant contextual and motion cues that enable learning of autonomous policies from in-the-wild human videos. We claim that with simple yet sufficiently powerful hardware for obtaining human data and our proposed framework AINA, we are now one significant step closer to achieving this dream. AINA enables learning multi-fingered policies from data collected by anyone, anywhere, and in any environment using Aria Gen 2 glasses. These glasses are lightweight and portable, feature a high-resolution RGB camera, provide accurate on-board 3D head and hand poses, and offer a wide stereo view that can be leveraged for depth estimation of the scene. This setup enables the learning of 3D point-based policies for multi-fingered hands that are robust to background changes and can be deployed directly without requiring any robot data (including online corrections, reinforcement learning, or simulation). We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks. Robot rollouts are best viewed on our website: https://aina-robot.github.io.

</details>
