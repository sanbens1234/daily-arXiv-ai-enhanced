{"id": "2511.15909", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.15909", "abs": "https://arxiv.org/abs/2511.15909", "authors": ["J. Cristobal", "A. Z. Zain Aldeen", "M. Izadi", "R. Faieghi"], "title": "Gimballed Rotor Mechanism for Omnidirectional Quadrotors", "comment": "6 pages, 7 figures, CASE 2025", "summary": "This paper presents the design of a gimballed rotor mechanism as a modular and efficient solution for constructing omnidirectional quadrotors. Unlike conventional quadrotors, which are underactuated, this class of quadrotors achieves full actuation, enabling independent motion in all six degrees of freedom. While existing omnidirectional quadrotor designs often require significant structural modifications, the proposed gimballed rotor system maintains a lightweight and easy-to-integrate design by incorporating servo motors within the rotor platforms, allowing independent tilting of each rotor without major alterations to the central structure of a quadrotor. To accommodate this unconventional design, we develop a new control allocation scheme in PX4 Autopilot and present successful flight tests, validating the effectiveness of the proposed approach.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4e07\u5411\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5728\u65cb\u7ffc\u5e73\u53f0\u4e0a\u96c6\u6210\u4f3a\u670d\u7535\u673a\u5b9e\u73b0\u6bcf\u4e2a\u65cb\u7ffc\u72ec\u7acb\u503e\u659c\uff0c\u4ece\u800c\u8fbe\u5230\u5168\u9a71\u52a8\uff0c\u4f7f\u98de\u884c\u5668\u80fd\u591f\u72ec\u7acb\u5730\u5728\u516d\u4e2a\u81ea\u7531\u5ea6\u4e0a\u8fd0\u52a8\u3002\u6b64\u5916\uff0c\u8fd8\u4e3a\u8fd9\u79cd\u8bbe\u8ba1\u5f00\u53d1\u4e86\u65b0\u7684\u63a7\u5236\u5206\u914d\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u6210\u529f\u7684\u98de\u884c\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7531\u4e8e\u5176\u7ed3\u6784\u9650\u5236\uff0c\u53ea\u80fd\u5b9e\u73b0\u6b20\u9a71\u52a8\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4eec\u4e0d\u80fd\u72ec\u7acb\u5730\u5728\u6240\u6709\u516d\u4e2a\u81ea\u7531\u5ea6\u4e0a\u79fb\u52a8\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e07\u5411\u8282\u8f6c\u5b50\u673a\u5236\u7684\u8bbe\u8ba1\uff0c\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u6a21\u5757\u5316\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u6784\u5efa\u5168\u65b9\u5411\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u3002", "method": "\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e07\u5411\u8282\u8f6c\u5b50\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u5728\u6bcf\u4e2a\u8f6c\u5b50\u5e73\u53f0\u4e2d\u52a0\u5165\u4f3a\u670d\u7535\u673a\u6765\u5141\u8bb8\u5404\u4e2a\u8f6c\u5b50\u72ec\u7acb\u503e\u659c\uff0c\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf\u5316\u548c\u6613\u4e8e\u96c6\u6210\u5230\u4f20\u7edf\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7ed3\u6784\u4e2d\u7684\u7279\u70b9\u3002\u4e3a\u4e86\u652f\u6301\u8fd9\u4e00\u521b\u65b0\u8bbe\u8ba1\uff0c\u4ed6\u4eec\u8fd8\u5728PX4\u81ea\u52a8\u9a7e\u9a76\u4eea\u8f6f\u4ef6\u4e2d\u5f00\u53d1\u4e86\u4e13\u95e8\u7684\u63a7\u5236\u5206\u914d\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4e07\u5411\u8282\u8f6c\u5b50\u673a\u5236\u4ee5\u53ca\u914d\u5957\u7684\u63a7\u5236\u7b56\u7565\u80fd\u591f\u6709\u6548\u5730\u5de5\u4f5c\uff0c\u4f7f\u5f97\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u80fd\u591f\u5728\u516d\u4e2a\u81ea\u7531\u5ea6\u4e0a\u8fdb\u884c\u72ec\u7acb\u64cd\u63a7\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u91c7\u7528\u4e07\u5411\u8282\u8f6c\u5b50\u673a\u5236\u5e76\u7ed3\u5408\u9002\u5f53\u7684\u63a7\u5236\u7b56\u7565\uff0c\u53ef\u4ee5\u6210\u529f\u5730\u5c06\u5e38\u89c4\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u6539\u9020\u6210\u4e3a\u5168\u65b9\u5411\u3001\u5168\u9a71\u52a8\u7684\u98de\u884c\u5e73\u53f0\uff0c\u4e3a\u672a\u6765\u66f4\u590d\u6742\u591a\u6837\u7684\u98de\u884c\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2511.15914", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.15914", "abs": "https://arxiv.org/abs/2511.15914", "authors": ["Debasmita Ghose", "Oz Gitelson", "Ryan Jin", "Grace Abawe", "Marynel Vazquez", "Brian Scassellati"], "title": "I've Changed My Mind: Robots Adapting to Changing Human Goals during Collaboration", "comment": "Accepted to RA-L", "summary": "For effective human-robot collaboration, a robot must align its actions with human goals, even as they change mid-task. Prior approaches often assume fixed goals, reducing goal prediction to a one-time inference. However, in real-world scenarios, humans frequently shift goals, making it challenging for robots to adapt without explicit communication. We propose a method for detecting goal changes by tracking multiple candidate action sequences and verifying their plausibility against a policy bank. Upon detecting a change, the robot refines its belief in relevant past actions and constructs Receding Horizon Planning (RHP) trees to actively select actions that assist the human while encouraging Differentiating Actions to reveal their updated goal. We evaluate our approach in a collaborative cooking environment with up to 30 unique recipes and compare it to three comparable human goal prediction algorithms. Our method outperforms all baselines, quickly converging to the correct goal after a switch, reducing task completion time, and improving collaboration efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u4eba\u7c7b\u76ee\u6807\u53d8\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddf\u8e2a\u591a\u4e2a\u5019\u9009\u52a8\u4f5c\u5e8f\u5217\u5e76\u9a8c\u8bc1\u5176\u5408\u7406\u6027\u6765\u5b9e\u73b0\u3002\u8be5\u65b9\u6cd5\u5728\u534f\u4f5c\u70f9\u996a\u73af\u5883\u4e2d\u5f97\u5230\u4e86\u8bc4\u4f30\uff0c\u5e76\u663e\u793a\u51fa\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u5728\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u534f\u4f5c\u6548\u7387\u4e0a\u90fd\u6709\u6240\u63d0\u9ad8\u3002", "motivation": "\u4e3a\u4e86\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\uff0c\u673a\u5668\u4eba\u9700\u8981\u4e0e\u4eba\u7c7b\u7684\u76ee\u6807\u4fdd\u6301\u4e00\u81f4\uff0c\u5373\u4f7f\u8fd9\u4e9b\u76ee\u6807\u5728\u4efb\u52a1\u4e2d\u9014\u53d1\u751f\u53d8\u5316\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u76ee\u6807\u662f\u56fa\u5b9a\u7684\uff0c\u5c06\u76ee\u6807\u9884\u6d4b\u7b80\u5316\u4e3a\u4e00\u6b21\u6027\u63a8\u7406\u3002\u7136\u800c\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u4eba\u4eec\u7ecf\u5e38\u6539\u53d8\u76ee\u6807\uff0c\u8fd9\u5bf9\u673a\u5668\u4eba\u6765\u8bf4\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u6ca1\u6709\u660e\u786e\u7684\u6c9f\u901a\u5c31\u5f88\u96be\u9002\u5e94\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ffd\u8e2a\u591a\u6761\u5019\u9009\u52a8\u4f5c\u5e8f\u5217\u5e76\u5bf9\u7167\u7b56\u7565\u5e93\u9a8c\u8bc1\u5b83\u4eec\u7684\u53ef\u4fe1\u5ea6\u6765\u68c0\u6d4b\u76ee\u6807\u53d8\u66f4\u3002\u4e00\u65e6\u68c0\u6d4b\u5230\u53d8\u66f4\uff0c\u673a\u5668\u4eba\u4f1a\u66f4\u65b0\u5bf9\u8fc7\u53bb\u76f8\u5173\u884c\u52a8\u7684\u4fe1\u4efb\uff0c\u5e76\u6784\u5efa\u56de\u6eaf\u89c4\u5212\uff08RHP\uff09\u6811\u4ee5\u4e3b\u52a8\u9009\u62e9\u80fd\u591f\u534f\u52a9\u4eba\u7c7b\u540c\u65f6\u4fc3\u8fdb\u533a\u5206\u6027\u884c\u52a8\u7684\u52a8\u4f5c\uff0c\u4ece\u800c\u63ed\u793a\u66f4\u65b0\u540e\u7684\u76ee\u6807\u3002", "result": "\u6b64\u65b9\u6cd5\u5728\u4e00\u4e2a\u5305\u542b\u591a\u8fbe30\u79cd\u72ec\u7279\u98df\u8c31\u7684\u5408\u4f5c\u70f9\u996a\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e14\u4e0e\u4e09\u79cd\u53ef\u6bd4\u8f83\u7684\u4eba\u7c7b\u76ee\u6807\u9884\u6d4b\u7b97\u6cd5\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u57fa\u7ebf\u4e0a\u90fd\u8868\u73b0\u5f97\u66f4\u597d\uff0c\u80fd\u591f\u5728\u5207\u6362\u540e\u8fc5\u901f\u6536\u655b\u81f3\u6b63\u786e\u7684\u76ee\u6807\uff0c\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u5e76\u63d0\u9ad8\u4e86\u5408\u4f5c\u6548\u7387\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u76d1\u6d4b\u548c\u9002\u5e94\u4eba\u7c7b\u4f19\u4f34\u4e0d\u65ad\u53d8\u5316\u7684\u76ee\u6807\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\u548c\u6548\u679c\u3002\u63d0\u51fa\u7684\u57fa\u4e8e\u591a\u52a8\u4f5c\u5e8f\u5217\u8ddf\u8e2a\u53ca\u7b56\u7565\u5e93\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u80fd\u591f\u5feb\u901f\u8bc6\u522b\u51fa\u76ee\u6807\u7684\u53d8\u5316\uff0c\u8fd8\u80fd\u6709\u6548\u5730\u8c03\u6574\u81ea\u8eab\u884c\u4e3a\u4ee5\u66f4\u597d\u5730\u652f\u6301\u4eba\u7c7b\u4f19\u4f34\u3002"}}
{"id": "2511.15956", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.15956", "abs": "https://arxiv.org/abs/2511.15956", "authors": ["Aliyah Smith", "Monroe Kennedy"], "title": "The Role of Consequential and Functional Sound in Human-Robot Interaction: Toward Audio Augmented Reality Interfaces", "comment": "9 pages, 6 figures", "summary": "As robots become increasingly integrated into everyday environments, understanding how they communicate with humans is critical. Sound offers a powerful channel for interaction, encompassing both operational noises and intentionally designed auditory cues. In this study, we examined the effects of consequential and functional sounds on human perception and behavior, including a novel exploration of spatial sound through localization and handover tasks. Results show that consequential sounds of the Kinova Gen3 manipulator did not negatively affect perceptions, spatial localization is highly accurate for lateral cues but declines for frontal cues, and spatial sounds can simultaneously convey task-relevant information while promoting warmth and reducing discomfort. These findings highlight the potential of functional and transformative auditory design to enhance human-robot collaboration and inform future sound-based interaction strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u901a\u8fc7\u58f0\u97f3\u4e0e\u4eba\u7c7b\u4e92\u52a8\u7684\u6548\u679c\uff0c\u53d1\u73b0\u64cd\u4f5c\u58f0\u97f3\u5bf9\u4eba\u7684\u611f\u77e5\u6ca1\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u4fa7\u5411\u58f0\u6e90\u5b9a\u4f4d\u51c6\u786e\u800c\u524d\u65b9\u58f0\u6e90\u5b9a\u4f4d\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u7a7a\u95f4\u58f0\u97f3\u80fd\u591f\u6709\u6548\u4f20\u8fbe\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u5e76\u63d0\u5347\u4eb2\u5207\u611f\u3001\u51cf\u5c11\u4e0d\u9002\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u8d8a\u6765\u8d8a\u591a\u5730\u878d\u5165\u65e5\u5e38\u751f\u6d3b\u73af\u5883\uff0c\u7406\u89e3\u5b83\u4eec\u5982\u4f55\u4e0e\u4eba\u7c7b\u6c9f\u901a\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u58f0\u97f3\u4f5c\u4e3a\u5f3a\u5927\u7684\u4ea4\u4e92\u6e20\u9053\uff0c\u4e0d\u4ec5\u5305\u62ec\u64cd\u4f5c\u566a\u97f3\uff0c\u8fd8\u6db5\u76d6\u4e86\u4e13\u95e8\u8bbe\u8ba1\u7684\u58f0\u97f3\u4fe1\u53f7\u3002", "method": "\u672c\u7814\u7a76\u68c0\u67e5\u4e86\u7ed3\u679c\u6027\u58f0\u97f3\u548c\u529f\u80fd\u6027\u58f0\u97f3\u5bf9\u4eba\u7c7b\u611f\u77e5\u548c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u9996\u6b21\u63a2\u7d22\u4e86\u901a\u8fc7\u5b9a\u4f4d\u548c\u4ea4\u63a5\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u58f0\u97f3\u6548\u679c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cKinova Gen3\u64cd\u7eb5\u5668\u7684\u7ed3\u679c\u6027\u58f0\u97f3\u5e76\u672a\u5bf9\u611f\u77e5\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff1b\u5bf9\u4e8e\u4fa7\u5411\u63d0\u793a\u97f3\u7684\u7a7a\u95f4\u5b9a\u4f4d\u975e\u5e38\u51c6\u786e\uff0c\u4f46\u5bf9\u4e8e\u524d\u65b9\u63d0\u793a\u97f3\u7684\u51c6\u786e\u6027\u6709\u6240\u4e0b\u964d\uff1b\u7a7a\u95f4\u58f0\u97f3\u53ef\u4ee5\u5728\u540c\u65f6\u4f20\u8fbe\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u7684\u540c\u65f6\u4fc3\u8fdb\u6e29\u6696\u611f\u5e76\u51cf\u5c11\u4e0d\u9002\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u529f\u80fd\u6027\u548c\u53d8\u9769\u6027\u542c\u89c9\u8bbe\u8ba1\u5728\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u57fa\u4e8e\u58f0\u97f3\u7684\u4ea4\u4e92\u7b56\u7565\u63d0\u4f9b\u4e86\u4fe1\u606f\u3002"}}
{"id": "2511.15995", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.15995", "abs": "https://arxiv.org/abs/2511.15995", "authors": ["Zili Tang", "Ying Zhang", "Meng Guo"], "title": "PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization", "comment": "20 pages, 24 figures. Accepted to IEEE Transactions on Robotics (T-RO), 2025", "summary": "Many robots are not equipped with a manipulator and many objects are not suitable for prehensile manipulation (such as large boxes and cylinders). In these cases, pushing is a simple yet effective non-prehensile skill for robots to interact with and further change the environment. Existing work often assumes a set of predefined pushing modes and fixed-shape objects. This work tackles the general problem of controlling a robotic fleet to push collaboratively numerous arbitrary objects to respective destinations, within complex environments of cluttered and movable obstacles. It incorporates several characteristic challenges for multi-robot systems such as online task coordination under large uncertainties of cost and duration, and for contact-rich tasks such as hybrid switching among different contact modes, and under-actuation due to constrained contact forces. The proposed method is based on combinatorial hybrid optimization over dynamic task assignments and hybrid execution via sequences of pushing modes and associated forces. It consists of three main components: (I) the decomposition, ordering and rolling assignment of pushing subtasks to robot subgroups; (II) the keyframe guided hybrid search to optimize the sequence of parameterized pushing modes for each subtask; (III) the hybrid control to execute these modes and transit among them. Last but not least, a diffusion-based accelerator is adopted to predict the keyframes and pushing modes that should be prioritized during hybrid search; and further improve planning efficiency. The framework is complete under mild assumptions. Its efficiency and effectiveness under different numbers of robots and general-shaped objects are validated extensively in simulations and hardware experiments, as well as generalizations to heterogeneous robots, planar assembly and 6D pushing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u5408\u6df7\u5408\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a7\u5236\u673a\u5668\u4eba\u56e2\u961f\u534f\u4f5c\u63a8\u52a8\u5404\u79cd\u5f62\u72b6\u7684\u7269\u4f53\u5230\u8fbe\u6307\u5b9a\u4f4d\u7f6e\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a\u63a8\u5b50\u4efb\u52a1\u7684\u5206\u89e3\u3001\u6392\u5e8f\u548c\u6eda\u52a8\u5206\u914d\uff1b\u5173\u952e\u5e27\u5f15\u5bfc\u7684\u6df7\u5408\u641c\u7d22\u4ee5\u4f18\u5316\u6bcf\u4e2a\u5b50\u4efb\u52a1\u7684\u53c2\u6570\u5316\u63a8\u52a8\u6a21\u5f0f\u5e8f\u5217\uff1b\u4ee5\u53ca\u6267\u884c\u8fd9\u4e9b\u6a21\u5f0f\u5e76\u8fdb\u884c\u5207\u6362\u7684\u6df7\u5408\u63a7\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u52a0\u901f\u5668\u6765\u9884\u6d4b\u5e94\u4f18\u5148\u8003\u8651\u7684\u5173\u952e\u5e27\u548c\u63a8\u52a8\u6a21\u5f0f\uff0c\u4ece\u800c\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u8bb8\u591a\u673a\u5668\u4eba\u6ca1\u6709\u914d\u5907\u64cd\u4f5c\u81c2\uff0c\u800c\u4e14\u5f88\u591a\u7269\u4f53\u4e0d\u9002\u5408\u6293\u53d6\u64cd\u4f5c\uff08\u5982\u5927\u578b\u7bb1\u5b50\u548c\u5706\u67f1\u4f53\uff09\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u63a8\u52a8\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u975e\u6293\u53d6\u6280\u80fd\uff0c\u53ef\u4ee5\u8ba9\u673a\u5668\u4eba\u4e0e\u73af\u5883\u4e92\u52a8\u5e76\u5bf9\u73af\u5883\u505a\u51fa\u6539\u53d8\u3002\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5047\u5b9a\u4e00\u7ec4\u9884\u5b9a\u4e49\u7684\u63a8\u52a8\u6a21\u5f0f\u548c\u56fa\u5b9a\u5f62\u72b6\u7684\u7269\u4f53\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u63a7\u5236\u673a\u5668\u4eba\u56e2\u961f\u534f\u540c\u63a8\u52a8\u4efb\u610f\u5f62\u72b6\u7684\u5927\u91cf\u7269\u4f53\u5230\u5404\u81ea\u76ee\u7684\u5730\u7684\u4e00\u822c\u95ee\u9898\uff0c\u5728\u5145\u6ee1\u969c\u788d\u7269\u4e14\u53ef\u79fb\u52a8\u7684\u590d\u6742\u73af\u5883\u4e2d\u5b8c\u6210\u6b64\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u57fa\u4e8e\u52a8\u6001\u4efb\u52a1\u5206\u914d\u548c\u901a\u8fc7\u4e00\u7cfb\u5217\u63a8\u52a8\u6a21\u5f0f\u53ca\u5173\u8054\u529b\u5b9e\u73b0\u7684\u6df7\u5408\u6267\u884c\u7684\u7ec4\u5408\u6df7\u5408\u4f18\u5316\u3002\u5b83\u7531\u4e09\u4e2a\u4e3b\u8981\u7ec4\u6210\u90e8\u5206\u6784\u6210\uff1a(I) \u5c06\u63a8\u52a8\u5b50\u4efb\u52a1\u5206\u89e3\u3001\u6392\u5e8f\uff0c\u5e76\u6eda\u52a8\u5206\u914d\u7ed9\u673a\u5668\u4eba\u5c0f\u7ec4\uff1b(II) \u901a\u8fc7\u5173\u952e\u5e27\u5f15\u5bfc\u7684\u6df7\u5408\u641c\u7d22\u4f18\u5316\u6bcf\u9879\u5b50\u4efb\u52a1\u4e2d\u53c2\u6570\u5316\u63a8\u52a8\u6a21\u5f0f\u7684\u987a\u5e8f\uff1b(III) \u5229\u7528\u6df7\u5408\u63a7\u5236\u6267\u884c\u8fd9\u4e9b\u6a21\u5f0f\u53ca\u5176\u4e4b\u95f4\u7684\u8f6c\u6362\u3002\u53e6\u5916\uff0c\u91c7\u7528\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u52a0\u901f\u5668\u6765\u9884\u6d4b\u5728\u6df7\u5408\u641c\u7d22\u671f\u95f4\u5e94\u8be5\u4f18\u5148\u5904\u7406\u7684\u5173\u952e\u5e27\u548c\u63a8\u52a8\u6a21\u5f0f\uff0c\u4ee5\u6b64\u8fdb\u4e00\u6b65\u63d0\u9ad8\u8ba1\u5212\u6548\u7387\u3002", "result": "\u8be5\u6846\u67b6\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u662f\u5b8c\u6574\u7684\u3002\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u6570\u91cf\u7684\u673a\u5668\u4eba\u548c\u4e00\u822c\u5f62\u72b6\u7269\u4f53\u4e0b\u7684\u6548\u7387\u4e0e\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5411\u5f02\u6784\u673a\u5668\u4eba\u3001\u5e73\u9762\u88c5\u914d\u548c6D\u63a8\u52a8\u4efb\u52a1\u6269\u5c55\u7684\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u63a7\u5236\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u534f\u540c\u63a8\u52a8\u4efb\u610f\u5f62\u72b6\u7269\u4f53\u81f3\u76ee\u6807\u5730\u70b9\u7684\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u6709\u6548\u89e3\u51b3\u4e86\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u53d7\u9650\u63a5\u89e6\u529b\u6761\u4ef6\u4e0b\u5728\u7ebf\u4efb\u52a1\u534f\u8c03\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4e5f\u63d0\u9ad8\u4e86\u89c4\u5212\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\u3002"}}
{"id": "2511.16048", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.16048", "abs": "https://arxiv.org/abs/2511.16048", "authors": ["Qing Zhang", "Jing Huang", "Mingyang Xu", "Jun Rekimoto"], "title": "Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud", "comment": "NeurIPS 2025 Creative AI Track, The Thirty-Ninth Annual Conference on Neural Information Processing Systems", "summary": "While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately \"lo-fi\" approach. We present the \"Semantic Glitch,\" a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a \"physical glitch\" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a \"narrative mind\" that complements the \"weak,\" historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling \"plan to execution\" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u6545\u610f\u91c7\u7528\u201c\u4f4e\u7cbe\u5ea6\u201d\u65b9\u6cd5\u7684\u521b\u610f\u6f5c\u529b\uff0c\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bed\u4e49\u6545\u969c\u201d\u7684\u8f6f\u98de\u884c\u673a\u5668\u4eba\u827a\u672f\u88c5\u7f6e\u3002\u8be5\u88c5\u7f6e\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9a\u6027\u3001\u8bed\u4e49\u7406\u89e3\u6765\u5bfc\u822a\uff0c\u5e76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4e3a\u673a\u5668\u4eba\u521b\u4f5c\u4e86\u751f\u7269\u542f\u53d1\u7684\u4eba\u683c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u6846\u67b6\u80fd\u591f\u6210\u529f\u5730\u521b\u5efa\u5177\u6709\u72ec\u7279\u4eba\u683c\u4f46\u4e0d\u5b8c\u7f8e\u7684\u4f34\u4fa3\uff0c\u5176\u6210\u529f\u6807\u51c6\u5728\u4e8e\u89d2\u8272\u9b45\u529b\u800c\u975e\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u6545\u610f\u91c7\u7528\u2018\u4f4e\u7cbe\u5ea6\u2019\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u9886\u57df\u4e2d\u7684\u521b\u610f\u6f5c\u529b\uff0c\u65e8\u5728\u521b\u9020\u4e00\u4e2a\u57fa\u4e8e\u8bed\u4e49\u7406\u89e3\u548c\u751f\u7269\u542f\u53d1\u4eba\u683c\u7684\u8f6f\u98de\u884c\u673a\u5668\u4eba\u827a\u672f\u88c5\u7f6e\uff0c\u4ee5\u5c55\u793a\u8fd9\u79cd\u975e\u4f20\u7edf\u65b9\u6cd5\u5728\u4ea7\u751f\u72ec\u7279\u4e14\u5438\u5f15\u4eba\u7684\u673a\u5668\u4eba\u7269\u683c\u65b9\u9762\u7684\u53ef\u80fd\u6027\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u540d\u4e3a\u2018\u8bed\u4e49\u6545\u969c\u2019\u7684\u827a\u672f\u88c5\u7f6e\uff0c\u5b83\u4f9d\u8d56\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u73af\u5883\u7684\u7406\u89e3\u8fdb\u884c\u5bfc\u822a\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u4f20\u7edf\u7684\u4f20\u611f\u5668\u5982LiDAR\u548cSLAM\u7cfb\u7edf\u3002\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u7ed9\u673a\u5668\u4eba\u8d4b\u4e88\u7279\u5b9a\u6027\u683c\u7279\u5f81\uff0c\u4ece\u800c\u5f62\u6210\u4e00\u79cd\u2018\u53d9\u4e8b\u601d\u7ef4\u2019\u4e0e\u5176\u5b9e\u9a8c\u6027\u8d28\u7684\u8eab\u4f53\u76f8\u5339\u914d\u3002", "result": "\u901a\u8fc713\u5206\u949f\u81ea\u4e3b\u98de\u884c\u65e5\u5fd7\u53ca\u540e\u7eed\u7814\u7a76\u7edf\u8ba1\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u7528\u4e8e\u521b\u4f5c\u51fa\u53ef\u91cf\u5316\u533a\u5206\u7684\u4e0d\u540c\u4eba\u683c\u3002\u5206\u6790\u63ed\u793a\u4e86\u4ece\u57fa\u4e8e\u5730\u6807\u5bfc\u822a\u5230\u4ee4\u4eba\u4fe1\u670d\u7684\u2018\u8ba1\u5212\u5230\u6267\u884c\u2019\u5dee\u8ddd\u7b49\u65b0\u5174\u884c\u4e3a\uff0c\u4ee5\u53ca\u7531\u4e8e\u7f3a\u4e4f\u7cbe\u786e\u672c\u4f53\u611f\u89c9\u800c\u4ea7\u751f\u7684\u4e0d\u53ef\u9884\u6d4b\u4f46\u5408\u7406\u7684\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5728\u57fa\u4e8e\u4f4e\u7cbe\u5ea6\u6280\u672f\u548c\u6709\u9650\u611f\u77e5\u80fd\u529b\u7684\u6846\u67b6\u4e5f\u80fd\u591f\u521b\u9020\u51fa\u5bcc\u6709\u9b45\u529b\u7684\u89d2\u8272\u578b\u4f34\u4fa3\u673a\u5668\u4eba\uff0c\u5f3a\u8c03\u4e86\u5728\u8bc4\u4ef7\u6b64\u7c7b\u4f5c\u54c1\u65f6\u5e94\u66f4\u52a0\u5173\u6ce8\u5176\u89d2\u8272\u5438\u5f15\u529b\u800c\u975e\u64cd\u4f5c\u6548\u7387\u3002"}}
{"id": "2511.16050", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16050", "abs": "https://arxiv.org/abs/2511.16050", "authors": ["Takeru Tsunoori", "Masato Kobayashi", "Yuki Uranishi"], "title": "Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers", "comment": null, "summary": "Underwater robotic manipulation is fundamentally challenged by extreme lighting variations, color distortion, and reduced visibility. We introduce Bi-AQUA, the first underwater bilateral control-based imitation learning framework that integrates lighting-aware visual processing for underwater robot arms. Bi-AQUA employs a hierarchical three-level lighting adaptation mechanism: a Lighting Encoder that extracts lighting representations from RGB images without manual annotation and is implicitly supervised by the imitation objective, FiLM modulation of visual backbone features for adaptive, lighting-aware feature extraction, and an explicit lighting token added to the transformer encoder input for task-aware conditioning. Experiments on a real-world underwater pick-and-place task under diverse static and dynamic lighting conditions show that Bi-AQUA achieves robust performance and substantially outperforms a bilateral baseline without lighting modeling. Ablation studies further confirm that all three lighting-aware components are critical. This work bridges terrestrial bilateral control-based imitation learning and underwater manipulation, enabling force-sensitive autonomous operation in challenging marine environments. For additional material, please check: https://mertcookimg.github.io/bi-aqua", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Bi-AQUA\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u53cc\u8fb9\u63a7\u5236\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u7279\u522b\u9488\u5bf9\u6c34\u4e0b\u673a\u5668\u4eba\u64cd\u4f5c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6574\u5408\u5149\u7167\u611f\u77e5\u89c6\u89c9\u5904\u7406\u6765\u5e94\u5bf9\u6781\u7aef\u5149\u7167\u53d8\u5316\u3001\u8272\u5f69\u5931\u771f\u548c\u53ef\u89c1\u5ea6\u964d\u4f4e\u7b49\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u9759\u6001\u548c\u52a8\u6001\u5149\u7167\u6761\u4ef6\u4e0b\u6267\u884c\u6c34\u4e0b\u53d6\u653e\u4efb\u52a1\u65f6\u8868\u73b0\u7a33\u5065\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u65e0\u5149\u7167\u5efa\u6a21\u7684\u53cc\u8fb9\u63a7\u5236\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u6c34\u4e0b\u673a\u5668\u4eba\u64cd\u4f5c\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u5305\u62ec\u6781\u7aef\u5149\u7167\u53d8\u5316\u3001\u8272\u5f69\u5931\u771f\u53ca\u80fd\u89c1\u5ea6\u964d\u4f4e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u53d1\u4e86Bi-AQUA\uff0c\u8fd9\u662f\u9996\u4e2a\u7ed3\u5408\u5149\u7167\u610f\u8bc6\u89c6\u89c9\u5904\u7406\u6280\u672f\u7684\u6c34\u4e0b\u53cc\u8fb9\u63a7\u5236\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u6539\u5584\u6c34\u4e0b\u673a\u68b0\u81c2\u7684\u64cd\u4f5c\u6027\u80fd\u3002", "method": "Bi-AQUA\u91c7\u7528\u4e86\u4e00\u79cd\u4e09\u5c42\u7ea7\u5149\u7167\u9002\u5e94\u673a\u5236\uff1a\u4e00\u662f\u4eceRGB\u56fe\u50cf\u4e2d\u63d0\u53d6\u5149\u7167\u8868\u793a\u800c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u7684\u5149\u7167\u7f16\u7801\u5668\uff08Lighting Encoder\uff09\uff0c\u5176\u8bad\u7ec3\u53d7\u5230\u6a21\u4eff\u76ee\u6807\u7684\u9690\u5f0f\u76d1\u7763\uff1b\u4e8c\u662f\u5229\u7528FiLM\u8c03\u5236\u89c6\u89c9\u9aa8\u5e72\u7279\u5f81\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u5149\u7167\u654f\u611f\u7684\u7279\u5f81\u63d0\u53d6\uff1b\u4e09\u662f\u5411Transformer\u7f16\u7801\u5668\u8f93\u5165\u6dfb\u52a0\u660e\u786e\u7684\u5149\u7167\u6807\u8bb0\u4ee5\u8fdb\u884c\u4efb\u52a1\u76f8\u5173\u7684\u6761\u4ef6\u8bbe\u7f6e\u3002", "result": "\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6c34\u4e0b\u53d6\u653e\u4efb\u52a1\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u591a\u79cd\u9759\u6001\u4e0e\u52a8\u6001\u5149\u7167\u6761\u4ef6\u4e0b\uff0cBi-AQUA\u80fd\u591f\u4fdd\u6301\u7a33\u5b9a\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u76f8\u6bd4\u6ca1\u6709\u8003\u8651\u5149\u7167\u56e0\u7d20\u7684\u4f20\u7edf\u53cc\u8fb9\u63a7\u5236\u65b9\u6cd5\u6709\u663e\u8457\u4f18\u52bf\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u6240\u6709\u4e09\u4e2a\u5149\u7167\u654f\u611f\u7ec4\u4ef6\u5bf9\u4e8e\u6574\u4f53\u6027\u80fd\u90fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c06\u9646\u5730\u4e0a\u7684\u57fa\u4e8e\u53cc\u8fb9\u63a7\u5236\u7684\u6a21\u4eff\u5b66\u4e60\u4e0e\u6c34\u4e0b\u64cd\u63a7\u76f8\u7ed3\u5408\uff0c\u4f7f\u5f97\u80fd\u591f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6d77\u6d0b\u73af\u5883\u4e2d\u5b9e\u73b0\u5bf9\u529b\u654f\u611f\u7684\u81ea\u4e3b\u64cd\u4f5c\u3002"}}
{"id": "2511.16158", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16158", "abs": "https://arxiv.org/abs/2511.16158", "authors": ["Lara Bergmann", "Cedric Grothues", "Klaus Neumann"], "title": "MagBotSim: Physics-Based Simulation and Reinforcement Learning Environments for Magnetic Robotics", "comment": null, "summary": "Magnetic levitation is about to revolutionize in-machine material flow in industrial automation. Such systems are flexibly configurable and can include a large number of independently actuated shuttles (movers) that dynamically rebalance production capacity. Beyond their capabilities for dynamic transportation, these systems possess the inherent yet unexploited potential to perform manipulation. By merging the fields of transportation and manipulation into a coordinated swarm of magnetic robots (MagBots), we enable manufacturing systems to achieve significantly higher efficiency, adaptability, and compactness. To support the development of intelligent algorithms for magnetic levitation systems, we introduce MagBotSim (Magnetic Robotics Simulation): a physics-based simulation for magnetic levitation systems. By framing magnetic levitation systems as robot swarms and providing a dedicated simulation, this work lays the foundation for next generation manufacturing systems powered by Magnetic Robotics. MagBotSim's documentation, videos, experiments, and code are available at: https://ubi-coro.github.io/MagBotSim/", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u78c1\u60ac\u6d6e\u6280\u672f\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff08MagBots\uff09\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u8fd0\u8f93\u548c\u64cd\u4f5c\u529f\u80fd\u6765\u63d0\u9ad8\u5236\u9020\u7cfb\u7edf\u7684\u6548\u7387\u3001\u9002\u5e94\u6027\u548c\u7d27\u51d1\u6027\uff0c\u5e76\u4e3a\u6b64\u5f00\u53d1\u4e86\u4eff\u771f\u5de5\u5177MagBotSim\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5de5\u4e1a\u81ea\u52a8\u5316\u4e2d\u6750\u6599\u6d41\u52a8\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u6587\u7ae0\u63d0\u51fa\u5c06\u78c1\u60ac\u6d6e\u7cfb\u7edf\u7684\u8fd0\u8f93\u4e0e\u64cd\u4f5c\u529f\u80fd\u7ed3\u5408\u8d77\u6765\uff0c\u5f62\u6210\u4e00\u4e2a\u534f\u8c03\u7684\u78c1\u6027\u673a\u5668\u4eba\u96c6\u7fa4(MagBots)\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u751f\u4ea7\u6548\u7387\u3001\u9002\u5e94\u6027\u548c\u7a7a\u95f4\u5229\u7528\u7387\u3002", "method": "\u4f5c\u8005\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aMagBotSim\u7684\u7269\u7406\u57fa\u7840\u6a21\u62df\u5668\uff0c\u4e13\u95e8\u7528\u4e8e\u78c1\u60ac\u6d6e\u7cfb\u7edf\u7684\u7814\u7a76\u3002\u8be5\u6a21\u62df\u5668\u652f\u6301\u667a\u80fd\u7b97\u6cd5\u7684\u53d1\u5c55\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u7531\u78c1\u6027\u673a\u5668\u4eba\u9a71\u52a8\u7684\u5236\u9020\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "result": "\u901a\u8fc7\u5c06\u78c1\u60ac\u6d6e\u7cfb\u7edf\u89c6\u4e3a\u673a\u5668\u4eba\u96c6\u7fa4\u5e76\u63d0\u4f9b\u4e13\u7528\u4eff\u771f\u5de5\u5177\uff0c\u7814\u7a76\u4e3a\u672a\u6765\u5236\u9020\u4e1a\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u516c\u5f00\u4e86MagBotSim\u7684\u76f8\u5173\u6587\u6863\u3001\u89c6\u9891\u3001\u5b9e\u9a8c\u53ca\u4ee3\u7801\u7b49\u8d44\u6e90\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165MagBots\u6982\u5ff5\u53ca\u914d\u5957\u7684MagBotSim\u4eff\u771f\u8f6f\u4ef6\uff0c\u4e3a\u5f00\u53d1\u66f4\u52a0\u9ad8\u6548\u7075\u6d3b\u7684\u65b0\u4e00\u4ee3\u5236\u9020\u7cfb\u7edf\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2511.16223", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16223", "abs": "https://arxiv.org/abs/2511.16223", "authors": ["Vincenzo Pomponi", "Paolo Franceschi", "Stefano Baraldo", "Loris Roveda", "Oliver Avram", "Luca Maria Gambardella", "Anna Valente"], "title": "DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks", "comment": null, "summary": "Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning.", "AI": {"tldr": "DynaMimicGen (D-MG) is a scalable dataset generation framework that allows for the creation of robust manipulation policies with minimal human supervision, supporting dynamic task environments. It segments human demonstrations into sub-tasks and uses Dynamic Movement Primitives to adapt these behaviors to new and changing scenarios. This method enhances the ability of robots to perform complex tasks, such as cube stacking or placing mugs in drawers, even when faced with unpredictable changes in the environment.", "motivation": "The motivation behind this work is to overcome the limitations associated with learning robust manipulation policies, which usually demand large, diverse datasets. Collecting such data is often time-consuming and impractical, especially in dynamic settings. The aim is to develop a solution that can effectively train policies from very limited human input while also being adaptable to dynamic environments.", "method": "The introduced method, DynaMimicGen (D-MG), operates by first segmenting a small number of human demonstrations into meaningful sub-tasks. It then utilizes Dynamic Movement Primitives (DMPs) to generalize and adapt these demonstrated behaviors to various and dynamically changing situations. D-MG generates smooth, realistic, and consistent Cartesian trajectories that can adjust in real-time to changes in object positions, robot states, or scene layout during task execution.", "result": "Robot agents trained using imitation learning on data generated by D-MG were able to demonstrate strong performance across a range of benchmarks, including long-horizon and contact-rich tasks. These tasks involved activities like stacking cubes and placing mugs inside drawers, even under conditions where the environment was subject to unexpected changes.", "conclusion": "D-MG presents a significant advancement in the field of autonomous robot learning by offering a more efficient and scalable alternative to traditional manual data collection methods. It successfully reduces the dependency on extensive human demonstrations and supports generalization in dynamic environments, thereby facilitating the development of versatile and adaptive robotic systems."}}
{"id": "2511.16233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16233", "abs": "https://arxiv.org/abs/2511.16233", "authors": ["Kewei Chen", "Yayu Long", "Shuai Li", "Mingsheng Shang"], "title": "FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models", "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u751f\u6210\u5f0f\u6570\u636e\u84b8\u998f\u6846\u67b6FT-NCFM\uff0c\u7528\u4e8e\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5bf9\u5927\u91cf\u5197\u4f59\u4e14\u4ef7\u503c\u4e0d\u5747\u7684\u6570\u636e\u96c6\u8fc7\u5ea6\u4f9d\u8d56\u7684\u95ee\u9898\u3002\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u5f52\u56e0\u548c\u7a0b\u5e8f\u5316\u5bf9\u6bd4\u9a8c\u8bc1\u7684\u81ea\u5305\u542b\u4e8b\u5b9e\u8ffd\u8e2a\u5f15\u64ce\u8bc4\u4f30\u6837\u672c\u5185\u5728\u4ef7\u503c\uff0c\u5e76\u6307\u5bfc\u5bf9\u6297\u6027NCFM\u8fc7\u7a0b\u5408\u6210\u4e0e\u6a21\u578b\u65e0\u5173\u3001\u4fe1\u606f\u5bc6\u96c6\u4e14\u53ef\u91cd\u7528\u7684\u6570\u636e\u8d44\u4ea7\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u591a\u4e2a\u4e3b\u6d41VLA\u57fa\u51c6\u4e0a\u4f7f\u7528\u4ec55%\u7ecf\u84b8\u998f\u7684\u6838\u5fc3\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u8fbe\u5230\u4e8685-90%\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8d85\u8fc780%\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u4e8e\u5176\u5bf9\u5927\u89c4\u6a21\u4f46\u5197\u4f59\u4e14\u4ef7\u503c\u5206\u5e03\u4e0d\u5747\u7684\u6570\u636e\u96c6\u7684\u9ad8\u5ea6\u4f9d\u8d56\uff0c\u8fd9\u963b\u788d\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u73b0\u6709\u7684\u4ee5\u6a21\u578b\u4e3a\u4e2d\u5fc3\u7684\u4f18\u5316\u8def\u5f84\uff0c\u5982\u6a21\u578b\u538b\u7f29\uff08\u901a\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff09\u6216\u7b56\u7565\u63d0\u70bc\uff08\u4ea7\u7269\u7279\u5b9a\u4e8e\u6a21\u578b\u4e14\u7f3a\u4e4f\u901a\u7528\u6027\uff09\uff0c\u672a\u80fd\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u8fd9\u4e00\u6570\u636e\u5c42\u9762\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86FT-NCFM\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u4e00\u79cd\u81ea\u6211\u5305\u542b\u7684\u4e8b\u5b9e\u8ffd\u8e2a(FT)\u5f15\u64ce\u6765\u7ed3\u5408\u56e0\u679c\u5f52\u5c5e\u5206\u6790\u548c\u7a0b\u5e8f\u5316\u7684\u5bf9\u6bd4\u9a8c\u8bc1\u65b9\u6cd5\u6765\u8bc4\u4ef7\u6837\u672c\u7684\u4ef7\u503c\u3002\u57fa\u4e8e\u8fd9\u4e9b\u8bc4\u4ef7\uff0c\u5229\u7528\u5bf9\u6297\u6027\u7684NCFM\u8fc7\u7a0b\u6765\u5408\u6210\u4e00\u4e2a\u4e0e\u6a21\u578b\u65e0\u5173\u3001\u4fe1\u606f\u5bc6\u96c6\u5e76\u4e14\u53ef\u4ee5\u91cd\u590d\u4f7f\u7528\u7684\u6570\u636e\u8d44\u4ea7\u3002", "result": "\u5728\u51e0\u4e2a\u4e3b\u6d41VLA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4f7f\u7528\u6211\u4eec\u7cbe\u7b80\u540e\u6838\u5fc3\u6570\u636e\u96c6\u76845%\uff0c\u8bad\u7ec3\u51fa\u7684\u6a21\u578b\u5c31\u80fd\u8fbe\u523085-90%\u7684\u6210\u529f\u7387\uff0c\u76f8\u8f83\u4e8e\u4f7f\u7528\u5b8c\u6574\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u60c5\u51b5\uff1b\u540c\u65f6\uff0c\u8fd9\u79cd\u65b9\u6cd5\u8fd8\u80fd\u591f\u5c06\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u8d85\u8fc780%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u667a\u80fd\u7684\u6570\u636e\u84b8\u998f\u4e3a\u6784\u5efa\u9ad8\u6548\u9ad8\u6027\u80fd\u7684VLA\u6a21\u578b\u5f00\u8f9f\u4e86\u4e00\u6761\u6781\u5177\u524d\u666f\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.16262", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16262", "abs": "https://arxiv.org/abs/2511.16262", "authors": ["Oliver Bimber", "Karl Dietrich von Ellenrieder", "Michael Haller", "Rakesh John Amala Arokia Nathan", "Gianni Lunardi", "Marco Camurri", "Mohamed Youssef", "Santos Miguel Orozco Soto", "Jeremy E. Niven"], "title": "How Robot Dogs See the Unseeable", "comment": null, "summary": "Peering, a side-to-side motion used by animals to estimate distance through motion parallax, offers a powerful bio-inspired strategy to overcome a fundamental limitation in robotic vision: partial occlusion. Conventional robot cameras, with their small apertures and large depth of field, render both foreground obstacles and background objects in sharp focus, causing occluders to obscure critical scene information. This work establishes a formal connection between animal peering and synthetic aperture (SA) sensing from optical imaging. By having a robot execute a peering motion, its camera describes a wide synthetic aperture. Computational integration of the captured images synthesizes an image with an extremely shallow depth of field, effectively blurring out occluding elements while bringing the background into sharp focus. This efficient, wavelength-independent technique enables real-time, high-resolution perception across various spectral bands. We demonstrate that this approach not only restores basic scene understanding but also empowers advanced visual reasoning in large multimodal models, which fail with conventionally occluded imagery. Unlike feature-dependent multi-view 3D vision methods or active sensors like LiDAR, SA sensing via peering is robust to occlusion, computationally efficient, and immediately deployable on any mobile robot. This research bridges animal behavior and robotics, suggesting that peering motions for synthetic aperture sensing are a key to advanced scene understanding in complex, cluttered environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6a21\u4eff\u52a8\u7269\u7684\u4fa7\u5411\u8fd0\u52a8\uff08Peering\uff09\uff0c\u5229\u7528\u5408\u6210\u5b54\u5f84\uff08SA\uff09\u611f\u5e94\u6280\u672f\u6765\u514b\u670d\u673a\u5668\u4eba\u89c6\u89c9\u4e2d\u7684\u90e8\u5206\u906e\u6321\u95ee\u9898\u3002\u901a\u8fc7\u8ba9\u673a\u5668\u4eba\u6267\u884c\u4fa7\u5411\u8fd0\u52a8\uff0c\u5176\u76f8\u673a\u53ef\u4ee5\u6355\u83b7\u4e00\u7cfb\u5217\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u7ecf\u8fc7\u8ba1\u7b97\u6574\u5408\u540e\uff0c\u80fd\u591f\u751f\u6210\u4e00\u4e2a\u5177\u6709\u6781\u6d45\u666f\u6df1\u7684\u56fe\u50cf\uff0c\u4ece\u800c\u6a21\u7cca\u6389\u524d\u666f\u969c\u788d\u7269\u5e76\u4f7f\u80cc\u666f\u6e05\u6670\u53ef\u89c1\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u6062\u590d\u4e86\u57fa\u672c\u573a\u666f\u7684\u7406\u89e3\u80fd\u529b\uff0c\u8fd8\u589e\u5f3a\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u9ad8\u7ea7\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u673a\u5668\u4eba\u6444\u50cf\u5934\u7531\u4e8e\u5c0f\u5149\u5708\u548c\u5927\u666f\u6df1\u7684\u7279\u70b9\uff0c\u5bfc\u81f4\u524d\u666f\u969c\u788d\u7269\u548c\u80cc\u666f\u7269\u4f53\u90fd\u5904\u4e8e\u6e05\u6670\u5bf9\u7126\u72b6\u6001\uff0c\u8fd9\u4f7f\u5f97\u906e\u6321\u7269\u4f1a\u906e\u6321\u5173\u952e\u7684\u573a\u666f\u4fe1\u606f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u53d7\u5230\u4e86\u52a8\u7269\u901a\u8fc7\u4fa7\u5411\u8fd0\u52a8\u4f30\u8ba1\u8ddd\u79bb\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4f7f\u7528\u5408\u6210\u5b54\u5f84\u611f\u5e94\u6280\u672f\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8ba9\u673a\u5668\u4eba\u6267\u884c\u7c7b\u4f3c\u52a8\u7269\u7684\u4fa7\u5411\u8fd0\u52a8\uff0c\u5176\u642d\u8f7d\u7684\u76f8\u673a\u5728\u79fb\u52a8\u8fc7\u7a0b\u4e2d\u8bb0\u5f55\u4e0b\u4e00\u7cfb\u5217\u56fe\u50cf\u3002\u4e4b\u540e\uff0c\u901a\u8fc7\u8ba1\u7b97\u624b\u6bb5\u5c06\u8fd9\u4e9b\u56fe\u50cf\u8fdb\u884c\u6574\u5408\uff0c\u4ee5\u5408\u6210\u4e00\u4e2a\u5177\u6709\u6781\u6d45\u666f\u6df1\u7684\u65b0\u56fe\u50cf\u3002\u8fd9\u6837\u505a\u7684\u7ed3\u679c\u662f\u80fd\u591f\u6709\u6548\u5730\u6a21\u7cca\u6389\u906e\u6321\u5143\u7d20\uff0c\u540c\u65f6\u4fdd\u6301\u80cc\u666f\u5bf9\u8c61\u6e05\u6670\u53ef\u89c1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u6062\u590d\u88ab\u4f20\u7edf\u65b9\u6cd5\u906e\u6321\u5f71\u50cf\u7684\u57fa\u672c\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u800c\u4e14\u8fd8\u80fd\u589e\u5f3a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u5bf9\u4e8e\u590d\u6742\u73af\u5883\u4e0b\u7684\u9ad8\u7ea7\u89c6\u89c9\u63a8\u7406\u6027\u80fd\u3002\u76f8\u6bd4\u4f9d\u8d56\u7279\u5f81\u7684\u591a\u89c6\u89d23D\u89c6\u89c9\u65b9\u6cd5\u6216\u50cfLiDAR\u8fd9\u6837\u7684\u4e3b\u52a8\u4f20\u611f\u5668\uff0c\u57fa\u4e8e\u4fa7\u5411\u8fd0\u52a8\u7684\u5408\u6210\u5b54\u5f84\u611f\u5e94\u6280\u672f\u66f4\u52a0\u9c81\u68d2\u4e8e\u906e\u6321\u60c5\u51b5\u3001\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5e76\u4e14\u53ef\u4ee5\u7acb\u5373\u90e8\u7f72\u5230\u4efb\u4f55\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u6a21\u4eff\u52a8\u7269\u884c\u4e3a\u7684\u4fa7\u5411\u8fd0\u52a8\u7ed3\u5408\u5408\u6210\u5b54\u5f84\u611f\u5e94\u6280\u672f\u4e3a\u673a\u5668\u4eba\u5728\u590d\u6742\u62e5\u6324\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u7ea7\u522b\u7684\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2511.16265", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16265", "abs": "https://arxiv.org/abs/2511.16265", "authors": ["Haru Fukatsu", "Ryoji Yasuda", "Yuki Funabora", "Shinji Doki"], "title": "Funabot-Upper: McKibben Actuated Haptic Suit Inducing Kinesthetic Perceptions in Trunk, Shoulder, Elbow, and Wrist", "comment": "8 pages, 8 figures. This work has been submitted to the IEEE for possible publication", "summary": "This paper presents Funabot-Upper, a wearable haptic suit that enables users to perceive 14 upper-body motions, including those of the trunk, shoulder, elbow, and wrist. Inducing kinesthetic perception through wearable haptic devices has attracted attention, and various devices have been developed in the past. However, these have been limited to verifications on single body parts, and few have applied the same method to multiple body parts as well. In our previous study, we developed a technology that uses the contraction of artificial muscles to deform clothing in three dimensions. Using this technology, we developed a haptic suit that induces kinesthetic perception of 7 motions in multiple upper body. However, perceptual mixing caused by stimulating multiple human muscles has occurred between the shoulder and the elbow. In this paper, we established a new, simplified design policy and developed a novel haptic suit that induces kinesthetic perceptions in the trunk, shoulder, elbow, and wrist by stimulating joints and muscles independently. We experimentally demonstrated the induced kinesthetic perception and examined the relationship between stimulation and perceived kinesthetic perception under the new design policy. Experiments confirmed that Funabot-Upper successfully induces kinesthetic perception across multiple joints while reducing perceptual mixing observed in previous designs. The new suit improved recognition accuracy from 68.8% to 94.6% compared to the previous Funabot-Suit, demonstrating its superiority and potential for future haptic applications.", "AI": {"tldr": "This paper introduces Funabot-Upper, a haptic suit designed to induce the perception of 14 upper-body motions. It improves upon previous designs by reducing perceptual mixing and increasing recognition accuracy from 68.8% to 94.6%, making it a promising tool for future haptic applications.", "motivation": "The motivation is to develop a haptic suit that can effectively induce kinesthetic perceptions across multiple joints of the upper body, addressing the issue of perceptual mixing observed in earlier designs, and thereby improving the overall recognition accuracy and user experience with wearable haptic devices.", "method": "A new, simplified design policy was established, leading to the development of Funabot-Upper, which stimulates joints and muscles independently to induce kinesthetic perceptions in the trunk, shoulder, elbow, and wrist. The effectiveness of this approach was verified through experiments that examined the relationship between the stimulation provided and the perceived kinesthetic sensations.", "result": "Experiments confirmed that Funabot-Upper successfully induces kinesthetic perception across multiple joints while significantly reducing the perceptual mixing that occurred in previous designs. The recognition accuracy was improved from 68.8% to 94.6%.", "conclusion": "Funabot-Upper represents an advancement in wearable haptic technology, offering a solution to the challenge of inducing clear kinesthetic perceptions across multiple upper body parts. Its success in minimizing perceptual mixing and enhancing recognition accuracy highlights its potential for broad application in the field of haptics."}}
{"id": "2511.16330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16330", "abs": "https://arxiv.org/abs/2511.16330", "authors": ["Shreyas Kumar", "Ravi Prakash"], "title": "Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) offers a powerful approach for robots to learn complex, collaborative skills by combining Dynamic Movement Primitives (DMPs) for motion and Variable Impedance Control (VIC) for compliant interaction. However, this model-free paradigm often risks instability and unsafe exploration due to the time-varying nature of impedance gains. This work introduces Certified Gaussian Manifold Sampling (C-GMS), a novel trajectory-centric RL framework that learns combined DMP and VIC policies while guaranteeing Lyapunov stability and actuator feasibility by construction. Our approach reframes policy exploration as sampling from a mathematically defined manifold of stable gain schedules. This ensures every policy rollout is guaranteed to be stable and physically realizable, thereby eliminating the need for reward penalties or post-hoc validation. Furthermore, we provide a theoretical guarantee that our approach ensures bounded tracking error even in the presence of bounded model errors and deployment-time uncertainties. We demonstrate the effectiveness of C-GMS in simulation and verify its efficacy on a real robot, paving the way for reliable autonomous interaction in complex environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6C-GMS\uff0c\u5b83\u7ed3\u5408\u4e86\u52a8\u6001\u8fd0\u52a8\u57fa\u5143(DMP)\u548c\u53ef\u53d8\u963b\u6297\u63a7\u5236(VIC)\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86Lyapunov\u7a33\u5b9a\u6027\u4ee5\u53ca\u81f4\u52a8\u5668\u7684\u53ef\u884c\u6027\u3002\u901a\u8fc7\u4ece\u4e00\u4e2a\u6570\u5b66\u5b9a\u4e49\u7684\u7a33\u5b9a\u589e\u76ca\u8c03\u5ea6\u6d41\u5f62\u4e2d\u91c7\u6837\u6765\u91cd\u65b0\u5b9a\u4e49\u7b56\u7565\u63a2\u7d22\uff0c\u786e\u4fdd\u6bcf\u4e2a\u7b56\u7565\u5c55\u5f00\u90fd\u662f\u7a33\u5b9a\u7684\u5e76\u4e14\u7269\u7406\u4e0a\u53ef\u884c\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u5b58\u5728\u6709\u754c\u6a21\u578b\u8bef\u5dee\u548c\u90e8\u7f72\u65f6\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u786e\u4fdd\u6709\u754c\u7684\u8ddf\u8e2a\u8bef\u5dee\u3002", "motivation": "\u4f20\u7edf\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u80fd\u591f\u5e2e\u52a9\u673a\u5668\u4eba\u5b66\u4e60\u590d\u6742\u7684\u534f\u4f5c\u6280\u80fd\uff0c\u4f46\u5176\u65e0\u6a21\u578b\u7279\u6027\u5f80\u5f80\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6027\u548c\u4e0d\u5b89\u5168\u7684\u63a2\u7d22\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u963b\u6297\u589e\u76ca\u968f\u65f6\u95f4\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u786e\u4fdd\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\uff0c\u63d0\u51fa\u4e86Certified Gaussian Manifold Sampling (C-GMS) \u65b9\u6cd5\u3002", "method": "C-GMS\u662f\u4e00\u79cd\u4ee5\u8f68\u8ff9\u4e3a\u4e2d\u5fc3\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u5c06DMP\u4e0eVIC\u7b56\u7565\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u6784\u9020\u4fdd\u8bc1\u4e86Lyapunov\u7a33\u5b9a\u6027\u548c\u81f4\u52a8\u5668\u7684\u53ef\u884c\u6027\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u7b56\u7565\u63a2\u7d22\u89c6\u4e3a\u4ece\u4e00\u4e2a\u7531\u6570\u5b66\u5b9a\u4e49\u7684\u7a33\u5b9a\u589e\u76ca\u8c03\u5ea6\u6d41\u5f62\u4e2d\u8fdb\u884c\u91c7\u6837\u7684\u8fc7\u7a0b\uff0c\u4ece\u800c\u786e\u4fdd\u6240\u6709\u751f\u6210\u7684\u7b56\u7565\u65e2\u7a33\u5b9a\u53c8\u5b9e\u9645\u53ef\u884c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cC-GMS\u4e0d\u4ec5\u5728\u6a21\u62df\u73af\u5883\u4e2d\u6709\u6548\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u6d4b\u8bd5\u4e5f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u7406\u8bba\u5206\u6790\u8fd8\u8bc1\u660e\u4e86\u5373\u4f7f\u9762\u5bf9\u6709\u754c\u6a21\u578b\u9519\u8bef\u548c\u6267\u884c\u65f6\u7684\u4e0d\u786e\u5b9a\u6027\uff0cC-GMS\u4f9d\u7136\u80fd\u591f\u4fdd\u8bc1\u6709\u9650\u7684\u8ddf\u8e2a\u8bef\u5dee\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165C-GMS\u6846\u67b6\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e0b\u5b9e\u73b0\u53ef\u9760\u81ea\u4e3b\u4ea4\u4e92\u94fa\u5e73\u4e86\u9053\u8def\u3002\u5b83\u89e3\u51b3\u4e86\u4f20\u7edfRL\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u9690\u60a3\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e2\u80fd\u4fdd\u8bc1\u5b66\u4e60\u6548\u7387\u53c8\u80fd\u4fdd\u969c\u5b89\u5168\u6027\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.16372", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16372", "abs": "https://arxiv.org/abs/2511.16372", "authors": ["Bowen Xu", "Zexuan Yan", "Minghao Lu", "Xiyu Fan", "Yi Luo", "Youshen Lin", "Zhiqiang Chen", "Yeke Chen", "Qiyuan Qiao", "Peng Lu"], "title": "Flow-Aided Flight Through Dynamic Clutters From Point To Motion", "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L), November, 2025", "summary": "Challenges in traversing dynamic clutters lie mainly in the efficient perception of the environmental dynamics and the generation of evasive behaviors considering obstacle movement. Previous solutions have made progress in explicitly modeling the dynamic obstacle motion for avoidance, but this key dependency of decision-making is time-consuming and unreliable in highly dynamic scenarios with occlusions. On the contrary, without introducing object detection, tracking, and prediction, we empower the reinforcement learning (RL) with single LiDAR sensing to realize an autonomous flight system directly from point to motion. For exteroception, a depth sensing distance map achieving fixed-shape, low-resolution, and detail-safe is encoded from raw point clouds, and an environment change sensing point flow is adopted as motion features extracted from multi-frame observations. These two are integrated into a lightweight and easy-to-learn representation of complex dynamic environments. For action generation, the behavior of avoiding dynamic threats in advance is implicitly driven by the proposed change-aware sensing representation, where the policy optimization is indicated by the relative motion modulated distance field. With the deployment-friendly sensing simulation and dynamics model-free acceleration control, the proposed system shows a superior success rate and adaptability to alternatives, and the policy derived from the simulator can drive a real-world quadrotor with safe maneuvers.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u6fc0\u5149\u96f7\u8fbe\u611f\u77e5\u7684\u81ea\u4e3b\u98de\u884c\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df1\u5ea6\u8ddd\u79bb\u56fe\u548c\u73af\u5883\u53d8\u5316\u70b9\u6d41\u6765\u8868\u793a\u590d\u6742\u52a8\u6001\u73af\u5883\uff0c\u5e76\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u9690\u5f0f\u5730\u9a71\u52a8\u65e0\u4eba\u673a\u907f\u5f00\u52a8\u6001\u969c\u788d\u7269\u3002\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6210\u529f\u7387\u548c\u9002\u5e94\u6027\uff0c\u4e14\u4ece\u6a21\u62df\u5668\u4e2d\u5f97\u51fa\u7684\u7b56\u7565\u80fd\u591f\u4f7f\u771f\u5b9e\u4e16\u754c\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5b89\u5168\u5730\u6267\u884c\u64cd\u4f5c\u3002", "motivation": "\u5728\u7a7f\u8d8a\u52a8\u6001\u6742\u4e71\u73af\u5883\u4e2d\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u6709\u6548\u611f\u77e5\u73af\u5883\u52a8\u6001\u5e76\u8003\u8651\u969c\u788d\u7269\u8fd0\u52a8\u4ea7\u751f\u907f\u8ba9\u884c\u4e3a\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u867d\u7136\u5728\u907f\u514d\u52a8\u6001\u969c\u788d\u7269\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u9ad8\u5ea6\u52a8\u6001\u573a\u666f\u4e0b\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u906e\u6321\u7684\u60c5\u51b5\u4e0b\uff0c\u51b3\u7b56\u7684\u5173\u952e\u4f9d\u8d56\u2014\u2014\u660e\u786e\u5efa\u6a21\u52a8\u6001\u969c\u788d\u7269\u8fd0\u52a8\u8017\u65f6\u4e14\u4e0d\u53ef\u9760\u3002", "method": "1. \u4ece\u539f\u59cb\u70b9\u4e91\u7f16\u7801\u5f97\u5230\u56fa\u5b9a\u5f62\u72b6\u3001\u4f4e\u5206\u8fa8\u7387\u4f46\u4fdd\u7559\u7ec6\u8282\u7684\u6df1\u5ea6\u8ddd\u79bb\u56fe\u3002\n2. \u4ece\u591a\u5e27\u89c2\u6d4b\u4e2d\u63d0\u53d6\u51fa\u4f5c\u4e3a\u8fd0\u52a8\u7279\u5f81\u7684\u73af\u5883\u53d8\u5316\u70b9\u6d41\u3002\n3. \u5c06\u4e0a\u8ff0\u4e24\u79cd\u65b9\u6cd5\u6574\u5408\u6210\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6613\u4e8e\u5b66\u4e60\u7684\u590d\u6742\u52a8\u6001\u73af\u5883\u8868\u793a\u65b9\u6cd5\u3002\n4. \u5229\u7528\u63d0\u51fa\u7684\u611f\u77e5\u8868\u5f81\u65b9\u5f0f\u9690\u5f0f\u9a71\u52a8\u907f\u8ba9\u884c\u4e3a\u751f\u6210\uff0c\u5176\u4e2d\u7b56\u7565\u4f18\u5316\u7531\u76f8\u5bf9\u8fd0\u52a8\u8c03\u8282\u7684\u8ddd\u79bb\u573a\u6307\u793a\u3002\n5. \u4f7f\u7528\u90e8\u7f72\u53cb\u597d\u7684\u611f\u77e5\u6a21\u62df\u4e0e\u65e0\u9700\u52a8\u529b\u5b66\u6a21\u578b\u7684\u52a0\u901f\u5ea6\u63a7\u5236\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u76f8\u5bf9\u4e8e\u5176\u4ed6\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u597d\u7684\u9002\u5e94\u6027\uff1b\u4ece\u6a21\u62df\u5668\u4e2d\u8bad\u7ec3\u51fa\u7684\u7b56\u7565\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\uff0c\u5b9e\u73b0\u5b89\u5168\u5bfc\u822a\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86\u4ec5\u4f9d\u9760\u5355\u6fc0\u5149\u96f7\u8fbe\u611f\u77e5\u5373\u53ef\u652f\u6301\u590d\u6742\u52a8\u6001\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u98de\u884c\u4efb\u52a1\uff0c\u4e3a\u5f00\u53d1\u66f4\u52a0\u7075\u6d3b\u53ef\u9760\u7684\u65e0\u4eba\u673a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.16390", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16390", "abs": "https://arxiv.org/abs/2511.16390", "authors": ["Ajith Anil Meera", "Poppy Collis", "Polina Arbuzova", "Abi\u00e1n Torres", "Paul F Kinghorn", "Ricardo Sanz", "Pablo Lanillos"], "title": "Robot Metacognition: Decision Making with Confidence for Tool Invention", "comment": "under review", "summary": "Robots today often miss a key ingredient of truly intelligent behavior: the ability to reflect on their own cognitive processes and decisions. In humans, this self-monitoring or metacognition is crucial for learning, decision making and problem solving. For instance, they can evaluate how confident they are in performing a task, thus regulating their own behavior and allocating proper resources. Taking inspiration from neuroscience, we propose a robot metacognition architecture centered on confidence (a second-order judgment on decisions) and we demonstrate it on the use case of autonomous tool invention. We propose the use of confidence as a metacognitive measure within the robot decision making scheme. Confidence-informed robots can evaluate the reliability of their decisions, improving their robustness during real-world physical deployment. This form of robotic metacognition emphasizes embodied action monitoring as a means to achieve better informed decisions. We also highlight potential applications and research directions for robot metacognition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u673a\u5668\u4eba\u5143\u8ba4\u77e5\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u4ee5\u4fe1\u5fc3\u4e3a\u4e2d\u5fc3\uff0c\u5e76\u901a\u8fc7\u81ea\u4e3b\u5de5\u5177\u53d1\u660e\u7684\u6848\u4f8b\u5c55\u793a\u4e86\u8fd9\u79cd\u67b6\u6784\u3002\u4fe1\u5fc3\u4f5c\u4e3a\u5143\u8ba4\u77e5\u5ea6\u91cf\u88ab\u7528\u4e8e\u673a\u5668\u4eba\u7684\u51b3\u7b56\u65b9\u6848\u4e2d\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u8bc4\u4f30\u5176\u51b3\u7b56\u7684\u53ef\u9760\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u5728\u5b9e\u9645\u7269\u7406\u90e8\u7f72\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u673a\u5668\u4eba\u901a\u5e38\u7f3a\u4e4f\u771f\u6b63\u667a\u80fd\u884c\u4e3a\u7684\u5173\u952e\u8981\u7d20\uff1a\u53cd\u601d\u81ea\u8eab\u8ba4\u77e5\u8fc7\u7a0b\u548c\u51b3\u7b56\u7684\u80fd\u529b\u3002\u8fd9\u79cd\u81ea\u6211\u76d1\u63a7\u6216\u5143\u8ba4\u77e5\u5bf9\u5b66\u4e60\u3001\u51b3\u7b56\u548c\u95ee\u9898\u89e3\u51b3\u81f3\u5173\u91cd\u8981\u3002\u53d7\u795e\u7ecf\u79d1\u5b66\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u4fe1\u5fc3\uff08\u5bf9\u51b3\u7b56\u7684\u4e8c\u9636\u5224\u65ad\uff09\u7684\u673a\u5668\u4eba\u5143\u8ba4\u77e5\u67b6\u6784\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u51b3\u7b56\u7684\u53ef\u9760\u6027\u548c\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u5fc3\u5ea6\u91cf\u7684\u673a\u5668\u4eba\u5143\u8ba4\u77e5\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u8ba9\u673a\u5668\u4eba\u80fd\u591f\u5728\u8fdb\u884c\u51b3\u7b56\u65f6\u8bc4\u4f30\u81ea\u5df1\u5bf9\u4e8e\u5b8c\u6210\u4efb\u52a1\u7684\u4fe1\u5fc3\u6c34\u5e73\uff0c\u8fdb\u800c\u8c03\u8282\u81ea\u8eab\u884c\u4e3a\u548c\u8d44\u6e90\u5206\u914d\u3002\u8fd9\u4e00\u65b9\u6cd5\u662f\u901a\u8fc7\u5177\u4f53\u7684\u5b9e\u9a8c\u573a\u666f\u2014\u2014\u81ea\u4e3b\u5de5\u5177\u53d1\u660e\u6765\u5c55\u793a\u7684\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u4fe1\u5fc3\u4f5c\u4e3a\u5143\u8ba4\u77e5\u5ea6\u91cf\u7eb3\u5165\u5230\u673a\u5668\u4eba\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u4ee5\u6709\u6548\u5e2e\u52a9\u673a\u5668\u4eba\u8bc4\u4f30\u5176\u51b3\u7b56\u7684\u53ef\u9760\u6027\uff0c\u8fd9\u6709\u52a9\u4e8e\u6539\u5584\u673a\u5668\u4eba\u5728\u9762\u5bf9\u771f\u5b9e\u4e16\u754c\u7269\u7406\u73af\u5883\u65f6\u7684\u884c\u4e3a\u8868\u73b0\u548c\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u4eba\u5143\u8ba4\u77e5\u67b6\u6784\uff0c\u5f3a\u8c03\u4e86\u8eab\u4f53\u52a8\u4f5c\u76d1\u63a7\u4f5c\u4e3a\u8fbe\u6210\u66f4\u660e\u667a\u51b3\u7b56\u624b\u6bb5\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u5143\u8ba4\u77e5\u9886\u57df\u672a\u6765\u53ef\u80fd\u7684\u5e94\u7528\u65b9\u5411\u4e0e\u7814\u7a76\u8d8b\u52bf\u3002"}}
{"id": "2511.16406", "categories": ["cs.RO", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2511.16406", "abs": "https://arxiv.org/abs/2511.16406", "authors": ["Luis Luna", "Isaac Chairez", "Andrey Polyakov"], "title": "Homogeneous Proportional-Integral-Derivative Controller in Mobile Robotic Manipulators", "comment": null, "summary": "Mobile robotic manipulators (MRMs), which integrate mobility and manipulation capabilities, present significant control challenges due to their nonlinear dynamics, underactuation, and coupling between the base and manipulator subsystems. This paper proposes a novel homogeneous Proportional-Integral-Derivative (hPID) control strategy tailored for MRMs to achieve robust and coordinated motion control. Unlike classical PID controllers, the hPID controller leverages the mathematical framework of homogeneous control theory to systematically enhance the stability and convergence properties of the closed-loop system, even in the presence of dynamic uncertainties and external disturbances involved into a system in a homogeneous way. A homogeneous PID structure is designed, ensuring improved convergence of tracking errors through a graded homogeneity approach that generalizes traditional PID gains to nonlinear, state-dependent functions. Stability analysis is conducted using Lyapunov-based methods, demonstrating that the hPID controller guarantees global asymptotic stability and finite-time convergence under mild assumptions. Experimental results on a representative MRM model validate the effectiveness of the hPID controller in achieving high-precision trajectory tracking for both the mobile base and manipulator arm, outperforming conventional linear PID controllers in terms of response time, steady-state error, and robustness to model uncertainties. This research contributes a scalable and analytically grounded control framework for enhancing the autonomy and reliability of next-generation mobile manipulation systems in structured and unstructured environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u540c\u8d28\u6bd4\u4f8b-\u79ef\u5206-\u5fae\u5206\uff08hPID\uff09\u63a7\u5236\u7b56\u7565\uff0c\u4e13\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u8bbe\u8ba1\uff0c\u4ee5\u5b9e\u73b0\u9c81\u68d2\u548c\u534f\u8c03\u7684\u8fd0\u52a8\u63a7\u5236\u3002\u901a\u8fc7\u91c7\u7528\u540c\u8d28\u63a7\u5236\u7406\u8bba\u7684\u6570\u5b66\u6846\u67b6\uff0c\u8be5\u63a7\u5236\u5668\u80fd\u591f\u7cfb\u7edf\u5730\u589e\u5f3a\u95ed\u73af\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\u5c5e\u6027\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u548c\u5916\u90e8\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u7ebf\u6027PID\u63a7\u5236\u5668\u76f8\u6bd4\uff0chPID\u63a7\u5236\u5668\u5728\u54cd\u5e94\u65f6\u95f4\u3001\u7a33\u6001\u8bef\u5dee\u4ee5\u53ca\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u9002\u7528\u4e8e\u4e0b\u4e00\u4ee3\u79fb\u52a8\u64cd\u4f5c\u7cfb\u7edf\u7684\u81ea\u4e3b\u6027\u548c\u53ef\u9760\u6027\u63d0\u5347\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u64cd\u4f5c\u5668\uff08MRMs\uff09\u7ed3\u5408\u4e86\u79fb\u52a8\u6027\u548c\u64cd\u7eb5\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u5176\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u3001\u6b20\u9a71\u52a8\u7279\u6027\u4ee5\u53ca\u57fa\u5ea7\u548c\u64cd\u4f5c\u81c2\u5b50\u7cfb\u7edf\u4e4b\u95f4\u7684\u8026\u5408\uff0c\u7ed9\u63a7\u5236\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u5e76\u63d0\u9ad8MRMs\u5728\u9762\u5bf9\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u53ca\u5916\u90e8\u5e72\u6270\u65f6\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u672c\u7814\u7a76\u3002", "method": "\u7814\u7a76\u4e2d\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u540c\u8d28\u63a7\u5236\u7406\u8bba\u7684\u65b0\u5f0fhPID\u63a7\u5236\u7b56\u7565\u3002\u6b64\u65b9\u6cd5\u901a\u8fc7\u5c06\u4f20\u7edfPID\u589e\u76ca\u63a8\u5e7f\u5230\u975e\u7ebf\u6027\u7684\u72b6\u6001\u4f9d\u8d56\u51fd\u6570\u6765\u6539\u5584\u8ddf\u8e2a\u8bef\u5dee\u7684\u6536\u655b\u6027\u3002\u5229\u7528Lyapunov\u65b9\u6cd5\u8fdb\u884c\u4e86\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5728\u6e29\u548c\u6761\u4ef6\u4e0bhPID\u63a7\u5236\u5668\u53ef\u4ee5\u4fdd\u8bc1\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u6027\u548c\u6709\u9650\u65f6\u95f4\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4ee3\u8868\u6027\u7684MRM\u6a21\u578b\u4e0a\u6d4b\u8bd5\u65f6\uff0c\u6240\u63d0\u51fa\u7684hPID\u63a7\u5236\u5668\u6bd4\u4f20\u7edf\u7684\u7ebf\u6027PID\u63a7\u5236\u5668\u5728\u54cd\u5e94\u65f6\u95f4\u3001\u7a33\u6001\u8bef\u5dee\u4ee5\u53ca\u5bf9\u6297\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u5f97\u66f4\u597d\u3002\u8fd9\u8bc1\u5b9e\u4e86hPID\u63a7\u5236\u5668\u5728\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8f68\u8ff9\u8ddf\u8e2a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u4e0b\u4e00\u4ee3\u79fb\u52a8\u64cd\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u7406\u8bba\u57fa\u7840\u7684\u63a7\u5236\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u8fd9\u4e9b\u7cfb\u7edf\u5728\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2511.16434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16434", "abs": "https://arxiv.org/abs/2511.16434", "authors": ["Chenming Wu", "Xiaofan Li", "Chengkai Dai"], "title": "From Prompts to Printable Models: Support-Effective 3D Generation via Offset Direct Preference Optimization", "comment": "Technical report (7 pages)", "summary": "The transition from digital 3D models to physical objects via 3D printing often requires support structures to prevent overhanging features from collapsing during the fabrication process. While current slicing technologies offer advanced support strategies, they focus on post-processing optimizations rather than addressing the underlying need for support-efficient design during the model generation phase. This paper introduces SEG (\\textit{\\underline{S}upport-\\underline{E}ffective \\underline{G}eneration}), a novel framework that integrates Direct Preference Optimization with an Offset (ODPO) into the 3D generation pipeline to directly optimize models for minimal support material usage. By incorporating support structure simulation into the training process, SEG encourages the generation of geometries that inherently require fewer supports, thus reducing material waste and production time. We demonstrate SEG's effectiveness through extensive experiments on two benchmark datasets, Thingi10k-Val and GPT-3DP-Val, showing that SEG significantly outperforms baseline models such as TRELLIS, DPO, and DRO in terms of support volume reduction and printability. Qualitative results further reveal that SEG maintains high fidelity to input prompts while minimizing the need for support structures. Our findings highlight the potential of SEG to transform 3D printing by directly optimizing models during the generative process, paving the way for more sustainable and efficient digital fabrication practices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6SEG\uff0c\u8be5\u6846\u67b6\u57283D\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u4f18\u5316\u6700\u5c0f\u652f\u6491\u6750\u6599\u7684\u4f7f\u7528\u3002\u901a\u8fc7\u5c06\u652f\u6491\u7ed3\u6784\u6a21\u62df\u6574\u5408\u5230\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cSEG\u4fc3\u8fdb\u4e86\u751f\u6210\u672c\u8d28\u4e0a\u9700\u8981\u8f83\u5c11\u652f\u6491\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u6750\u6599\u6d6a\u8d39\u548c\u751f\u4ea7\u65f6\u95f4\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0cSEG\u5728\u51cf\u5c11\u652f\u6491\u4f53\u79ef\u548c\u53ef\u6253\u5370\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u4e14\u4fdd\u6301\u4e86\u5bf9\u8f93\u5165\u63d0\u793a\u7684\u9ad8\u5ea6\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5f53\u524d\u7684\u5207\u7247\u6280\u672f\u867d\u7136\u63d0\u4f9b\u4e86\u5148\u8fdb\u7684\u652f\u6491\u7b56\u7565\uff0c\u4f46\u4e3b\u8981\u96c6\u4e2d\u5728\u540e\u5904\u7406\u4f18\u5316\u4e0a\uff0c\u800c\u4e0d\u662f\u89e3\u51b3\u5728\u6a21\u578b\u751f\u6210\u9636\u6bb5\u5c31\u9700\u8981\u9ad8\u6548\u652f\u6491\u8bbe\u8ba1\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86SEG\u6846\u67b6\uff0c\u65e8\u5728\u76f4\u63a5\u4f18\u5316\u6a21\u578b\u4ee5\u51cf\u5c11\u652f\u6491\u6750\u6599\u7684\u4f7f\u7528\uff0c\u4ece\u800c\u63d0\u9ad83D\u6253\u5370\u8fc7\u7a0b\u4e2d\u7684\u53ef\u6301\u7eed\u6027\u548c\u6548\u7387\u3002", "method": "SEG\u6846\u67b6\u91c7\u7528\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316\u52a0\u504f\u79fb\u91cf\uff08ODPO\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u4e863D\u751f\u6210\u7ba1\u9053\u4e2d\u3002\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u652f\u6491\u7ed3\u6784\u4eff\u771f\uff0c\u4f7f\u5f97\u751f\u6210\u7684\u51e0\u4f55\u4f53\u5929\u7136\u5730\u8981\u6c42\u8f83\u5c11\u7684\u652f\u6491\u7269\uff0c\u8fdb\u800c\u964d\u4f4e\u4e86\u6750\u6599\u6d6a\u8d39\u548c\u751f\u4ea7\u65f6\u95f4\u3002", "result": "\u901a\u8fc7\u5bf9\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6Thingi10k-Val\u548cGPT-3DP-Val\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u53d1\u73b0SEG\u663e\u8457\u4f18\u4e8eTRELLIS\u3001DPO\u548cDRO\u7b49\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u652f\u6491\u4f53\u79ef\u51cf\u5c11\u548c\u652f\u6301\u7ed3\u6784\u9700\u6c42\u6700\u5c0f\u5316\u7684\u540c\u65f6\u4fdd\u8bc1\u4e86\u9ad8\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86SEG\u80fd\u591f\u901a\u8fc7\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u4f18\u5316\u6a21\u578b\u6765\u6539\u53d83D\u6253\u5370\u9886\u57df\uff0c\u4e3a\u66f4\u52a0\u53ef\u6301\u7eed\u548c\u9ad8\u6548\u7684\u6570\u5b57\u5236\u9020\u5b9e\u8df5\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.16518", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.16518", "abs": "https://arxiv.org/abs/2511.16518", "authors": ["Xiaoshuai Hao", "Lei Zhou", "Zhijian Huang", "Zhiwen Hou", "Yingbo Tang", "Lingfeng Zhang", "Guang Li", "Zheng Lu", "Shuhuai Ren", "Xianhui Meng", "Yuchen Zhang", "Jing Wu", "Jinghui Lu", "Chenxu Dang", "Jiayi Guan", "Jianhua Wu", "Zhiyi Hou", "Hanbing Li", "Shumeng Xia", "Mingliang Zhou", "Yinan Zheng", "Zihao Yue", "Shuhao Gu", "Hao Tian", "Yuannan Shen", "Jianwei Cui", "Wen Zhang", "Shaoqing Xu", "Bing Wang", "Haiyang Sun", "Zeyu Zhu", "Yuncheng Jiang", "Zibin Guo", "Chuhong Gong", "Chaofan Zhang", "Wenbo Ding", "Kun Ma", "Guang Chen", "Rui Cai", "Diyun Xiang", "Heng Qu", "Fuli Luo", "Hangjun Ye", "Long Chen"], "title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report", "comment": "Code: https://github.com/XiaomiMiMo/MiMo-Embodied Model: https://huggingface.co/XiaomiMiMo/MiMo-Embodied-7B", "summary": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aMiMo-Embodied\u7684\u8de8\u5b9e\u4f53\u57fa\u7840\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u5b9e\u4f53AI\u4e24\u4e2a\u9886\u57df\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u8fd9\u4e24\u4e2a\u9886\u57df\u4e4b\u95f4\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6b63\u5411\u8fc1\u79fb\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5728\u81ea\u52a8\u9a7e\u9a76\u4e0e\u5b9e\u4f53\u4eba\u5de5\u667a\u80fd\u4e24\u5927\u9886\u57df\u5185\u5747\u8868\u73b0\u51fa\u8272\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u8bc1\u660e\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u6784\u5efa\u3001\u591a\u9636\u6bb5\u5b66\u4e60\u4ee5\u53caCoT/RL\u5fae\u8c03\u7b49\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u540c\u4f46\u76f8\u5173\u7684\u4efb\u52a1\u95f4\u5b9e\u73b0\u79ef\u6781\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "method": "\u91c7\u7528\u4e86\u591a\u9636\u6bb5\u5b66\u4e60\u3001\u7cbe\u9009\u6570\u636e\u96c6\u6784\u9020\u4ee5\u53ca\u94fe\u5f0f\u601d\u7ef4/\u5f3a\u5316\u5b66\u4e60\uff08CoT/RL\uff09\u5fae\u8c03\u6280\u672f\u6765\u8bad\u7ec3MiMo-Embodied\u6a21\u578b\u3002", "result": "MiMo-Embodied\u6a21\u578b\u572817\u4e2a\u5b9e\u4f53AI\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u4efb\u52a1\u89c4\u5212\u3001\u53ef\u64cd\u4f5c\u6027\u9884\u6d4b\u548c\u7a7a\u95f4\u7406\u89e3\uff09\u53ca12\u4e2a\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\uff08\u6db5\u76d6\u73af\u5883\u611f\u77e5\u3001\u72b6\u6001\u9884\u6d4b\u548c\u9a7e\u9a76\u89c4\u5212\uff09\u4e2d\u521b\u4e0b\u4e86\u65b0\u7eaa\u5f55\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u3001\u95ed\u6e90\u53ca\u4e13\u95e8\u5316\u57fa\u7ebf\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u91c7\u7528\u7279\u5b9a\u7684\u5b66\u4e60\u7b56\u7565\u548c\u6280\u672f\u624b\u6bb5\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e0e\u5b9e\u4f53AI\u8fd9\u4e24\u4e2a\u770b\u4f3c\u4e0d\u540c\u7684\u9886\u57df\u4e4b\u95f4\u53ef\u4ee5\u89c2\u5bdf\u5230\u5f3a\u70c8\u7684\u6b63\u9762\u8fc1\u79fb\u6548\u5e94\uff0c\u4ece\u800c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.16651", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16651", "abs": "https://arxiv.org/abs/2511.16651", "authors": ["Yang Tian", "Yuyin Yang", "Yiman Xie", "Zetao Cai", "Xu Shi", "Ning Gao", "Hangxu Liu", "Xuekun Jiang", "Zherui Qiu", "Feng Yuan", "Yaping Li", "Ping Wang", "Junhao Cai", "Jia Zeng", "Hao Dong", "Jiangmiao Pang"], "title": "InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy", "comment": null, "summary": "Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $\u03c0$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $\u03c0_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $\u03c0_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u8bc1\u660e\u4e86\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\uff08InternData-A1\uff09\u53ef\u4ee5\u4e0e\u6700\u5f3a\u7684\u73b0\u5b9e\u673a\u5668\u4eba\u6570\u636e\u96c6\u5728\u9884\u8bad\u7ec3VLA\u6a21\u578b\u65f6\u8fbe\u5230\u76f8\u540c\u7684\u6027\u80fd\uff0c\u663e\u793a\u51fa\u5927\u89c4\u6a21\u6a21\u62df\u7684\u91cd\u8981\u4ef7\u503c\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u8fd8\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u96f6\u6837\u672c\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u867d\u7136\u5f53\u524d\u7684VLA\u6a21\u578b\u5df2\u7ecf\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u771f\u5b9e\u673a\u5668\u4eba\u9884\u8bad\u7ec3\u7684\u5f3a\u5927\u6709\u6548\u6027\uff0c\u4f46\u5408\u6210\u6570\u636e\u5728\u6b64\u4e4b\u524d\u5c1a\u672a\u5728\u89c4\u6a21\u4e0a\u5c55\u793a\u51fa\u53ef\u6bd4\u7684\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5408\u6210\u6570\u636e\u662f\u5426\u80fd\u591f\u5355\u72ec\u5339\u914d\u6700\u5f3a\u5927\u7684\u03c0-\u6570\u636e\u96c6\u5728\u9884\u8bad\u7ec3VLA\u6a21\u578b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u5176\u5bf9\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u96f6\u6837\u672csim-to-real\u8f6c\u79fb\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aInternData-A1\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7630,000\u4e2a\u8f68\u8ff9\u548c7,433\u5c0f\u65f6\u7684\u6570\u636e\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u64cd\u4f5c\u573a\u666f\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u4e00\u4e2a\u9ad8\u5ea6\u81ea\u4e3b\u3001\u5b8c\u5168\u89e3\u8026\u4e14\u7ec4\u5408\u5f0f\u7684\u4eff\u771f\u7ba1\u9053\u751f\u6210\uff0c\u652f\u6301\u957f\u65f6\u95f4\u6280\u80fd\u7ec4\u5408\u3001\u7075\u6d3b\u7684\u4efb\u52a1\u7ec4\u88c5\u4ee5\u53ca\u5f02\u6784\u4f53\u73b0\u5f62\u5f0f\u3002\u4f7f\u7528\u4e0e\u03c0_0\u76f8\u540c\u67b6\u6784\uff0c\u7814\u7a76\u8005\u5b8c\u5168\u57fa\u4e8eInternData-A1\u8fdb\u884c\u4e86\u6a21\u578b\u9884\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b8c\u5168\u57fa\u4e8eInternData-A1\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u572849\u9879\u6a21\u62df\u4efb\u52a1\u30015\u9879\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4ee5\u53ca4\u9879\u957f\u65f6\u7075\u5de7\u4efb\u52a1\u4e0a\u4e0e\u5b98\u65b9\u03c0_0\u7684\u8868\u73b0\u76f8\u5339\u914d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u9996\u4e2a\u8bc1\u636e\uff0c\u8bc1\u660e\u4ec5\u4f9d\u9760\u5408\u6210\u6570\u636e\u5373\u53ef\u4e3aVLA\u6a21\u578b\u63d0\u4f9b\u4e0e\u5b9e\u9645\u673a\u5668\u4eba\u6570\u636e\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u9884\u8bad\u7ec3\u6548\u679c\uff0c\u5f3a\u8c03\u4e86\u5927\u89c4\u6a21\u6a21\u62df\u5bf9\u4e8e\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u73b0\u96f6\u6837\u672csim-to-real\u8f6c\u79fb\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.16661", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16661", "abs": "https://arxiv.org/abs/2511.16661", "authors": ["Irmak Guzey", "Haozhi Qi", "Julen Urain", "Changhao Wang", "Jessica Yin", "Krishna Bodduluri", "Mike Lambeta", "Lerrel Pinto", "Akshara Rai", "Jitendra Malik", "Tingfan Wu", "Akash Sharma", "Homanga Bharadhwaj"], "title": "Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations", "comment": null, "summary": "Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been bottle-necked by the embodiment gap between humans and robots, as well as by difficulties in extracting relevant contextual and motion cues that enable learning of autonomous policies from in-the-wild human videos. We claim that with simple yet sufficiently powerful hardware for obtaining human data and our proposed framework AINA, we are now one significant step closer to achieving this dream. AINA enables learning multi-fingered policies from data collected by anyone, anywhere, and in any environment using Aria Gen 2 glasses. These glasses are lightweight and portable, feature a high-resolution RGB camera, provide accurate on-board 3D head and hand poses, and offer a wide stereo view that can be leveraged for depth estimation of the scene. This setup enables the learning of 3D point-based policies for multi-fingered hands that are robust to background changes and can be deployed directly without requiring any robot data (including online corrections, reinforcement learning, or simulation). We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks. Robot rollouts are best viewed on our website: https://aina-robot.github.io.", "AI": {"tldr": "The paper introduces AINA, a framework that uses Aria Gen 2 glasses to learn multi-fingered robot policies from human daily task videos, overcoming the embodiment gap and enabling robust policy learning without needing robot-specific data.", "motivation": "The motivation is to reduce the reliance on labor-intensive robot data collection by learning multi-fingered robot manipulation policies directly from human demonstrations in natural environments, thus advancing toward more generalizable robot use in human settings.", "method": "The method involves using Aria Gen 2 glasses, which are equipped with a high-resolution RGB camera, accurate 3D head and hand pose estimation, and a wide stereo view for depth estimation. This hardware, combined with the AINA framework, allows for the extraction of relevant contextual and motion cues necessary to learn 3D point-based policies for multi-fingered hands.", "result": "The results show that the AINA framework can learn policies that are robust to background changes and can be deployed across nine everyday manipulation tasks without the need for any robot data, including online corrections, reinforcement learning, or simulation.", "conclusion": "The conclusion is that with the AINA framework and the use of Aria Gen 2 glasses, the research represents a significant step towards achieving the goal of learning multi-fingered robot policies from human demonstrations, thereby making robot manipulation more adaptable to human environments."}}
