<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 21]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [PrediFlow: A Flow-Based Prediction-Refinement Framework for Real-Time Human Motion Prediction in Human-Robot Collaboration](https://arxiv.org/abs/2512.13903)
*Sibo Tian,Minghui Zheng,Xiao Liang*

Main category: cs.RO

TL;DR: 本文提出了一种新的预测-精炼框架，该框架结合了人类和机器人观察到的动作来精炼初始预测结果，以实现实时、逼真且交互感知的人类动作预测。实验表明，该方法显著提高了预测准确性，同时保持了人类动作的不确定性和多模态性，并且总推理时间仍在预算内。


<details>
  <summary>Details</summary>
Motivation: 随机人类运动预测对于工业再制造中安全有效的人机协作至关重要，因为它能够捕捉确定性方法无法处理的人类运动不确定性及多模态行为。尽管早期研究强调高度多样化的预测，但它们往往生成不真实的人类动作；而较新的方法则侧重于准确性和实时性能，但在不超出时间预算的情况下仍有提高预测质量的空间。此外，当前关于人机协作中随机人类运动预测的研究通常仅考虑孤立的人类运动，忽略了机器人运动对人类行为的影响。为了解决这些研究空白，实现实时、逼真且交互感知的人类运动预测，提出了这一新框架。

Method: 本文提出的方法是一种新颖的预测-精炼框架，它整合了人类与机器人观察到的运动信息来精炼由预训练的最佳预测器产生的初始预测。精炼模块采用流匹配结构（Flow Matching）来考虑不确定性因素。

Result: 在HRC桌面拆卸数据集上的实验研究表明，所提出的方法显著提升了预测精度，同时保留了人类运动的不确定性和多模态特性。此外，整个框架的推理时间保持在可接受范围内，证明了其有效性和实用性。

Conclusion: 通过结合人类与机器人的互动信息，所提出的预测-精炼框架能够更准确地预测人类在与机器人协作环境下的运动模式，同时确保预测结果的真实性和多样性。这为人机协作领域提供了新的解决方案。

Abstract: Stochastic human motion prediction is critical for safe and effective human-robot collaboration (HRC) in industrial remanufacturing, as it captures human motion uncertainties and multi-modal behaviors that deterministic methods cannot handle. While earlier works emphasize highly diverse predictions, they often generate unrealistic human motions. More recent methods focus on accuracy and real-time performance, yet there remains potential to improve prediction quality further without exceeding time budgets. Additionally, current research on stochastic human motion prediction in HRC typically considers human motion in isolation, neglecting the influence of robot motion on human behavior. To address these research gaps and enable real-time, realistic, and interaction-aware human motion prediction, we propose a novel prediction-refinement framework that integrates both human and robot observed motion to refine the initial predictions produced by a pretrained state-of-the-art predictor. The refinement module employs a Flow Matching structure to account for uncertainty. Experimental studies on the HRC desktop disassembly dataset demonstrate that our method significantly improves prediction accuracy while preserving the uncertainties and multi-modalities of human motion. Moreover, the total inference time of the proposed framework remains within the time budget, highlighting the effectiveness and practicality of our approach.

</details>


### [2] [Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline](https://arxiv.org/abs/2512.13974)
*Hossein Naderi,Alireza Shojaei,Philip Agee,Kereshmeh Afsari,Abiola Akanmu*

Main category: cs.RO

TL;DR: 本文提出了一种结合机器人技术和AI的多层框架，用于自动建筑工地安全检查和报告生成。该系统通过SLAM和自主导航实现重复覆盖，并利用视觉语言模型(VLM)生成场景描述、检索OSHA与现场政策、评估安全状况，最后由大型语言模型(LLM)生成安全报告。实验结果表明，该方法在召回率上表现出色，精度与最先进的闭源模型相当。


<details>
  <summary>Details</summary>
Motivation: 当前建筑安全检查主要依赖人工，自动化方法需要特定任务的数据集，在快速变化的施工环境中难以维护；同时，使用机器人进行现场检查仍需人工远程操作和手动报告，效率低下。为解决这些问题，本文旨在开发一种能够将机器人在自主导航过程中观察到的信息与常见的建筑工地安全规则相联系的系统，以自动生成安全检查报告。

Method: 本文提出了一个包含两个主要模块（机器人技术与AI）的多层框架。机器人技术方面，采用SLAM和自主导航技术提供可重复的覆盖范围及通过航点的目标重访。AI方面，则构建了一个基于视觉语言模型(VLM)的层级来生成场景描述；另一个基于VLM的层级则根据规则评估安全状况；还包括一个依据OSHA指南和现场政策对描述进行解释的检索组件；最终，使用大型语言模型(LLM)根据前几层的输出生成安全报告。

Result: 通过概念验证实现并在模拟常见危险情况的实验室环境中进行了评估，结果显示该框架具有较高的召回率以及与现有最先进闭源模型相比具有竞争力的精确度。

Conclusion: 本研究贡献了一条透明且可泛化的管道，超越了黑盒模型，通过展示每一层的中间产物并让人类保持在循环中。这项工作为未来扩展至更多任务和其他设置提供了基础。

Abstract: Construction safety inspection remains mostly manual, and automated approaches still rely on task-specific datasets that are hard to maintain in fast-changing construction environments due to frequent retraining. Meanwhile, field inspection with robots still depends on human teleoperation and manual reporting, which are labor-intensive. This paper aims to connect what a robot sees during autonomous navigation to the safety rules that are common in construction sites, automatically generating a safety inspection report. To this end, we proposed a multi-layer framework with two main modules: robotics and AI. On the robotics side, SLAM and autonomous navigation provide repeatable coverage and targeted revisits via waypoints. On AI side, a Vision Language Model (VLM)-based layer produces scene descriptions; a retrieval component powered grounds those descriptions in OSHA and site policies; Another VLM-based layer assesses the safety situation based on rules; and finally Large Language Model (LLM) layer generates safety reports based on previous outputs. The framework is validated with a proof-of-concept implementation and evaluated in a lab environment that simulates common hazards across three scenarios. Results show high recall with competitive precision compared to state-of-the-art closed-source models. This paper contributes a transparent, generalizable pipeline that moves beyond black-box models by exposing intermediate artifacts from each layer and keeping the human in the loop. This work provides a foundation for future extensions to additional tasks and settings within and beyond construction context.

</details>


### [3] [Impact of Robot Facial-Audio Expressions on Human Robot Trust Dynamics and Trust Repair](https://arxiv.org/abs/2512.13981)
*Hossein Naderi,Alireza Shojaei,Philip Agee,Kereshmeh Afsari,Abiola Akanmu*

Main category: cs.RO

TL;DR: 本文研究了机器人在完成任务时的表现及其对结果的表达反应如何随着时间塑造人类信任的动态变化。实验通过两个受建筑启发的任务（材料递送和信息收集）来测量参与者的信任度，并发现成功增加信任，失败则导致信任急剧下降，而基于道歉的表达可以部分恢复信任。此外，年龄组和先前态度也影响了信任动态。


<details>
  <summary>Details</summary>
Motivation: 尽管在AEC行业中机器人技术和人机协作取得了进展，但信任通常被视为静态因素，缺乏关于在协作过程中它是如何变化的指导。因此，本研究旨在探索机器人任务表现及其对结果的情感回应如何随时间影响人对机器人的信任度。

Method: 设计了一个控制条件下的单因素被试内研究，包括两个灵感来自建筑行业的任务：材料递送（物理协助）和信息收集（感知协助）。使用14项的人机交互信任感知量表加上重新分配选择来反复测量信任（每个任务四次）。机器人展示两种多模态表情：“高兴”的显示，在成功后简短确认；“悲伤”的显示，在失败后道歉并请求第二次机会。该研究在一个实验室环境中进行，有30名参与者和一个四足平台。

Result: 结果显示，机器人成功可靠地增加了信任感，失败则导致了信任感的急剧下降，而基于道歉的表情部分恢复了信任（在材料递送任务中恢复了44%；在信息收集任务中恢复了38%）。项目级分析表明，恢复的信任主要由互动和沟通因素驱动，能力部分恢复，自主性方面变化最小。另外，年龄组和先前的态度调节了信任动态，年轻参与者表现出较大但持续时间较短的变化，20多岁的参与者显示出最持久的信任修复，而年长参与者表现出最为保守的信任动态。

Conclusion: 这项工作为未来根据任务需求和用户特征调整修复策略提供了基础，以支持在建筑工地安全、高效地采用机器人。

Abstract: Despite recent advances in robotics and human-robot collaboration in the AEC industry, trust has mostly been treated as a static factor, with little guidance on how it changes across events during collaboration. This paper investigates how a robot's task performance and its expressive responses after outcomes shape the dynamics of human trust over time. To this end, we designed a controlled within-subjects study with two construction-inspired tasks, Material Delivery (physical assistance) and Information Gathering (perceptual assistance), and measured trust repeatedly (four times per task) using the 14-item Trust Perception Scale for HRI plus a redelegation choice. The robot produced two multimodal expressions, a "glad" display with a brief confirmation after success, and a "sad" display with an apology and a request for a second chance after failure. The study was conducted in a lab environment with 30 participants and a quadruped platform, and we evaluated trust dynamics and repair across both tasks. Results show that robot success reliably increases trust, failure causes sharp drops, and apology-based expressions partially restores trust (44% recovery in Material Delivery; 38% in Information Gathering). Item-level analysis indicates that recovered trust was driven mostly by interaction and communication factors, with competence recovering partially and autonomy aspects changing least. Additionally, age group and prior attitudes moderated trust dynamics with younger participants showed larger but shorter-lived changes, mid-20s participants exhibited the most durable repair, and older participants showed most conservative dynamics. This work provides a foundation for future efforts that adapt repair strategies to task demands and user profiles to support safe, productive adoption of robots on construction sites.

</details>


### [4] [CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth](https://arxiv.org/abs/2512.14001)
*Zhuo Zhang,Yonghui Liu,Meijie Zhang,Feiyang Tan,Yikang Ding*

Main category: cs.RO

TL;DR: 本研究提出了一种新的相机-激光雷达对齐方法CLAIM，通过最小化基于块状皮尔逊相关性的结构损失和基于互信息的纹理损失来寻找最优变换。该方法简单且适应性强，在多个公开数据集上表现出优于现有技术的性能。


<details>
  <summary>Details</summary>
Motivation: 为了释放单深度模型在相机-激光雷达校准中的潜力，并简化现有的对齐过程，即减少复杂的数据处理、特征提取或特征匹配步骤。

Method: 提出了CLAIM方法，该方法利用从粗到细的搜索策略，依据图像与激光雷达点云对之间的初始猜测找到最优变换，以最小化特定设计的结构损失和纹理损失。

Result: 在KITTI、Waymo以及MIAS-LCEC等公共数据集上的实验结果表明，CLAIM方法相较于当前最先进的方法表现更优。

Conclusion: CLAIM提供了一个有效且简单的解决方案来解决相机-激光雷达校准问题，适用于大多数场景，并且在实际应用中显示出了优越性。

Abstract: In this paper, we unleash the potential of the powerful monodepth model in camera-LiDAR calibration and propose CLAIM, a novel method of aligning data from the camera and LiDAR. Given the initial guess and pairs of images and LiDAR point clouds, CLAIM utilizes a coarse-to-fine searching method to find the optimal transformation minimizing a patched Pearson correlation-based structure loss and a mutual information-based texture loss. These two losses serve as good metrics for camera-LiDAR alignment results and require no complicated steps of data processing, feature extraction, or feature matching like most methods, rendering our method simple and adaptive to most scenes. We validate CLAIM on public KITTI, Waymo, and MIAS-LCEC datasets, and the experimental results demonstrate its superior performance compared with the state-of-the-art methods. The code is available at https://github.com/Tompson11/claim.

</details>


### [5] [Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model](https://arxiv.org/abs/2512.14031)
*Zhaofeng Hu,Hongrui Yu,Vaidhyanathan Chandramouli,Ci-Jyun Liang*

Main category: cs.RO

TL;DR: 本研究评估了两种用于教授建筑机器人新技能的方法：视觉-语言-动作（VLA）模型和强化学习（RL）方法，以了解它们在建筑自动化中的适用性。通过一系列实验对比，发现VLA模型在泛化能力和少量样本学习方面表现出色，相比而言DQN虽然有效但需要更多调优工作。


<details>
  <summary>Details</summary>
Motivation: 为了理解不同教学方法对建筑机器人执行任务的表现以及实际部署所需的工作量，旨在为建筑自动化找到更有效的解决方案。

Method: 开发了两个遥操作界面来控制机器人并收集所需演示数据；进行了三阶段评估，包括比较MLP策略与DQN模仿模型、训练并比较三种不同的VLA模型、以及将选定的RL基线与VLA模型进行基准测试。

Result: VLA模型展现出强大的泛化能力和少量样本学习能力，在拾取阶段达到了60%至100%的成功率。相比之下，DQN可以通过增加噪声提高鲁棒性，但这增加了调优工作量。

Conclusion: VLA模型通过减少编程工作量并在最少的数据下实现良好性能而提供实用优势，而当可以接受足够的调优努力时，DQN也提供了一个可行的基线。

Abstract: This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.

</details>


### [6] [E-Navi: Environmental Adaptive Navigation for UAVs on Resource Constrained Platforms](https://arxiv.org/abs/2512.14046)
*Boyang Li,Zhongpeng Jin,Shuai Zhao,Jiahui Liao,Tian Liu,Han Liu,Yuanhai Zhang,Kai Huang*

Main category: cs.RO

TL;DR: 本文提出了一种针对无人机的环境自适应导航系统E-Navi，该系统能够根据环境变化动态调整CPU上的任务执行情况，从而优化了飞行性能和减少了计算负担。


<details>
  <summary>Details</summary>
Motivation: 当前无人机自主导航系统通常采用固定执行配置，不考虑基于可用计算资源的环境动态变化，导致刚性的飞行策略和过度计算，最终降低飞行性能甚至导致无人机故障。尽管需要一个自适应系统来解决这些问题，但因难以量化环境复杂性和建模环境与系统配置之间的关系，动态调整工作负载仍然具有挑战性。

Method: 提出了E-Navi，一种用于无人机的环境自适应导航系统，它能够根据可获得的计算资源对环境变化作出响应，通过定量评估环境复杂性来动态调整地图分辨率和执行频率，同时支持跨不同计算能力硬件平台的灵活部署。

Result: 广泛的硬件在环测试和真实世界实验表明，所提出的系统在各种硬件平台上显著优于基线方法，实现了高达53.9%的导航任务工作量减少、高达63.8%的飞行时间节省，并提供了更稳定的飞行速度控制。

Conclusion: E-Navi展示了通过动态调整以适应环境变化的能力，不仅提高了无人机导航系统的效率还增强了其适应性。

Abstract: The ability to adapt to changing environments is crucial for the autonomous navigation systems of Unmanned Aerial Vehicles (UAVs). However, existing navigation systems adopt fixed execution configurations without considering environmental dynamics based on available computing resources, e.g., with a high execution frequency and task workload. This static approach causes rigid flight strategies and excessive computations, ultimately degrading flight performance or even leading to failures in UAVs. Despite the necessity for an adaptive system, dynamically adjusting workloads remains challenging, due to difficulties in quantifying environmental complexity and modeling the relationship between environment and system configuration. Aiming at adapting to dynamic environments, this paper proposes E-Navi, an environmental-adaptive navigation system for UAVs that dynamically adjusts task executions on the CPUs in response to environmental changes based on available computational resources. Specifically, the perception-planning pipeline of UAVs navigation system is redesigned through dynamic adaptation of mapping resolution and execution frequency, driven by the quantitative environmental complexity evaluations. In addition, E-Navi supports flexible deployment across hardware platforms with varying levels of computing capability. Extensive Hardware-In-the-Loop and real-world experiments demonstrate that the proposed system significantly outperforms the baseline method across various hardware platforms, achieving up to 53.9% navigation task workload reduction, up to 63.8% flight time savings, and delivering more stable velocity control.

</details>


### [7] [Expert Switching for Robust AAV Landing: A Dual-Detector Framework in Simulation](https://arxiv.org/abs/2512.14054)
*Humaira Tasnim,Ashik E Rasul,Bruce Jo,Hyung-Jin Yoon*

Main category: cs.RO

TL;DR: 提出了一种自适应双专家感知框架，用于提高自主飞行器在降落过程中对直升机停机坪的检测可靠性。该框架利用两个YOLOv8模型分别针对远距离和近距离检测进行了优化，并通过几何门控机制选择最合适的预测结果，从而提高了检测精度、着陆准确性和整体鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在GPS受限或视觉条件差的情况下，可靠的直升机停机坪检测对于自主飞行器（AAV）着陆至关重要。但现有单模型检测方案难以应对从高空到接近地面过程中目标尺寸的巨大变化，导致检测性能下降。

Method: 设计了一个规模自适应的双专家感知框架，其中包含两个专门训练过的YOLOv8模型：一个擅长检测远距离的小型低分辨率停机坪；另一个则专注于当目标占据大部分视野时提供高精度定位。此外，还引入了几何门控机制来决定哪个专家的预测与AAV视角最为一致。

Result: 实验表明，在闭环着陆环境中，相较于单一检测器基线，所提出的双专家系统显著提升了对齐稳定性、着陆精度及总体鲁棒性。

Conclusion: 通过为着陆问题量身定制的规模意识专家路由策略，这项研究推进了基于视觉的韧性感知技术的发展，为未来多专家AAV框架奠定了基础。

Abstract: Reliable helipad detection is essential for Autonomous Aerial Vehicle (AAV) landing, especially under GPS-denied or visually degraded conditions. While modern detectors such as YOLOv8 offer strong baseline performance, single-model pipelines struggle to remain robust across the extreme scale transitions that occur during descent, where helipads appear small at high altitude and large near touchdown. To address this limitation, we propose a scale-adaptive dual-expert perception framework that decomposes the detection task into far-range and close-range regimes. Two YOLOv8 experts are trained on scale-specialized versions of the HelipadCat dataset, enabling one model to excel at detecting small, low-resolution helipads and the other to provide high-precision localization when the target dominates the field of view. During inference, both experts operate in parallel, and a geometric gating mechanism selects the expert whose prediction is most consistent with the AAV's viewpoint. This adaptive routing prevents the degradation commonly observed in single-detector systems when operating across wide altitude ranges. The dual-expert perception module is evaluated in a closed-loop landing environment that integrates CARLA's photorealistic rendering with NASA's GUAM flight-dynamics engine. Results show substantial improvements in alignment stability, landing accuracy, and overall robustness compared to single-detector baselines. By introducing a scale-aware expert routing strategy tailored to the landing problem, this work advances resilient vision-based perception for autonomous descent and provides a foundation for future multi-expert AAV frameworks.

</details>


### [8] [Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning](https://arxiv.org/abs/2512.14057)
*Amir M. Soufi Enayati,Homayoun Honari,Homayoun Najjaran*

Main category: cs.RO

TL;DR: 本文提出了一种名为CRAFT的新模型，它通过仅从状态和奖励序列中推断任务表示来改进机器人控制中的强化学习方法。与依赖于动作信息的传统方法相比，CRAFT促进了更快的任务适应、更好的泛化能力和更有效的探索。


<details>
  <summary>Details</summary>
Motivation: 传统的上下文自适应元强化学习方法通常需要完整的动作信息来进行任务推理，这使得任务推理紧密依赖于特定的策略。该研究旨在通过开发一种不依赖于动作的新方法来解决这一问题，从而实现模块化训练并支持可扩展信念更新。

Method: CRAFT（Context Representation via Action Free Transformer encoder-decoder）是一种基于transformer编码器-解码器架构的信念模型，利用旋转位置嵌入来捕捉长时间范围的时间依赖性，并且能够稳健地编码参数化和非参数化的任务变化。该模型通过去除对动作的依赖，将任务推理与策略优化分离开来。

Result: 在MetaWorld ML-10机器人操作基准测试上的实验表明，与上下文自适应元-RL基线相比，CRAFT实现了更快的适应速度、改善了泛化性能以及更高效的探索。

Conclusion: 研究表明，无动作推理作为机器人控制领域中可扩展强化学习的基础具有巨大潜力。CRAFT模型为提高机器人的适应性和泛化能力提供了一个新的方向。

Abstract: Reinforcement learning (RL) enables robots to operate in uncertain environments, but standard approaches often struggle with poor generalization to unseen tasks. Context-adaptive meta reinforcement learning addresses these limitations by conditioning on the task representation, yet they mostly rely on complete action information in the experience making task inference tightly coupled to a specific policy. This paper introduces Context Representation via Action Free Transformer encoder decoder (CRAFT), a belief model that infers task representations solely from sequences of states and rewards. By removing the dependence on actions, CRAFT decouples task inference from policy optimization, supports modular training, and leverages amortized variational inference for scalable belief updates. Built on a transformer encoder decoder with rotary positional embeddings, the model captures long range temporal dependencies and robustly encodes both parametric and non-parametric task variations. Experiments on the MetaWorld ML-10 robotic manipulation benchmark show that CRAFT achieves faster adaptation, improved generalization, and more effective exploration compared to context adaptive meta--RL baselines. These findings highlight the potential of action-free inference as a foundation for scalable RL in robotic control.

</details>


### [9] [Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field](https://arxiv.org/abs/2512.14111)
*Chenzui Li,Yiming Chen,Xi Wu,Tao Teng,Sylvain Calinon,Darwin Caldwell,Fei Chen*

Main category: cs.RO

TL;DR: 本文提出了一种名为CSEF（配置空间人体工程学场）的方法，用于工业人机协作中的实时人体工程学意识规划。该方法在基准测试中表现优于基于任务空间的人体工程学规划器，并在实际硬件实验中降低了肌肉激活程度和人体工程学成本，展示了其实用价值。


<details>
  <summary>Details</summary>
Motivation: 工业人机协作需要无碰撞、响应迅速且符合人体工程学的安全运动规划，以减少疲劳和肌肉骨骼风险。

Method: 提出了配置空间人体工程学场(CSEF)，这是一种覆盖人类关节空间的连续可微分场，能够量化人体工程学质量并提供梯度支持实时人体工程学意识规划。通过一种高效算法从已建立的指标出发，结合关节级权重和任务条件来构建CSEF，并将其集成到与阻抗控制机器人兼容的基于梯度的规划器中。

Result: 在2-自由度基准测试中，基于CSEF的规划相较于基于任务空间的人体工程学规划器实现了更高的成功率、更低的人体工程学成本以及更快的计算速度。硬件实验表明，相比点对点基线，在单臂引导、协作钻孔及双臂共同搬运任务中，基于CSEF的规划方法能够更快速地降低人体工程学成本，更接近于优化后的关节目标定位，并减少了关键肌群的激活程度。

Conclusion: 基于CSEF的规划方法在实际应用中显示出明显优势，对于协作钻孔任务平均人体工程学评分下降了10.31%，而对于双臂共同搬运任务则下降了5.60%，同时减少了关键肌群的激活，表明其在现实世界部署中有实际益处。

Abstract: Industrial human-robot collaboration requires motion planning that is collision-free, responsive, and ergonomically safe to reduce fatigue and musculoskeletal risk. We propose the Configuration Space Ergonomic Field (CSEF), a continuous and differentiable field over the human joint space that quantifies ergonomic quality and provides gradients for real-time ergonomics-aware planning. An efficient algorithm constructs CSEF from established metrics with joint-wise weighting and task conditioning, and we integrate it into a gradient-based planner compatible with impedance-controlled robots. In a 2-DoF benchmark, CSEF-based planning achieves higher success rates, lower ergonomic cost, and faster computation than a task-space ergonomic planner. Hardware experiments with a dual-arm robot in unimanual guidance, collaborative drilling, and bimanual cocarrying show faster ergonomic cost reduction, closer tracking to optimized joint targets, and lower muscle activation than a point-to-point baseline. CSEF-based planning method reduces average ergonomic scores by up to 10.31% for collaborative drilling tasks and 5.60% for bimanual co-carrying tasks while decreasing activation in key muscle groups, indicating practical benefits for real-world deployment.

</details>


### [10] [SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry](https://arxiv.org/abs/2512.14189)
*Johannes A. Gaus,Daniel Häufle,Woo-Jeong Baek*

Main category: cs.RO

TL;DR: 本文提出了一种名为SUPER的框架，该框架通过敏感性传播不确定性以进行VIO中的实时风险评估。它能够预测轨迹退化并启动停止或重新定位策略，且在长时程映射中也具有应用性。


<details>
  <summary>Details</summary>
Motivation: 尽管许多视觉里程计（VO）、视觉惯性里程计（VIO）和SLAM系统达到了很高的精度，但大多数现有方法未能在运行时评估风险。

Method: 开发了SUPER（基于敏感性的不确定性感知性能与风险评估）框架，该框架利用Gauss-Newton正规矩阵的Schur补块来传播不确定性，并基于残差大小、几何条件及短期时间趋势估计风险。

Result: 相比基线，SUPER能够在50帧前可靠地预测轨迹退化，准确率提高了20%；对于触发停止或重定位策略，其召回率达到89.1%。此外，该框架对后端无特定要求，且实际运行时CPU额外开销低于0.2%。

Conclusion: 实验表明，SUPER提供了连贯的不确定性估计，并且适用于长时程映射，证明了其作为VIO系统中有效风险评估工具的价值。

Abstract: While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime. This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO. The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties. Practically, the Schur complement captures the sensitivity that reflects the influence of the uncertainty on the risk occurrence. Our framework estimates risks on the basis of the residual magnitudes, geometric conditioning, and short horizon temporal trends without requiring ground truth knowledge. Our framework enables to reliably predict trajectory degradation 50 frames ahead with an improvement of 20% to the baseline. In addition, SUPER initiates a stop or relocalization policy with 89.1% recall. The framework is backend agnostic and operates in real time with less than 0.2% additional CPU cost. Experiments show that SUPER provides consistent uncertainty estimates. A SLAM evaluation highlights the applicability to long horizon mapping.

</details>


### [11] [Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments](https://arxiv.org/abs/2512.14206)
*Mayank Sewlia,Christos K. Verginis,Dimos V. Dimarogonas*

Main category: cs.RO

TL;DR: 本文提出了一种多速率规划和控制框架，用于解决在障碍物众多且高度受限的环境中，由移动多操作器系统执行协同操作的问题。通过离线生成满足STL的对象轨迹和无碰撞基座足迹，结合在线约束逆运动学和连续时间反馈控制，实现了多个操作器在跟踪期望对象运动时的协调重构。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决在充满障碍物和狭窄通道的高度受限环境中，移动多操作器系统如何协同搬运物体的问题。这一问题需要同时考虑连续的机器人动力学以及由障碍物引起的离散几何约束。

Method: 为了解决这个问题，作者提出了一个多速率规划与控制框架。该框架包括两部分：一是离线阶段，生成符合STL（信号时序逻辑）要求的目标物路径及无障碍基底轨迹；二是在线阶段，实施带约束条件下的逆向运动学计算和连续时间内的反馈控制策略。

Result: 通过使用三个Franka Emika Panda移动操作器牢固抓取一个物体进行高保真物理仿真测试，验证了所提方法的有效性。结果显示，该闭合回路系统能够使多个操作臂在追踪指定物体运动的同时实现协调重组。

Conclusion: 本文介绍的方法为在复杂环境下实现多移动操作器协同工作提供了一个有效的解决方案，特别是在需要精确控制与避障的情况下。

Abstract: We consider the problem of cooperative manipulation by a mobile multi-manipulator system operating in obstacle-cluttered and highly constrained environments under spatio-temporal task specifications. The task requires transporting a grasped object while respecting both continuous robot dynamics and discrete geometric constraints arising from obstacles and narrow passages. To address this hybrid structure, we propose a multi-rate planning and control framework that combines offline generation of an STL-satisfying object trajectory and collision-free base footprints with online constrained inverse kinematics and continuous-time feedback control. The resulting closed-loop system enables coordinated reconfiguration of multiple manipulators while tracking the desired object motion. The approach is evaluated in high-fidelity physics simulations using three Franka Emika Panda mobile manipulators rigidly grasping an object.

</details>


### [12] [CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics](https://arxiv.org/abs/2512.14270)
*Zixin Tang,Yiming Chen,Quentin Rouxel,Dianxi Li,Shuang Wu,Fei Chen*

Main category: cs.RO

TL;DR: 本文提出了一种名为CaFe-TeleVision的粗到细远程操作系统，它通过沉浸式现场可视化技术提高了人体工程学性能。该系统在六个具有挑战性的双手机械操作任务中进行了验证，并且用户研究表明它显著降低了任务负荷，同时在成功率和完成时间上超越了其他比较方法。


<details>
  <summary>Details</summary>
Motivation: 尽管最近有所进展，但当前的远程操作系统在效率和人体工程学方面仍存在局限性，特别是在挑战性场景下。为了解决这些问题并提高远程控制与机器人本体感觉数据收集的效果，提出了CaFe-TeleVision系统。

Method: CaFe-TeleVision采用了从粗到细的控制机制来解决工作空间差异问题，并结合了按需现场可视化技术以减少多视图处理时的认知负担。该系统基于类人协作机器人构建，并通过一系列复杂的双手机械操作任务进行测试。

Result: 用户研究显示，CaFe-TeleVision显著改善了人体工程学特性，降低了任务执行难度，并提高了用户的接受度。定量结果表明，在六个测试任务中，该系统的成功率比对比方法高出至多28.89%，并且完成时间加快了26.81%。

Conclusion: CaFe-TeleVision提供了一种新的远程操作方案，通过引入粗到细控制机制及沉浸式可视化反馈有效提升了远程操控的人体工学体验及效率。

Abstract: Teleoperation presents a promising paradigm for remote control and robot proprioceptive data collection. Despite recent progress, current teleoperation systems still suffer from limitations in efficiency and ergonomics, particularly in challenging scenarios. In this paper, we propose CaFe-TeleVision, a coarse-to-fine teleoperation system with immersive situated visualization for enhanced ergonomics. At its core, a coarse-to-fine control mechanism is proposed in the retargeting module to bridge workspace disparities, jointly optimizing efficiency and physical ergonomics. To stream immersive feedback with adequate visual cues for human vision systems, an on-demand situated visualization technique is integrated in the perception module, which reduces the cognitive load for multi-view processing. The system is built on a humanoid collaborative robot and validated with six challenging bimanual manipulation tasks. User study among 24 participants confirms that CaFe-TeleVision enhances ergonomics with statistical significance, indicating a lower task load and a higher user acceptance during teleoperation. Quantitative results also validate the superior performance of our system across six tasks, surpassing comparative methods by up to 28.89% in success rate and accelerating by 26.81% in completion time. Project webpage: https://clover-cuhk.github.io/cafe_television/

</details>


### [13] [ARCADE: Adaptive Robot Control with Online Changepoint-Aware Bayesian Dynamics Learning](https://arxiv.org/abs/2512.14331)
*Rishabh Dev Yadav,Avirup Das,Hongyu Song,Samuel Kaski,Wei Pan*

Main category: cs.RO

TL;DR: 提出了一种能够实时从流数据中更新的非线性动态建模框架，适用于机器人系统在不断变化的操作条件下运行。该方法将表征学习与在线适应解耦，并引入了一个基于数据似然性的变点感知机制来处理持续或突然的变化。实验验证表明，此框架比相关基线具有更高的预测准确性、更快的恢复速度和更精确的闭环跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的机器人必须在由操作条件变化、外部干扰和未建模效应引起的演变动力学下工作。这些变化可能表现为逐渐漂移、短暂波动或突然转变，要求具备既能应对短期变动又能响应长期变化的实时适应能力。

Method: 提出一种新的框架，用于对可以实时从流数据中更新的机器人系统的非线性动力学进行建模。该方法将离线学习到的潜在表示用于支持在线闭式贝叶斯更新。为了处理变化的情况，还介绍了一种带有从数据似然性推断出的潜变量的变点感知机制，以指示连续性或转变。

Result: 通过证明，该框架的自适应遗憾仅随时间对数增长，且随着转变次数线性增加，这与知道转变时机的理想情况相比具有竞争力。在模拟倒立摆实验以及实际四旋翼飞行器携带摇摆载荷和飞行中掉落的情况下进行了验证，显示了比相关基线更好的预测精度、更快的恢复速度和更准确的闭环跟踪。

Conclusion: 所提出的框架能够在保持校准不确定性的同时支持关于瞬态、渐进或结构性变化的概率推理，为机器人系统提供了有效应对复杂动态环境的方法。

Abstract: Real-world robots must operate under evolving dynamics caused by changing operating conditions, external disturbances, and unmodeled effects. These may appear as gradual drifts, transient fluctuations, or abrupt shifts, demanding real-time adaptation that is robust to short-term variation yet responsive to lasting change. We propose a framework for modeling the nonlinear dynamics of robotic systems that can be updated in real time from streaming data. The method decouples representation learning from online adaptation, using latent representations learned offline to support online closed-form Bayesian updates. To handle evolving conditions, we introduce a changepoint-aware mechanism with a latent variable inferred from data likelihoods that indicates continuity or shift. When continuity is likely, evidence accumulates to refine predictions; when a shift is detected, past information is tempered to enable rapid re-learning. This maintains calibrated uncertainty and supports probabilistic reasoning about transient, gradual, or structural change. We prove that the adaptive regret of the framework grows only logarithmically in time and linearly with the number of shifts, competitive with an oracle that knows timings of shift. We validate on cartpole simulations and real quadrotor flights with swinging payloads and mid-flight drops, showing improved predictive accuracy, faster recovery, and more accurate closed-loop tracking than relevant baselines.

</details>


### [14] [Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments](https://arxiv.org/abs/2512.14340)
*Aleksi Karhunen,Teemu Hakala,Väinö Karjalainen,Eija Honkavaara*

Main category: cs.RO

TL;DR: 该研究实现了一个基于轻量级激光雷达的自主飞行四旋翼无人机，并在真实森林环境中对其进行了测试。经过93次飞行测试，优化后的系统在中等密度和密集森林中的成功率分别为12/15和15/15（目标速度1m/s）以及12/15和5/15（目标速度2m/s）。此外，提出了标准化测试设置和评估标准，以促进同类系统的性能比较、可重复性和技术进步。


<details>
  <summary>Details</summary>
Motivation: 尽管无人飞行器(UAV)在林冠上方的飞行已达到高度自主化水平，但其在林下环境中的导航仍面临重大挑战。使用自主UAV可以减少数据收集的工作负担，这激发了对林下自主飞行解决方案的研究兴趣。然而，现有文献中的实验及其报告缺乏严谨性，很少有关于测试森林的密度与难度信息或多次飞行尝试及其成功率的数据。本研究旨在通过开发一个基于轻量级激光雷达并采用开源算法支持的自主飞行四旋翼无人机来填补这一空白。

Method: 研究者们首先构建了一个利用IPC路径规划器和LTA-OM SLAM算法的四旋翼无人机原型，并在真实的森林环境中进行了一系列严格的实验。根据前33次飞行的结果反馈，团队进一步改进了系统设计。最终，在完成总共93次试飞后，他们不仅证明了所提方案的有效性，还提出了一套标准化的测试框架及评价指标体系。

Result: 优化后的无人机系统展现了更高的可靠性和更快的任务完成时间，在中等密度森林里以1米/秒的目标速度飞行时成功率为80%，而在密集森林中则达到了100%；当目标速度提高到2米/秒时，这两个场景下的成功率分别降为80%和33.3%。

Conclusion: 研究表明，基于轻量级激光雷达与特定SLAM算法的自主飞行四旋翼无人机能够有效地在不同密度的森林环境中执行任务。提出的标准化测试方法和评价标准有助于未来类似系统之间的性能对比，促进了研究领域的可重复性和技术创新。

Abstract: The interest in the usage of uncrewed aerial vehicles (UAVs) for forest applications has increased in recent years. While above-canopy flight has reached a high level of autonomy, navigating under-canopy remains a significant challenge. The use of autonomous UAVs could reduce the burden of data collection, which has motivated the development of numerous solutions for under-canopy autonomous flight. However, the experiments conducted in the literature and their reporting lack rigor. Very rarely, the density and the difficulty of the test forests are reported, or multiple flights are flown, and the success rate of those flights is reported. The aim of this study was to implement an autonomously flying quadrotor based on a lightweight lidar using openly available algorithms and test its behavior in real forest environments. A set of rigorous experiments was conducted with a quadrotor prototype utilizing the IPC path planner and LTA-OM SLAM algorithm. Based on the results of the first 33 flights, the original system was further enhanced. With the optimized system, 60 flights were performed, resulting in a total of 93 test flights. The optimized system performed significantly better in terms of reliability and flight mission completion times, achieving success rates of 12/15 in a medium-density forest and 15/15 in a dense forest, at a target flight velocity of 1 m/s. At a target flight velocity of 2 m/s, it had a success rate of 12/15 and 5/15, respectively. Furthermore, a standardized testing setup and evaluation criteria were proposed, enabling consistent performance comparisons of autonomous under-canopy UAV systems, enhancing reproducibility, guiding system improvements, and accelerating progress in forest robotics.

</details>


### [15] [CoLD Fusion: A Real-time Capable Spline-based Fusion Algorithm for Collective Lane Detection](https://arxiv.org/abs/2512.14355)
*Jörg Gamerdinger,Sven Teufel,Georg Volk,Oliver Bringmann*

Main category: cs.RO

TL;DR: 本文提出了一种基于样条估计的实时集体感知车道的方法，通过车辆间通信扩展了局部感知能力，将感知范围提高了200%。


<details>
  <summary>Details</summary>
Motivation: 在许多情况下，由于传感器范围有限、遮挡和弯道等因素，无法完全感知其他物体或车道。对于无法精确定位的情况或者没有高清地图的道路，自动驾驶汽车必须仅依赖其感知到的道路信息。因此，通过车辆间通信利用集体感知来扩展局部感知能力是一个有前景但尚未被探索用于车道检测的策略。

Method: 本文提出了一种能够实现实时处理的集体感知车道方法，该方法基于未检测道路段的样条估计。

Result: 提出的融合算法在各种情况和道路类型下进行了评估，实现了实时性能，并且将感知范围扩大了高达200%。

Conclusion: 这项研究展示了一种有效的集体感知方法来提高自动驾驶汽车的车道检测能力，特别是在那些传统手段难以达到良好效果的情况下。

Abstract: Comprehensive environment perception is essential for autonomous vehicles to operate safely. It is crucial to detect both dynamic road users and static objects like traffic signs or lanes as these are required for safe motion planning. However, in many circumstances a complete perception of other objects or lanes is not achievable due to limited sensor ranges, occlusions, and curves. In scenarios where an accurate localization is not possible or for roads where no HD maps are available, an autonomous vehicle must rely solely on its perceived road information. Thus, extending local sensing capabilities through collective perception using vehicle-to-vehicle communication is a promising strategy that has not yet been explored for lane detection. Therefore, we propose a real-time capable approach for collective perception of lanes using a spline-based estimation of undetected road sections. We evaluate our proposed fusion algorithm in various situations and road types. We were able to achieve real-time capability and extend the perception range by up to 200%.

</details>


### [16] [A Comprehensive Safety Metric to Evaluate Perception in Autonomous Systems](https://arxiv.org/abs/2512.14367)
*Georg Volk,Jörg Gamerdinger,Alexander von Bernuth,Oliver Bringmann*

Main category: cs.RO

TL;DR: 提出了一种新的安全度量标准，该标准综合考虑了目标的速度、方向、距离、大小以及潜在碰撞损害等因素，为自动驾驶汽车的目标感知提供了一个易于理解的安全评估分数，并通过真实世界和虚拟数据集进行了评估。


<details>
  <summary>Details</summary>
Motivation: 现有的目标感知评估指标未能充分考虑到不同目标因其速度、方向、距离、大小或因未检测到而可能引发的碰撞损害差异所带来的不同重要性，因此需要一种能更全面地反映这些因素的新安全评价体系。

Method: 开发了一种新安全度量方法，它将目标的速度、方向、距离、大小及潜在碰撞损害等参数整合进一个单一且容易解读的安全评估分数中。

Result: 这种新提出的度量标准在处理来自真实世界与虚拟环境的数据时显示出了其有效性，并与当前最先进的一些度量方法进行了对比分析。

Conclusion: 研究表明，所提出的新安全度量能够更有效地评估自动驾驶车辆周围环境中的目标感知安全性，为提升整体驾驶安全提供了有力支持。

Abstract: Complete perception of the environment and its correct interpretation is crucial for autonomous vehicles. Object perception is the main component of automotive surround sensing. Various metrics already exist for the evaluation of object perception. However, objects can be of different importance depending on their velocity, orientation, distance, size, or the potential damage that could be caused by a collision due to a missed detection. Thus, these additional parameters have to be considered for safety evaluation. We propose a new safety metric that incorporates all these parameters and returns a single easily interpretable safety assessment score for object perception. This new metric is evaluated with both real world and virtual data sets and compared to state of the art metrics.

</details>


### [17] [Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids](https://arxiv.org/abs/2512.14411)
*Mohammed Ayman Habib,Aldo Petruzzelli*

Main category: cs.RO

TL;DR: Omnia利用合成数据驱动的管道加速军事化人形机器人的训练、验证和部署准备，通过将第一人称空间观察转换为特定任务的合成数据集，并结合自动标注和模型训练，实现快速迭代感知、导航和决策能力，支持在复杂竞争环境中更快的开发周期和改进的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了加速军事化人形机器人的训练、验证以及部署准备过程，同时减少实地试验带来的成本、风险和时间限制。

Method: 通过收集来自第一视角记录、智能眼镜、增强现实头盔等设备的空间观察数据，将其转换成可扩展的任务特定合成数据集；生成大量高保真模拟场景并与自动标注及模型训练相结合。

Result: 能够快速迭代感知、导航和决策能力而无需依赖于广泛的实际现场测试；生成的数据集可以针对新的操作环境和威胁条件进行快速调整，支持基础的人形机器人性能以及高级子系统如多模态感应、反侦测生存能力和CBRNE相关的侦察行为。

Conclusion: 该工作旨在通过早期发展阶段就让人形系统暴露于广泛的场景多样性中，从而在复杂且充满挑战的环境中促进更快的研发周期和提升系统的鲁棒性。

Abstract: Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabilities without the cost, risk, or time constraints of extensive field trials. The resulting datasets can be tuned quickly for new operational environments and threat conditions, supporting both baseline humanoid performance and advanced subsystems such as multimodal sensing, counter-detection survivability, and CBRNE-relevant reconnaissance behaviors. This work targets faster development cycles and improved robustness in complex, contested settings by exposing humanoid systems to broad scenario diversity early in the development process.

</details>


### [18] [Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations](https://arxiv.org/abs/2512.14428)
*Aaron Kurda,Simon Steuernagel,Lukas Jung,Marcus Baum*

Main category: cs.RO

TL;DR: 提出了Odyssey数据集，专为GNSS信号缺失环境（如隧道、停车场）设计的LIO数据集，使用了基于环形激光陀螺仪的惯性导航系统来提供精确的真实轨迹，适用于长时间研究。


<details>
  <summary>Details</summary>
Motivation: 现有的LIO和SLAM系统依赖于GNSS作为基础，但在遮挡环境中GNSS信号可能变得不可靠。虽然一些数据集通过结合IMU测量值来补偿这种不足，但这些解决方案并不适合长期研究GNSS信号缺失的情况。

Method: 创建了一个名为Odyssey的数据集，它专注于GNSS信号缺失环境，并且利用了配备有环形激光陀螺仪(RLG)的导航级惯性导航系统(INS)，以提供比现有数据集中使用的IMU更稳定的真实轨迹。此外，该数据集还支持其他任务，比如地点识别等。

Result: Odyssey成为首个公开可用的基于RLG-INS的数据集，能够在诸如隧道和停车场等GNSS信号缺失环境下进行长时间且准确的研究。同时，它也为其他相关任务提供了支持。

Conclusion: 通过引入Odyssey数据集，研究人员现在能够更好地评估和发展在GNSS信号缺失条件下的LIO和SLAM系统性能。

Abstract: The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth. The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal. While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments. To close this gap, we present Odyssey, a LIO dataset with a focus on GNSS-denied environments such as tunnels and parking garages as well as other underrepresented, yet ubiquitous situations such as stop-and-go-traffic, bumpy roads and wide open fields. Our ground truth is derived from a navigation-grade Inertial Navigation System (INS) equipped with a Ring Laser Gyroscope (RLG), offering exceptional bias stability characteristics compared to IMUs used in existing datasets and enabling the prolonged and accurate study of GNSS-denied environments. This makes Odyssey the first publicly available dataset featuring a RLG-based INS. Besides providing data for LIO, we also support other tasks, such as place recognition, through the threefold repetition of all trajectories as well as the integration of external mapping data by providing precise geodetic coordinates. All data, dataloader and other material is available online at https://odyssey.uni-goettingen.de/ .

</details>


### [19] [Geometric Parameter Optimization of a Novel 3-(PP(2-(UPS))) Redundant Parallel Mechanism based on Workspace Determination](https://arxiv.org/abs/2512.14434)
*Quan Yuan,Daqian Cao,Weibang Bai*

Main category: cs.RO

TL;DR: 本文提出了一种新型3-(PP(2-(UPS)))冗余并联机构，并通过定义扭转能力指数TI_1和倾斜能力指数TI_2来评估该机构的方向性能。通过数值模拟研究，为这种及类似冗余并联机构的参数优化提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 为了满足比传统并联机器人更高的精度、负载能力和工作空间要求，本文针对冗余并联机器人的基础配置及其几何参数优化难题提出了解决方案。

Method: 首先提出了一个通用性强的3-(PP(2-(UPS)))冗余并联机制模型；接着分析了关键几何参数对工作空间体积、形状、边界完整性和方向能力的影响；定义了两个新的性能指标：扭转能力指数(TI_1)和倾斜能力指数(TI_2)，用于评价机构的方向表现；最后通过数值仿真验证了这些分析结果。

Result: 研究表明，所提出的3-(PP(2-(UPS)))机构具有良好的工作空间特性和方向性能；通过调整特定的几何参数可以有效改善其工作空间属性；此外，新定义的能力指数为评估冗余并联机构的方向性能提供了一个实用的方法。

Conclusion: 本文不仅介绍了一种新颖且具广泛适用性的冗余并联机构设计，还深入探讨了几何参数对该类机构工作空间特性的影响，并通过引入创新的性能评估指标，为相关领域的参数优化提供了有价值的见解。

Abstract: Redundant parallel robots are normally employed in scenarios requiring good precision, high load capability, and large workspace compared to traditional parallel mechanisms. However, the elementary robotic configuration and geometric parameter optimization are still quite challenging. This paper proposes a novel 3-(PP(2-(UPS))) redundant parallel mechanism, with good generalizability first, and further investigates the kinematic optimization issue by analyzing and investigating how its key geometric parameters influence the volume, shape, boundary completeness, and orientation capabilities of its workspace. The torsional capability index TI_1 and tilting capability index TI_2 are defined to evaluate the orientation performance of the mechanism. Numerical simulation studies are completed to indicate the analysis, providing reasonable but essential references for the parameter optimization of 3-(PP(2-(UPS))) and other similar redundant parallel mechanisms.

</details>


### [20] [EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models](https://arxiv.org/abs/2512.14666)
*Zechen Bai,Chen Gao,Mike Zheng Shou*

Main category: cs.RO

TL;DR: 本文提出了一种名为EVOLVE-VLA的测试时训练框架，它使视觉-语言-动作模型能够在与环境交互过程中持续适应，几乎不需要或完全不需要特定任务的演示。通过学习进度估计器提供密集反馈，并采用两种机制来处理噪声信号：累积进度估计机制和平滑点估计、渐进式视野扩展策略以实现政策逐步演变。实验结果表明，在长周期任务和一次性学习上均有显著提升，并且在没有特定任务演示的情况下对未见过的任务也表现出了成功。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言-动作（VLA）模型虽然利用了大型语言模型来提高机器人操控能力，但仍然受限于监督微调方法，需要大量演示数据，而且无法很好地应对部署条件与训练条件不符的情况。为了克服这些限制，研究者们希望能够开发出一种能够通过与环境互动而不断自我改进的智能体。

Method: 提出了一个名为EVOLVE-VLA的新框架，该框架允许VLA模型在几乎没有或完全没有针对特定任务的演示情况下，通过与环境的互动连续适应。为了解决缺乏真实奖励信号的问题，使用了一个学习到的进步估计器来提供密集反馈。此外，设计了两种机制来处理这种本质上嘈杂的信号：一种是累积进步评估机制，可以平滑单点估计中的噪音；另一种是逐渐扩展范围的策略，支持策略逐步进化。

Result: EVOLVE-VLA在长周期任务上实现了8.6%的成功率提升，在一次学习中提升了22.0%，并且在没有任何特定任务演示训练的情况下对于新任务也能达到20.8%的成功率。质量分析还揭示了一些在演示中未曾出现的能力，如错误恢复和新颖策略。

Conclusion: 这项工作代表了向真正能够学习和适应的VLA迈出的重要一步，超越了静态模仿，朝着持续自我改进的方向发展。

Abstract: Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.

</details>


### [21] [CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation](https://arxiv.org/abs/2512.14689)
*Sirui Chen,Zi-ang Cao,Zhengyi Luo,Fernando Castañeda,Chenran Li,Tingwu Wang,Ye Yuan,Linxi "Jim" Fan,C. Karen Liu,Yuke Zhu*

Main category: cs.RO

TL;DR: 提出了一种名为CHIP的模块，能够让人形机器人在保持动态参考动作敏捷跟踪的同时，实现可控的末端执行器刚度，从而完成各种需要不同末端执行器顺应性的强力操作任务。


<details>
  <summary>Details</summary>
Motivation: 尽管人形机器人已经在灵巧运动技能方面取得了进展，但它们仍然难以完成如移动物体、擦拭和推车等需要施力的操作任务。因此，研究者们提出了一个简易实施且无需额外数据或奖励调整的方案来解决这一问题。

Method: 通过引入名为CHIP（Compliance Humanoid control through hIsight Perturbation）的即插即用模块，该方法允许在保持对动态参考动作敏捷追踪的同时控制末端执行器的刚度。

Result: 使用CHIP训练的一般性运动跟踪控制器能够执行一系列需要不同末端执行器顺应性的强力操作任务，包括多机器人协作、擦拭、箱子运送及开门等。

Conclusion: CHIP模块为提高人形机器人在执行强力操作任务时的能力提供了一个有效解决方案，同时保持了对复杂动态动作的良好跟随性能。

Abstract: Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.

</details>
