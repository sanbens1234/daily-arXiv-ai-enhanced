<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 30]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Beyond the Vehicle: Cooperative Localization by Fusing Point Clouds for GPS-Challenged Urban Scenarios](https://arxiv.org/abs/2602.03908)
*Kuo-Yi Chao,Ralph Rasshofer,Alois Christian Knoll*

Main category: cs.RO

TL;DR: 本文提出了一种结合车对车(V2V)和车对基础设施(V2I)系统的多传感器、多模式定位方法，通过点云配准的SLAM算法来提高城市环境中车辆定位的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在城市环境中，由于GPS信号常常不可靠，精确的车辆定位是一个关键挑战。为了克服这一问题，文章提出了一个合作式的多传感器和多模式定位方案。

Method: 该方法整合了来自车辆间通信及车辆与基础设施之间的数据，并使用基于点云配准的同时定位与地图构建(SLAM)算法。系统处理来自不同传感器的数据，包括车载激光雷达和立体相机，以及部署在交叉路口处的传感器。

Result: 通过利用基础设施共享的数据，该方法显著提高了复杂且GPS信号嘈杂的城市环境下的定位准确性和鲁棒性。

Conclusion: 本研究提出的合作式多传感器融合定位方法为解决城市环境中GPS信号不稳定导致的车辆定位难题提供了一个有效的解决方案。

Abstract: Accurate vehicle localization is a critical challenge in urban environments where GPS signals are often unreliable. This paper presents a cooperative multi-sensor and multi-modal localization approach to address this issue by fusing data from vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) systems. Our approach integrates cooperative data with a point cloud registration-based simultaneous localization and mapping (SLAM) algorithm. The system processes point clouds generated from diverse sensor modalities, including vehicle-mounted LiDAR and stereo cameras, as well as sensors deployed at intersections. By leveraging shared data from infrastructure, our method significantly improves localization accuracy and robustness in complex, GPS-noisy urban scenarios.

</details>


### [2] [How Users Understand Robot Foundation Model Performance through Task Success Rates and Beyond](https://arxiv.org/abs/2602.03920)
*Isaac Sheidlower,Jindan Huang,James Staley,Bingyu Wu,Qicong Chen,Reuben Aronson,Elaine Short*

Main category: cs.RO

TL;DR: 本文研究了非机器人专家如何理解来自机器人基础模型（RFM）评估的性能信息，特别是任务成功率（TSR），发现非专家用户不仅能够按照预期使用TSR，而且也非常重视诸如失败案例等其他类型的信息。此外，用户希望获得RFM先前评估的真实数据以及机器人对自己执行新任务表现的估计。


<details>
  <summary>Details</summary>
Motivation: 鉴于机器人基础模型（RFMs）拥有广泛的能力，但用户可能会要求它们执行未经过训练或测试的任务。为了确保用户明白尝试这些新任务可能带来的风险，并且了解RFM真正能处理的情境和任务，有必要探究非专业用户是如何理解和利用从RFM评价中得到的表现信息的。

Method: 通过进行一项研究，让用户看到包括任务成功率(TSR)、失败案例描述以及视频在内的真实评估数据，这些数据来源于多个已发布的RFM研究项目。

Result: 研究表明，非专业人士确实会按照专家们的预期那样去利用TSR指标；同时，他们也十分看重那些不常出现在RFM评估报告中的其他类型信息，比如失败案例。另外，用户还希望能够获取到有关RFM过去评估的实际数据以及对于新任务执行效果的一个预估。

Conclusion: 这项工作强调了在向非专业用户传达RFM能力时提供全面信息的重要性，包括更详细地分享失败案例以及对新任务成功可能性做出预测。

Abstract: Robot Foundation Models (RFMs) represent a promising approach to developing general-purpose home robots. Given the broad capabilities of RFMs, users will inevitably ask an RFM-based robot to perform tasks that the RFM was not trained or evaluated on. In these cases, it is crucial that users understand the risks associated with attempting novel tasks due to the relatively high cost of failure. Furthermore, an informed user who understands an RFM's capabilities will know what situations and tasks the robot can handle. In this paper, we study how non-roboticists interpret performance information from RFM evaluations. These evaluations typically report task success rate (TSR) as the primary performance metric. While TSR is intuitive to experts, it is necessary to validate whether novices also use this information as intended. Toward this end, we conducted a study in which users saw real evaluation data, including TSR, failure case descriptions, and videos from multiple published RFM research projects. The results highlight that non-experts not only use TSR in a manner consistent with expert expectations but also highly value other information types, such as failure cases that are not often reported in RFM evaluations. Furthermore, we find that users want access to both real data from previous evaluations of the RFM and estimates from the robot about how well it will do on a novel task.

</details>


### [3] [VLS: Steering Pretrained Robot Policies via Vision-Language Models](https://arxiv.org/abs/2602.03973)
*Shuo Liu,Ishneet Sukhvinder Singh,Yiqing Xu,Jiafei Duan,Ranjay Krishna*

Main category: cs.RO

TL;DR: 提出了一种无需训练的框架Vision-Language Steering (VLS)，用于在推理时自适应调整已冻结的生成型机器人策略，通过视觉-语言模型合成轨迹可微奖励函数来指导去噪过程，以满足测试时的空间和任务要求。


<details>
  <summary>Details</summary>
Motivation: 预训练的扩散或流匹配策略在执行靠近障碍物、在偏移的支持面上或轻微杂乱环境中的相同任务时失败，这揭示了模仿学习在训练-测试迁移下的局限性。重新训练或微调成本高昂且概念上不一致。

Method: 提出了Vision-Language Steering (VLS)框架，在不修改策略参数的情况下，通过响应超出分布的观察-语言输入来引导预训练扩散或流匹配策略的采样过程。利用视觉-语言模型合成轨迹可微奖励函数，指导去噪朝向满足测试时间空间和任务需求的动作轨迹。

Result: 在模拟和实际评估中，VLS始终优于先前的转向方法，在CALVIN上实现了31%的改进，在LIBERO-PRO上获得了13%的增益。Franka机器人的实际部署进一步证明了其在测试时间空间和语义变化下的强大推理时间适应能力。

Conclusion: VLS为解决预训练策略在面对新的环境或任务条件时的适应问题提供了一个有效的解决方案，无需额外的训练或微调。

Abstract: Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation of imitation learning under train-test shifts, where action generation is tightly coupled to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively adapted at test time. We propose Vision-Language Steering (VLS), a training-free framework for inference-time adaptation of frozen generative robot policies. VLS treats adaptation as an inference-time control problem, steering the sampling process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation-language inputs without modifying policy parameters. By leveraging vision-language models to synthesize trajectory-differentiable reward functions, VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements. Across simulation and real-world evaluations, VLS consistently outperforms prior steering methods, achieving a 31% improvement on CALVIN and a 13% gain on LIBERO-PRO. Real-world deployment on a Franka robot further demonstrates robust inference-time adaptation under test-time spatial and semantic shifts. Project page: https://vision-language-steering.github.io/webpage/

</details>


### [4] [Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement](https://arxiv.org/abs/2602.03983)
*Weikang Qiu,Tinglin Huang,Aosong Feng,Rex Ying*

Main category: cs.RO

TL;DR: 本文提出了一种名为SD-VLA的框架，通过将视觉输入分解为多级静态和动态标记来解决VLA模型面临的长期上下文有限和推理效率低下的问题。实验结果表明，该方法在新基准上比基线提高了39.8%的成功率，在SimplerEnv基准上提高了3.9%，并且在相同基准上实现了2.26倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision-Language-Action (VLA) 模型虽然在各种任务中表现出色，但面临两个主要挑战：有限的长期视野背景以及由于二次注意力复杂性和大量参数导致的低效推理。研究者们观察到轨迹中的许多视觉信息（例如背景）在不同时间步是保持不变的，基于这一特性提出了改进方案。

Method: 提出了SD-VLA框架，它将视觉输入分解为多层次的静态与动态token，使得(1)可以在帧间保留一份静态token以显著减少上下文长度；(2)通过轻量级的重缓存门仅在必要时更新静态token的键值(KV)缓存。此外，还引入了一个新的评估基准，更有效地测试了VLAs对长时间依赖关系建模的能力。

Result: 实验结果显示，所提方法在这个新基准上的成功率相比基线提高了39.8%，在SimplerEnv基准上也获得了3.9%的增长。另外，SD-VLA相比基础VLA模型在同一基准下实现了2.26倍的推理加速。

Conclusion: SD-VLA框架通过有效利用视觉信息中的静态部分，成功解决了VLA模型中存在的长期视野限制和低效推理问题，不仅提升了任务执行的成功率，同时也大大加快了推理速度，为实际应用提供了更佳的选择。

Abstract: Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment.

</details>


### [5] [FDA Flocking: Future Direction-Aware Flocking via Velocity Prediction](https://arxiv.org/abs/2602.04012)
*Hossein B. Jond,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出了一种基于生物启发的预测性群集模型，称为未来方向感知（FDA）群集。该模型结合了反应性对齐与基于邻居未来速度短期估计的预测项，通过可调节参数平衡反应性和预测性行为。仿真结果表明，FDA相比纯反应性模型，在一致性、群集平移位移以及对延迟和噪声的鲁棒性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 受到鸟类姿态和翼拍信号以及多旋翼无人机在改变方向前的姿态倾斜的启发，为了增强协调性并克服现有模型中忽略预判线索的问题。

Method: 提出了一个名为未来方向感知（FDA）群集的框架，其中个体不仅进行反应式对齐还结合了一个基于邻居未来速度短期估计的预测项，通过可调参数来平衡反应式与预测式行为之间的关系。

Result: 模拟实验结果显示，FDA相较于纯反应性模型能够实现更快更高的对齐度、更好的群集平移位移效果，并且对于传感及通信延迟和测量噪声具有更强的鲁棒性。

Conclusion: 未来方向感知（FDA）群集提供了一种新的方法以提高群集机器人的协调性能；未来的研究将探索自适应混合策略、加权预测方案以及在多旋翼无人机群上的实际验证。

Abstract: Understanding self-organization in natural collectives such as bird flocks inspires swarm robotics, yet most flocking models remain reactive, overlooking anticipatory cues that enhance coordination. Motivated by avian postural and wingbeat signals, as well as multirotor attitude tilts that precede directional changes, this work introduces a principled, bio-inspired anticipatory augmentation of reactive flocking termed Future Direction-Aware (FDA) flocking. In the proposed framework, agents blend reactive alignment with a predictive term based on short-term estimates of neighbors' future velocities, regulated by a tunable blending parameter that interpolates between reactive and anticipatory behaviors. This predictive structure enhances velocity consensus and cohesion-separation balance while mitigating the adverse effects of sensing and communication delays and measurement noise that destabilize reactive baselines. Simulation results demonstrate that FDA achieves faster and higher alignment, enhanced translational displacement of the flock, and improved robustness to delays and noise compared to a purely reactive model. Future work will investigate adaptive blending strategies, weighted prediction schemes, and experimental validation on multirotor drone swarms.

</details>


### [6] [An Anatomy-specific Guidewire Shaping Robot for Improved Vascular Navigation](https://arxiv.org/abs/2602.04050)
*Aabha Tamhankar,Jay Patil,Giovanni Pittiglio*

Main category: cs.RO

TL;DR: 本文介绍了一种能够根据需要自动成形导丝的机器人，以辅助神经内血管手术中的导丝定位。通过实验数据校准的模型可以将所需的导丝形状转换为机器人的动作，并且能够在2D和3D中生成临床常见的导丝尖端几何形状。该机器人还展示了从岩骨段颈内动脉到后交通动脉复杂内腔导航的能力。


<details>
  <summary>Details</summary>
Motivation: 当前神经内血管通路常依赖于手动塑形的微导丝，这不仅过程复杂，尤其在解剖结构迂回时，而且高度依赖外科医生的经验水平。为了实现标准化的自主塑形，研究者开发了这种机器人系统。

Method: 研究人员开发了一个工作台顶导丝塑形机器人，它能够产生针对特定导航需求的导丝配置。他们提出了一种可以通过实验数据进行校准的模型，用于将所需的导丝形状映射到具体的机器人操作上。此外，还在2D与3D环境下验证了机器人生成常见导丝尖端几何形状（如C型、S型、角度型及钩型）的能力。

Result: 机器人能够准确地生产出临床上常见的几种导丝尖端形状，并且这些形状与模型预测相吻合，在所有形状上的平均误差仅为0.56毫米。进一步测试显示，该系统还能完成从岩骨段颈内动脉至后交通动脉之间复杂的内腔导航任务。

Conclusion: 这项研究表明，所提出的机器人系统能有效地辅助神经内血管介入手术，提供一种新的方法来实现导丝的精确控制和标准化制作，从而有可能提高手术效率并减少对医生经验的依赖。

Abstract: Neuroendovascular access often relies on passive microwires that are hand-shaped at the back table and then used to track a microcatheter to the target. Neuroendovascular surgeons determine the shape of the wire by examining the patient pre-operative images and using their experience to identify anatomy specific shapes of the wire that would facilitate reaching the target. This procedure is particularly complex in convoluted anatomical structures and is heavily dependent on the level of expertise of the surgeon. Towards enabling standardized autonomous shaping, we present a bench-top guidewire shaping robot capable of producing navigation-specific desired wire configurations. We present a model that can map the desired wire shape into robot actions, calibrated using experimental data. We show that the robot can produce clinically common tip geometries (C, S, Angled, Hook) and validate them with respect to the model-predicted shapes in 2D. Our model predicts the shape with a Root Mean Square (RMS) error of 0.56mm across all shapes when compared to the experimental results. We also demonstrate 3D tip shaping capabilities and the ability to traverse complex endoluminal navigation from the petrous Internal Carotid Artery (ICA) to the Posterior Communicating Artery (PComm).

</details>


### [7] [Control and State Estimation of Vehicle-Mounted Aerial Systems in GPS-Denied, Non-Inertial Environments](https://arxiv.org/abs/2602.04057)
*Riming Xu,Obadah Wali,Yasmine Marani,Eric Feron*

Main category: cs.RO

TL;DR: 提出了一种基于外部位置测量和扩展卡尔曼滤波器处理未知输入（EKF-UI）的鲁棒控制与估计框架，专门针对GNSS缺乏且惯性传感器不可靠的非惯性环境中的四旋翼飞行器。该方法不依赖IMU或GNSS，而是通过结合高精度动作捕捉系统来提高在移动平台如卡车或电梯上的稳定性及轨迹跟踪能力。


<details>
  <summary>Details</summary>
Motivation: 在GNSS信号缺失并且由于平台引起的加速度导致惯性传感器变得不可靠的非惯性环境中，传统的估计器无法准确区分测得的加速度是来自四旋翼本身还是非惯性平台，从而引起漂移和控制性能下降。为了解决这一问题，本文提出了一种新的方法。

Method: 本研究的方法完全依赖于外部的位置测量，并采用了带有未知输入的扩展卡尔曼滤波器(EKF-UI)来考虑平台运动的影响。该估计器与一个级联PID控制器配对，以实现全3D跟踪。所有测试都使用了高精度的动作捕捉系统来进行，以隔离定位误差对估计器性能的影响。

Result: 在一个移动小车测试平台上进行的实验结果验证了所提方法的有效性，无论是在X轴还是Y轴方向上的位移情况下都能保持良好表现。相比于标准EKF，所提出的方法在不需要惯性反馈的情况下显著提升了稳定性和轨迹跟踪性能。

Conclusion: 这项工作展示了一种创新的方法，使得四旋翼飞行器能够在没有GNSS覆盖且惯性传感器不可靠的动态环境中有效运行。它不仅提高了稳定性还增强了轨迹跟踪能力，为四旋翼飞行器在诸如卡车或电梯等移动平台上的实际应用铺平了道路。

Abstract: We present a robust control and estimation framework for quadrotors operating in Global Navigation Satellite System(GNSS)-denied, non-inertial environments where inertial sensors such as Inertial Measurement Units (IMUs) become unreliable due to platform-induced accelerations. In such settings, conventional estimators fail to distinguish whether the measured accelerations arise from the quadrotor itself or from the non-inertial platform, leading to drift and control degradation. Unlike conventional approaches that depend heavily on IMU and GNSS, our method relies exclusively on external position measurements combined with a Extended Kalman Filter with Unknown Inputs (EKF-UI) to account for platform motion. The estimator is paired with a cascaded PID controller for full 3D tracking. To isolate estimator performance from localization errors, all tests are conducted using high-precision motion capture systems. Experimental results in a moving-cart testbed validate our approach under both translational in X-axis and Y-axis dissonance. Compared to standard EKF, the proposed method significantly improves stability and trajectory tracking without requiring inertial feedback, enabling practical deployment on moving platforms such as trucks or elevators.

</details>


### [8] [Comparative Analysis of Autonomous Robotic and Manual Techniques for Ultrasonic Sacral Osteotomy: A Preliminary Study](https://arxiv.org/abs/2602.04076)
*Daniyal Maroufi,Yash Kulkarni,Justin E. Bird,Jeffrey H. Siewerdsen,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 本文介绍了一种自主超声骶骨截骨机器人系统，该系统将超声骨刀与七自由度机械臂结合，并由光学跟踪系统引导。实验结果显示，机器人系统在轨迹精度和切割深度控制方面明显优于手动操作，为更安全、更精确的骶骨切除手术奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够克服手动骶骨截骨术关键限制的自主机器人系统，以提高手术的安全性和精确性。

Method: 通过整合超声骨刀与一个七自由度机械臂，并使用光学跟踪系统进行指导，构建了一个自主超声骶骨截骨（USO）机器人系统。为了评估该系统的多方向控制能力，在相同的截骨条件下对人工USO(MUSO)与机器人USO(RUSO)进行了定量比较。

Result: RUSO系统实现了亚毫米级的轨迹精度(0.11 mm RMSE)，比MUSO(1.10 mm RMSE)提高了约一个数量级。此外，MUSO试验显示出显著的过度穿透(实际达到16.0 mm vs. 目标8.0 mm)，而RUSO系统保持了精准的深度控制(8.1 mm)。

Conclusion: 机器人辅助手术可以有效克服手动骶骨截骨术的关键局限性，为实现更加安全和准确的骶骨切除提供了可能。

Abstract: In this paper, we introduce an autonomous Ultrasonic Sacral Osteotomy (USO) robotic system that integrates an ultrasonic osteotome with a seven-degree-of-freedom (DoF) robotic manipulator guided by an optical tracking system. To assess multi-directional control along both the surface trajectory and cutting depth of this system, we conducted quantitative comparisons between manual USO (MUSO) and robotic USO (RUSO) in Sawbones phantoms under identical osteotomy conditions. The RUSO system achieved sub-millimeter trajectory accuracy (0.11 mm RMSE), an order of magnitude improvement over MUSO (1.10 mm RMSE). Moreover, MUSO trials showed substantial over-penetration (16.0 mm achieved vs. 8.0 mm target), whereas the RUSO system maintained precise depth control (8.1 mm). These results demonstrate that robotic procedures can effectively overcome the critical limitations of manual osteotomy, establishing a foundation for safer and more precise sacral resections.

</details>


### [9] [Shaping Expressiveness in Robotics: The Role of Design Tools in Crafting Embodied Robot Movements](https://arxiv.org/abs/2602.04137)
*Elisabetta Zibetti,Alexandra Mercader,Hélène Duval,Florent Levillain,Audrey Rochette,David St-Onge*

Main category: cs.RO

TL;DR: 本文介绍了一种以动作为中心的设计教学法，旨在支持工程师创建具有表现力的机器人手臂动作。通过结合舞蹈分析框架和专门的动画软件，该方法有效地连接了人类意图与机器人表现力之间的差距，从而产生了更直观且吸引人的机器人手臂动作。


<details>
  <summary>Details</summary>
Motivation: 随着机器人越来越多地进入共享的人类空间，它们的动作需要超越基本功能，融入表达性品质以增强参与度和交流。

Method: 本文提出的方法包括一个基于动手互动工作坊的教学法，该工作坊借鉴跨学科方法论，并使用了一个定制的手动遥控器来实现对机械臂的交互式实时操控。此外，还利用了专用动画软件来支持可视化、详细的运动序列化以及精确参数控制。

Result: 通过对这种互动设计过程进行定性分析，发现所提出的“工具箱”能够有效地填补人类意图与机器人表现力之间的空白，产生更加直观和吸引人的表现型机器人手臂动作。

Conclusion: 本研究提出的方法为工程师提供了创造富有表现力的机器人手臂动作的有效途径，促进了人机之间更加自然流畅的沟通。

Abstract: As robots increasingly become part of shared human spaces, their movements must transcend basic functionality by incorporating expressive qualities to enhance engagement and communication. This paper introduces a movement-centered design pedagogy designed to support engineers in creating expressive robotic arm movements. Through a hands-on interactive workshop informed by interdisciplinary methodologies, participants explored various creative possibilities, generating valuable insights into expressive motion design. The iterative approach proposed integrates analytical frameworks from dance, enabling designers to examine motion through dynamic and embodied dimensions. A custom manual remote controller facilitates interactive, real-time manipulation of the robotic arm, while dedicated animation software supports visualization, detailed motion sequencing, and precise parameter control. Qualitative analysis of this interactive design process reveals that the proposed "toolbox" effectively bridges the gap between human intent and robotic expressiveness resulting in more intuitive and engaging expressive robotic arm movements.

</details>


### [10] [A Modern System Recipe for Situated Embodied Human-Robot Conversation with Real-Time Multimodal LLMs and Tool-Calling](https://arxiv.org/abs/2602.04157)
*Dong Won Lee,Sarah Gillet,Louis-Philippe Morency,Cynthia Breazeal,Hae Won Park*

Main category: cs.RO

TL;DR: 研究介绍了一种结合实时多模态语言模型与注意力及主动感知工具接口的系统，用于提升机器人在情境化对话中的表现。通过六个家庭场景的研究表明，这种系统在提高交互质量和正确性方面具有潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决机器人在进行情境化对话时面临的挑战，即如何在严格的延迟限制下交错进行实时对话和主动感知（决定看什么、何时看以及说什么）。

Method: 研究设计了一个简单的系统方案，将一个实时多模态语言模型与一套小型注意力及主动感知工具接口相结合，并在四个系统变体上进行了评估，包括对回合级工具决策正确性的评价以及收集关于互动质量的主观评分。

Result: 结果显示，使用实时多模态大型语言模型与主动感知工具相结合的方法，在实践中的情境化对话领域展现出了良好的前景。

Conclusion: 结合实时多模态语言模型与主动感知能力为机器人在复杂环境下的有效沟通提供了一个有希望的方向。

Abstract: Situated embodied conversation requires robots to interleave real-time dialogue with active perception: deciding what to look at, when to look, and what to say under tight latency constraints. We present a simple, minimal system recipe that pairs a real-time multimodal language model with a small set of tool interfaces for attention and active perception. We study six home-style scenarios that require frequent attention shifts and increasing perceptual scope. Across four system variants, we evaluate turn-level tool-decision correctness against human annotations and collect subjective ratings of interaction quality. Results indicate that real-time multimodal large language models and tool use for active perception is a promising direction for practical situated embodied conversation.

</details>


### [11] [SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models](https://arxiv.org/abs/2602.04208)
*Hyeonbeom Choi,Daechul Ahn,Youhan Lee,Taewook Kang,Seongwon Cho,Jonghyun Choi*

Main category: cs.RO

TL;DR: 提出了一种名为SCALE的推理策略，用于改进视觉-语言-动作模型在机器人控制中的性能。该方法基于自我不确定性同时调整视觉感知和动作选择，无需额外训练或验证步骤，仅需一次前向传播即可实现，在不确定情况下扩大探索范围，在有信心时专注于利用，从而适应不同条件下的执行。实验表明，SCALE提高了现有VLA模型的表现，并且优于现有的测试时间缩放方法，同时保持了单次传递效率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作（VLA）模型虽然在通用机器人控制方面展现出潜力，但为了提高超出训练环境之外的鲁棒性所采用的测试时间缩放（TTS）方法存在需要额外训练、验证器以及多次前向传递的问题，这使得它们难以实际部署。此外，这些方法只在动作解码阶段进行干预而忽视了感知层面的变化，这在面对感知模糊时显得不足，因为在这种情况下重新考虑如何感知与决定做什么同样重要。

Method: 提出了SCALE，这是一种基于“自我不确定性”的简单推理策略，灵感来源于主动推理理论中的不确定性驱动探索。SCALE不需要任何额外的训练过程、验证机制或者多次前向传递就能共同调节视觉感知与行动决策。当面临高度不确定性时，SCALE能够拓宽感知和行动上的探索范围；而在信心较高时，则侧重于利用已有的信息来做出决策，这样可以适应多种变化条件下的任务执行。

Result: 通过模拟及真实世界基准测试显示，SCALE不仅超越了现有的VLA模型表现，而且相比其他测试时间缩放方法也展现出了更好的效果，同时还保持了单次传递的高效性。

Conclusion: SCALE作为一种新颖且高效的推理策略，成功解决了现有VLA模型在应用测试时间缩放技术时遇到的实际部署难题，通过动态地调整对环境的理解和响应方式，即使是在充满不确定性的场景下也能实现更加灵活可靠的机器人控制。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.

</details>


### [12] [ALORE: Autonomous Large-Object Rearrangement with a Legged Manipulator](https://arxiv.org/abs/2602.04214)
*Zhihai Bi,Yushan Zhang,Kai Chen,Guoyang Zhao,Yulin Li,Jun Ma*

Main category: cs.RO

TL;DR: 本文介绍了一种名为ALORE的自主大型物体重新排列系统，该系统专为腿部操作器设计，能够在多种场景中高效地重新排列各种大型物体。通过层次强化学习训练管道、统一交互配置表示和物体速度估计器模块以及任务与运动规划框架，ALORE能够实现对多物体环境的学习及稳定操作。实验表明，该系统在策略泛化、物体速度跟踪精度和多物体重排效率方面均优于强基线方法。


<details>
  <summary>Details</summary>
Motivation: 赋予机器人重新排列诸如家具等大而重物品的能力可以显著减轻人类的工作负担。然而，这项任务非常具有挑战性，因为需要与多样化的物体进行互动，并在复杂的环境中有效地重新排列多个物体，同时确保无碰撞的移动-操控。

Method: 提出了ALORE系统，它包括三个主要特征：(i) 一个多对象环境学习的层次强化学习训练流程，其中高级别的物体速度控制器建立在低级别的全身控制器之上，以实现跨多个物体的有效且稳定的联合学习；(ii) 两个关键模块——统一交互配置表示和物体速度估算器，使单一策略能够准确调节不同物体的平面速度；(iii) 一个任务与运动规划框架，用于共同优化物体访问顺序和物体到目标的分配，在提高任务效率的同时支持在线重新规划。

Result: 与强大的基准相比，ALORE在政策概括能力、物体速度追踪准确性以及多物体重新排列效率上显示出一致的优势。此外，通过广泛的模拟和实际测试验证了整个系统的鲁棒性和有效性，成功地在近40分钟内连续完成8个循环，重新排列了32把椅子，没有发生任何故障，并执行了大约40米路线上的长距离自动重新排列。

Conclusion: 本研究提出的ALORE系统证明了其在复杂环境下有效执行多物体重新排列任务的能力，展示了其在政策泛化、精确控制及任务效率方面的优势。

Abstract: Endowing robots with the ability to rearrange various large and heavy objects, such as furniture, can substantially alleviate human workload. However, this task is extremely challenging due to the need to interact with diverse objects and efficiently rearrange multiple objects in complex environments while ensuring collision-free loco-manipulation. In this work, we present ALORE, an autonomous large-object rearrangement system for a legged manipulator that can rearrange various large objects across diverse scenarios. The proposed system is characterized by three main features: (i) a hierarchical reinforcement learning training pipeline for multi-object environment learning, where a high-level object velocity controller is trained on top of a low-level whole-body controller to achieve efficient and stable joint learning across multiple objects; (ii) two key modules, a unified interaction configuration representation and an object velocity estimator, that allow a single policy to regulate planar velocity of diverse objects accurately; and (iii) a task-and-motion planning framework that jointly optimizes object visitation order and object-to-target assignment, improving task efficiency while enabling online replanning. Comparisons against strong baselines show consistent superiority in policy generalization, object-velocity tracking accuracy, and multi-object rearrangement efficiency. Key modules are systematically evaluated, and extensive simulations and real-world experiments are conducted to validate the robustness and effectiveness of the entire system, which successfully completes 8 continuous loops to rearrange 32 chairs over nearly 40 minutes without a single failure, and executes long-distance autonomous rearrangement over an approximately 40 m route. The open-source packages are available at https://zhihaibi.github.io/Alore/.

</details>


### [13] [OAT: Ordered Action Tokenization](https://arxiv.org/abs/2602.04215)
*Chaoqi Liu,Xiaoshen Han,Jiawei Gao,Yue Zhao,Haonan Chen,Yilun Du*

Main category: cs.RO

TL;DR: 本文提出了一种名为有序动作标记化（OAT）的方法，该方法通过变换器与寄存器、有限标量量化和顺序诱导训练机制将动作块离散成有序的标记序列。此方法在超过20个任务中表现出色，优于先前的标记化方案和基于扩散的基线，并且在推理时提供了更大的灵活性。


<details>
  <summary>Details</summary>
Motivation: 自回归策略为可扩展的机器人学习提供了一个有吸引力的基础，但应用于连续机器人动作需要有效的动作标记化方案。现有方法要么产生过长的标记序列，要么缺乏结构，限制了它们与下一个标记预测的兼容性。因此，研究旨在开发一种满足高压缩率、完全解码能力和从左到右因果排序标记空间这三个要求的动作标记化方法。

Method: 提出了Ordered Action Tokenization (OAT)，这是一种学习型动作标记器，它利用变换器与寄存器、有限标量量化以及促进顺序性的训练机制来将动作片段转化为有序的标记序列。生成的标记空间自然地与自回归生成相匹配，并允许基于前缀的去标记化，从而在推理成本和动作保真度之间实现随时权衡。

Result: 在四个模拟基准测试和现实世界设置中的超过20项任务上，装备了OAT的自回归策略始终优于以前的标记化方案和基于扩散的基线，在推理时还提供了显著更大的灵活性。

Conclusion: OAT作为一种新的动作标记化方案，成功解决了连续机器人动作高效编码的问题，不仅提高了执行效率，也增强了与自回归模型的兼容性，为未来机器人学习领域的研究开辟了新路径。

Abstract: Autoregressive policies offer a compelling foundation for scalable robot learning by enabling discrete abstraction, token-level reasoning, and flexible inference. However, applying autoregressive modeling to continuous robot actions requires an effective action tokenization scheme. Existing approaches either rely on analytical discretization methods that produce prohibitively long token sequences, or learned latent tokenizers that lack structure, limiting their compatibility with next-token prediction. In this work, we identify three desiderata for action tokenization - high compression, total decodability, and a left-to-right causally ordered token space - and introduce Ordered Action Tokenization (OAT), a learned action tokenizer that satisfies all three. OAT discretizes action chunks into an ordered sequence of tokens using transformer with registers, finite scalar quantization, and ordering-inducing training mechanisms. The resulting token space aligns naturally with autoregressive generation and enables prefix-based detokenization, yielding an anytime trade-off between inference cost and action fidelity. Across more than 20 tasks spanning four simulation benchmarks and real-world settings, autoregressive policies equipped with OAT consistently outperform prior tokenization schemes and diffusion-based baselines, while offering significantly greater flexibility at inference time.

</details>


### [14] [Reshaping Action Error Distributions for Reliable Vision-Language-Action Models](https://arxiv.org/abs/2602.04228)
*Shuanghao Bai,Dakai Wang,Cheng Chi,Wanqi Zhou,Jing Lyu,Xiaoguang Zhao,Pengwei Wang,Zhongyuan Wang,Lei Xing,Shanghang Zhang,Badong Chen*

Main category: cs.RO

TL;DR: 本文提出了一种新的训练方法，通过在视觉-语言-动作（VLA）模型中引入最小误差熵（MEE）目标函数，改善了连续动作预测的准确性与鲁棒性。实验结果表明，该方法不仅提高了标准、少量样本和噪声环境下的成功率和鲁棒性，而且在不平衡数据条件下也表现出色，同时对训练成本和推理效率影响极小。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言-动作(VLA)框架主要依赖于传统的监督学习目标，如交叉熵损失用于离散动作预测，均方误差(MSE)用于连续动作回归，这些方法对单个预测施加了很强的点对点约束。本文旨在超越常规的基于MSE的回归方法，通过重塑动作错误分布来改进连续动作VLA模型的学习效果。

Method: 本文引入了信息论原理中的最小误差熵(MEE)概念，并将其整合到现代VLA架构中，提出了一个轨迹级别的MEE目标函数及其两种加权变体，与MSE结合使用以训练连续动作VLA模型。

Result: 实验评估显示，在标准设置、少量样本情况以及存在噪声的环境中，所提方法能够一致地提高多个代表性VLA架构的成功率和鲁棒性。即使是在数据不平衡的情况下，该方法仍然能够在特定的操作范围内保持性能增益，且几乎不增加额外的训练成本或影响推理效率。

Conclusion: 通过将最小误差熵(MEE)引入到连续动作VLA模型的训练过程中，可以有效地提升模型的准确性和鲁棒性。此外，理论分析进一步解释了为什么基于MEE的监督是有效的，并描述了其实用范围。

Abstract: In robotic manipulation, vision-language-action (VLA) models have emerged as a promising paradigm for learning generalizable and scalable robot policies. Most existing VLA frameworks rely on standard supervised objectives, typically cross-entropy for discrete actions and mean squared error (MSE) for continuous action regression, which impose strong pointwise constraints on individual predictions. In this work, we focus on continuous-action VLA models and move beyond conventional MSE-based regression by reshaping action error distributions during training. Drawing on information-theoretic principles, we introduce Minimum Error Entropy (MEE) into modern VLA architectures and propose a trajectory-level MEE objective, together with two weighted variants, combined with MSE for continuous-action VLA training. We evaluate our approaches across standard, few-shot, and noisy settings on multiple representative VLA architectures, using simulation benchmarks such as LIBERO and SimplerEnv as well as real-world robotic manipulation tasks. Experimental results demonstrate consistent improvements in success rates and robustness across these settings. Under imbalanced data regimes, the gains persist within a well-characterized operating range, while incurring negligible additional training cost and no impact on inference efficiency. We further provide theoretical analyses that explain why MEE-based supervision is effective and characterize its practical range. Project Page: https://cognition2actionlab.github.io/VLA-TMEE.github.io/

</details>


### [15] [GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning](https://arxiv.org/abs/2602.04231)
*Rui Tang,Guankun Wang,Long Bai,Huxin Gao,Jiewen Lai,Chi Kit Ng,Jiazheng Wang,Fan Zhang,Hongliang Ren*

Main category: cs.RO

TL;DR: 提出了一种端到端的多任务框架GeoLanG，基于CLIP架构统一了视觉和语言输入，通过深度引导几何模块（DGGM）和自适应密集通道整合技术来提高在杂乱、遮挡或低纹理场景中的目标识别与抓取性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于多阶段管道将物体感知和抓取分开处理，这导致跨模态融合有限、计算冗余，并且在杂乱、遮挡或低纹理场景中泛化能力差。为了解决这些问题，作者提出了GeoLanG框架。

Method: GeoLanG是一个基于CLIP架构构建的端到端多任务框架，它将视觉和语言输入统一到一个共享表示空间中以实现鲁棒的语义对齐和改进的泛化能力。此外，通过引入深度引导几何模块（DGGM）转换深度信息为显式的几何先验并将其注入注意力机制中，以及采用自适应密集通道整合来自适应地平衡多层特征贡献，从而生成更具辨别性和泛化性的视觉表示。

Result: 在OCID-VLG数据集上进行了广泛的实验，并且在仿真环境和真实硬件上也进行了测试，结果表明GeoLanG能够在复杂且杂乱的环境中实现精确而鲁棒的语言指导下的抓取，为现实世界以人为中心的多模态机器人操作铺平了道路。

Conclusion: GeoLanG通过有效利用深度信息及自适应调整多层特征的重要性，展示了其在处理杂乱、遮挡或低纹理场景下进行语言指导抓取任务时的优势，为实现更可靠的人机交互式机器人操作提供了新的解决方案。

Abstract: Language-guided grasping has emerged as a promising paradigm for enabling robots to identify and manipulate target objects through natural language instructions, yet it remains highly challenging in cluttered or occluded scenes. Existing methods often rely on multi-stage pipelines that separate object perception and grasping, which leads to limited cross-modal fusion, redundant computation, and poor generalization in cluttered, occluded, or low-texture scenes. To address these limitations, we propose GeoLanG, an end-to-end multi-task framework built upon the CLIP architecture that unifies visual and linguistic inputs into a shared representation space for robust semantic alignment and improved generalization. To enhance target discrimination under occlusion and low-texture conditions, we explore a more effective use of depth information through the Depth-guided Geometric Module (DGGM), which converts depth into explicit geometric priors and injects them into the attention mechanism without additional computational overhead. In addition, we propose Adaptive Dense Channel Integration, which adaptively balances the contributions of multi-layer features to produce more discriminative and generalizable visual representations. Extensive experiments on the OCID-VLG dataset, as well as in both simulation and real-world hardware, demonstrate that GeoLanG enables precise and robust language-guided grasping in complex, cluttered environments, paving the way toward more reliable multimodal robotic manipulation in real-world human-centric settings.

</details>


### [16] [Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation](https://arxiv.org/abs/2602.04243)
*Pengfei Yi,Yifan Han,Junyan Li,Litao Liu,Wenzhao Lian*

Main category: cs.RO

TL;DR: 提出了MAE-Select框架，用于单摄像头机器人系统中的主动视角选择，通过利用预训练的多视图掩码自动编码器表征，在每个时间块动态选择最信息量丰富的视角，而无需标记视角。实验表明，该方法提高了单摄像头系统的性能，甚至在某些情况下超越了多摄像头设置。


<details>
  <summary>Details</summary>
Motivation: 当前的模仿学习方法通常依赖于固定摄像机设置，这限制了适应性和覆盖范围。受人类主动感知启发，人们可以通过动态调整视角来捕捉最相关和噪声最少的信息，因此需要一种新的框架来改进机器人系统中的视角选择机制。

Method: 开发了一个名为MAE-Select的新框架，它充分利用了预先训练好的多视图掩码自动编码器表示，并能够不依靠标签信息就从一系列可能的视角中动态挑选出下一个最具信息价值的视角。

Result: 广泛的实验证明，MAE-Select增强了单一摄像头系统的功能，在特定场景下其表现甚至优于多个摄像头组成的系统。

Conclusion: MAE-Select提供了一种有效的手段来改善单摄像头机器人系统的视角选取问题，通过模拟人类主动感知过程达到了提高系统灵活性与效率的目的。

Abstract: Robotic manipulation continues to be a challenge, and imitation learning (IL) enables robots to learn tasks from expert demonstrations. Current IL methods typically rely on fixed camera setups, where cameras are manually positioned in static locations, imposing significant limitations on adaptability and coverage. Inspired by human active perception, where humans dynamically adjust their viewpoint to capture the most relevant and least noisy information, we propose MAE-Select, a novel framework for active viewpoint selection in single-camera robotic systems. MAE-Select fully leverages pre-trained multi-view masked autoencoder representations and dynamically selects the next most informative viewpoint at each time chunk without requiring labeled viewpoints. Extensive experiments demonstrate that MAE-Select improves the capabilities of single-camera systems and, in some cases, even surpasses multi-camera setups. The project will be available at https://mae-select.github.io.

</details>


### [17] [Towards Next-Generation SLAM: A Survey on 3DGS-SLAM Focusing on Performance, Robustness, and Future Directions](https://arxiv.org/abs/2602.04251)
*Li Wang,Ruixuan Gong,Yumo Han,Lei Yang,Lu Yang,Ying Li,Bin Xu,Huaping Liu,Rong Fu*

Main category: cs.RO

TL;DR: 该综述探讨了3D高斯点绘（3DGS）与SLAM系统集成的关键技术方法，从渲染质量、跟踪精度、重建速度和内存消耗四个方面分析了代表性方法的性能优化，并讨论了如何提高在复杂环境下的鲁棒性以及未来挑战和发展趋势。


<details>
  <summary>Details</summary>
Motivation: 传统的SLAM系统面临诸如渲染质量粗糙、场景细节恢复不足以及在动态环境中鲁棒性差等问题。3DGS以其高效的显式表示和高质量的渲染能力为SLAM提供了一种新的重建范式。

Method: 本研究通过综合回顾将3DGS与SLAM集成的技术手段，分析了代表方法在渲染质量、跟踪准确性、重建速度及内存使用四个关键维度上的表现优化，同时深入探究了设计原理和突破点。此外，还考察了增强3DGS-SLAM在诸如运动模糊和动态环境等复杂条件下的鲁棒性的方法。

Result: 综述揭示了不同方法在性能优化方面的优劣，指出了当前技术方案对于提高SLAM系统真实感、效率和鲁棒性的潜力，并为未来研究提供了方向。

Conclusion: 这项调查旨在为研究人员提供技术参考，促进具有高保真度、高效性和鲁棒性的下一代SLAM系统的发展。

Abstract: Traditional Simultaneous Localization and Mapping (SLAM) systems often face limitations including coarse rendering quality, insufficient recovery of scene details, and poor robustness in dynamic environments. 3D Gaussian Splatting (3DGS), with its efficient explicit representation and high-quality rendering capabilities, offers a new reconstruction paradigm for SLAM. This survey comprehensively reviews key technical approaches for integrating 3DGS with SLAM. We analyze performance optimization of representative methods across four critical dimensions: rendering quality, tracking accuracy, reconstruction speed, and memory consumption, delving into their design principles and breakthroughs. Furthermore, we examine methods for enhancing the robustness of 3DGS-SLAM in complex environments such as motion blur and dynamic environments. Finally, we discuss future challenges and development trends in this area. This survey aims to provide a technical reference for researchers and foster the development of next-generation SLAM systems characterized by high fidelity, efficiency, and robustness.

</details>


### [18] [AppleVLM: End-to-end Autonomous Driving with Advanced Perception and Planning-Enhanced Vision-Language Models](https://arxiv.org/abs/2602.04256)
*Yuxuan Han,Kunyuan Wu,Qianyi Shao,Renxiang Xiao,Zilu Wang,Cansen Jiang,Yi Xiao,Liang Hu,Yunjiang Lou*

Main category: cs.RO

TL;DR: 提出了AppleVLM，一种增强的视觉-语言模型，通过引入新的视觉编码器和规划策略编码器来提高端到端自动驾驶在不同场景下的鲁棒性。该模型在CARLA基准测试中表现优异，并在AGV平台上成功展示了复杂户外环境中的真实世界端到端自动驾驶能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉-语言模型（VLMs）的方法在车道感知、语言理解偏差以及处理边缘情况方面仍面临挑战。为了克服这些问题并提高端到端驾驶模型在多样化及未见场景中的鲁棒性和泛化能力。

Method: 开发了AppleVLM，它包含一个新颖的视觉编码器用于融合多视角图像的时间空间信息，以及一个专门的规划模态来编码鸟瞰视角的空间信息，减少导航指令中的语言偏误。此外，采用层次化的思维链对VLM解码器进行微调，以整合视觉、语言与规划特征输出稳健的驾驶路径点。

Result: AppleVLM在两个CARLA基准上的闭环实验中达到了最先进的驾驶性能。同时，在AGV平台上的部署也证明了其能够在复杂的户外环境中实现真实的端到端自主驾驶。

Conclusion: AppleVLM为解决现有基于VLM方法面临的挑战提供了一个有效的解决方案，并通过实验证明了其在提高自动驾驶系统鲁棒性方面的潜力。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm integrating perception, decision-making, and control within a unified learning framework. Recently, Vision-Language Models (VLMs) have gained significant attention for their potential to enhance the robustness and generalization of end-to-end driving models in diverse and unseen scenarios. However, existing VLM-based approaches still face challenges, including suboptimal lane perception, language understanding biases, and difficulties in handling corner cases. To address these issues, we propose AppleVLM, an advanced perception and planning-enhanced VLM model for robust end-to-end driving. AppleVLM introduces a novel vision encoder and a planning strategy encoder to improve perception and decision-making. Firstly, the vision encoder fuses spatial-temporal information from multi-view images across multiple timesteps using a deformable transformer mechanism, enhancing robustness to camera variations and facilitating scalable deployment across different vehicle platforms. Secondly, unlike traditional VLM-based approaches, AppleVLM introduces a dedicated planning modality that encodes explicit Bird's-Eye-View spatial information, mitigating language biases in navigation instructions. Finally, a VLM decoder fine-tuned by a hierarchical Chain-of-Thought integrates vision, language, and planning features to output robust driving waypoints. We evaluate AppleVLM in closed-loop experiments on two CARLA benchmarks, achieving state-of-the-art driving performance. Furthermore, we deploy AppleVLM on an AGV platform and successfully showcase real-world end-to-end autonomous driving in complex outdoor environments.

</details>


### [19] [Quantile Transfer for Reliable Operating Point Selection in Visual Place Recognition](https://arxiv.org/abs/2602.04401)
*Dhyey Manish Rajani,Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 提出了一种根据用户定义的精度要求自动选择视觉地点识别系统操作点以最大化召回率的方法，通过小规模校准遍历和相似度分数分布的分位数标准化来转移阈值，从而在不同环境和操作条件下保持性能稳定。实验表明该方法相比现有技术可提高高达25%的召回率。


<details>
  <summary>Details</summary>
Motivation: 视觉地点识别（VPR）在GPS不可用环境中的定位非常重要，但其性能严重依赖于图像匹配阈值的选择，而这个阈值通常是针对特定环境手动调整并固定的，导致环境变化时性能下降。

Method: 本文提出了一种新方法，在给定用户定义的精度需求下，自动选取VPR系统的操作点以最大化召回率。该方法基于一小段具有已知对应关系的校准轨迹，并通过相似度得分分布的分位数归一化将阈值转移到部署中。

Result: 实验结果显示，所提方法在多个最先进VPR技术和数据集上表现一致优于现有技术水平，特别是在高精度操作模式下，能实现高达25%的召回率提升。

Conclusion: 该方法消除了手动调优的需求，能够适应新环境并在不同操作条件下通用，展示了其在实际应用中的潜力。

Abstract: Visual Place Recognition (VPR) is a key component for localisation in GNSS-denied environments, but its performance critically depends on selecting an image matching threshold (operating point) that balances precision and recall. Thresholds are typically hand-tuned offline for a specific environment and fixed during deployment, leading to degraded performance under environmental change. We propose a method that, given a user-defined precision requirement, automatically selects the operating point of a VPR system to maximise recall. The method uses a small calibration traversal with known correspondences and transfers thresholds to deployment via quantile normalisation of similarity score distributions. This quantile transfer ensures that thresholds remain stable across calibration sizes and query subsets, making the method robust to sampling variability. Experiments with multiple state-of-the-art VPR techniques and datasets show that the proposed approach consistently outperforms the state-of-the-art, delivering up to 25% higher recall in high-precision operating regimes. The method eliminates manual tuning by adapting to new environments and generalising across operating conditions. Our code will be released upon acceptance.

</details>


### [20] [HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation](https://arxiv.org/abs/2602.04412)
*Puyue Wang,Jiawei Hu,Yan Gao,Junyan Wang,Yu Zhang,Gillian Dobbie,Tao Gu,Wafa Johal,Ting Dang,Hong Jia*

Main category: cs.RO

TL;DR: 提出了一种名为HoRD的两阶段学习框架，用于在动态变化下提高人形机器人的鲁棒控制性能。通过历史条件强化学习训练高性能教师策略，并通过在线蒸馏将这种能力转移到基于变压器的学生策略中，从而实现对未见领域的零样本适应。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在动力学、任务规格或环境设置发生微小变化时性能显著下降的问题。

Method: 1. 通过历史条件强化学习训练一个高性能的教师策略，该策略能够从最近的状态-动作轨迹推断出潜在的动力学背景，以适应多样化的随机动力学。
2. 对基于transformer的学生策略执行在线蒸馏，使其能够利用稀疏根相对3D关节关键点轨迹进行操作。
结合历史条件适应与在线蒸馏，使单一策略能够在没有每个领域重新训练的情况下零样本适应未见过的领域。

Result: 广泛的实验表明，HoRD在鲁棒性和迁移性方面优于强大的基线方法，特别是在面对未知领域和外部扰动时表现突出。

Conclusion: 本研究提出的HoRD框架有效提升了人形机器人在不同环境下的自适应能力和鲁棒性，为开发更灵活的人形机器人控制系统提供了新的解决方案。

Abstract: Humanoid robots can suffer significant performance drops under small changes in dynamics, task specifications, or environment setup. We propose HoRD, a two-stage learning framework for robust humanoid control under domain shift. First, we train a high-performance teacher policy via history-conditioned reinforcement learning, where the policy infers latent dynamics context from recent state--action trajectories to adapt online to diverse randomized dynamics. Second, we perform online distillation to transfer the teacher's robust control capabilities into a transformer-based student policy that operates on sparse root-relative 3D joint keypoint trajectories. By combining history-conditioned adaptation with online distillation, HoRD enables a single policy to adapt zero-shot to unseen domains without per-domain retraining. Extensive experiments show HoRD outperforms strong baselines in robustness and transfer, especially under unseen domains and external perturbations. Code and project page are available at \href{https://tonywang-0517.github.io/hord/}{https://tonywang-0517.github.io/hord/}.

</details>


### [21] [Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning](https://arxiv.org/abs/2602.04419)
*Heqing Yang,Ziyuan Jiao,Shu Wang,Yida Niu,Si Liu,Hangxin Liu*

Main category: cs.RO

TL;DR: 提出了一种名为EPoG的框架，它结合了基于图的全局规划器和基于大型语言模型（LLM）的情境局部规划器，能够在部分已知环境中实现探索与任务规划的有效结合。通过46个现实家庭场景和5项长期日常物体运输任务的消融研究，EPoG达成了91.3%的成功率，并平均减少了36.1%的行进距离。此外，在未知和动态环境中，物理移动操纵器成功执行了复杂任务，展示了EPoG在实际应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 在部分已知的环境中，机器人需要结合探索以收集信息以及任务规划来高效执行工作。为了解决这一挑战，提出了EPoG框架，旨在无缝整合探索与顺序操作规划。

Method: EPoG框架利用基于图的全局规划器与基于大型语言模型（LLM）的情境局部规划器相结合的方法。通过持续更新一个信念图来表示已知和未知的对象，该信念图根据观察结果和LLM预测进行更新。动作序列是通过对目标图和信念图之间计算图编辑操作生成的，按照时间依赖性和移动成本排序。

Result: 经过对46个现实家庭场景和5项长期日常物体运输任务的研究表明，EPoG达到了91.3%的成功率，并且平均减少了36.1%的行进距离。此外，物理移动操纵器在未知和动态环境中成功完成了复杂的任务。

Conclusion: EPoG框架有效地结合了探索与任务规划，适用于未知及动态环境下的复杂任务执行，显示出在真实世界应用中的巨大潜力。

Abstract: In partially known environments, robots must combine exploration to gather information with task planning for efficient execution. To address this challenge, we propose EPoG, an Exploration-based sequential manipulation Planning framework on Scene Graphs. EPoG integrates a graph-based global planner with a Large Language Model (LLM)-based situated local planner, continuously updating a belief graph using observations and LLM predictions to represent known and unknown objects. Action sequences are generated by computing graph edit operations between the goal and belief graphs, ordered by temporal dependencies and movement costs. This approach seamlessly combines exploration and sequential manipulation planning. In ablation studies across 46 realistic household scenes and 5 long-horizon daily object transportation tasks, EPoG achieved a success rate of 91.3%, reducing travel distance by 36.1% on average. Furthermore, a physical mobile manipulator successfully executed complex tasks in unknown and dynamic environments, demonstrating EPoG's potential for real-world applications.

</details>


### [22] [Gust Estimation and Rejection with a Disturbance Observer for Proprioceptive Underwater Soft Morphing Wings](https://arxiv.org/abs/2602.04438)
*Tobias Cook,Leo Micklem,Huazhi Dong,Yunjie Yang,Michael Mistry,Francesco Giorgio Serchi*

Main category: cs.RO

TL;DR: 受海洋生物应对水流扰动策略的启发，本文提出了一种具备本体感知能力的软变形翼，用于缓解无人水下航行器在浅水区作业时遇到的流体动力学干扰问题。通过建立液压驱动软翼的动态模型并实验验证，研究证明了基于曲率感知可以准确估计攻角扰动，并展示了一个利用这些本体感知估计值来抑制升力响应扰动的控制器。


<details>
  <summary>Details</summary>
Motivation: 为了解决无人水下航行器在执行维护和调查任务时因波浪、洋流等不稳定流动引起的快速方向与速度变化导致稳定性及操控性受损的问题，模仿海洋生物使用自身感觉反馈与柔性鳍尾对抗扰动的方法。

Method: 开发了一种具有可控弯度的液压驱动软翼及其动态模型；通过实验验证该模型，并展示了如何利用基于曲率的感知技术来精确估计攻击角的变化；最后，设计并测试了一个能够利用这些感知信息来抵消软翼升力反应中扰动影响的控制器。

Result: 成功地创建并验证了一个能够反映真实条件下软翼行为的动态模型；证明了通过监测翼面弯曲程度的变化可以有效地评估外部流体环境的变化；实现了对软翼升力响应扰动的有效抑制。

Conclusion: 结合本体感知与扰动观测器的技术不仅模拟了生物学策略，而且为软体水下机器人在危险环境中保持稳定提供了一条可行之路。

Abstract: Unmanned underwater vehicles are increasingly employed for maintenance and surveying tasks at sea, but their operation in shallow waters is often hindered by hydrodynamic disturbances such as waves, currents, and turbulence. These unsteady flows can induce rapid changes in direction and speed, compromising vehicle stability and manoeuvrability. Marine organisms contend with such conditions by combining proprioceptive feedback with flexible fins and tails to reject disturbances. Inspired by this strategy, we propose soft morphing wings endowed with proprioceptive sensing to mitigate environmental perturbations. The wing's continuous deformation provides a natural means to infer dynamic disturbances: sudden changes in camber directly reflect variations in the oncoming flow. By interpreting this proprioceptive signal, a disturbance observer can reconstruct flow parameters in real time. To enable this, we develop and experimentally validate a dynamic model of a hydraulically actuated soft wing with controllable camber. We then show that curvature-based sensing allows accurate estimation of disturbances in the angle of attack. Finally, we demonstrate that a controller leveraging these proprioceptive estimates can reject disturbances in the lift response of the soft wing. By combining proprioceptive sensing with a disturbance observer, this technique mirrors biological strategies and provides a pathway for soft underwater vehicles to maintain stability in hazardous environments.

</details>


### [23] [EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models](https://arxiv.org/abs/2602.04515)
*Yu Bai,MingMing Yu,Chaojie Li,Ziyi Bai,Xinlong Wang,Börje F. Karlsson*

Main category: cs.RO

TL;DR: 提出了EgoActing任务和EgoActor模型，旨在通过整合感知、移动和操作来解决在真实世界环境中部署人形机器人面临的挑战。EgoActor能够根据高层次指令预测移动基元、头部动作、操纵命令及人机交互，并在模拟和真实环境测试中展示了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在真实世界环境中部署人形机器人面临许多挑战，包括在信息不全且环境动态变化的情况下实现感知、移动与操作的紧密结合，以及平滑地在不同类型子任务间切换。为应对这些挑战，本文提出了一种新的任务框架。

Method: 开发了名为EgoActing的新任务，该任务要求将高级别指令直接转化为多种精确的空间感知型人形机器人动作；同时提出了EgoActor模型，这是一种统一且可扩展的视觉-语言模型（VLM），能够从第一人称视角仅基于RGB数据的学习中预测出移动基元、头部运动、操控命令及人机互动行为。

Result: 广泛的评估显示，无论是在仿真还是实际环境中，EgoActor都能够有效地连接抽象的任务规划与具体的电机执行过程，并且能够在多样化的任务和未见过的环境中表现出良好的适应性。

Conclusion: EgoActor作为一个统一的视觉-语言模型，在处理复杂的人形机器人行动任务时展现出了强大的潜力，能够依据高层级指令快速准确地做出反应，并成功应用于多种场景下。

Abstract: Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.

</details>


### [24] [TACO: Temporal Consensus Optimization for Continual Neural Mapping](https://arxiv.org/abs/2602.04516)
*Xunlan Zhou,Hongrui Zhao,Negar Mehr*

Main category: cs.RO

TL;DR: 提出了TACO，一种无需重放历史数据的连续神经映射框架，通过将映射视为时间一致性优化问题来适应环境变化，同时平衡了内存效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于神经隐式映射的方法在实际机器人部署中无法支持对变化环境的持续适应，并且通常假设场景是静态的，这限制了它们在动态机器人设置中的应用。

Method: TACO (TemporAl Consensus Optimization) 是一个不依赖于重放历史观测值的连续学习框架，它将过去模型快照视为时间邻居，并通过强制执行与历史表示的一致性来更新当前地图。该方法允许可靠的旧几何形状约束优化，同时允许不可靠或过时区域根据新观察结果进行修订。

Result: 通过广泛的模拟和真实世界实验表明，TACO能够稳健地适应场景变化，并且在连续学习基准测试中始终优于其他方法。

Conclusion: TACO提供了一种有效的解决方案，能够在满足严格内存和计算限制的同时，使机器人导航系统能够适应不断变化的环境。

Abstract: Neural implicit mapping has emerged as a powerful paradigm for robotic navigation and scene understanding. However, real-world robotic deployment requires continual adaptation to changing environments under strict memory and computation constraints, which existing mapping systems fail to support. Most prior methods rely on replaying historical observations to preserve consistency and assume static scenes. As a result, they cannot adapt to continual learning in dynamic robotic settings. To address these challenges, we propose TACO (TemporAl Consensus Optimization), a replay-free framework for continual neural mapping. We reformulate mapping as a temporal consensus optimization problem, where we treat past model snapshots as temporal neighbors. Intuitively, our approach resembles a model consulting its own past knowledge. We update the current map by enforcing weighted consensus with historical representations. Our method allows reliable past geometry to constrain optimization while permitting unreliable or outdated regions to be revised in response to new observations. TACO achieves a balance between memory efficiency and adaptability without storing or replaying previous data. Through extensive simulated and real-world experiments, we show that TACO robustly adapts to scene changes, and consistently outperforms other continual learning baselines.

</details>


### [25] [A Unified Complementarity-based Approach for Rigid-Body Manipulation and Motion Prediction](https://arxiv.org/abs/2602.04522)
*Bingkun Huang,Xin Ma,Nilanjan Chakraborty,Riddhiman Laha*

Main category: cs.RO

TL;DR: 本文提出了一种统一的离散时间建模框架(Unicomp)，能够一致地捕捉自由运动和摩擦接触，适用于实时优化规划。该模型基于互补刚体动力学，解决了从平面推送到全身接触操作等任务中的稳定性和物理一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人操作规划和仿真框架通常将自由空间运动与环境接触分离处理，或依赖简化的接触表示，特别是在建模非凸或分布式接触区域时。这些近似限制了接触模式转换的真实度，并阻碍了富含接触行为的实时鲁棒执行。

Method: 1. 基于互补性的刚体动力学，将自由空间运动和接触交互作为耦合线性及非线性互补问题进行公式化。
2. 对于平面斑块接触，从最大功率耗散原理出发导出一个摩擦接触模型，其中可接受的接触力矩集合由椭球形极限表面表示。
3. 提出的公式产生了一个离散时间预测模型，通过二次约束关联广义速度与接触力矩，适合实时基于优化的规划。

Result: 实验结果表明，所提出的方法能够在从平面推送到富含接触的全身动作等各种任务中以交互速度实现稳定且物理上一致的行为。

Conclusion: 本文介绍的Unicomp为机器人操作提供了一种新的方法，它能同时准确地模拟自由运动和摩擦接触，无需假设固定接触点，从而支持更加丰富复杂的接触行为实时规划。

Abstract: Robotic manipulation in unstructured environments requires planners to reason jointly about free-space motion and sustained, frictional contact with the environment. Existing (local) planning and simulation frameworks typically separate these regimes or rely on simplified contact representations, particularly when modeling non-convex or distributed contact patches. Such approximations limit the fidelity of contact-mode transitions and hinder the robust execution of contact-rich behaviors in real time. This paper presents a unified discrete-time modeling framework for robotic manipulation that consistently captures both free motion and frictional contact within a single mathematical formalism (Unicomp). Building on complementarity-based rigid-body dynamics, we formulate free-space motion and contact interactions as coupled linear and nonlinear complementarity problems, enabling principled transitions between contact modes without enforcing fixed-contact assumptions. For planar patch contact, we derive a frictional contact model from the maximum power dissipation principle in which the set of admissible contact wrenches is represented by an ellipsoidal limit surface. This representation captures coupled force-moment effects, including torsional friction, while remaining agnostic to the underlying pressure distribution across the contact patch. The resulting formulation yields a discrete-time predictive model that relates generalized velocities and contact wrenches through quadratic constraints and is suitable for real-time optimization-based planning. Experimental results show that the proposed approach enables stable, physically consistent behavior at interactive speeds across tasks, from planar pushing to contact-rich whole-body maneuvers.

</details>


### [26] [Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data](https://arxiv.org/abs/2602.04600)
*Jialiang Li,Yi Qiao,Yunhan Guo,Changwen Chen,Wenzhao Lian*

Main category: cs.RO

TL;DR: 本文提出了一种认知和记忆感知的视觉-语言-动作框架CoMe-VLA，通过利用大规模的人类自我中心数据来学习多样的探索和操作先验，从而实现主动感知。实验表明该方法在多种主动感知场景下具有强大的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 为了实现复杂环境中的通用操作，机器人需要能够积极解决信息不确定性，即拥有主动感知的能力。当前的方法往往受限于有限类型的感知行为，限制了它们在复杂环境中的应用。

Method: 作者将主动感知形式化为一个由信息增益和决策分支驱动的非马尔可夫过程，并提出了CoMe-VLA框架。该框架包括一个用于自主子任务转换的认知辅助头以及一个双轨记忆系统，以融合本体感觉和视觉时间上下文来维持一致的自我与环境意识。模型训练分为三个阶段进行，旨在统一的第一人称视角动作空间中对齐人类和机器人的手眼协调行为。

Result: 广泛的实验显示，在轮式类人机器人上执行的各种长期任务中，所提出的方法在多个主动感知场景下展现了强大的鲁棒性和适应能力。

Conclusion: 通过引入CoMe-VLA框架，本文提供了一种新的方式来增强机器人的主动感知能力，使其能够在更复杂的环境中有效地执行任务。

Abstract: Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios.

</details>


### [27] [Radar-Inertial Odometry For Computationally Constrained Aerial Navigation](https://arxiv.org/abs/2602.04631)
*Jan Michalczyk*

Main category: cs.RO

TL;DR: 本文提出了雷达-惯性里程计(RIO)算法，该算法结合了IMU和低成本FMCW雷达的数据，以实现在资源受限的嵌入式计算机上实时估计无人机(UAV)的导航状态。此外，还展示了一种利用深度学习从稀疏且有噪声的雷达点云中检索3D点对应关系的新方法。


<details>
  <summary>Details</summary>
Motivation: 由于某些外部传感器在极端环境条件下（如极强光照或存在烟雾等微粒）可能无法正常工作，而雷达对这些因素具有较高的抗干扰能力。因此，研究者们旨在开发一种雷达-惯性里程计算法，使用雷达和IMU数据来估计机器人的姿态，特别适用于无人机，并能在资源受限的计算平台上实时运行。

Method: 提出的方法基于多状态紧耦合扩展卡尔曼滤波器(EKF)和因子图(FG)，将低成本、现成的调频连续波(FMCW)雷达提供的瞬时速度与到3D点的距离信息同IMU读数相结合。此外，还介绍了一种新方法，即通过深度学习技术从稀疏且噪声较大的雷达点云中提取3D点对应关系。

Result: 所提出的RIO方法能够有效地融合来自IMU和雷达的信息，以估计无人机的导航状态。实验表明，即使在使用成本低廉的消费级传感器情况下，该系统也能在便携式资源受限的嵌入式计算机上实现实时运行。同时，利用深度学习处理雷达点云的方法也展示了良好的性能。

Conclusion: 本研究成功地开发出了一种新颖的雷达-惯性里程计算法，不仅能够克服传统外感传感器在恶劣环境下失效的问题，而且还能确保在有限计算资源下高效准确地估计无人机的姿态。这为未来开发更加自主、可靠的机器人系统提供了新的可能性。

Abstract: Recently, the progress in the radar sensing technology consisting in the miniaturization of the packages and increase in measuring precision has drawn the interest of the robotics research community. Indeed, a crucial task enabling autonomy in robotics is to precisely determine the pose of the robot in space. To fulfill this task sensor fusion algorithms are often used, in which data from one or several exteroceptive sensors like, for example, LiDAR, camera, laser ranging sensor or GNSS are fused together with the Inertial Measurement Unit (IMU) measurements to obtain an estimate of the navigation states of the robot. Nonetheless, owing to their particular sensing principles, some exteroceptive sensors are often incapacitated in extreme environmental conditions, like extreme illumination or presence of fine particles in the environment like smoke or fog. Radars are largely immune to aforementioned factors thanks to the characteristics of electromagnetic waves they use. In this thesis, we present Radar-Inertial Odometry (RIO) algorithms to fuse the information from IMU and radar in order to estimate the navigation states of a (Uncrewed Aerial Vehicle) UAV capable of running on a portable resource-constrained embedded computer in real-time and making use of inexpensive, consumer-grade sensors. We present novel RIO approaches relying on the multi-state tightly-coupled Extended Kalman Filter (EKF) and Factor Graphs (FG) fusing instantaneous velocities of and distances to 3D points delivered by a lightweight, low-cost, off-the-shelf Frequency Modulated Continuous Wave (FMCW) radar with IMU readings. We also show a novel way to exploit advances in deep learning to retrieve 3D point correspondences in sparse and noisy radar point clouds.

</details>


### [28] [Dull, Dirty, Dangerous: Understanding the Past, Present, and Future of a Key Motivation for Robotics](https://arxiv.org/abs/2602.04746)
*Nozomi Nakajima,Pedro Reynolds-Cuéllar,Caitrin Lynch,Kate Darling*

Main category: cs.RO

TL;DR: 本文对1980年至2024年间提及'枯燥、肮脏、危险'(DDD)工作的机器人学出版物进行了实证分析，发现仅有少数文献定义了DDD或提供了具体例子。文章还回顾了社会科学中关于DDD工作的文献，并提出了一个框架来帮助机器人技术领域考虑其技术的工作背景，鼓励从更知情的角度看待机器人技术可能如何影响人类劳动。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过实证分析与文献回顾，为机器人技术领域提供对于'枯燥、肮脏、危险'(DDD)工作更加清晰的定义和理解，从而促进该领域在开发相关技术时能够更好地考虑到这些因素对人类劳动的影响。

Method: 采用实证分析方法，对特定时间段内涉及DDD概念的机器人学出版物进行统计与内容审查；同时，基于社会科学研究成果，为DDD工作提供定义及概念化指导。

Result: 发现仅2.7%的研究文献明确界定了DDD的概念，而8.7%的文章给出了具体的DDD任务或工作示例。此外，文中提出了一种新框架，用于指导机器人技术开发者更加全面地考量技术应用对工作环境的影响。

Conclusion: 当前机器人学领域对DDD工作的理解和定义尚不充分，需要借助社会科学视角深化认识。所提出的框架有助于机器人社区成员在开发新技术时考虑到更广泛的社会经济影响，特别是对人类劳动力市场的影响。

Abstract: In robotics, the concept of "dull, dirty, and dangerous" (DDD) work has been used to motivate where robots might be useful. In this paper, we conduct an empirical analysis of robotics publications between 1980 and 2024 that mention DDD, and find that only 2.7% of publications define DDD and 8.7% of publications provide concrete examples of tasks or jobs that are DDD. We then review the social science literature on "dull," "dirty," and "dangerous" work to provide definitions and guidance on how to conceptualize DDD for robotics. Finally, we propose a framework that helps the robotics community consider the job context for our technology, encouraging a more informed perspective on how robotics may impact human labor.

</details>


### [29] [PDF-HR: Pose Distance Fields for Humanoid Robots](https://arxiv.org/abs/2602.04851)
*Yi Gu,Yukang Gao,Yangchen Zhou,Xingyu Chen,Yixiao Feng,Mingle Zhao,Yunyang Mo,Zhaorui Wang,Lixin Xu,Renjing Xu*

Main category: cs.RO

TL;DR: 提出了一种名为PDF-HR的新方法，作为一种轻量级的先验知识，用于表示类人机器人姿态分布为连续可微流形，以预测任意姿态与大量重新定向的机器人姿态之间的距离，从而提供一种平滑的姿态合理性度量。该方法可以作为奖励塑造项、正则化器或独立的合理性评分器集成到多种流程中，并在各种类人任务上显示了其增强基准方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管在人类运动恢复领域已经广泛研究了姿态和运动先验知识，并且有多种模型可用，但它们在类人机器人中的应用仍然有限，主要原因是高质量类人机器人运动数据的稀缺。

Method: 本文介绍了Pose Distance Fields for Humanoid Robots (PDF-HR)，这是一种轻量级的先验知识，能够将机器人的姿态分布表示成一个连续且可微分的流形。给定任意姿态，PDF-HR能预测它与大量的重定位机器人姿态之间的距离，从而给出一种适合于优化和控制的姿态合理性的平滑测量。

Result: 通过在单轨迹运动跟踪、一般运动跟踪、基于风格的运动模仿以及一般运动重定位等不同类人任务上的评估，实验表明这种即插即用的先验知识始终显著地增强了强大的基线方法。

Conclusion: PDF-HR 提供了一种新的方式来利用姿态先验，对于优化和控制类人机器人的运动来说非常有用。此外，它还能够在不同的应用场景下提高已有方法的表现。

Abstract: Pose and motion priors play a crucial role in humanoid robotics. Although such priors have been widely studied in human motion recovery (HMR) domain with a range of models, their adoption for humanoid robots remains limited, largely due to the scarcity of high-quality humanoid motion data. In this work, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that represents the robot pose distribution as a continuous and differentiable manifold. Given an arbitrary pose, PDF-HR predicts its distance to a large corpus of retargeted robot poses, yielding a smooth measure of pose plausibility that is well suited for optimization and control. PDF-HR can be integrated as a reward shaping term, a regularizer, or a standalone plausibility scorer across diverse pipelines. We evaluate PDF-HR on various humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and general motion retargeting. Experiments show that this plug-and-play prior consistently and substantially strengthens strong baselines. Code and models will be released.

</details>


### [30] [Capturing Visual Environment Structure Correlates with Control Performance](https://arxiv.org/abs/2602.04880)
*Jiahua Dong,Yunze Man,Pavel Tokmakov,Yu-Xiong Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种新的方法，通过测量预训练视觉编码器支持从图像中解码环境状态（包括几何、物体结构和物理属性）的能力来评估其性能。这种方法在不同环境和学习设置下与下游策略表现之间有很强的相关性，比先前的指标更有效，并为选择合适的表示提供了效率上的提升。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估视觉表示的方法主要集中在捕捉视觉世界的狭窄方面，比如物体形状上，这限制了跨环境的泛化能力。本文旨在找到一种更有效的度量方式，能够更好地预测视觉表示对通用机器人策略的支持程度。

Method: 研究者们通过测量预训练视觉编码器支持从图像中解码出环境状态（如几何、物体结构及物理特性）的能力来进行分析。利用具有真实状态访问权限的模拟环境，展示了这种探测准确性与多种环境下策略执行效果之间的强关联性。

Result: 实验表明，所提出的探查准确度与各种环境和学习场景下的策略性能之间存在显著相关性，优于现有指标，并且能够促进高效的选择视觉表示。

Conclusion: 本研究表明，学习如何编码环境潜在物理状态对于实现可泛化的操作控制是一个很有前景的目标；同时，提出的新方法为评估和选择有助于扩大通用机器人策略规模的视觉表示提供了一个有效途径。

Abstract: The choice of visual representation is key to scaling generalist robot policies. However, direct evaluation via policy rollouts is expensive, even in simulation. Existing proxy metrics focus on the representation's capacity to capture narrow aspects of the visual world, like object shape, limiting generalization across environments. In this paper, we take an analytical perspective: we probe pretrained visual encoders by measuring how well they support decoding of environment state -- including geometry, object structure, and physical attributes -- from images. Leveraging simulation environments with access to ground-truth state, we show that this probing accuracy strongly correlates with downstream policy performance across diverse environments and learning settings, significantly outperforming prior metrics and enabling efficient representation selection. More broadly, our study provides insight into the representational properties that support generalizable manipulation, suggesting that learning to encode the latent physical state of the environment is a promising objective for control.

</details>
