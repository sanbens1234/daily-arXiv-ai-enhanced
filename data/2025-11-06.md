<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Toward an Agricultural Operational Design Domain: A Framework](https://arxiv.org/abs/2511.02937)
*Mirco Felske,Jannik Redenius,Georg Happich,Julius Schöning*

Main category: cs.RO

TL;DR: 本文提出了一种农业ODD（Ag-ODD）框架，用于明确和验证自主农业系统的操作边界。该框架由三部分组成：Ag-ODD描述概念、7层模型以及迭代验证过程，旨在支持环境描述的标准化与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 农业领域越来越多地依赖于在复杂多变环境中运行的自主系统。然而，现有的操作设计域(ODD)概念尚未解决农业应用的独特挑战。为了解决这种复杂性并确保开发和验证过程的一致性，需要一种结构化、透明且经过验证的方法来描述环境。

Method: 提出了农业ODD（Ag-ODD）框架，包括三个核心元素：1. Ag-ODD描述概念，采用ASAM Open ODD和CityGML中的概念提供了一个结构化方法来明确定义环境和操作参数；2. 从PEGASUS 6层模型派生出的7层模型，增加了工艺层以捕捉动态农业作业；3. 迭代验证过程，通过将Ag-ODD与其对应的逻辑场景进行对比来确保其完整性和一致性。

Result: Ag-ODD框架提供了一种一致的方法来创建无歧义且可验证的操作设计域。演示用例展示了Ag-ODD框架如何支持自主农业系统环境描述的标准化和可扩展性。

Conclusion: 通过引入Ag-ODD框架，研究人员能够更好地处理自主农业系统中遇到的独特挑战，并促进这些系统在不同条件下的可靠部署。

Abstract: The agricultural sector increasingly relies on autonomous systems that
operate in complex and variable environments. Unlike on-road applications,
agricultural automation integrates driving and working processes, each of which
imposes distinct operational constraints. Handling this complexity and ensuring
consistency throughout the development and validation processes requires a
structured, transparent, and verified description of the environment. However,
existing Operational Design Domain (ODD) concepts do not yet address the unique
challenges of agricultural applications.
  Therefore, this work introduces the Agricultural ODD (Ag-ODD) Framework,
which can be used to describe and verify the operational boundaries of
autonomous agricultural systems. The Ag-ODD Framework consists of three core
elements. First, the Ag-ODD description concept, which provides a structured
method for unambiguously defining environmental and operational parameters
using concepts from ASAM Open ODD and CityGML. Second, the 7-Layer Model
derived from the PEGASUS 6-Layer Model, has been extended to include a process
layer to capture dynamic agricultural operations. Third, the iterative
verification process verifies the Ag-ODD against its corresponding logical
scenarios, derived from the 7-Layer Model, to ensure the Ag-ODD's completeness
and consistency.
  Together, these elements provide a consistent approach for creating
unambiguous and verifiable Ag-ODD. Demonstrative use cases show how the Ag-ODD
Framework can support the standardization and scalability of environmental
descriptions for autonomous agricultural systems.

</details>


### [2] [Comprehensive Assessment of LiDAR Evaluation Metrics: A Comparative Study Using Simulated and Real Data](https://arxiv.org/abs/2511.02994)
*Syed Mostaquim Ali,Taufiq Rahman,Ghazal Farhani,Mohamed H. Zaki,Benoit Anctil,Dominique Charlebois*

Main category: cs.RO

TL;DR: 本研究探索了用于比较现实世界和模拟LiDAR扫描的评估指标，发现密度感知切比雪夫距离(DCD)在所有情况下表现最佳。通过使用真实LiDAR数据生成虚拟测试环境(VTE)，并对比实际与模拟LiDAR扫描，在几何相似性和模型感知方面得到了一定的相似性，但仍有差异。


<details>
  <summary>Details</summary>
Motivation: 为了开发安全的自动驾驶系统(ADS)，需要进行严格的测试以确保它们可以安全地部署上路。由于全面的传统物理测试因成本和安全问题而不可行，因此采用虚拟测试环境(VTE)作为替代方案。比较VTE生成的传感器输出与其现实世界的对应物可以帮助验证VTE是否准确反映了现实情况。

Method: 本工作采用了实验方法来寻找适合于比较现实世界与模拟LiDAR扫描的评价指标，并且对这些指标进行了敏感度和准确性测试，包括不同噪声、密度、失真、传感器方向以及通道设置的情况。此外，还基于真实的LiDAR扫描数据构建了一个虚拟测试环境，并使用相同姿态生成了模拟LiDAR扫描。然后，将模拟和实际的LiDAR扫描从模型感知和几何相似性的角度进行了比较。

Result: 结果表明，密度感知切比雪夫距离(DCD)在所有情况下都是最适合的评价指标。实际和模拟LiDAR扫描之间存在轻微的几何属性差异，同时模型输出间也存在一定显著差异。然而，在调整后的强度下，两者之间的语义分割输出具有21%的平均交并比(mIoU)，并且平均密度感知切比雪夫距离(DCD)为0.63。这表明尽管存在一些差异，但模拟确实能够较好地反映实际情况。

Conclusion: 该研究表明，密度感知切比雪夫距离(DCD)是连接感知方法与几何相似性分析的最佳指标之一。此外，利用真实LiDAR数据创建的虚拟测试环境能够提供与实际场景相接近的模拟LiDAR扫描，从而为自动驾驶系统的安全测试提供了有价值的工具。

Abstract: For developing safe Autonomous Driving Systems (ADS), rigorous testing is
required before they are deemed safe for road deployments. Since comprehensive
conventional physical testing is impractical due to cost and safety concerns,
Virtual Testing Environments (VTE) can be adopted as an alternative. Comparing
VTE-generated sensor outputs against their real-world analogues can be a strong
indication that the VTE accurately represents reality. Correspondingly, this
work explores a comprehensive experimental approach to finding evaluation
metrics suitable for comparing real-world and simulated LiDAR scans. The
metrics were tested in terms of sensitivity and accuracy with different noise,
density, distortion, sensor orientation, and channel settings. From comparing
the metrics, we found that Density Aware Chamfer Distance (DCD) works best
across all cases. In the second step of the research, a Virtual Testing
Environment was generated using real LiDAR scan data. The data was collected in
a controlled environment with only static objects using an instrumented vehicle
equipped with LiDAR, IMU and cameras. Simulated LiDAR scans were generated from
the VTEs using the same pose as real LiDAR scans. The simulated and LiDAR scans
were compared in terms of model perception and geometric similarity. Actual and
simulated LiDAR scans have a similar semantic segmentation output with a mIoU
of 21\% with corrected intensity and an average density aware chamfer distance
(DCD) of 0.63. This indicates a slight difference in the geometric properties
of simulated and real LiDAR scans and a significant difference between model
outputs. During the comparison, density-aware chamfer distance was found to be
the most correlated among the metrics with perception methods.

</details>


### [3] [WorldPlanner: Monte Carlo Tree Search and MPC with Action-Conditioned Visual World Models](https://arxiv.org/abs/2511.03077)
*R. Khorrambakht,Joaquim Ortiz-Haro,Joseph Amigo,Omar Mostafa,Daniel Dugas,Franziska Meier,Ludovic Righetti*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型的方法，通过收集少量非结构化游戏数据来学习动作条件视觉世界模型、基于扩散的动作采样器以及可选的奖励模型。结合这些组件，使用蒙特卡洛树搜索（MCTS）规划器优化长序列动作，并通过零阶模型预测控制器在机器人上执行计划。实验结果表明，在标准操作测试环境中，该方法相比行为克隆基线有显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于策略的行为克隆方法难以迁移到新任务及生成训练数据困难的问题。旨在通过更少且容易获取的数据来让机器人理解和推理其环境，从而完成复杂任务。

Method: 采用模型为基础的方法，利用几小时简单易得的游戏数据训练动作条件视觉世界模型、基于扩散的动作采样器和可选奖励模型。使用这些模型加上蒙特卡洛树搜索规划器优化动作序列，并通过模型预测控制执行于实际机器人上。

Result: 所提出的动作采样器能够减少世界模型在规划过程中的幻觉现象；在3个不同复杂度的真实世界机器人任务中验证了方法的有效性；实验显示，与行为克隆基线相比，该方法在标准操控测试环境中取得了显著性能提升。

Conclusion: 通过基于模型的方法，结合动作条件视觉世界模型、基于扩散的动作采样器及奖励模型，可以有效提高机器人在复杂任务上的表现，尤其是在需要长期规划的情况下。

Abstract: Robots must understand their environment from raw sensory inputs and reason
about the consequences of their actions in it to solve complex tasks. Behavior
Cloning (BC) leverages task-specific human demonstrations to learn this
knowledge as end-to-end policies. However, these policies are difficult to
transfer to new tasks, and generating training data is challenging because it
requires careful demonstrations and frequent environment resets. In contrast to
such policy-based view, in this paper we take a model-based approach where we
collect a few hours of unstructured easy-to-collect play data to learn an
action-conditioned visual world model, a diffusion-based action sampler, and
optionally a reward model. The world model -- in combination with the action
sampler and a reward model -- is then used to optimize long sequences of
actions with a Monte Carlo Tree Search (MCTS) planner. The resulting plans are
executed on the robot via a zeroth-order Model Predictive Controller (MPC). We
show that the action sampler mitigates hallucinations of the world model during
planning and validate our approach on 3 real-world robotic tasks with varying
levels of planning and modeling complexity. Our experiments support the
hypothesis that planning leads to a significant improvement over BC baselines
on a standard manipulation test environment.

</details>


### [4] [Learning Natural and Robust Hexapod Locomotion over Complex Terrains via Motion Priors based on Deep Reinforcement Learning](https://arxiv.org/abs/2511.03167)
*Xin Liu,Jinze Wu,Yinghui Li,Chenkun Qi,Yufei Xue,Feng Gao*

Main category: cs.RO

TL;DR: 本研究通过基于运动先验的方法，利用深度强化学习算法成功地让一个真实的六足机器人学会了自然步态，并在复杂地形上展示了良好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多足机器人能够提供增强的稳定性来穿越复杂的地形，但如何有效地协调多个腿在更大的动作探索空间中生成自然且稳健的移动是一个关键问题。

Method: 研究者引入了一种基于运动先验的方法，应用了深度强化学习算法到一个真实的六足机器人上。他们生成了一个优化过的运动先验数据集，并训练了一个基于这些先验的对抗判别器来指导六足机器人学习自然步态。

Result: 所学得的策略被成功转移到一个真实的六足机器人上，在没有视觉信息的情况下于复杂地形上展示出了自然步态模式和显著的鲁棒性。

Conclusion: 这是首次使用强化学习控制器实现真实六足机器人在复杂地形上的行走。

Abstract: Multi-legged robots offer enhanced stability to navigate complex terrains
with their multiple legs interacting with the environment. However, how to
effectively coordinate the multiple legs in a larger action exploration space
to generate natural and robust movements is a key issue. In this paper, we
introduce a motion prior-based approach, successfully applying deep
reinforcement learning algorithms to a real hexapod robot. We generate a
dataset of optimized motion priors, and train an adversarial discriminator
based on the priors to guide the hexapod robot to learn natural gaits. The
learned policy is then successfully transferred to a real hexapod robot, and
demonstrate natural gait patterns and remarkable robustness without visual
information in complex terrains. This is the first time that a reinforcement
learning controller has been used to achieve complex terrain walking on a real
hexapod robot.

</details>


### [5] [Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control](https://arxiv.org/abs/2511.03181)
*Rewida Ali,Cristian C. Beltran-Hernandez,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 本文提出了一种基于学习的框架，结合了由大型语言模型驱动的高级任务规划器与模仿学习和强化学习策略，特别设计了一个子任务感知机器人Transformer（START），用于处理如礼品包装这样的长时序操作问题。该方法通过引入子任务ID来明确时间定位，从而在整个包装过程中实现稳健表现，并在实际包装任务中达到了97%的成功率。


<details>
  <summary>Details</summary>
Motivation: 人机协作在需要频繁处理可变形物体（如纸张、袋子和织物）的环境中至关重要，但这类材料的不可预测动力学特性和对适应性力控制的需求使得协调机器人的动作和人类的帮助变得困难。为了解决这个问题，研究聚焦于礼品包装任务，这是一个涉及精确折叠、受控折痕和纸张固定等复杂步骤的长时间操作问题。

Method: 研究人员提出了一个基于学习的框架，该框架整合了由大型语言模型支持的高级任务规划器以及模仿学习和强化学习相结合的低级策略。核心组件是一个名为START的子任务感知机器人Transformer，它能够从人类演示中学习统一的策略。相较于传统的ACT方法仅适用于短期任务，本方法通过引入子任务ID来捕捉整个包装序列中的长期时间依赖关系。

Result: 实验结果表明，所提出的框架在现实世界中的包装任务上取得了97%的成功率。此外，统一的基于Transformer的策略减少了对专门化模型的需求，允许更加灵活的人类监督方式，并有效地将高层次意图与细粒度的力量控制结合起来，这对于可变形物体的操作是必要的。

Conclusion: 通过引入一种新颖的学习框架，包括利用大型语言模型进行任务规划及采用模仿学习与强化学习混合策略，特别是开发出能够理解并执行复杂顺序任务的START模型，研究证明了即使面对具有挑战性的可变形物体操作任务也能实现高效的人机协作。

Abstract: Human-robot cooperation is essential in environments such as warehouses and
retail stores, where workers frequently handle deformable objects like paper,
bags, and fabrics. Coordinating robotic actions with human assistance remains
difficult due to the unpredictable dynamics of deformable materials and the
need for adaptive force control. To explore this challenge, we focus on the
task of gift wrapping, which exemplifies a long-horizon manipulation problem
involving precise folding, controlled creasing, and secure fixation of paper.
Success is achieved when the robot completes the sequence to produce a neatly
wrapped package with clean folds and no tears.
  We propose a learning-based framework that integrates a high-level task
planner powered by a large language model (LLM) with a low-level hybrid
imitation learning (IL) and reinforcement learning (RL) policy. At its core is
a Sub-task Aware Robotic Transformer (START) that learns a unified policy from
human demonstrations. The key novelty lies in capturing long-range temporal
dependencies across the full wrapping sequence within a single model. Unlike
vanilla Action Chunking with Transformer (ACT), typically applied to short
tasks, our method introduces sub-task IDs that provide explicit temporal
grounding. This enables robust performance across the entire wrapping process
and supports flexible execution, as the policy learns sub-goals rather than
merely replicating motion sequences.
  Our framework achieves a 97% success rate on real-world wrapping tasks. We
show that the unified transformer-based policy reduces the need for specialized
models, allows controlled human supervision, and effectively bridges high-level
intent with the fine-grained force control required for deformable object
manipulation.

</details>


### [6] [Collaborative Assembly Policy Learning of a Sightless Robot](https://arxiv.org/abs/2511.03189)
*Zeqing Zhang,Weifeng Lu,Lei Yang,Wei Jing,Bowei Tang,Jia Pan*

Main category: cs.RO

TL;DR: 本文提出了一种新的强化学习方法，该方法利用人为设计的顺应性控制器来促进更积极的机器人行为并减少人类在协作任务中的努力。实验表明，这种方法在成功率和任务完成时间方面优于传统的顺应性控制，并且显著降低了所需的人力。


<details>
  <summary>Details</summary>
Motivation: 当前用于物理人机协作任务（pHRC）的方法，如顺应性控制，难以准确估计人的意图，限制了机器人在协作任务中提供帮助的能力；而其他使用强化学习的方法由于安全约束和稀疏奖励的问题并不适用于板插入任务。为解决这些问题，作者提出了一个新方法。

Method: 提出了一种新的强化学习方法，结合了人为设计的顺应性控制器以促进更积极的机器人参与度，并通过模拟与实际操作实验验证了其有效性。

Result: 实验证明，所提出的方法相比传统顺应性控制，在成功率和任务完成时间上表现更好，同时显著减少了所需施加的力量/扭矩。

Conclusion: 本研究展示了一种将强化学习与顺应性控制相结合的新方法，可以提高物理人机协作任务的表现，特别是在安全性要求高及奖励稀疏的情况下。

Abstract: This paper explores a physical human-robot collaboration (pHRC) task
involving the joint insertion of a board into a frame by a sightless robot and
a human operator. While admittance control is commonly used in pHRC tasks, it
can be challenging to measure the force/torque applied by the human for
accurate human intent estimation, limiting the robot's ability to assist in the
collaborative task. Other methods that attempt to solve pHRC tasks using
reinforcement learning (RL) are also unsuitable for the board-insertion task
due to its safety constraints and sparse rewards. Therefore, we propose a novel
RL approach that utilizes a human-designed admittance controller to facilitate
more active robot behavior and reduce human effort. Through simulation and
real-world experiments, we demonstrate that our approach outperforms admittance
control in terms of success rate and task completion time. Additionally, we
observed a significant reduction in measured force/torque when using our
proposed approach compared to admittance control. The video of the experiments
is available at https://youtu.be/va07Gw6YIog.

</details>


### [7] [GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained Robot Policy Enhancement](https://arxiv.org/abs/2511.03400)
*Minquan Gao,Xinyi Li,Qing Yan,Xiaojian Sun,Xiaopan Zhang,Chien-Ming Huang,Jiachen Li*

Main category: cs.RO

TL;DR: GUIDES框架通过将基础模型的语义指导整合到预训练的机器人策略中，无需重新设计架构，就能提高任务成功率和动作精度。


<details>
  <summary>Details</summary>
Motivation: 预训练的机器人策略虽然包含了丰富的实体知识，但通常缺乏基础模型所具有的语义意识。直接替换这些策略既不经济也容易导致知识流失。因此，需要一种方法来增强现有策略而不完全替换它们。

Method: 提出了GUIDES框架，利用微调后的视觉-语言模型生成上下文指令，并由辅助模块编码为指导嵌入。这些嵌入被注入到策略的潜在空间中，使旧模型能够通过简短的目标微调适应新的语义输入。此外，基于大型语言模型的Reflector在执行过程中监控信心水平，并在必要时启动推理循环以优化后续行动。

Result: 在RoboCasa模拟环境中对多种策略架构进行了广泛验证，显示了任务成功率的一致性和显著改进。实际部署于UR5机器人上进一步证明了GUIDES对于抓取等关键子任务的动作精度有所提升。

Conclusion: GUIDES提供了一种实用且资源高效的途径来升级而非替换已验证的机器人策略，实现了性能上的重要改进。

Abstract: Pre-trained robot policies serve as the foundation of many validated robotic
systems, which encapsulate extensive embodied knowledge. However, they often
lack the semantic awareness characteristic of foundation models, and replacing
them entirely is impractical in many situations due to high costs and the loss
of accumulated knowledge. To address this gap, we introduce GUIDES, a
lightweight framework that augments pre-trained policies with semantic guidance
from foundation models without requiring architectural redesign. GUIDES employs
a fine-tuned vision-language model (Instructor) to generate contextual
instructions, which are encoded by an auxiliary module into guidance
embeddings. These embeddings are injected into the policy's latent space,
allowing the legacy model to adapt to this new semantic input through brief,
targeted fine-tuning. For inference-time robustness, a large language
model-based Reflector monitors the Instructor's confidence and, when confidence
is low, initiates a reasoning loop that analyzes execution history, retrieves
relevant examples, and augments the VLM's context to refine subsequent actions.
Extensive validation in the RoboCasa simulation environment across diverse
policy architectures shows consistent and substantial improvements in task
success rates. Real-world deployment on a UR5 robot further demonstrates that
GUIDES enhances motion precision for critical sub-tasks such as grasping.
Overall, GUIDES offers a practical and resource-efficient pathway to upgrade,
rather than replace, validated robot policies.

</details>


### [8] [Value Elicitation for a Socially Assistive Robot Addressing Social Anxiety: A Participatory Design Approach](https://arxiv.org/abs/2511.03444)
*Vesna Poprcova,Iulia Lefter,Martijn Warnier,Frances Brazier*

Main category: cs.RO

TL;DR: 本研究通过与心理健康学术研究人员共同举办的设计工作坊，探讨了设计用于社交焦虑支持的社会辅助机器人的核心价值观，包括适应性、接纳性和有效性。


<details>
  <summary>Details</summary>
Motivation: 为了开发有效的解决方案，需要理解形成有意义、可接受和有帮助的支撑的价值观。社会机器人技术的进步为补充传统心理健康提供了机会。

Method: 进行了参与式设计工作坊，通过创意、反思和展望活动，探索场景和设计方案，系统地引出与机器人支持干预相关的价值观、期望、需求和偏好。

Result: 研究揭示了与设计相关的核心价值观，如适应性、接纳性和有效性，这些对于支持患有社交焦虑的人至关重要。

Conclusion: 研究表明，在开发社会辅助机器人时，采用以研究为主导的方法来引出价值观的重要性，并强调用户中心和情境感知的设计考虑。

Abstract: Social anxiety is a prevalent mental health condition that can significantly
impact overall well-being and quality of life. Despite its widespread effects,
adequate support or treatment for social anxiety is often insufficient.
Advances in technology, particularly in social robotics, offer promising
opportunities to complement traditional mental health. As an initial step
toward developing effective solutions, it is essential to understand the values
that shape what is considered meaningful, acceptable, and helpful. In this
study, a participatory design workshop was conducted with mental health
academic researchers to elicit the underlying values that should inform the
design of socially assistive robots for social anxiety support. Through
creative, reflective, and envisioning activities, participants explored
scenarios and design possibilities, allowing for systematic elicitation of
values, expectations, needs, and preferences related to robot-supported
interventions. The findings reveal rich insights into design-relevant
values-including adaptivity, acceptance, and efficacy-that are core to support
for individuals with social anxiety. This study highlights the significance of
a research-led approach to value elicitation, emphasising user-centred and
context-aware design considerations in the development of socially assistive
robots.

</details>


### [9] [Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control](https://arxiv.org/abs/2511.03481)
*Jianbo Yuan,Haohua Zhu,Jing Dai,Sheng Yi*

Main category: cs.RO

TL;DR: 介绍了Dex-Hand 021，一种轻量级、高性能的五指机器人手，具有19个自由度，通过本体感觉力感知为基础的顺应控制方法增强了操作性能。


<details>
  <summary>Details</summary>
Motivation: 为了在保持工程约束的同时复制人类手部多功能能力（包括运动、感知和协调操作），开发了一种灵巧的机器人手。

Method: 设计了Dex-Hand 021，这是一种由电缆驱动的五指机器人手，拥有12个主动和7个被动自由度，并提出了一种基于本体感觉力感知的顺应控制方法来增强操作性。

Result: 实验结果表明其优越性能：单指负载能力超过10N，指尖重复精度小于0.001m，力估计误差低于0.2N；与PID控制相比，在多对象抓取时关节扭矩减少了31.19%，显著提高了力感测能力同时防止碰撞过载。

Conclusion: 这项工作促进了轻量级工业级灵巧手的设计及本体感受控制的进步，为机器人操作和智能制造做出了贡献。

Abstract: The human hand plays a vital role in daily life and industrial applications,
yet replicating its multifunctional capabilities-including motion, sensing, and
coordinated manipulation-with robotic systems remains a formidable challenge.
Developing a dexterous robotic hand requires balancing human-like agility with
engineering constraints such as complexity, size-to-weight ratio, durability,
and force-sensing performance. This letter presents Dex-Hand 021, a
high-performance, cable-driven five-finger robotic hand with 12 active and 7
passive degrees of freedom (DoFs), achieving 19 DoFs dexterity in a lightweight
1 kg design. We propose a proprioceptive force-sensing-based admittance control
method to enhance manipulation. Experimental results demonstrate its superior
performance: a single-finger load capacity exceeding 10 N, fingertip
repeatability under 0.001 m, and force estimation errors below 0.2 N. Compared
to PID control, joint torques in multi-object grasping are reduced by 31.19%,
significantly improves force-sensing capability while preventing overload
during collisions. The hand excels in both power and precision grasps,
successfully executing 33 GRASP taxonomy motions and complex manipulation
tasks. This work advances the design of lightweight, industrial-grade dexterous
hands and enhances proprioceptive control, contributing to robotic manipulation
and intelligent manufacturing.

</details>


### [10] [ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied AI Applications](https://arxiv.org/abs/2511.03497)
*Lei Fu,Sahar Salimpour,Leonardo Militano,Harry Edelman,Jorge Peña Queralta,Giovanni Toffetti*

Main category: cs.RO

TL;DR: 本研究介绍了一种用于分析ROS和ROS 2数据包的MCP服务器，允许通过语言模型(LLMs)和视觉语言模型(VLMs)自然语言处理机器人数据。研究中构建了专门针对移动机器人的工具，并提供了用户界面来评估不同LLM的表现。实验结果表明，在工具调用能力方面存在显著差异，Kimi K2和Claude Sonnet 4表现最佳。


<details>
  <summary>Details</summary>
Motivation: 鉴于Agentic AI系统和物理或具身AI系统是人工智能和机器人学领域的重要研究方向，但两者结合的研究相对较少，本论文旨在填补这一空白，提供一个能够促进Agentic Embodied AI应用发展的解决方案。

Method: 开发了一个MCP服务器，专门设计用来分析ROS和ROS 2的数据包文件，通过集成语言模型（包括专有模型如Anthropic, OpenAI以及开源选项）实现了对机器人轨迹、激光扫描等数据的自然语言处理与可视化。此外，还创建了一个轻量级UI，用于测试不同语言模型在工具调用上的性能。

Result: 实验结果显示，不同语言模型之间在工具调用能力上存在很大差距，其中Kimi K2和Claude Sonnet 4展现了明显更优的表现。同时发现，影响成功率的因素包括但不限于工具描述模式、参数数量及可供使用的工具数量。

Conclusion: 该工作为利用MCP增强Agentic Embodied AI的应用奠定了基础，揭示了当前语言模型在执行特定任务时面临的挑战，并指出未来研究可能的方向。

Abstract: Agentic AI systems and Physical or Embodied AI systems have been two key
research verticals at the forefront of Artificial Intelligence and Robotics,
with Model Context Protocol (MCP) increasingly becoming a key component and
enabler of agentic applications. However, the literature at the intersection of
these verticals, i.e., Agentic Embodied AI, remains scarce. This paper
introduces an MCP server for analyzing ROS and ROS 2 bags, allowing for
analyzing, visualizing and processing robot data with natural language through
LLMs and VLMs. We describe specific tooling built with robotics domain
knowledge, with our initial release focused on mobile robotics and supporting
natively the analysis of trajectories, laser scan data, transforms, or time
series data. This is in addition to providing an interface to standard ROS 2
CLI tools ("ros2 bag list" or "ros2 bag info"), as well as the ability to
filter bags with a subset of topics or trimmed in time. Coupled with the MCP
server, we provide a lightweight UI that allows the benchmarking of the tooling
with different LLMs, both proprietary (Anthropic, OpenAI) and open-source
(through Groq). Our experimental results include the analysis of tool calling
capabilities of eight different state-of-the-art LLM/VLM models, both
proprietary and open-source, large and small. Our experiments indicate that
there is a large divide in tool calling capabilities, with Kimi K2 and Claude
Sonnet 4 demonstrating clearly superior performance. We also conclude that
there are multiple factors affecting the success rates, from the tool
description schema to the number of arguments, as well as the number of tools
available to the models. The code is available with a permissive license at
https://github.com/binabik-ai/mcp-rosbags.

</details>


### [11] [Indicating Robot Vision Capabilities with Augmented Reality](https://arxiv.org/abs/2511.03550)
*Hong Wang,Ridhima Phatak,James Ocampo,Zhao Han*

Main category: cs.RO

TL;DR: 研究发现人们可能错误地认为机器人和人类有相同的视野，这可能导致人机协作任务中的失败。为此，研究人员提出了四种增强现实（AR）中的视野指示器，并通过用户实验评估了它们在准确性、信心、任务效率和工作负荷方面的表现。结果显示，以任务空间为中心的指示器具有最高的准确性，但对机器人视野的解释存在延迟；而以自我为中心的更深眼窝设计也提高了准确性。所有指示器下参与者的信心都较高且认知负荷较低。最后，为实践者提供了六条指导方针，以帮助调整人们对机器人视觉能力的心理模型。


<details>
  <summary>Details</summary>
Motivation: 研究表明，人们可能会错误地认为机器人拥有与人类相同的视野范围，这种误解可能导致在执行需要人机合作的任务时出现失败情况，尤其是在机器人没有机会扫描周围环境来更新其世界模型的情况下。为了改善这一问题，本文旨在通过增强现实技术提出并测试几种视野指示器，以更好地匹配人们对机器人视觉能力的理解。

Method: 本研究设计了四种不同的增强现实（AR）视野指示器，这些指示器覆盖了从以自我为中心（基于机器人的视角）到以他物为中心（基于任务区域）的不同层面。随后进行了一个包含41名参与者的人类被试实验，用以评估这些指示器在准确性、用户信心、任务完成效率以及认知负荷等方面的表现。

Result: 实验结果表明，在任务区域内设置的以他物为中心的方块状指示器达到了最高的识别准确率，尽管对于理解机器人视野范围来说存在一定的延迟。另外，采用更深层次的眼窝作为以自我为中心的设计方式也能有效提高用户的识别准确率。总体而言，所有类型的指示器均能让参与者保持较高的自信心水平，并且不会显著增加他们的认知负担。

Conclusion: 通过这项研究，我们不仅验证了几种不同类型的AR视野指示器的有效性，还为如何利用这些指示器或物理修改来校准人们对机器人视觉能力的认知提供了具体的指导建议。这对于促进未来更加高效和谐的人机交互具有重要意义。

Abstract: Research indicates that humans can mistakenly assume that robots and humans
have the same field of view (FoV), possessing an inaccurate mental model of
robots. This misperception may lead to failures during human-robot
collaboration tasks where robots might be asked to complete impossible tasks
about out-of-view objects. The issue is more severe when robots do not have a
chance to scan the scene to update their world model while focusing on assigned
tasks. To help align humans' mental models of robots' vision capabilities, we
propose four FoV indicators in augmented reality (AR) and conducted a user
human-subjects experiment (N=41) to evaluate them in terms of accuracy,
confidence, task efficiency, and workload. These indicators span a spectrum
from egocentric (robot's eye and head space) to allocentric (task space).
Results showed that the allocentric blocks at the task space had the highest
accuracy with a delay in interpreting the robot's FoV. The egocentric indicator
of deeper eye sockets, possible for physical alteration, also increased
accuracy. In all indicators, participants' confidence was high while cognitive
load remained low. Finally, we contribute six guidelines for practitioners to
apply our AR indicators or physical alterations to align humans' mental models
with robots' vision capabilities.

</details>


### [12] [OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera](https://arxiv.org/abs/2511.03571)
*Hao Shi,Ze Wang,Shangwei Guo,Mengfei Duan,Song Wang,Teng Chen,Kailun Yang,Lin Wang,Kaiwei Wang*

Main category: cs.RO

TL;DR: OneOcc是一个专为腿部/类人机器人设计的全景3D语义占用框架，通过结合双投影融合、双网格体素化、轻量解码器和步态位移补偿等技术，实现更好的全方位感知。此外，还发布了两个新的全景占用基准数据集QuadOcc和Human360Occ，并在这些基准上取得了领先的成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的大部分语义场景补全（SSC）系统主要针对配备前向传感器的轮式平台设计，而对腿部/类人机器人至关重要的鲁棒3D语义占用却较少被关注。因此，研究旨在开发一种适用于腿部运动引入的身体抖动及360度连续性的视觉仅依赖型全景SSC框架。

Method: OneOcc框架整合了四项关键技术：(i) 双投影融合(DP-ER)，利用环形全景图及其等距矩形展开图，保持360度连续性和网格对齐；(ii) 双网格体素化(BGV)，允许在笛卡尔坐标系与圆柱极坐标系间进行推理，减少离散化偏差并锐化自由/占用边界；(iii) 采用层次化AMoE-3D的轻量级解码器，支持动态多尺度融合及更优的长距离/遮挡推理；(iv) 即插即用的步态位移补偿(GDC)，学习特征层面的动作校正而不需额外传感器。

Result: OneOcc不仅在新发布的两个全景占用基准数据集——QuadOcc（真实四足动物，第一人称360度视角）和Human360Occ (H3O)（CARLA人类自我360度视角，包含RGB、深度信息和语义占用；标准内/跨城市分割）上表现优异，而且其模块轻量化特性使得该解决方案易于部署于腿部/类人机器人以实现全周感知能力。特别是在H3O数据集上获得了+3.83 mIoU（城内）和+8.08 mIoU（跨城）的显著提升。

Conclusion: 通过提出OneOcc框架以及配套的新型全景占用数据集QuadOcc和Human360Occ，本研究成功地为腿部/类人机器人提供了先进的全方位感知解决方案。实验结果表明，OneOcc能够在多个评价指标上超越现有强大的视觉基线方法和流行的激光雷达方案，展示了其在实际应用中的潜力。

Abstract: Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most
semantic scene completion (SSC) systems target wheeled platforms with
forward-facing sensors. We present OneOcc, a vision-only panoramic SSC
framework designed for gait-introduced body jitter and 360{\deg} continuity.
OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular
panorama and its equirectangular unfolding, preserving 360{\deg} continuity and
grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and
cylindrical-polar spaces, reducing discretization bias and sharpening
free/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D
for dynamic multi-scale fusion and better long-range/occlusion reasoning; and
(iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level
motion correction without extra sensors. We also release two panoramic
occupancy benchmarks: QuadOcc (real quadruped, first-person 360{\deg}) and
Human360Occ (H3O) (CARLA human-ego 360{\deg} with RGB, Depth, semantic
occupancy; standardized within-/cross-city splits). OneOcc sets new
state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and
popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08
(cross-city). Modules are lightweight, enabling deployable full-surround
perception for legged/humanoid robots. Datasets and code will be publicly
available at https://github.com/MasterHow/OneOcc.

</details>


### [13] [Multi-User Personalisation in Human-Robot Interaction: Using Quantitative Bipolar Argumentation Frameworks for Preferences Conflict Resolution](https://arxiv.org/abs/2511.03576)
*Aniol Civit,Antonio Andriella,Carles Sierra,Guillem Alenyà*

Main category: cs.RO

TL;DR: 提出了一种基于定量双极论证框架的多用户个性化框架(MUP-QBAF)，旨在解决人机交互中多个用户偏好冲突的问题。该框架能够根据环境动态调整，并通过一个实际案例研究进行了验证，展示了如何在护理者和被护理者之间存在冲突偏好的情况下辅助机器人进行调解。


<details>
  <summary>Details</summary>
Motivation: 现有的人类-机器人互动(HRI)个性化方法主要关注单个用户的适应性问题，而忽略了涉及具有潜在冲突偏好的多个利益相关者的场景。为了解决这个问题，作者提出了一个新的多用户个性化框架。

Method: 开发了一个名为MUP-QBAF的新框架，该框架基于定量双极论证框架(QBAFs)，明确地建模并解决了多用户偏好冲突。此方法不仅考虑了用户的论点，还结合了机器人对环境的动态观察，使系统能够随着时间适应并响应变化的情境。

Result: 通过一个现实案例研究呈现并验证了该框架的特性和能力，在这项研究中，辅助机器人在一个脆弱性评估任务中调解了护理者与被护理者之间的偏好冲突。此外，还进行了论据基础分数的敏感性分析，展示了用户输入和情境观察如何影响偏好结果。

Conclusion: 这项工作通过提供一种透明、结构化且对上下文敏感的方法来解决竞争用户偏好问题，推动了多用户HRI领域的发展。它为数据驱动方法提供了一种有原则的替代方案，使机器人能够在真实环境中导航冲突。

Abstract: While personalisation in Human-Robot Interaction (HRI) has advanced
significantly, most existing approaches focus on single-user adaptation,
overlooking scenarios involving multiple stakeholders with potentially
conflicting preferences. To address this, we propose the Multi-User Preferences
Quantitative Bipolar Argumentation Framework (MUP-QBAF), a novel multi-user
personalisation framework based on Quantitative Bipolar Argumentation
Frameworks (QBAFs) that explicitly models and resolves multi-user preference
conflicts. Unlike prior work in Argumentation Frameworks, which typically
assumes static inputs, our approach is tailored to robotics: it incorporates
both users' arguments and the robot's dynamic observations of the environment,
allowing the system to adapt over time and respond to changing contexts.
Preferences, both positive and negative, are represented as arguments whose
strength is recalculated iteratively based on new information. The framework's
properties and capabilities are presented and validated through a realistic
case study, where an assistive robot mediates between the conflicting
preferences of a caregiver and a care recipient during a frailty assessment
task. This evaluation further includes a sensitivity analysis of argument base
scores, demonstrating how preference outcomes can be shaped by user input and
contextual observations. By offering a transparent, structured, and
context-sensitive approach to resolving competing user preferences, this work
advances the field of multi-user HRI. It provides a principled alternative to
data-driven methods, enabling robots to navigate conflicts in real-world
environments.

</details>


### [14] [Manifold-constrained Hamilton-Jacobi Reachability Learning for Decentralized Multi-Agent Motion Planning](https://arxiv.org/abs/2511.03591)
*Qingyi Chen,Ruiqi Ni,Jun Kim,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 提出了一种在任务约束下的多智能体安全运动规划方法，通过结合流形约束的Hamilton-Jacobi可达性学习框架来生成既安全又满足任务要求的运动计划。


<details>
  <summary>Details</summary>
Motivation: 针对机器人在执行任务时需要遵守多种约束条件（如服务机器人持杯移动时需保持杯子直立且避免与人或其他机器人碰撞）的同时进行动态环境导航这一挑战，现有方法难以有效地将这些约束条件融入到分散式多智能体运动规划中。

Method: 开发了一个受流形约束的Hamilton-Jacobi可达性(HJR)学习框架，用于分散式多智能体运动规划。该方法首先解决流形约束下的HJR问题以捕捉任务感知的安全条件，并将这些条件整合进一个分散式的轨迹优化规划器中。

Result: 实验表明，所提方法不仅优于现有的受限运动规划器，而且运行速度达到了实际应用的要求。此外，该方法能够适用于不同的受流形约束的任务，并能有效扩展至高维多智能体操作问题。

Conclusion: 通过引入流形约束的HJR学习框架，本研究为解决复杂的多智能体运动规划问题提供了一条新途径，使机器人能够在不假设其他智能体策略的情况下生成既安全又符合任务需求的动作计划。

Abstract: Safe multi-agent motion planning (MAMP) under task-induced constraints is a
critical challenge in robotics. Many real-world scenarios require robots to
navigate dynamic environments while adhering to manifold constraints imposed by
tasks. For example, service robots must carry cups upright while avoiding
collisions with humans or other robots. Despite recent advances in
decentralized MAMP for high-dimensional systems, incorporating manifold
constraints remains difficult. To address this, we propose a
manifold-constrained Hamilton-Jacobi reachability (HJR) learning framework for
decentralized MAMP. Our method solves HJR problems under manifold constraints
to capture task-aware safety conditions, which are then integrated into a
decentralized trajectory optimization planner. This enables robots to generate
motion plans that are both safe and task-feasible without requiring assumptions
about other agents' policies. Our approach generalizes across diverse
manifold-constrained tasks and scales effectively to high-dimensional
multi-agent manipulation problems. Experiments show that our method outperforms
existing constrained motion planners and operates at speeds suitable for
real-world applications. Video demonstrations are available at
https://youtu.be/RYcEHMnPTH8 .

</details>


### [15] [Multi-robot searching with limited sensing range for static and mobile intruders](https://arxiv.org/abs/2511.03622)
*Swadhin Agrawal,Sujoy Bhore,Joseph S. B. Mitchell,P. B. Sujit,Aayush Gohil*

Main category: cs.RO

TL;DR: 本研究探讨了在正交多边形区域内使用多个搜索机器人寻找入侵者的问题，即使对于静止的入侵者，该问题也被证明是NP难的。为此，提出了基于空间填充曲线、随机搜索和协作随机搜索的有效算法，并评估了搜索机器人数量与完成搜索所需时间之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决在具有限制感知能力的多个搜索机器人的情况下，如何有效率地在一个简单的正交多边形领域内搜寻侵入者的问题。考虑到即使是静态的侵入者，这个问题也是NP难的，因此转向开发高效且鲁棒的算法解决方案。

Method: 研究采用了基于空间填充曲线、随机搜索以及合作式随机搜索的方法来设计算法。同时，对每种算法进行了分析，以评估搜索机器人数量与搜索过程完成时间之间的关系，同时考虑了连通正交搜索区域的几何特性。

Result: 结果表明，所提出的几种算法能够在一定程度上解决给定问题，并且根据不同的几何属性和搜索机器人的数量，存在不同的效率表现。

Conclusion: 尽管找到一个入侵者的问题是NP难的，但通过采用基于空间填充曲线、随机搜索及合作式随机搜索等方法，可以有效地进行搜索。此外，研究还指出了在不同条件下，选择合适的搜索策略的重要性。

Abstract: We consider the problem of searching for an intruder in a geometric domain by
utilizing multiple search robots. The domain is a simply connected orthogonal
polygon with edges parallel to the cartesian coordinate axes. Each robot has a
limited sensing capability. We study the problem for both static and mobile
intruders. It turns out that the problem of finding an intruder is NP-hard,
even for a stationary intruder. Given this intractability, we turn our
attention towards developing efficient and robust algorithms, namely methods
based on space-filling curves, random search, and cooperative random search.
Moreover, for each proposed algorithm, we evaluate the trade-off between the
number of search robots and the time required for the robots to complete the
search process while considering the geometric properties of the connected
orthogonal search area.

</details>


### [16] [Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural](https://arxiv.org/abs/2511.03651)
*Andrei A. Korigodskii,Oleg D. Kalachev,Artem E. Vasiunik,Matvei V. Urvantsev,Georgii E. Bondar*

Main category: cs.RO

TL;DR: 本文介绍了一种创新设计并成功部署的自主无人飞行系统，用于绘制世界上最大的无人机壁画。该系统结合了红外运动捕捉相机和激光雷达技术，以实现精确的位置跟踪，并采用独特的控制架构来确保轨迹跟踪的准确性与线条渲染的稳定性。此外，还开发了专门的喷漆机制以适应无人机螺旋桨产生的湍流气流，保护无人机的关键组件免受油漆损害。实验结果证明了该系统的鲁棒性和在不同条件下的精确度，展示了其在大规模艺术创作中的潜力以及扩展机器人在创意领域应用的可能性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决在户外恶劣条件下（如风、直射阳光）保持艺术精度和操作可靠性的问题，通过开发一种能够高精度导航并在户外绘画的坚固系统，以实现前所未有的大型壁画创作。

Method: 提出了一种将红外运动捕捉相机和激光雷达技术相结合的新颖导航系统，适用于大规模艺术应用；设计了一种独特的控制系统，在相对于计划路径的切向和法向上使用不同的调节方式，以保证精确的轨迹跟踪和稳定的线条绘制；开发了轨迹规划与路径优化算法，支持复杂曲线绘制及区域填充；创建了一个定制的喷漆机制，特别设计以应对无人机螺旋桨产生的湍流气流，同时防止油漆对无人机关键部件造成损害。

Result: 实验结果显示，所提出的系统能够在各种条件下表现出强大的鲁棒性和精确性，不仅验证了其作为自动大规模艺术创作工具的有效性，也为其在更广泛领域的应用提供了可能性。

Conclusion: 本研究展示了一种新颖且有效的解决方案，即利用自主无人机完成高质量的大规模壁画绘制工作，表明了机器人技术在创意产业中有着广阔的应用前景。

Abstract: This paper presents the innovative design and successful deployment of a
pioneering autonomous unmanned aerial system developed for executing the
world's largest mural painted by a drone. Addressing the dual challenges of
maintaining artistic precision and operational reliability under adverse
outdoor conditions such as wind and direct sunlight, our work introduces a
robust system capable of navigating and painting outdoors with unprecedented
accuracy. Key to our approach is a novel navigation system that combines an
infrared (IR) motion capture camera and LiDAR technology, enabling precise
location tracking tailored specifically for largescale artistic applications.
We employ a unique control architecture that uses different regulation in
tangential and normal directions relative to the planned path, enabling precise
trajectory tracking and stable line rendering. We also present algorithms for
trajectory planning and path optimization, allowing for complex curve drawing
and area filling. The system includes a custom-designed paint spraying
mechanism, specifically engineered to function effectively amidst the turbulent
airflow generated by the drone's propellers, which also protects the drone's
critical components from paint-related damage, ensuring longevity and
consistent performance. Experimental results demonstrate the system's
robustness and precision in varied conditions, showcasing its potential for
autonomous large-scale art creation and expanding the functional applications
of robotics in creative fields.

</details>


### [17] [Motion Planning Under Temporal Logic Specifications In Semantically Unknown Environments](https://arxiv.org/abs/2511.03652)
*Azizollah Taheri,Derya Aksaray*

Main category: cs.RO

TL;DR: 提出了一种新的自动机理论方法，通过构建特殊乘积自动机来处理语义标签的不确定性，并为每个边设计了奖励函数，利用值迭代进行在线重新规划，以在不确定环境中实现时空逻辑任务。


<details>
  <summary>Details</summary>
Motivation: 解决在存在环境语义标签不确定性的情况下，如何有效地规划路径以完成时空逻辑任务的问题。

Method: 采用一种新颖的自动机理论方法，其中构造了一个特殊的乘积自动机来捕捉与语义标签相关的不确定性，并为该乘积自动机的每条边设计了奖励函数。所提出的算法利用值迭代来进行在线重规划。

Result: 展示了部分理论成果并通过一些仿真/实验表明了所提方法的有效性。

Conclusion: 本文介绍的方法能够有效应对带有不确定性的时空逻辑任务规划问题，通过结合自动机理论和值迭代技术提供了一种创新解决方案。

Abstract: This paper addresses a motion planning problem to achieve
spatio-temporal-logical tasks, expressed by syntactically co-safe linear
temporal logic specifications (scLTL\next), in uncertain environments. Here,
the uncertainty is modeled as some probabilistic knowledge on the semantic
labels of the environment. For example, the task is "first go to region 1, then
go to region 2"; however, the exact locations of regions 1 and 2 are not known
a priori, instead a probabilistic belief is available. We propose a novel
automata-theoretic approach, where a special product automaton is constructed
to capture the uncertainty related to semantic labels, and a reward function is
designed for each edge of this product automaton. The proposed algorithm
utilizes value iteration for online replanning. We show some theoretical
results and present some simulations/experiments to demonstrate the efficacy of
the proposed approach.

</details>


### [18] [Unconscious and Intentional Human Motion Cues for Expressive Robot-Arm Motion Design](https://arxiv.org/abs/2511.03676)
*Taito Tashiro,Tomoko Yonezawa,Hirotake Yamazoe*

Main category: cs.RO

TL;DR: 研究了人类动作线索如何用于设计表达性的机器人手臂运动，通过分析游戏Geister中玩家的自然游戏动作和指示性表达，并调整机器人运动的速度和停止时间，发现后期运动时机尤其是撤回阶段对印象形成很重要，物理实体增强了动作线索的可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索基于人类动作线索来设计能够传达特定印象的机器人手臂运动的可能性。

Method: 使用名为Geister的游戏分析了两种类型的人类棋子移动动作：自然游戏（无意识倾向）和指示性表达（有意图的提示）。基于这些发现，通过改变移动速度和停止时间创建了相位特定的机器人动作，并在两种呈现模式下评估观察者的印象：实体机器人和录制视频。

Result: 结果表明，在形成印象时，后期的动作时机，特别是撤退期间，起着重要作用，并且物理体现增强了动作线索的可解释性。

Conclusion: 本研究为根据人类的时间行为设计表达性机器人动作提供了见解。

Abstract: This study investigates how human motion cues can be used to design
expressive robot-arm movements. Using the imperfect-information game Geister,
we analyzed two types of human piece-moving motions: natural gameplay
(unconscious tendencies) and instructed expressions (intentional cues). Based
on these findings, we created phase-specific robot motions by varying movement
speed and stop duration, and evaluated observer impressions under two
presentation modalities: a physical robot and a recorded video. Results
indicate that late-phase motion timing, particularly during withdrawal, plays
an important role in impression formation and that physical embodiment enhances
the interpretability of motion cues. These findings provide insights for
designing expressive robot motions based on human timing behavior.

</details>


### [19] [Source-Free Bistable Fluidic Gripper for Size-Selective and Stiffness-Adaptive Grasping](https://arxiv.org/abs/2511.03691)
*Zhihang Qin,Yueheng Zhang,Wan Su,Linxin Hou,Shenghao Zhou,Zhijun Chen,Yu Jun Tan,Cecilia Laschi*

Main category: cs.RO

TL;DR: 本文介绍了一种自给自足的软体夹持器，通过内部液体在三个互连双稳态弹跳腔室之间的重新分配来工作。它能够在接触物体时自动调整抓握压力，并且不需要持续的能量输入。这种设计为轻量级、适应刚度的流体驱动操作提供了新的可能性，特别适合水下和野外环境中的目标特定采样和操作。


<details>
  <summary>Details</summary>
Motivation: 传统的流体驱动软体夹持器通常依赖于外部能源，这限制了其便携性和长期自主性。因此，需要一种不依赖外部源、能够自我维持运作的新方法。

Method: 研究者们开发了一种固定尺寸的自包含软体夹持器，该夹持器通过内部液体在三个相互连接的双稳态转换腔室之间重新分布来单独操作。当顶部感应腔室因接触而变形时，被置换的液体会触发抓取腔室的快速膨胀，从而实现无需连续能量输入即可进行稳定和大小选择性的抓取。

Result: 实验结果表明，该软体夹持器能够有效地根据物体硬度被动调整抓握压力，实现了对不同大小和硬度物体的稳定抓取。此外，由于其内部液压反馈机制的存在，使得夹持器可以根据物体的硬度自动调节抓握力。

Conclusion: 这项研究展示了一种新颖的无源紧凑型设计方案，为轻量化、刚度适应性流体驱动操控开辟了新途径，在软机器人领域特别是在水下及野外环境中针对特定尺寸样本采集与操作方面具有重要应用前景。

Abstract: Conventional fluid-driven soft grippers typically depend on external sources,
which limit portability and long-term autonomy. This work introduces a
self-contained soft gripper with fixed size that operates solely through
internal liquid redistribution among three interconnected bistable snap-through
chambers. When the top sensing chamber deforms upon contact, the displaced
liquid triggers snap-through expansion of the grasping chambers, enabling
stable and size-selective grasping without continuous energy input. The
internal hydraulic feedback further allows passive adaptation of gripping
pressure to object stiffness. This source-free and compact design opens new
possibilities for lightweight, stiffness-adaptive fluid-driven manipulation in
soft robotics, providing a feasible approach for targeted size-specific
sampling and operation in underwater and field environments.

</details>
