<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization](https://arxiv.org/abs/2512.07969)
*Alan Papalia,Nikolas Sanderson,Haoyu Han,Heng Yang,Hanumant Singh,Michael Everett*

Main category: cs.RO

TL;DR: 本文提出了一种针对具有规范对称性问题的变量投影（VarPro）方案，该方法同时利用了可分离性和稀疏性，通过一次性预处理步骤构建了一个无矩阵的舒尔补算子。此算子能够有效评估简化问题的成本、梯度和Hessian-向量积，并且可以轻松地与标准迭代NLS求解器集成。在SLAM、SNL和SfM的真实基准测试中，我们的方法比现有技术快2到35倍，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 机器人感知经常需要解决大规模非线性最小二乘问题。尽管稀疏性已被很好地利用来扩展求解器，但另一种被忽视的结构是可分离性——其中一些变量（如视觉地标）在线性残差中出现，并且对于剩余变量（例如姿态）的任何估计，都有一个封闭形式的解。变量投影(VarPro)方法是一系列利用这种结构的技术，通过分析消除线性变量并呈现剩余变量中的简化问题。然而，在机器人感知中VarPro的应用有限；主要挑战来自于规范对称性，这在感知中很常见并且给标准VarPro方法带来特定的计算挑战。

Method: 作者们提出了一种专为具有规范对量的问题设计的VarPro方案，它同时利用了可分离性和稀疏性。这个方法可以作为一个一次性预处理步骤来构建一个无矩阵舒尔补算子。这个算子允许有效地评估简化问题的成本、梯度和Hessian-向量积，并且容易与标准迭代NLS求解器整合。

Result: 在SLAM、SNL和SfM的合成和真实基准测试中，该方法相比最先进的方法实现了高达2至35倍更快的运行时间，同时保持了准确度。

Conclusion: 提出的VarPro方案成功地解决了带有规范对称性的大规模非线性最小二乘问题，展示了显著的速度提升而不牺牲准确性。此外，研究者还公开了C++实现以及所有实验数据集，以促进进一步的研究和应用。

Abstract: Robotic perception often requires solving large nonlinear least-squares (NLS) problems. While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution. Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties. However, VarPro has seen limited use in robotic perception; a major challenge arises from gauge symmetries (e.g., cost invariance to global shifts and rotations), which are common in perception and induce specific computational challenges in standard VarPro approaches. We present a VarPro scheme designed for problems with gauge symmetries that jointly exploits separability and sparsity. Our method can be applied as a one-time preprocessing step to construct a \emph{matrix-free Schur complement operator}. This operator allows efficient evaluation of costs, gradients, and Hessian-vector products of the reduced problem and readily integrates with standard iterative NLS solvers. We provide precise conditions under which our method applies, and describe extensions when these conditions are only partially met. Across synthetic and real benchmarks in SLAM, SNL, and SfM, our approach achieves up to \textbf{2$\times$--35$\times$ faster runtimes} than state-of-the-art methods while maintaining accuracy. We release an open-source C++ implementation and all datasets from our experiments.

</details>


### [2] [VLD: Visual Language Goal Distance for Reinforcement Learning Navigation](https://arxiv.org/abs/2512.07976)
*Lazar Milikic,Manthan Patel,Jonas Frey*

Main category: cs.RO

TL;DR: 本文提出了一种名为视觉-语言距离（VLD）学习的框架，用于目标条件导航，该框架将感知学习与策略学习分离。通过首先在大规模互联网视频数据上训练一个自监督的目标距离预测器，然后使用强化学习策略最小化这个距离信号来实现导航。这种方法提高了导航策略的可扩展性和多模态适应性。


<details>
  <summary>Details</summary>
Motivation: 当前基于图像数据训练端到端策略以直接预测机器人系统的导航动作面临两大挑战：模拟到现实的迁移差距以及带有动作标签的训练数据量有限。为了解决这些问题，提出了视觉-语言距离（VLD）学习框架。

Method: 1. 利用互联网规模的视频数据训练一个自监督的距离至目标预测器。
2. 该预测器能够跨图像和文本目标提供通用的距离信号。
3. 使用强化学习(RL)策略，在模拟环境中完全利用特权几何距离信号进行训练，并添加噪声模仿训练好的距离预测器的不确定性。
4. 部署时，策略依据VLD预测运行，结合了大规模视觉训练中的语义目标信息同时保留从模拟中习得的稳健低级导航行为。

Result: 实验表明，所提出的解耦设计不仅能够在仿真中达到竞争性的导航性能，而且还支持灵活的目标模式，为可靠、多模态导航策略提供了另一种且更重要的可扩展路径。此外，VLD在性能上优于先前的时间距离方法，如ViNT和VIP。

Conclusion: 视觉-语言距离(VLD)学习框架通过分离感知与策略学习，提供了一条解决现有导航策略局限性的途径，尤其强调了其对于提高导航系统可扩展性和多模态适应性的重要性。

Abstract: Training end-to-end policies from image data to directly predict navigation actions for robotic systems has proven inherently difficult. Existing approaches often suffer from either the sim-to-real gap during policy transfer or a limited amount of training data with action labels. To address this problem, we introduce Vision-Language Distance (VLD) learning, a scalable framework for goal-conditioned navigation that decouples perception learning from policy learning. Instead of relying on raw sensory inputs during policy training, we first train a self-supervised distance-to-goal predictor on internet-scale video data. This predictor generalizes across both image- and text-based goals, providing a distance signal that can be minimized by a reinforcement learning (RL) policy. The RL policy can be trained entirely in simulation using privileged geometric distance signals, with injected noise to mimic the uncertainty of the trained distance predictor. At deployment, the policy consumes VLD predictions, inheriting semantic goal information-"where to go"-from large-scale visual training while retaining the robust low-level navigation behaviors learned in simulation. We propose using ordinal consistency to assess distance functions directly and demonstrate that VLD outperforms prior temporal distance approaches, such as ViNT and VIP. Experiments show that our decoupled design achieves competitive navigation performance in simulation while supporting flexible goal modalities, providing an alternative and, most importantly, scalable path toward reliable, multimodal navigation policies.

</details>


### [3] [DIJIT: A Robotic Head for an Active Observer](https://arxiv.org/abs/2512.07998)
*Mostafa Kamali Tabrizi,Mingshi Chi,Bir Bikram Dey,Yu Qing Yuan,Markus D. Solbach,Yiqian Liu,Michael Jenkin,John K. Tsotsos*

Main category: cs.RO

TL;DR: 本文介绍了一种名为DIJIT的新型双目机器人头部设计，它具有九个机械自由度和四个光学自由度，旨在研究主动视觉以及类似人类的眼睛和头部-颈部动作。DIJIT还被用来探索人类视觉与当前计算机视觉方法在执行视觉任务时使用眼/头运动方式上的差异。此外，文中提出了一种新的用于实现接近人类准确度的扫视相机移动的方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够模仿人类眼睛和头部-颈部运动的机器人头部，以便更好地理解这些运动如何促进视觉能力，并探索它们对于解决视觉任务的重要性。同时也为了研究人类视觉系统与现有计算机视觉技术之间的区别。

Method: 通过构建一个具备九个机械自由度（包括调节、版本控制和旋转向量）及额外四个光学自由度的双目机器人头部来达到研究目的。提出并实施了一种新的基于直接映射相机方向到电机值关系的方法来进行快速而精确的相机扫视运动。

Result: 成功地创建了一个功能齐全的DIJIT原型，其性能方面如运动范围和速度都与人类相似。新提出的相机扫视运动方法能够产生接近人类精度的动作表现。

Conclusion: DIJIT为研究主动视觉提供了强有力的支持，并且有助于增进我们对人眼与头部协调工作以完成复杂视觉任务的理解。同时，它也为改进现有的计算机视觉算法提供了一种新思路。

Abstract: We present DIJIT, a novel binocular robotic head expressly designed for mobile agents that behave as active observers. DIJIT's unique breadth of functionality enables active vision research and the study of human-like eye and head-neck motions, their interrelationships, and how each contributes to visual ability. DIJIT is also being used to explore the differences between how human vision employs eye/head movements to solve visual tasks and current computer vision methods. DIJIT's design features nine mechanical degrees of freedom, while the cameras and lenses provide an additional four optical degrees of freedom. The ranges and speeds of the mechanical design are comparable to human performance. Our design includes the ranges of motion required for convergent stereo, namely, vergence, version, and cyclotorsion. The exploration of the utility of these to both human and machine vision is ongoing. Here, we present the design of DIJIT and evaluate aspects of its performance. We present a new method for saccadic camera movements. In this method, a direct relationship between camera orientation and motor values is developed. The resulting saccadic camera movements are close to human movements in terms of their accuracy.

</details>


### [4] [An Introduction to Deep Reinforcement and Imitation Learning](https://arxiv.org/abs/2512.08052)
*Pedro Santana*

Main category: cs.RO

TL;DR: 本文介绍了在具身智能体（如机器人和虚拟角色）中应用深度强化学习(DRL)与深度模仿学习(DIL)的方法，以解决复杂的序列决策问题。文章采用深入浅出的方式讲解了从马尔可夫决策过程到REINFORCE、近端策略优化(PPO)等DRL算法，以及行为克隆、数据集聚合(DAgger)和生成对抗模仿学习(GAIL)等DML技术。


<details>
  <summary>Details</summary>
Motivation: 面对具身智能体需要处理的复杂序列决策问题，手动设计控制器变得十分困难。因此，基于学习的方法，特别是深度强化学习和深度模仿学习，成为了解决这类问题的有效途径。

Method: 通过介绍一系列基础性的算法和技术，包括但不限于马尔可夫决策过程、REINFORCE、近端策略优化用于深度强化学习部分；行为克隆、数据集聚合及生成对抗模仿学习则构成了深度模仿学习部分的核心内容。

Result: 该文档为读者提供了一个关于如何利用深度强化学习和深度模仿学习来训练具身智能体执行任务的全面指南。

Conclusion: 本文档旨在为对使用深度强化学习和深度模仿学习感兴趣的读者提供一个易于理解的学习资源，专注于少数几个关键算法的理解而非广泛覆盖整个领域。

Abstract: Embodied agents, such as robots and virtual characters, must continuously select actions to execute tasks effectively, solving complex sequential decision-making problems. Given the difficulty of designing such controllers manually, learning-based approaches have emerged as promising alternatives, most notably Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL). DRL leverages reward signals to optimize behavior, while DIL uses expert demonstrations to guide learning. This document introduces DRL and DIL in the context of embodied agents, adopting a concise, depth-first approach to the literature. It is self-contained, presenting all necessary mathematical and machine learning concepts as they are needed. It is not intended as a survey of the field; rather, it focuses on a small set of foundational algorithms and techniques, prioritizing in-depth understanding over broad coverage. The material ranges from Markov Decision Processes to REINFORCE and Proximal Policy Optimization (PPO) for DRL, and from Behavioral Cloning to Dataset Aggregation (DAgger) and Generative Adversarial Imitation Learning (GAIL) for DIL.

</details>


### [5] [RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features](https://arxiv.org/abs/2512.08170)
*Haoxin Zhang,Shuaixin Li,Xiaozhou Zhu,Hongbo Chen,Wen Yao*

Main category: cs.RO

TL;DR: 本文提出了一种用户友好的LiDAR-相机校准工具包，它兼容多种传感器，并且只需要一对激光点和一张相机图像即可在无目标环境中工作。通过Gluestick流程来建立2D-3D特征对应关系以获得鲁棒的初始估计，并通过量化分析特征分布对校准结果的影响来自适应地调整每个特征的成本权重。实验验证了该方法在不同设置下的优越性，并且代码已经在GitHub上开源。


<details>
  <summary>Details</summary>
Motivation: 开发一种不依赖于特定目标物、能够处理大范围位姿偏差并且对于各种LiDAR-相机传感器都适用的简便而准确的校准工具。

Method: 利用Gluestick流程建立2D-3D点线特征对应；量化分析特征分布对校准结果的影响并自适应加权；优化外参以减少不良特征带来的负面影响。

Result: 本方法在多种室内与室外环境下的不同LiDAR-相机传感器上进行了广泛测试，展示了相比现有技术更好的鲁棒性和准确性。

Conclusion: 所提出的LiDAR-相机校准工具包不仅简化了校准过程，而且提高了校准精度和鲁棒性，为研究者们提供了便利。

Abstract: In this paper, we present a user-friendly LiDAR-camera calibration toolkit that is compatible with various LiDAR and camera sensors and requires only a single pair of laser points and a camera image in targetless environments. Our approach eliminates the need for an initial transform and remains robust even with large positional and rotational LiDAR-camera extrinsic parameters. We employ the Gluestick pipeline to establish 2D-3D point and line feature correspondences for a robust and automatic initial guess. To enhance accuracy, we quantitatively analyze the impact of feature distribution on calibration results and adaptively weight the cost of each feature based on these metrics. As a result, extrinsic parameters are optimized by filtering out the adverse effects of inferior features. We validated our method through extensive experiments across various LiDAR-camera sensors in both indoor and outdoor settings. The results demonstrate that our method provides superior robustness and accuracy compared to SOTA techniques. Our code is open-sourced on GitHub to benefit the community.

</details>


### [6] [Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation](https://arxiv.org/abs/2512.08186)
*Meng Wei,Chenyang Wan,Jiaqi Peng,Xiqian Yu,Yuqiang Yang,Delin Feng,Wenzhe Cai,Chenming Zhu,Tai Wang,Jiangmiao Pang,Xihui Liu*

Main category: cs.RO

TL;DR: 提出了DualVLN，一种双系统视觉-语言导航基础模型，结合高层次推理与低层次动作执行，通过VLM全局规划和轻量级多模态条件扩散变换器策略实现平滑轨迹生成，在复杂动态环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言导航方法通常依赖于端到端的管道，直接将视觉-语言输入映射为短期离散动作，导致运动片段化、延迟高，并难以应对现实世界中的挑战如动态避障。

Method: 开发了DualVLN，该模型包含两个子系统：基于VLM的全局规划者（System 2）负责中期路点目标预测；轻量级多模态条件扩散变换器策略（System 1）则利用来自System 2的具体像素目标及潜在特征来生成平稳精确的轨迹。

Result: DualVLN在所有视觉-语言导航基准测试中超越了先前的方法，并且实验证明其具有强大的长期规划能力和实时适应性，特别是在动态环境当中。

Conclusion: DualVLN通过整合高级别的思考与底层的动作实施，实现了鲁棒性的实时控制以及在复杂多变环境下的自适应局部决策制定能力，标志着向更高效视觉-语言导航解决方案迈进了一步。

Abstract: While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, "grounds slowly" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, "moves fast" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.

</details>


### [7] [Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model](https://arxiv.org/abs/2512.08188)
*Wenjiang Xu,Cindy Wang,Rui Fang,Mingkang Zhang,Lusong Li,Jing Xu,Jiayuan Gu,Zecui Zeng,Rui Chen*

Main category: cs.RO

TL;DR: 提出了一种新的Embodied Tree of Thoughts (EToT)框架，该框架利用基于物理的交互式数字孪生作为体现的世界模型，并通过先验分支和反思分支两种机制扩展树搜索，以确保生成的计划遵循刚体动力学和碰撞约束。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型在机器人操作规划中往往缺乏严格的物理基础，导致幻觉产生以及无法在长时序内保持物理一致性。

Method: Embodied Tree of Thoughts (EToT)，一种新的Real2Sim2Real规划框架，它利用基于物理的互动数字孪生作为体现的世界模型。该方法通过先验分支（Priori Branching）和反思分支（Reflective Branching）两种协同机制来展开操纵规划的树搜索。

Result: EToT在一系列短时和长时操作任务上得到了验证，与基线相比，它能够有效地预测物理动态并适应潜在的失败情况。

Conclusion: EToT框架通过将高层次推理建立于物理模拟器之上，保证了所生成计划符合刚体动力学及碰撞限制条件，从而为解决现有视频生成模型在机器人操作规划中的局限性提供了有效方案。

Abstract: World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .

</details>


### [8] [High-Performance Dual-Arm Task and Motion Planning for Tabletop Rearrangement](https://arxiv.org/abs/2512.08206)
*Duo Zhang,Junshan Huang,Jingjin Yu*

Main category: cs.RO

TL;DR: 提出了一种新的双臂同步重排规划器（SDAR），能够高效解决桌面物体重组任务，尤其适用于起始和目标配置紧密交织的情况。通过将任务规划与动作规划紧密结合，并采用先进的GPU并行处理技术，SDAR在复杂、非单调的长周期任务中实现了100%的成功率，且解决方案质量远超现有技术水平。


<details>
  <summary>Details</summary>
Motivation: 针对双臂机器人在密闭空间内合作完成物体重组任务时面临的挑战，特别是当物体的起始位置与目标位置高度交错的情况下，需要一种有效的方法来生成最优的任务与动作规划方案。

Method: SDAR结合了依赖性驱动的任务规划器(SDAR-T)和同步双臂运动规划器(SDAR-M)，其中SDAR-T通过分解全局对象依赖图产生更优的双臂任务计划；而SDAR-M则利用最新的基于GPU SIMD的运动规划工具，采取分层运动规划策略从多个任务计划中筛选出最佳同步双臂运动计划。

Result: 综合评估表明，SDAR在解决复杂的、非单调的、长时间范围内的桌面重组任务方面达到了100%的成功率，其解决方案的质量远高于之前的最先进水平。此外，在两个UR-5e机械臂上的实验进一步证实了SDAR可以直接可靠地转移到机器人硬件上。

Conclusion: 研究成功开发了一种名为SDAR的新框架，它能够显著提高双臂机器人执行复杂桌面重组任务的能力。该方法不仅理论上有优势，在实际应用中也表现出了很好的适应性和可靠性。

Abstract: We propose Synchronous Dual-Arm Rearrange- ment Planner (SDAR), a task and motion planning (TAMP) framework for tabletop rearrangement, where two robot arms equipped with 2-finger grippers must work together in close proximity to rearrange objects whose start and goal config- urations are strongly entangled. To tackle such challenges, SDAR tightly knit together its dependency-driven task planner (SDAR-T) and synchronous dual-arm motion planner (SDAR- M), to intelligently sift through a large number of possible task and motion plans. Specifically, SDAR-T applies a simple yet effective strategy to decompose the global object dependency graph induced by the rearrangement task, to produce more optimal dual-arm task plans than solutions derived from optimal task plans for a single arm. Leveraging state-of-the-art GPU SIMD-based motion planning tools, SDAR-M employs a layered motion planning strategy to sift through many task plans for the best synchronous dual-arm motion plan while ensuring high levels of success rate. Comprehensive evaluation demonstrates that SDAR delivers a 100% success rate in solving complex, non-monotone, long-horizon tabletop rearrangement tasks with solution quality far exceeding the previous state- of-the-art. Experiments on two UR-5e arms further confirm SDAR directly and reliably transfers to robot hardware.

</details>


### [9] [Semantic-Metric Bayesian Risk Fields: Learning Robot Safety from Human Videos with a VLM Prior](https://arxiv.org/abs/2512.08233)
*Timothy Chen,Marcus Dominguez-Kuhne,Aiden Swann,Xu Liu,Mac Schwager*

Main category: cs.RO

TL;DR: 本文提出了一种从安全的人类演示视频和视觉-语言模型常识中提取隐含人类风险模型的框架，通过贝叶斯公式定义风险，并能够生成与像素对齐的风险图像。这些图像可以用于机器人规划任务或投影到3D以产生类似人类的动作，使得自主系统能够内化类似于人类的风险认知。


<details>
  <summary>Details</summary>
Motivation: 人们对于安全的理解不是二元的，而是基于连续、依赖于上下文和空间的风险概念。尽管风险是主观的，但人们形成了理性的心理模型来指导在动态环境中的行动选择。为了使机器学习模型更好地理解并模仿这种人类风险感知的方式，本文提出了一个新框架。

Method: 该研究引入了一种新颖的、语义条件下的空间变化风险参数化方法，直接由安全的人类示范视频和视觉-语言模型（VLM）常识监督。通过贝叶斯公式定义风险，其中先验由预训练的视觉-语言模型提供；而为了使风险估计更符合人类的认知，利用一个学习得到的视觉变换器（ViT）作为似然函数调整先验，从而产生相对风险度量。

Result: 所提出的框架能够生成与像素对齐的风险图，可用于机器人规划任务或转换为三维空间以优化路径规划，实现更接近人类偏好的动作。此外，还展示了几个下游应用示例，如作为视觉运动规划者的价值学习工具或与经典轨迹优化算法结合使用。

Conclusion: 这项工作表明，所提出的框架在生产符合人类偏好情境下风险方面表现良好，并且有潜力应用于更大规模的数据集训练。特别是引入的贝叶斯框架允许模型快速适应额外观察或常识规则的变化，标志着向让自主系统内化类人风险迈出了重要一步。

Abstract: Humans interpret safety not as a binary signal but as a continuous, context- and spatially-dependent notion of risk. While risk is subjective, humans form rational mental models that guide action selection in dynamic environments. This work proposes a framework for extracting implicit human risk models by introducing a novel, semantically-conditioned and spatially-varying parametrization of risk, supervised directly from safe human demonstration videos and VLM common sense. Notably, we define risk through a Bayesian formulation. The prior is furnished by a pretrained vision-language model. In order to encourage the risk estimate to be more human aligned, a likelihood function modulates the prior to produce a relative metric of risk. Specifically, the likelihood is a learned ViT that maps pretrained features, to pixel-aligned risk values. Our pipeline ingests RGB images and a query object string, producing pixel-dense risk images. These images that can then be used as value-predictors in robot planning tasks or be projected into 3D for use in conventional trajectory optimization to produce human-like motion. This learned mapping enables generalization to novel objects and contexts, and has the potential to scale to much larger training datasets. In particular, the Bayesian framework that is introduced enables fast adaptation of our model to additional observations or common sense rules. We demonstrate that our proposed framework produces contextual risk that aligns with human preferences. Additionally, we illustrate several downstream applications of the model; as a value learner for visuomotor planners or in conjunction with a classical trajectory optimization algorithm. Our results suggest that our framework is a significant step toward enabling autonomous systems to internalize human-like risk. Code and results can be found at https://riskbayesian.github.io/bayesian_risk/.

</details>


### [10] [Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation](https://arxiv.org/abs/2512.08271)
*Srijan Dokania,Dharini Raghavan*

Main category: cs.RO

TL;DR: Zero-Splat TeleAssist 是一种零样本传感器融合管道，它将普通CCTV流转换为共享的6自由度世界模型，用于多边遥操作。通过整合视觉-语言分割、单目深度、加权PCA姿态提取和3D高斯点绘技术，TeleAssist能够在没有标记物或深度传感器的情况下，向每个操作员提供多个机器人实时的全局位置和方向信息。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在开发一种无需特殊标记或深度传感器就能在多边遥操作环境中提供给所有操作员一个共享且精确的世界模型的方法。

Method: 采用视觉-语言分割、单目深度估计、基于加权PCA的姿态提取以及3D高斯点绘等技术相结合的方式构建了一个名为Zero-Splat TeleAssist的系统。

Result: 成功实现了从商用CCTV视频流中生成共享的6自由度世界模型，并能够实时更新多个机器人的位置与姿态信息给所有参与者。

Conclusion: Zero-Splat TeleAssist展示了一种新的方法来支持无标记多边遥操作系统中的空间感知需求，为未来更广泛的应用场景提供了可能性。

Abstract: We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.

</details>


### [11] [Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging](https://arxiv.org/abs/2512.08333)
*Yajat Yadav,Zhiyuan Zhou,Andrew Wagenmaker,Karl Pertsch,Sergey Levine*

Main category: cs.RO

TL;DR: 本文提出了一种通过插值微调模型和预训练模型权重的方法，使得通用机器人策略在学习新技能的同时保持其原有的广泛能力。实验表明，该方法不仅能够使单一模型稳健地解决新任务，还能在持续学习过程中不断获取新技能而不牺牲先前学到的通用能力。


<details>
  <summary>Details</summary>
Motivation: 尽管通用机器人策略在多样化的数据集上训练后能够执行多种行为，并适应各种真实环境，但在面对未包含在训练数据中的新任务时表现不佳。直接对少量新任务示例进行微调会导致过拟合问题，即丧失了处理广泛任务的能力，同时在新任务上的泛化能力也较差。为了解决这个问题，研究旨在开发一种方法，在微调过程中保留通用策略的泛化能力，允许单一策略稳定地将新技能融入其技能库中。

Method: 研究采用了一种简单而有效的方法：将微调后的模型权重与预训练模型权重进行插值。这种模型合并技术旨在创建一个既能继承基础模型的通用能力又能学会稳健解决新任务的单一模型。

Result: 通过广泛的模拟和现实世界实验验证，发现这种方法确实能够产生一个既拥有基础模型泛化能力又能在新任务上表现出色（尤其是在分布外变体上）的单一模型。此外，研究表明模型合并还支持在终身学习场景下连续获取新技能，而不会牺牲之前学得的通用能力。

Conclusion: 本研究表明，通过简单地插值微调模型与预训练模型之间的权重，可以有效地让通用机器人策略在不失去原有广泛能力的前提下学会新技能。这为开发更加灵活且可扩展的机器人系统提供了新的思路。

Abstract: Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.

</details>


### [12] [Learning Robot Manipulation from Audio World Models](https://arxiv.org/abs/2512.08405)
*Fan Zhang,Michael Gienger*

Main category: cs.RO

TL;DR: 本文提出了一种生成性潜在流匹配模型来预测未来的音频观察，以支持机器人策略中的长期推理。通过两个需要感知自然音频或音乐信号的操作任务，证明了该系统相比没有未来前瞻的方法具有更好的能力，并强调了准确预测未来音频状态对于成功学习机器人动作的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于许多机器人学习任务本质上要求多模态推理，仅依赖视觉信息可能不足以完成任务，例如在给水瓶加水时，视觉信息可能是模糊或不完整的，因此需要考虑音频随时间变化的物理属性和音高模式来进行推理。

Method: 提出了一种生成性的潜在流匹配模型，用于预测未来的音频观察，这有助于系统在集成到机器人策略中时对长期后果进行推理。

Result: 通过两个需要感知自然音频或音乐信号的操作任务表明，所提系统比那些不具备未来预见能力的方法表现更优。

Conclusion: 成功的机器人动作学习不仅依赖于多模态输入，而且关键在于能够准确地预测代表内在节奏模式的未来音频状态。

Abstract: World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.

</details>


### [13] [Prospect Theory in Physical Human-Robot Interaction: A Pilot Study of Probability Perception](https://arxiv.org/abs/2512.08481)
*Yixiang Lin,Tiancheng Yang,Jonathan Eden,Ying Tan*

Main category: cs.RO

TL;DR: 该研究通过一个物理耦合的目标到达任务，考察了人类在与机器人互动过程中面对不确定性时的行为模式。结果发现参与者可以根据干扰概率调整其反应，分为'权衡'组和'总是补偿'组两大行为类别。这表明个体间存在显著差异，并强调了使用如累积前景理论这样的模型来更好地理解和预测这些行为的重要性，以指导未来适应性机器人控制器的设计。


<details>
  <summary>Details</summary>
Motivation: 理解人类如何应对不确定性对于设计安全有效的物理人机交互至关重要。由于与机器人进行物理合作会引入信任、舒适度和感知安全性等多方面的不确定性，而传统的人机交互控制框架通常基于最优控制理论，假设人类行为旨在最小化成本函数；然而，在不确定性条件下，人类的行为往往偏离这种最优模式。因此，需要对不确定性下的人类行为有更深入的理解。

Method: 研究者实施了一个物理耦合的目标到达任务，其中机器人以系统变化的概率（10%到90%）提供帮助或制造干扰。通过对参与者施加的力量输入及决策策略的分析，揭示了不同概率下人们的行为模式。

Result: 分析显示存在两种明显不同的行为集群：一种是根据干扰可能性调节其物理响应的“权衡”组；另一种则是表现出强烈风险规避倾向的“总是补偿”组，无论概率高低都倾向于过度反应。

Conclusion: 研究表明，在物理人机交互中人类的决策过程高度个性化，且对概率的认知可能与其真实值有所偏差。为此，研究指出需要采用更加可解释的行为模型，比如累积前景理论，以便更准确地捕捉这些行为特征，并为未来适应性机器人控制器的设计提供信息。

Abstract: Understanding how humans respond to uncertainty is critical for designing safe and effective physical human-robot interaction (pHRI), as physically working with robots introduces multiple sources of uncertainty, including trust, comfort, and perceived safety. Conventional pHRI control frameworks typically build on optimal control theory, which assumes that human actions minimize a cost function; however, human behavior under uncertainty often departs from such optimal patterns. To address this gap, additional understanding of human behavior under uncertainty is needed. This pilot study implemented a physically coupled target-reaching task in which the robot delivered assistance or disturbances with systematically varied probabilities (10\% to 90\%). Analysis of participants' force inputs and decision-making strategies revealed two distinct behavioral clusters: a "trade-off" group that modulated their physical responses according to disturbance likelihood, and an "always-compensate" group characterized by strong risk aversion irrespective of probability. These findings provide empirical evidence that human decision-making in pHRI is highly individualized and that the perception of probability can differ to its true value. Accordingly, the study highlights the need for more interpretable behavioral models, such as cumulative prospect theory (CPT), to more accurately capture these behaviors and inform the design of future adaptive robot controllers.

</details>


### [14] [SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking](https://arxiv.org/abs/2512.08518)
*Nadezhda Kushina,Ko Watanabe,Aarthi Kannan,Ashita Ashok,Andreas Dengel,Karsten Berns*

Main category: cs.RO

TL;DR: 本研究探索了用户与人形机器人Ameca互动时的舒适度，通过移动眼动追踪和主观报告在四个受控距离下（0.5米至2.0米）进行评估。结果显示，决策树分类器基于凝视特征估计舒适度表现最佳(F1分数=0.73)，瞳孔直径是最关键的预测指标。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索眼动追踪特性是否能够可靠地估计人类与人形机器人互动时的舒适度，填补先前研究仅关注人际互动中的空白。

Method: 采用移动眼动追踪技术结合主观报告法，在不同实验控制的距离上评估参与者对名为Ameca的人形机器人的舒适感受，并使用多种机器学习及深度学习模型来基于凝视特征估算舒适度。

Result: 与之前关于人际互动的研究结果相反，在本次研究中，决策树分类器表现出色，F1分数达到0.73；最小瞳孔直径被确定为最重要的预测因子。

Conclusion: 该研究表明，人类-机器人互动中生理舒适阈值与人际互动存在差异，且可以通过可解释性强的方法有效建模。

Abstract: Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot "Ameca" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.

</details>


### [15] [vEDGAR - Can CARLA Do HiL?](https://arxiv.org/abs/2512.08541)
*Nils Gehrke,David Brecht,Dominik Kulmer,Dheer Patel,Frank Diermeyer*

Main category: cs.RO

TL;DR: 本文旨在创建一个仿真框架，以测试自动化驾驶软件在其专用硬件上运行的情况，并确定其极限。通过修改CARLA模拟器并开发vEDGAR软件，研究评估了CARLA作为高保真度在环测试工具的适用性。


<details>
  <summary>Details</summary>
Motivation: 当前开源模拟器如CARLA被广泛用于训练、评估和软件在环测试新的自动化驾驶算法，但缺乏对研究及自动化驾驶车辆连同它们完整的传感器与执行器堆栈进行实时仿真的评估。

Method: 首先定义需求并指定和实现了一个仿真架构。基于这些需求，评估了所提出的vEDGAR软件，最终得出关于CARLA应用于自动化车辆HiL测试可行性的结论。

Result: 开发了vEDGAR软件，并对修改后的CARLA模拟器进行了评估，结果表明该方法有助于提高开放源代码自动化驾驶功能开发流程的一致性和效率。

Conclusion: 通过创建针对自动化软件及其硬件的仿真框架，可以更好地识别自动化驾驶功能的限制，从而为整个开发过程中的持续评估提供支持。

Abstract: Simulation offers advantages throughout the development process of automated driving functions, both in research and product development. Common open-source simulators like CARLA are extensively used in training, evaluation, and software-in-the-loop testing of new automated driving algorithms. However, the CARLA simulator lacks an evaluation where research and automated driving vehicles are simulated with their entire sensor and actuation stack in real time. The goal of this work is therefore to create a simulation framework for testing the automation software on its dedicated hardware and identifying its limits. Achieving this goal would greatly benefit the open-source development workflow of automated driving functions, designating CARLA as a consistent evaluation tool along the entire development process. To achieve this goal, in a first step, requirements are derived, and a simulation architecture is specified and implemented. Based on the formulated requirements, the proposed vEDGAR software is evaluated, resulting in a final conclusion on the applicability of CARLA for HiL testing of automated vehicles. The tool is available open source: Modified CARLA fork: https://github.com/TUMFTM/carla, vEDGAR Framework: https://github.com/TUMFTM/vEDGAR

</details>


### [16] [Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations](https://arxiv.org/abs/2512.08548)
*Yuchi Zhang,Churui Sun,Shiqi Liang,Diyuan Liu,Chao Ji,Wei-Nan Zhang,Ting Liu*

Main category: cs.RO

TL;DR: 研究提出了一种基于语义的语用表示方法，用于归一化动作以提高预训练效率。该方法通过强调方向性而非数值大小来减少分布偏移，并缩小了动作标记与标准词汇标记之间的特征距离，从而提高了机器人操作任务中的泛化性能和迁移性。


<details>
  <summary>Details</summary>
Motivation: 当前端到端机器人操作研究中越来越多地采用受大型语言模型启发的架构，但不同机器人平台和任务间动作命令数值的巨大差异导致了严重的分布偏移问题，限制了预训练知识的有效转移。

Method: 提出一种语义基础的语言表达方式，用于动作标准化处理，特别强调方向而非数值规模的影响，以此来缓解分布偏移并拉近动作令牌与标准词汇令牌间的模态差距。

Result: 在两个基准测试上的多任务实验表明，所提方法显著提升了机器人操作任务中的泛化表现和跨任务迁移能力。

Conclusion: 本研究介绍的方法通过引入更加通用的动作表示形式，有效解决了因数值尺度变化引起的分布偏移问题，促进了预训练模型在多样化机器人平台及任务上的应用。

Abstract: Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks.

</details>


### [17] [RVC-NMPC: Nonlinear Model Predictive Control with Reciprocal Velocity Constraints for Mutual Collision Avoidance in Agile UAV Flight](https://arxiv.org/abs/2512.08574)
*Vit Kratky,Robert Penicka,Parakh M. Gupta,Ondrej Prochazka,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出了一种基于非线性模型预测控制（NMPC）和时变互惠速度约束（RVCs）的相互避碰方法，仅依赖于对其他机器人可观察信息，无需过多通信。该算法计算效率高，整个流程运行频率可达100Hz，并且在涉及多达10架无人机、速度高达25m/s的模拟实验以及加速度达到30m/s^2的真实世界实验中进行了评估，与现有技术相比，在具有挑战性的场景中飞行时间减少了31%，同时所有试验中均保持无碰撞导航。


<details>
  <summary>Details</summary>
Motivation: 传统的避碰方法往往需要大量的机器间通讯来协调行动，这对资源是一个不小的负担。为了减少对通讯的依赖并提高处理效率，研究者们提出了这种基于非线性模型预测控制(NMPC)结合时变互惠速度约束(RVCs)的新方法。

Method: 本研究采用的方法是将时变互惠速度约束直接集成到非线性模型预测控制的问题公式化过程中，通过设计一个计算效率高的RVCs算法，使得整个系统能够以100赫兹的速度运行。此方法考虑了无人机的非线性动力学特性，适用于敏捷飞行。

Result: 通过对仿真环境下的多UAV测试（最高速度达25m/s）及真实环境中加速度高达30m/s^2的情况进行验证，结果表明新方法比现有技术表现更好，在困难条件下能减少31%的飞行时间，并且保证了全程无碰撞。

Conclusion: 所提出的方法有效提高了无人机编队飞行时的避障效率，降低了对通信资源的需求，为实现更高效、更安全的无人飞行器操作提供了新的途径。

Abstract: This paper presents an approach to mutual collision avoidance based on Nonlinear Model Predictive Control (NMPC) with time-dependent Reciprocal Velocity Constraints (RVCs). Unlike most existing methods, the proposed approach relies solely on observable information about other robots, eliminating the necessity of excessive communication use. The computationally efficient algorithm for computing RVCs, together with the direct integration of these constraints into NMPC problem formulation on a controller level, allows the whole pipeline to run at 100 Hz. This high processing rate, combined with modeled nonlinear dynamics of the controlled Uncrewed Aerial Vehicles (UAVs), is a key feature that facilitates the use of the proposed approach for an agile UAV flight. The proposed approach was evaluated through extensive simulations emulating real-world conditions in scenarios involving up to 10 UAVs and velocities of up to 25 m/s, and in real-world experiments with accelerations up to 30 m/s$^2$. Comparison with state of the art shows 31% improvement in terms of flight time reduction in challenging scenarios, while maintaining a collision-free navigation in all trials.

</details>


### [18] [Mind to Hand: Purposeful Robotic Control via Embodied Reasoning](https://arxiv.org/abs/2512.08580)
*Peijun Tang,Shangjin Xie,Binyan Sun,Baifu Huang,Kuncheng Luo,Haotian Yang,Weiqi Jin,Jianan Wang*

Main category: cs.RO

TL;DR: 本文介绍了一种名为Lumo-1的新型视觉-语言-动作模型，该模型通过三阶段预训练流程将机器人推理与物理动作相结合，从而在多种复杂的机器人任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管互联网规模的数据已经使AI系统具备了广泛的推理能力，但将这些能力落实到具体的物理行动上仍然是一个主要挑战。研究旨在通过开发一种能够结合思维和行动的通用模型来解决这一问题。

Method: Lumo-1基于预先训练好的视觉-语言模型（VLMs）发展而来，并通过三个阶段的预训练流程进行扩展：继续使用精选的视觉-语言数据对VLM进行预训练以增强具身推理技能；跨实体机器人数据与视觉-语言数据联合训练；以及在Astribot S1双臂移动操作器上收集的动作轨迹上进行动作训练。此外，还集成了强化学习以进一步提高推理-行动的一致性。

Result: 实验表明，Lumo-1在具身视觉-语言推理方面取得了显著的性能提升，这是实现通用机器人控制的关键组成部分。真实世界评估显示，Lumo-1不仅超过了强大的基准模型，在面对新物体和环境时展现出强大的泛化能力，而且特别擅长处理需要长时间规划及理解人类自然指令的任务。

Conclusion: Lumo-1为结合机器人推理与实际操作提供了一个有效的方法，这标志着向创建更加灵活且适应性强的机器人助手迈出了重要一步。

Abstract: Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning ("mind") with robot action ("hand"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.

</details>


### [19] [Multi-Task Bayesian Optimization for Tuning Decentralized Trajectory Generation in Multi-UAV Systems](https://arxiv.org/abs/2512.08630)
*Marta Manzoni,Alessandro Nazzari,Roberto Rubinacci,Marco Lovera*

Main category: cs.RO

TL;DR: 本文探讨了多任务贝叶斯优化在调整多无人机系统中分散轨迹生成算法的应用，通过模拟表明单任务优化虽然能随着无人机群规模的增加而逐渐缩短任务时间，但其优化所需的时间远大于跨任务平均优化方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何利用多任务贝叶斯优化来调优多无人机系统中的分散轨迹生成算法，并通过建模不同场景之间的关系以实现高效的信息传递与优化。

Method: 采用了多任务高斯过程来捕捉任务间的共享结构，同时比较了两种策略：一种是针对所有任务平均任务时间进行优化，另一种则是对每个任务单独进行优化。

Result: 结果显示，尽管单任务优化方法能够随着无人机群规模增长而逐步减少任务执行时间，但是相比于采用跨任务平均优化的方法，它需要消耗更多的时间来进行优化。

Conclusion: 结论指出，在考虑优化效率时，对于多无人机系统的轨迹生成算法来说，采取跨任务平均优化策略可能是更优的选择。

Abstract: This paper investigates the use of Multi-Task Bayesian Optimization for tuning decentralized trajectory generation algorithms in multi-drone systems. We treat each task as a trajectory generation scenario defined by a specific number of drone-to-drone interactions. To model relationships across scenarios, we employ Multi-Task Gaussian Processes, which capture shared structure across tasks and enable efficient information transfer during optimization. We compare two strategies: optimizing the average mission time across all tasks and optimizing each task individually. Through a comprehensive simulation campaign, we show that single-task optimization leads to progressively shorter mission times as swarm size grows, but requires significantly more optimization time than the average-task approach.

</details>


### [20] [A Sensor-Aware Phenomenological Framework for Lidar Degradation Simulation and SLAM Robustness Evaluation](https://arxiv.org/abs/2512.08653)
*Doumegna Mawuto Koudjo Felix,Xianjia Yu,Zhuo Zou,Tomi Westerlund*

Main category: cs.RO

TL;DR: 本文提出了一种传感器感知的、现象学框架，用于在真实点云上直接模拟可解释的激光雷达退化，从而实现可控和可重复的SLAM压力测试。


<details>
  <summary>Details</summary>
Motivation: 现有的基于激光雷达的SLAM系统对遮挡、噪声以及视野降级等不利条件非常敏感，但当前的鲁棒性评估方法要么缺乏物理基础，要么无法捕捉特定于传感器的行为。

Method: 该框架通过在保持每个点几何、强度和时间结构的同时，应用结构化丢弃、视野缩减、高斯噪声、遮挡掩模、稀疏化和运动失真等手段来模拟激光雷达退化。此外，该系统具有自动主题和传感器检测功能，模块化配置支持四个严重程度级别（轻度至极端），并且与ROS工作流程兼容，每帧处理时间少于20毫秒。

Result: 实验验证了三种激光雷达架构和五种最先进的SLAM系统，在不同传感器设计和环境背景下显示出不同的鲁棒性模式。

Conclusion: 开源实现为在物理意义上重要的退化场景下基准测试基于激光雷达的SLAM提供了实用基础。

Abstract: Lidar-based SLAM systems are highly sensitive to adverse conditions such as occlusion, noise, and field-of-view (FoV) degradation, yet existing robustness evaluation methods either lack physical grounding or do not capture sensor-specific behavior. This paper presents a sensor-aware, phenomenological framework for simulating interpretable lidar degradations directly on real point clouds, enabling controlled and reproducible SLAM stress testing. Unlike image-derived corruption benchmarks (e.g., SemanticKITTI-C) or simulation-only approaches (e.g., lidarsim), the proposed system preserves per-point geometry, intensity, and temporal structure while applying structured dropout, FoV reduction, Gaussian noise, occlusion masking, sparsification, and motion distortion. The framework features autonomous topic and sensor detection, modular configuration with four severity tiers (light--extreme), and real-time performance (less than 20 ms per frame) compatible with ROS workflows. Experimental validation across three lidar architectures and five state-of-the-art SLAM systems reveals distinct robustness patterns shaped by sensor design and environmental context. The open-source implementation provides a practical foundation for benchmarking lidar-based SLAM under physically meaningful degradation scenarios.

</details>


### [21] [Sim2Swim: Zero-Shot Velocity Control for Agile AUV Maneuvering in 3 Minutes](https://arxiv.org/abs/2512.08656)
*Lauritz Rismark Fosso,Herman Biørn Amundsen,Marios Xanthidis,Sveinung Johan Ohrem*

Main category: cs.RO

TL;DR: 本文提出了一种名为Sim2Swim的基于深度强化学习的速度控制器，能够在不进行后期处理或调优的情况下，实现对具有不同特性的自主水下航行器（AUVs）的现场部署控制策略。通过领域随机化和大规模并行训练，该控制器能够应对复杂的水静力学、水动力学以及由于载荷变化导致的动力学频繁变化等挑战，并在池试中针对多种配置进行了广泛验证，展示了对于高度敏捷动作的鲁棒性控制。


<details>
  <summary>Details</summary>
Motivation: 全向自主水下航行器具备硬件能力以实现在平移和旋转自由度上的灵活操作，但因水下环境复杂特性如复杂的水静力学与水动力学、参数不确定性及载荷变化引起动力学频繁改变等因素，使得其控制变得困难。通常情况下，控制系统需要针对特定平台配置精细调整，并且在不同的载荷与流体动力条件下还需重新调整。因此，在实践中很少使用同时追踪平移和旋转自由度上时变参考点的敏捷操纵方式。

Method: 提出了Sim2Swim方法，这是一种基于最新DRL位置控制启发的零样本sim2real深度强化学习速度控制器。它利用领域随机化和大规模并行训练来生成无需后处理或调优即可用于具有可变特征的AUV的现场部署控制策略。

Result: Sim2Swim方法经过了广泛的池试验证，适用于多种配置情况，展示了对于高度敏捷运动的强大控制能力。此外，该控制器仅需3分钟的训练时间就能收敛到可用于现场部署的控制策略。

Conclusion: Sim2Swim是首个能够实现对不同特性AUV进行6自由度敏捷操控的一般性零样本sim2real DRL速度控制器。它通过结合领域随机化与大规模并行训练技术，解决了传统上需要为每种独特平台配置精心调整控制器的问题，从而促进了AUV在多样化应用场景中的实际应用。

Abstract: Holonomic autonomous underwater vehicles (AUVs) have the hardware ability for agile maneuvering in both translational and rotational degrees of freedom (DOFs). However, due to challenges inherent to underwater vehicles, such as complex hydrostatics and hydrodynamics, parametric uncertainties, and frequent changes in dynamics due to payload changes, control is challenging. Performance typically relies on carefully tuned controllers targeting unique platform configurations, and a need for re-tuning for deployment under varying payloads and hydrodynamic conditions. As a consequence, agile maneuvering with simultaneous tracking of time-varying references in both translational and rotational DOFs is rarely utilized in practice. To the best of our knowledge, this paper presents the first general zero-shot sim2real deep reinforcement learning-based (DRL) velocity controller enabling path following and agile 6DOF maneuvering with a training duration of just 3 minutes. Sim2Swim, the proposed approach, inspired by state-of-the-art DRL-based position control, leverages domain randomization and massively parallelized training to converge to field-deployable control policies for AUVs of variable characteristics without post-processing or tuning. Sim2Swim is extensively validated in pool trials for a variety of configurations, showcasing robust control for highly agile motions.

</details>


### [22] [Ergodic Trajectory Planning with Dynamic Sensor Footprints](https://arxiv.org/abs/2512.08661)
*Ziyue Zheng,Yongce Liu,Hesheng Wang,Zhongqiang Ren*

Main category: cs.RO

TL;DR: 本文提出了一种新的度量方法，以考虑动态传感器足迹，从而改进了用于信息收集的轨迹规划，并在实验中展示了比传统方法更好的遍历性。


<details>
  <summary>Details</summary>
Motivation: 现有的遍历规划通常简化了感知模型，假设传感器为点传感器或具有恒定形状和分辨率的足迹。然而实际上，随着机器人移动，如配备向下摄像头的空中机器人，其视野会根据方向和高度大幅变化。为了克服这一限制，研究者提出了新方法。

Method: 研究者提出了一种新的度量方法来考虑动态变化的传感器足迹，分析了理论上的局部最优条件，并提出了数值轨迹优化算法。

Result: 实验结果表明，所提出的方法可以同时优化轨迹与传感器足迹，相比传统方法，在遍历性方面提高了多达一个数量级。此外，该方法还被应用于多无人机系统，在三维空间内遍历覆盖物体。

Conclusion: 通过引入能够处理动态传感器足迹的新度量方法，本研究显著提升了遍历规划的有效性和效率，为信息收集任务提供了更优解。

Abstract: This paper addresses the problem of trajectory planning for information gathering with a dynamic and resolution-varying sensor footprint. Ergodic planning offers a principled framework that balances exploration (visiting all areas) and exploitation (focusing on high-information regions) by planning trajectories such that the time spent in a region is proportional to the amount of information in that region. Existing ergodic planning often oversimplifies the sensing model by assuming a point sensor or a footprint with constant shape and resolution. In practice, the sensor footprint can drastically change over time as the robot moves, such as aerial robots equipped with downward-facing cameras, whose field of view depends on the orientation and altitude. To overcome this limitation, we propose a new metric that accounts for dynamic sensor footprints, analyze the theoretic local optimality conditions, and propose numerical trajectory optimization algorithms. Experimental results show that the proposed approach can simultaneously optimize both the trajectories and sensor footprints, with up to an order of magnitude better ergodicity than conventional methods. We also deploy our approach in a multi-drone system to ergodically cover an object in 3D space.

</details>


### [23] [Non Normalized Shared-Constraint Dynamic Games for Human-Robot Collaboration with Asymmetric Responsibility](https://arxiv.org/abs/2512.08688)
*Mark Pustilnik,Francesco Borrelli*

Main category: cs.RO

TL;DR: 本文提出了一种动态博弈公式，用于在有障碍物的共享工作空间中进行人机协作导航。通过引入一种针对共享约束的非标准化均衡结构，允许双方根据安全需求如避碰和玩家间距贡献不同程度的努力，并将此非标准化均衡嵌入到一个滚动时域最优控制方案中。


<details>
  <summary>Details</summary>
Motivation: 为了在存在障碍物的共享工作空间里实现更安全有效的人与机器人合作，特别是在需要共同完成任务的同时满足基本的安全约束条件（比如避免碰撞）的情况下。

Method: 采用动态游戏建模方法来描述人与机器人之间的互动关系；提出了一个新的非标准化均衡概念以处理不同主体对于安全约束的不同贡献程度问题；最后，将上述模型整合进一个基于滚动优化控制框架内。

Result: 成功地建立了一个能够支持人-机器人团队在复杂环境中协同工作的数学模型，该模型不仅考虑到了两者间的相互作用还特别强调了如何合理分配资源以满足安全要求。

Conclusion: 这项研究为设计更加智能、灵活且安全的人机协作系统提供了新的思路和技术手段，特别是对于那些需要在有限或拥挤的空间内运作的应用场景而言具有重要意义。

Abstract: This paper proposes a dynamic game formulation for cooperative human-robot navigation in shared workspaces with obstacles, where the human and robot jointly satisfy shared safety constraints while pursuing a common task. A key contribution is the introduction of a non-normalized equilibrium structure for the shared constraints. This structure allows the two agents to contribute different levels of effort towards enforcing safety requirements such as collision avoidance and inter-players spacing. We embed this non-normalized equilibrium into a receding-horizon optimal control scheme.

</details>


### [24] [A Multi-Robot Platform for Robotic Triage Combining Onboard Sensing and Foundation Models](https://arxiv.org/abs/2512.08754)
*Jason Hughes,Marcel Hussing,Edward Zhang,Shenbagaraj Kannapiran,Joshua Caswell,Kenneth Chaney,Ruichen Deng,Michaela Feehery,Agelos Kratimenos,Yi Fan Li,Britny Major,Ethan Sanchez,Sumukh Shrote,Youkang Wang,Jeremy Wang,Daudi Zein,Luying Zhang,Ruijun Zhang,Alex Zhou,Tenzi Zhouga,Jeremy Cannon,Zaffir Qasim,Jay Yelon,Fernando Cladera,Kostas Daniilidis,Camillo J. Taylor,Eric Eaton*

Main category: cs.RO

TL;DR: 本报告介绍了一种用于大规模伤亡事件中远程初步分类的异构机器人系统，结合了无人机和地面无人车来定位受害者、评估伤害并优先提供医疗援助。


<details>
  <summary>Details</summary>
Motivation: 在大规模伤亡事件中，为避免救援人员的生命危险，同时有效地对受害者进行定位、伤情评估及优先级排序，以提高救援效率。

Method: 通过协调无人机（UAVs）和地面无人车辆（UGVs）组成空中-地面团队。无人机负责从上方识别和查看伤者；而装备有专门传感器的UGV则用来测量生命体征、检测与定位身体受伤情况。

Result: 该系统能够完成整个分诊流程：受害者定位、生命体征测量、伤情严重程度分类、精神状态评估以及为第一响应者整合数据。

Conclusion: 作为DARPA分诊挑战的一部分开发的这种多机器人系统方法展示了如何增强人类在灾难响应场景中的能力，以最大限度地挽救生命。

Abstract: This report presents a heterogeneous robotic system designed for remote primary triage in mass-casualty incidents (MCIs). The system employs a coordinated air-ground team of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to locate victims, assess their injuries, and prioritize medical assistance without risking the lives of first responders. The UAV identify and provide overhead views of casualties, while UGVs equipped with specialized sensors measure vital signs and detect and localize physical injuries. Unlike previous work that focused on exploration or limited medical evaluation, this system addresses the complete triage process: victim localization, vital sign measurement, injury severity classification, mental status assessment, and data consolidation for first responders. Developed as part of the DARPA Triage Challenge, this approach demonstrates how multi-robot systems can augment human capabilities in disaster response scenarios to maximize lives saved.

</details>
