<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 12]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Revisiting Continuous-Time Trajectory Estimation via Gaussian Processes and the Magnus Expansion](https://arxiv.org/abs/2601.03360)
*Timothy Barfoot,Cedric Le Gentil,Sven Lilge*

Main category: cs.RO

TL;DR: 本文提出了一种通过Magnus展开在李群上导出全局高斯过程先验的方法，用于连续时间轨迹估计，并与局部高斯过程方法进行了数值比较。


<details>
  <summary>Details</summary>
Motivation: 为了处理异步和高速率测量、给估计值引入平滑性、事后在不同于测量时间点查询估计值以及解决与移动中扫描传感器相关的某些可观测性问题，连续时间状态估计被证明是一种有效的方法。虽然已有工作通过一系列局部高斯过程来表示李群上的状态，但这种方法缺乏理论上的优雅性。

Method: 本文采用完全的线性时变高斯过程方法来进行连续时间轨迹估计，利用Magnus展开在李群上推导出一个更优雅且通用的全局高斯过程先验。

Result: 文章提供了新提出的全局GP方法与基于局部GP方法之间的数值对比，并讨论了各自的相对优点。

Conclusion: 通过Magnus展开导出的全局GP为连续时间轨迹估计提供了一个更加优雅且通用的解决方案，尽管文中也提到了两种方法各自的优势。

Abstract: Continuous-time state estimation has been shown to be an effective means of (i) handling asynchronous and high-rate measurements, (ii) introducing smoothness to the estimate, (iii) post hoc querying the estimate at times other than those of the measurements, and (iv) addressing certain observability issues related to scanning-while-moving sensors. A popular means of representing the trajectory in continuous time is via a Gaussian process (GP) prior, with the prior's mean and covariance functions generated by a linear time-varying (LTV) stochastic differential equation (SDE) driven by white noise. When the state comprises elements of Lie groups, previous works have resorted to a patchwork of local GPs each with a linear time-invariant SDE kernel, which while effective in practice, lacks theoretical elegance. Here we revisit the full LTV GP approach to continuous-time trajectory estimation, deriving a global GP prior on Lie groups via the Magnus expansion, which offers a more elegant and general solution. We provide a numerical comparison between the two approaches and discuss their relative merits.

</details>


### [2] [Cost-Effective Radar Sensors for Field-Based Water Level Monitoring with Sub-Centimeter Accuracy](https://arxiv.org/abs/2601.03447)
*Anna Zavei-Boroda,J. Toby Minear,Kyle Harlow,Dusty Woods,Christoffer Heckman*

Main category: cs.RO

TL;DR: 该研究探索了基于雷达的低成本水位监测方法，通过实地测试和统计滤波技术提高了精度，证明单个雷达传感器可以实现厘米级精度，适用于无人机和机器人平台的自主水位监测。


<details>
  <summary>Details</summary>
Motivation: 传统水位监测方法成本高且覆盖范围有限，而有效的水位监测对于洪水管理、水资源分配和生态评估至关重要。因此，寻找一种低成本且高效的替代方案成为必要。

Method: 本研究采用商用雷达传感器进行实际场地测试，并应用统计过滤技术来提高测量准确性。

Result: 结果表明，单一雷达传感器在最小校准的情况下即可达到厘米级的精确度。

Conclusion: 雷达传感提供了一种非接触式、对环境条件鲁棒性强的低成本水位估计解决方案，非常适合于使用无人机和机器人平台执行自动化的水位监控任务。

Abstract: Water level monitoring is critical for flood management, water resource allocation, and ecological assessment, yet traditional methods remain costly and limited in coverage. This work explores radar-based sensing as a low-cost alternative for water level estimation, leveraging its non-contact nature and robustness to environmental conditions. Commercial radar sensors are evaluated in real-world field tests, applying statistical filtering techniques to improve accuracy. Results show that a single radar sensor can achieve centimeter-scale precision with minimal calibration, making it a practical solution for autonomous water monitoring using drones and robotic platforms.

</details>


### [3] [A Vision-Language-Action Model with Visual Prompt for OFF-Road Autonomous Driving](https://arxiv.org/abs/2601.03519)
*Liangdong Zhang,Yiming Nie,Haoyang Li,Fanjie Kong,Baobao Zhang,Shunxin Huang,Kai Fu,Chen Min,Liang Xiao*

Main category: cs.RO

TL;DR: 本文提出了一种名为OFF-EMMA的新型端到端多模态框架，旨在解决越野自动驾驶场景中视觉-语言-动作模型的空间感知不足和推理不稳定的问题。通过引入视觉提示块和具有自一致性的思维链（COT-SC）推理策略，该方法在RELLIS-3D越野数据集上的实验结果表明，与现有方法相比，能够显著提高轨迹规划的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的越野地形自动车辆轨迹规划方法在动态环境中的适应性有限，存在空间感知不足及推理不稳定的问题。为了解决这些问题，作者提出了新的解决方案。

Method: 提出OFF-EMMA框架，包括使用语义分割掩码作为视觉提示来增强预训练视觉-语言模型对复杂地形的空间理解能力，以及采用带有自一致性的思维链(COT-SC)推理策略以减少异常值对规划性能的影响。

Result: 在RELLIS-3D越野数据集上进行测试后发现，相比于Qwen基础模型，OFF-EMMA将平均L2误差降低了13.3%，同时失败率从16.52%降至6.56%。

Conclusion: 研究证明了所提出的OFF-EMMA框架在提升越野条件下自动驾驶车辆轨迹规划准确性与鲁棒性方面的有效性。

Abstract: Efficient trajectory planning in off-road terrains presents a formidable challenge for autonomous vehicles, often necessitating complex multi-step pipelines. However, traditional approaches exhibit limited adaptability in dynamic environments. To address these limitations, this paper proposes OFF-EMMA, a novel end-to-end multimodal framework designed to overcome the deficiencies of insufficient spatial perception and unstable reasoning in visual-language-action (VLA) models for off-road autonomous driving scenarios. The framework explicitly annotates input images through the design of a visual prompt block and introduces a chain-of-thought with self-consistency (COT-SC) reasoning strategy to enhance the accuracy and robustness of trajectory planning. The visual prompt block utilizes semantic segmentation masks as visual prompts, enhancing the spatial understanding ability of pre-trained visual-language models for complex terrains. The COT- SC strategy effectively mitigates the error impact of outliers on planning performance through a multi-path reasoning mechanism. Experimental results on the RELLIS-3D off-road dataset demonstrate that OFF-EMMA significantly outperforms existing methods, reducing the average L2 error of the Qwen backbone model by 13.3% and decreasing the failure rate from 16.52% to 6.56%.

</details>


### [4] [From Score to Sound: An End-to-End MIDI-to-Motion Pipeline for Robotic Cello Performance](https://arxiv.org/abs/2601.03562)
*Samantha Sudhoff,Pranesh Velmurugan,Jiashu Liu,Vincent Zhao,Yung-Hsiang Lu,Kristen Yeon-Ji Yun*

Main category: cs.RO

TL;DR: 本文提出了一种端到端的MIDI乐谱到机器人动作流程，使UR5e机器人能够直接将音乐输入转换为防碰撞的拉弓动作，并通过实时数据交换记录机器人演奏时的关节数据。此外，还引入了音乐图灵测试来评估机器人的表演与人类基线相比的有效性，并提出了残差强化学习方法以改进基础的机器人控制，从而提高过弦效率和音质。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人大提琴手往往依赖昂贵的动作捕捉技术，并且无法像人类一样视读音乐。为了克服这些限制，作者们旨在开发一种不需要动捕技术、能够以类似人类的方式演奏音乐的机器人系统，同时提供开放的数据集供研究社区使用。

Method: 1. 设计了一个从MIDI乐谱到机器人拉弓动作的端到端流程。
2. 利用Universal Robot Freedrive功能实现更加自然的声音产生。
3. 通过RTDE协议在机器人演奏过程中收集实时关节数据。
4. 引入音乐图灵测试，让132名参与者对机器人与人类演奏者的表现进行比较评价。
5. 探索基于残差强化学习的方法来进一步优化机器人控制。

Result: 1. 成功实现了不依赖于动捕技术的人类相似度高的机器人演奏。
2. 分享了包含五个标准曲目的标记化机器人演奏数据集。
3. 音乐图灵测试结果表明，所提出的系统在某种程度上可以与人类演奏者相媲美。
4. 提出的残差强化学习框架为未来改进提供了方向。

Conclusion: 本研究介绍了一种新的无需动捕技术即可实现高质量音乐演奏的机器人系统设计，并通过音乐图灵测试证明了其有效性。所提供的数据集及探索性的强化学习方法为该领域内后续的研究工作奠定了坚实的基础。

Abstract: Robot musicians require precise control to obtain proper note accuracy, sound quality, and musical expression. Performance of string instruments, such as violin and cello, presents a significant challenge due to the precise control required over bow angle and pressure to produce the desired sound. While prior robotic cellists focus on accurate bowing trajectories, these works often rely on expensive motion capture techniques, and fail to sightread music in a human-like way.
  We propose a novel end-to-end MIDI score to robotic motion pipeline which converts musical input directly into collision-aware bowing motions for a UR5e robot cellist. Through use of Universal Robot Freedrive feature, our robotic musician can achieve human-like sound without the need for motion capture. Additionally, this work records live joint data via Real-Time Data Exchange (RTDE) as the robot plays, providing labeled robotic playing data from a collection of five standard pieces to the research community. To demonstrate the effectiveness of our method in comparison to human performers, we introduce the Musical Turing Test, in which a collection of 132 human participants evaluate our robot's performance against a human baseline. Human reference recordings are also released, enabling direct comparison for future studies. This evaluation technique establishes the first benchmark for robotic cello performance. Finally, we outline a residual reinforcement learning methodology to improve upon baseline robotic controls, highlighting future opportunities for improved string-crossing efficiency and sound quality.

</details>


### [5] [Locomotion Beyond Feet](https://arxiv.org/abs/2601.03607)
*Tae Hoon Yang,Haochen Shi,Jiacheng Hu,Zhicong Zhang,Daniel Jiang,Weizhuo Wang,Yao He,Zhen Wu,Yuming Chen,Yifan Hou,Monroe Kennedy,Shuran Song,C. Karen Liu*

Main category: cs.RO

TL;DR: 该论文提出了一种名为'超越脚部的移动'的系统，旨在通过结合基于物理的关键帧动画与强化学习来实现类人机器人在复杂环境下的全身移动。


<details>
  <summary>Details</summary>
Motivation: 当前大多数类人机器人的移动方法主要集中在腿部步态上，但自然双足生物经常依靠手、膝盖和肘部来增加接触点以提高稳定性和支持。为了解决这一问题，本文开发了一个新的系统，使机器人能够在极其复杂的地形中进行全身移动。

Method: 该研究结合了基于物理的关键帧动画和强化学习技术。关键帧编码了人类对运动技能的知识，并且是针对特定实体设计的，可以在模拟或硬件上轻松验证；而强化学习则将这些参考转换为稳健且物理上准确的动作。此外，还采用了包括地形特异性动作跟踪策略、故障恢复机制以及基于视觉的技能规划器在内的层次框架。

Result: 实验证明，'超越脚部的移动'系统能够实现鲁棒性的全身移动，并且可以跨障碍物尺寸、障碍实例及地形序列泛化。

Conclusion: 本研究介绍的方法为类人机器人提供了一种新颖有效的解决方案，使其能够在包含低间隙空间、膝高墙壁、平台及陡峭楼梯等极端挑战性地形中进行全身移动。

Abstract: Most locomotion methods for humanoid robots focus on leg-based gaits, yet natural bipeds frequently rely on hands, knees, and elbows to establish additional contacts for stability and support in complex environments. This paper introduces Locomotion Beyond Feet, a comprehensive system for whole-body humanoid locomotion across extremely challenging terrains, including low-clearance spaces under chairs, knee-high walls, knee-high platforms, and steep ascending and descending stairs. Our approach addresses two key challenges: contact-rich motion planning and generalization across diverse terrains. To this end, we combine physics-grounded keyframe animation with reinforcement learning. Keyframes encode human knowledge of motor skills, are embodiment-specific, and can be readily validated in simulation or on hardware, while reinforcement learning transforms these references into robust, physically accurate motions. We further employ a hierarchical framework consisting of terrain-specific motion-tracking policies, failure recovery mechanisms, and a vision-based skill planner. Real-world experiments demonstrate that Locomotion Beyond Feet achieves robust whole-body locomotion and generalizes across obstacle sizes, obstacle instances, and terrain sequences.

</details>


### [6] [PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation](https://arxiv.org/abs/2601.03782)
*Wenlong Huang,Yu-Wei Chao,Arsalan Mousavian,Ming-Yu Liu,Dieter Fox,Kaichun Mo,Li Fei-Fei*

Main category: cs.RO

TL;DR: A 3D world model, PointWorld, has been developed to predict 3D responses to robot actions, enabling a wide range of manipulation tasks for a real-world robot without extra training.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to develop a model that allows robots to predict how the 3D world will respond to their actions, similar to human anticipation, which is crucial for effective robotic manipulation. The goal is to create a unified 3D representation that works across different robot embodiments and can generalize from simulations to the real world, enabling robots to perform various manipulation tasks with minimal adaptation.

Method: The researchers created PointWorld, which uses 3D point flows to represent both state and action in a shared 3D space. They trained this model on a large-scale dataset that includes both real and simulated robotic manipulation scenarios, totaling about 2 million trajectories and 500 hours of data. The dataset was collected using a single-arm Franka and a bimanual humanoid robot. The team also conducted extensive empirical studies to establish design principles for large-scale 3D world modeling.

Result: The result is a highly efficient and scalable 3D world model, PointWorld, which can be used in real-time for model-predictive control (MPC) and enables a real-world Franka robot to carry out a variety of manipulation tasks including pushing, manipulating deformable and articulated objects, and using tools, all based on a single pre-trained checkpoint and a single image, without further training or demonstrations.

Conclusion: PointWorld, a large pre-trained 3D world model, can predict 3D point flows from RGB-D images and action commands, allowing for versatile manipulation tasks by a real-world robot without additional training or demonstrations. The model is capable of handling rigid, deformable, and articulated objects as well as tool use, all in real-time, and it can be integrated into the model-predictive control (MPC) framework.

Abstract: Humans anticipate, from a glance and a contemplated action of their bodies, how the 3D world will respond, a capability that is equally vital for robotic manipulation. We introduce PointWorld, a large pre-trained 3D world model that unifies state and action in a shared 3D space as 3D point flows: given one or few RGB-D images and a sequence of low-level robot action commands, PointWorld forecasts per-pixel displacements in 3D that respond to the given actions. By representing actions as 3D point flows instead of embodiment-specific action spaces (e.g., joint positions), this formulation directly conditions on physical geometries of robots while seamlessly integrating learning across embodiments. To train our 3D world model, we curate a large-scale dataset spanning real and simulated robotic manipulation in open-world environments, enabled by recent advances in 3D vision and simulated environments, totaling about 2M trajectories and 500 hours across a single-arm Franka and a bimanual humanoid. Through rigorous, large-scale empirical studies of backbones, action representations, learning objectives, partial observability, data mixtures, domain transfers, and scaling, we distill design principles for large-scale 3D world modeling. With a real-time (0.1s) inference speed, PointWorld can be efficiently integrated in the model-predictive control (MPC) framework for manipulation. We demonstrate that a single pre-trained checkpoint enables a real-world Franka robot to perform rigid-body pushing, deformable and articulated object manipulation, and tool use, without requiring any demonstrations or post-training and all from a single image captured in-the-wild. Project website at https://point-world.github.io/.

</details>


### [7] [Generational Replacement and Learning for High-Performing and Diverse Populations in Evolvable Robots](https://arxiv.org/abs/2601.03807)
*K. Ege de Bruin,Kyrre Glette,Kai Olav Ellefsen*

Main category: cs.RO

TL;DR: 本文提出了一种结合世代更替和生命周期内学习的方法，以在进化机器人学中增加多样性同时保持性能。此外，还强调了在研究形态演变的机器人学习时性能度量的重要性。


<details>
  <summary>Details</summary>
Motivation: 进化机器人学面临的一个挑战是身体和控制的同时优化，因为控制器需要时间来适应不断变化的形态，这可能会阻碍新的有前途的设计进入进化群体。另一个问题是进化群体中经常缺乏多样性，因为进化过程过快地缩小到少数几个有希望的设计上。

Method: 本文采用的方法是为进化群体中的每个个体添加一个额外的控制器优化循环（即生命周期内学习），以及实施全世代替换策略，其中后代机器人将完全取代整个群体。

Result: 通过结合世代更替与生命周期内学习，可以提高进化机器人群体的多样性，同时维持其表现水平。研究还指出，在评估具有学习能力且形态进化的机器人时，根据功能评估对比依据进化代数进行评价可能会得出不同的结论。

Conclusion: 结合全世代替换与生命周期内学习提供了一个有效手段，既增加了进化机器人设计的多样性也保持了良好的性能。此外，选择合适的性能指标对于准确理解学习对形态演变机器人影响至关重要。

Abstract: Evolutionary Robotics offers the possibility to design robots to solve a specific task automatically by optimizing their morphology and control together. However, this co-optimization of body and control is challenging, because controllers need some time to adapt to the evolving morphology - which may make it difficult for new and promising designs to enter the evolving population. A solution to this is to add intra-life learning, defined as an additional controller optimization loop, to each individual in the evolving population. A related problem is the lack of diversity often seen in evolving populations as evolution narrows the search down to a few promising designs too quickly. This problem can be mitigated by implementing full generational replacement, where offspring robots replace the whole population. This solution for increasing diversity usually comes at the cost of lower performance compared to using elitism. In this work, we show that combining such generational replacement with intra-life learning can increase diversity while retaining performance. We also highlight the importance of performance metrics when studying learning in morphologically evolving robots, showing that evaluating according to function evaluations versus according to generations of evolution can give different conclusions.

</details>


### [8] [Integrating Sample Inheritance into Bayesian Optimization for Evolutionary Robotics](https://arxiv.org/abs/2601.03813)
*K. Ege de Bruin,Kyrre Glette,Kai Olav Ellefsen*

Main category: cs.RO

TL;DR: 该研究通过在进化机器人学中引入贝叶斯优化和样本继承的方法，解决了体脑协同优化问题。实验结果表明，在控制器学习预算有限的情况下，重新评估父母的最佳样本（一种样本继承方式）对后代最为有利，尤其是在环境更具挑战性时。此外，跨代积累的学习适应性可以补偿单个形态学学习预算不足的问题。


<details>
  <summary>Details</summary>
Motivation: 在进化机器人学领域，机器人形态与控制需共同优化。但为每个新形态从头开始优化控制器可能需要大量的学习资源。本研究旨在通过有效利用贝叶斯优化及其强大的探索能力，并结合样本继承机制来解决这个问题，以减少控制器学习所需的预算。

Method: 采用了贝叶斯优化来进行控制器优化，并测试了两种类型的样本继承方法：一是将父代的所有样本直接作为先验信息传递给子代；二是仅重新评估父代的最佳样本并应用于子代。这些方法被用来与不使用任何继承机制的基线进行比较。所有实验都在一个故意设定较低的控制器学习预算下执行。

Result: 结果显示，重新评估父代最佳样本的方法表现最好，而基于先验信息的继承也优于完全没有继承的情况。进一步分析发现，虽然对于单一形态来说学习预算是不够的，但是通过世代间的继承可以在几代之间累积学习到的适应性变化，从而弥补这一缺陷。此外，相似于其父母的后代形态受益最大。

Conclusion: 研究表明，在进化机器人中采用继承机制能够显著提高性能，即使是在控制器学习预算非常有限的情况下也能实现。这为设计更高效、能力更强的机器人提供了一条有前景的道路。

Abstract: In evolutionary robotics, robot morphologies are designed automatically using evolutionary algorithms. This creates a body-brain optimization problem, where both morphology and control must be optimized together. A common approach is to include controller optimization for each morphology, but starting from scratch for every new body may require a high controller learning budget. We address this by using Bayesian optimization for controller optimization, exploiting its sample efficiency and strong exploration capabilities, and using sample inheritance as a form of Lamarckian inheritance. Under a deliberately low controller learning budget for each morphology, we investigate two types of sample inheritance: (1) transferring all the parent's samples to the offspring to be used as prior without evaluating them, and (2) reevaluating the parent's best samples on the offspring. Both are compared to a baseline without inheritance. Our results show that reevaluation performs best, with prior-based inheritance also outperforming no inheritance. Analysis reveals that while the learning budget is too low for a single morphology, generational inheritance compensates for this by accumulating learned adaptations across generations. Furthermore, inheritance mainly benefits offspring morphologies that are similar to their parents. Finally, we demonstrate the critical role of the environment, with more challenging environments resulting in more stable walking gaits. Our findings highlight that inheritance mechanisms can boost performance in evolutionary robotics without needing large learning budgets, offering an efficient path toward more capable robot design.

</details>


### [9] [Towards Safe Autonomous Driving: A Real-Time Motion Planning Algorithm on Embedded Hardware](https://arxiv.org/abs/2601.03904)
*Korbinian Moller,Glenn Johannes Tungka,Lucas Jürgens,Johannes Betz*

Main category: cs.RO

TL;DR: 本文提出了一种基于轻量级采样轨迹规划器的主动安全扩展方案，用于自动驾驶在主规划器失效时仍能安全运行。实验结果表明该方案在认证的安全硬件上具有确定性的时序行为和有限的延迟及抖动，证明了其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有的保障概念如在线验证（OV）提供了检测不可行规划输出的安全层，但缺乏一种当主规划器出现故障时确保安全操作的主动机制。为了填补这一空白，作者们旨在开发一种即使在系统故障情况下也能保持可控性的运动规划模块。

Method: 研究者们在一个汽车级别的嵌入式平台上部署了一个轻量级的基于采样的轨迹规划器，该平台运行着实时操作系统（RTOS）。这个规划器能够在受限的计算资源下持续计算轨迹，为未来紧急情况下的规划架构奠定了基础。

Result: 实验结果显示了确定性的时间行为、有界的延迟以及最小的抖动，这证实了在可以进行安全认证的硬件上执行轨迹规划是可行的。

Conclusion: 这项研究表明了将主动后备机制作为下一代安全保障框架的一部分整合起来的可能性及其面临的挑战，并强调了在自动驾驶车辆中实现故障操作安全的重要性。

Abstract: Ensuring the functional safety of Autonomous Vehicles (AVs) requires motion planning modules that not only operate within strict real-time constraints but also maintain controllability in case of system faults. Existing safeguarding concepts, such as Online Verification (OV), provide safety layers that detect infeasible planning outputs. However, they lack an active mechanism to ensure safe operation in the event that the main planner fails. This paper presents a first step toward an active safety extension for fail-operational Autonomous Driving (AD). We deploy a lightweight sampling-based trajectory planner on an automotive-grade, embedded platform running a Real-Time Operating System (RTOS). The planner continuously computes trajectories under constrained computational resources, forming the foundation for future emergency planning architectures. Experimental results demonstrate deterministic timing behavior with bounded latency and minimal jitter, validating the feasibility of trajectory planning on safety-certifiable hardware. The study highlights both the potential and the remaining challenges of integrating active fallback mechanisms as an integral part of next-generation safeguarding frameworks. The code is available at: https://github.com/TUM-AVS/real-time-motion-planning

</details>


### [10] [An Event-Based Opto-Tactile Skin](https://arxiv.org/abs/2601.03907)
*Mohammadreza Koolani,Simeon Bamford,Petr Trunin,Simon F. Müller-Cleve,Matteo Lo Preti,Fulvio Mastrogiovanni,Lucia Beccai,Chiara Bartolozzi*

Main category: cs.RO

TL;DR: 本研究介绍了一种基于动态视觉传感器（DVS）与柔性硅光学波导皮肤集成的神经形态、事件驱动触觉传感系统。该系统通过侧视方式使用两个DVS相机来检测亮度变化产生事件，并利用DBSCAN算法估计按压位置。实验表明，在数据量大幅减少的情况下，该系统仍能有效定位按压点，且具有较低的延迟，这为未来在软体机器人和互动环境中的应用提供了可能性。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够覆盖大面积、柔软并具有良好响应性的触觉传感系统，以满足软体机器人及互动环境的需求。

Method: 采用两台DVS相机侧向观察并通过柔性硅制光学波导皮肤检测亮度变化产生的事件，然后通过三角测量法结合DBSCAN算法确定2D皮肤表面上的压力位置。为了测试系统的性能，研究者在一个4620平方毫米的皮肤区域上进行了蜿蜒光栅扫描测试；同时，也探索了当事件数据量极大减少时，系统的表现如何。

Result: 对于95%可以被双摄像机捕捉到的按压动作，系统达到了4.66毫米的均方根误差(RMSE)。即使将事件流减少到原来的1/1024，系统仍然能够在85%的情况下提供有效的按压定位，尽管平均定位误差从4.66毫米增加到了9.33毫米。此外，系统的检测延迟分布宽度约为31毫秒。

Conclusion: 这项工作展示了所提出的方法在实现宽面积、灵活且反应迅速的触觉传感器方面的潜力，特别是在软体机器人技术和交互环境中。此外，即便是在极端的数据稀疏条件下，该传感方法也能保持功能性，这对降低功耗和计算负载是非常有前景的。

Abstract: This paper presents a neuromorphic, event-driven tactile sensing system for soft, large-area skin, based on the Dynamic Vision Sensors (DVS) integrated with a flexible silicone optical waveguide skin. Instead of repetitively scanning embedded photoreceivers, this design uses a stereo vision setup comprising two DVS cameras looking sideways through the skin. Such a design produces events as changes in brightness are detected, and estimates press positions on the 2D skin surface through triangulation, utilizing Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to find the center of mass of contact events resulting from pressing actions. The system is evaluated over a 4620 mm2 probed area of the skin using a meander raster scan. Across 95 % of the presses visible to both cameras, the press localization achieved a Root-Mean-Squared Error (RMSE) of 4.66 mm. The results highlight the potential of this approach for wide-area flexible and responsive tactile sensors in soft robotics and interactive environments. Moreover, we examined how the system performs when the amount of event data is strongly reduced. Using stochastic down-sampling, the event stream was reduced to 1/1024 of its original size. Under this extreme reduction, the average localization error increased only slightly (from 4.66 mm to 9.33 mm), and the system still produced valid press localizations for 85 % of the trials. This reduction in pass rate is expected, as some presses no longer produce enough events to form a reliable cluster for triangulation. These results show that the sensing approach remains functional even with very sparse event data, which is promising for reducing power consumption and computational load in future implementations. The system exhibits a detection latency distribution with a characteristic width of 31 ms.

</details>


### [11] [CoINS: Counterfactual Interactive Navigation via Skill-Aware VLM](https://arxiv.org/abs/2601.03956)
*Kangjie Zhou,Zhejia Wen,Zhiyong Zhuo,Zike Yan,Pengying Wu,Ieng Hou U,Shuaiyang Li,Han Gao,Kang Ding,Wenhan Cao,Wei Pan,Chang Liu*

Main category: cs.RO

TL;DR: 本文提出了一种名为CoINS的新框架，该框架通过技能感知的视觉-语言模型来改进机器人在交互式导航中的表现。通过对InterNav-VLM进行微调，并结合强化学习开发的技能库，使机器人能够理解环境中的物理限制并判断何时以及如何与物体互动以清理路径。实验表明，这种方法显著提高了导航成功率，特别是在复杂长距离场景中。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉-语言模型（VLM）的导航系统主要局限于被动避障，无法主动考虑如何与环境中的物体交互以清除障碍物。为了解决这一问题，提出了Counterfactual Interactive Navigation via Skill-aware VLM (CoINS)框架，旨在让机器人能够根据自身能力评估是否需要以及如何移除物体来创建可通行路径。

Method: 1. 开发了InterNav-VLM，一种经过微调的VLM，可以将技能适用性和具体约束参数纳入输入上下文中，并将其映射到度量级环境表示。
2. 利用提出的InterNav数据集对模型进行微调，使其能够隐式地评估移除物体对导航连通性的影响，从而决定是否需要互动及选择目标。
3. 通过强化学习建立了一个全面的技能库，特别引入了面向可穿越性的策略来处理不同类型的物体以清除路径。

Result: 广泛的模拟和现实世界测试表明，与代表性基线相比，CoINS的整体成功率提高了17%，在复杂的长距离场景中性能提升了超过80%。

Conclusion: 本研究成功地展示了CoINS框架在增强机器人交互式导航能力方面的有效性，特别是在面对需要主动改变环境以完成任务的情况时。

Abstract: Recent Vision-Language Models (VLMs) have demonstrated significant potential in robotic planning. However, they typically function as semantic reasoners, lacking an intrinsic understanding of the specific robot's physical capabilities. This limitation is particularly critical in interactive navigation, where robots must actively modify cluttered environments to create traversable paths. Existing VLM-based navigators are predominantly confined to passive obstacle avoidance, failing to reason about when and how to interact with objects to clear blocked paths. To bridge this gap, we propose Counterfactual Interactive Navigation via Skill-aware VLM (CoINS), a hierarchical framework that integrates skill-aware reasoning and robust low-level execution. Specifically, we fine-tune a VLM, named InterNav-VLM, which incorporates skill affordance and concrete constraint parameters into the input context and grounds them into a metric-scale environmental representation. By internalizing the logic of counterfactual reasoning through fine-tuning on the proposed InterNav dataset, the model learns to implicitly evaluate the causal effects of object removal on navigation connectivity, thereby determining interaction necessity and target selection. To execute the generated high-level plans, we develop a comprehensive skill library through reinforcement learning, specifically introducing traversability-oriented strategies to manipulate diverse objects for path clearance. A systematic benchmark in Isaac Sim is proposed to evaluate both the reasoning and execution aspects of interactive navigation. Extensive simulations and real-world experiments demonstrate that CoINS significantly outperforms representative baselines, achieving a 17\% higher overall success rate and over 80\% improvement in complex long-horizon scenarios compared to the best-performing baseline

</details>


### [12] [CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos](https://arxiv.org/abs/2601.04061)
*Chubin Zhang,Jianan Wang,Zifeng Gao,Yue Su,Tianru Dai,Cai Zhou,Jiwen Lu,Yansong Tang*

Main category: cs.RO

TL;DR: 提出了一种新的框架CLAP，通过对比学习将视频中的视觉潜在空间与机器人轨迹的本体感觉潜在空间对齐，以解决视觉-语言-动作模型中由于机器人数据稀缺而受限的问题。该框架能够有效地从人类视频演示转移技能到机器人执行上，并且引入了两种形式的VLA框架和一种知识匹配正则化策略来提高性能和防止微调时的灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 目前，视觉-语言-动作模型的发展受到机器人数据相对不足的限制，而现有的潜在动作模型虽然尝试利用视频数据，但往往因为视觉纠缠问题捕捉到了噪音而非操作技巧。

Method: 提出了对比学习潜行动作预训练（CLAP）框架，它将来自视频的视觉潜在空间与来自机器人轨迹的本体感受潜在空间对齐。此外，还引入了一个双公式VLA框架，包括擅长指令跟随和对象泛化的自回归模型CLAP-NTP以及为高频精确操控设计的基于矫正流策略的CLAP-RF。同时提出了知识匹配(KM)正则化策略来减轻微调过程中的灾难性遗忘。

Result: 广泛的实验表明，CLAP在多个方面显著优于强大的基线方法，实现了从人类视频到机器人执行的有效技能转移。

Conclusion: CLAP框架通过有效结合视频资料与机器人轨迹信息，解决了当前视觉-语言-动作模型面临的挑战，促进了技能从人类演示向机器人执行的有效迁移。

Abstract: Generalist Vision-Language-Action models are currently hindered by the scarcity of robotic data compared to the abundance of human video demonstrations. Existing Latent Action Models attempt to leverage video data but often suffer from visual entanglement, capturing noise rather than manipulation skills. To address this, we propose Contrastive Latent Action Pretraining (CLAP), a framework that aligns the visual latent space from videos with a proprioceptive latent space from robot trajectories. By employing contrastive learning, CLAP maps video transitions onto a quantized, physically executable codebook. Building on this representation, we introduce a dual-formulation VLA framework offering both CLAP-NTP, an autoregressive model excelling at instruction following and object generalization, and CLAP-RF, a Rectified Flow-based policy designed for high-frequency, precise manipulation. Furthermore, we propose a Knowledge Matching (KM) regularization strategy to mitigate catastrophic forgetting during fine-tuning. Extensive experiments demonstrate that CLAP significantly outperforms strong baselines, enabling the effective transfer of skills from human videos to robotic execution. Project page: https://lin-shan.com/CLAP/.

</details>
