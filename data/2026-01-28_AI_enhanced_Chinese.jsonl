{"id": "2601.18923", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18923", "abs": "https://arxiv.org/abs/2601.18923", "authors": ["Manthan Patel", "Jonas Frey", "Mayank Mittal", "Fan Yang", "Alexander Hansson", "Amir Bar", "Cesar Cadena", "Marco Hutter"], "title": "DeFM: Learning Foundation Representations from Depth for Robotics", "comment": "Under review, 19 pages, 15 Figures, 9 Tables", "summary": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: https://de-fm.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86DeFM\uff0c\u4e00\u79cd\u4e13\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u800c\u8bbe\u8ba1\u7684\u3001\u5b8c\u5168\u57fa\u4e8e\u6df1\u5ea6\u56fe\u50cf\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578b\u3002\u901a\u8fc7\u57286000\u4e07\u5f20\u6df1\u5ea6\u56fe\u50cf\u4e0a\u4f7f\u7528DINO\u98ce\u683c\u7684\u81ea\u84b8\u998f\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff0cDeFM\u5b66\u4e60\u4e86\u51e0\u4f55\u548c\u8bed\u4e49\u8868\u793a\uff0c\u5e76\u4e14\u80fd\u591f\u9002\u5e94\u5404\u79cd\u73af\u5883\u3001\u4efb\u52a1\u548c\u4f20\u611f\u5668\u3002\u8be5\u6a21\u578b\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8f93\u5165\u5f52\u4e00\u5316\u7b56\u7565\u4ee5\u4fdd\u6301\u8de8\u5c3a\u5ea6\u7684\u5ea6\u91cf\u610f\u8bc6\uff0c\u5e76\u53ef\u88ab\u63d0\u70bc\u6210\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u673a\u5668\u4eba\u7684\u7d27\u51d1\u578b\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDeFM\u5728\u57fa\u4e8e\u6df1\u5ea6\u7684\u5206\u7c7b\u3001\u5206\u5272\u3001\u5bfc\u822a\u3001\u8fd0\u52a8\u548c\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4ece\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u73af\u5883\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u4f20\u611f\u5668\u5e7f\u6cdb\u90e8\u7f72\u4e8e\u5404\u7c7b\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5e76\u4e14\u5feb\u901f\u9ad8\u4fdd\u771f\u7684\u6df1\u5ea6\u4eff\u771f\u6280\u672f\u8fdb\u6b65\u4f7f\u5f97\u57fa\u4e8e\u6df1\u5ea6\u89c2\u5bdf\u8bad\u7ec3\u51fa\u7684\u673a\u5668\u4eba\u7b56\u7565\u80fd\u591f\u5b9e\u73b0\u5bf9\u4e8e\u591a\u79cd\u4efb\u52a1\u7684\u7a33\u5065\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\uff0c\u4f46\u4e0eRGB\u6a21\u6001\u76f8\u6bd4\uff0c\u9488\u5bf9\u6df1\u5ea6\u6a21\u6001\u7684\u8868\u5f81\u5b66\u4e60\u4ecd\u8f83\u5c11\u63a2\u7d22\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u95e8\u7528\u4e8e\u673a\u5668\u4eba\u5e94\u7528\u7684\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578bDeFM\uff0c\u65e8\u5728\u6539\u5584\u6df1\u5ea6\u6a21\u6001\u4e0b\u7684\u5b66\u4e60\u8868\u73b0\u3002", "method": "\u91c7\u7528DINO\u98ce\u683c\u7684\u81ea\u6211\u84b8\u998f\u76ee\u6807\uff0c\u5728\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\uff08\u5305\u542b6000\u4e07\u5f20\u6df1\u5ea6\u56fe\u50cf\uff09\u4e0a\u8bad\u7ec3\u4e86\u540d\u4e3aDeFM\u7684\u57fa\u7840\u6a21\u578b\uff1b\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8f93\u5165\u6807\u51c6\u5316\u7b56\u7565\u6765\u4fdd\u6301\u4e0d\u540c\u5c3a\u5ea6\u4e0a\u7684\u5ea6\u91cf\u611f\u77e5\u6027\uff1b\u5c06\u5927\u578b\u7684DeFM\u6a21\u578b\u63d0\u70bc\u6210\u66f4\u5c0f\u7248\u672c\uff0c\u9002\u5408\u5e94\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "result": "DeFM\u5728\u57fa\u4e8e\u6df1\u5ea6\u7684\u5206\u7c7b\u3001\u5206\u5272\u3001\u5bfc\u822a\u3001\u79fb\u52a8\u4ee5\u53ca\u64cd\u63a7\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u53d6\u5f97\u4e86\u9876\u5c16\u7684\u8868\u73b0\uff1b\u5b83\u5c55\u793a\u51fa\u4e86\u4ece\u6a21\u62df\u73af\u5883\u5411\u771f\u5b9e\u4e16\u754c\u73af\u5883\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DeFM\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6ca1\u6709\u7279\u5b9a\u4efb\u52a1\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u76f4\u63a5\u5e94\u7528\u4e8e\u57fa\u4e8e\u6df1\u5ea6\u4fe1\u606f\u7684\u673a\u5668\u4eba\u5b66\u4e60\u4efb\u52a1\uff0c\u4e3a\u6df1\u5ea6\u6a21\u6001\u4e0b\u7684\u8868\u5f81\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18963", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18963", "abs": "https://arxiv.org/abs/2601.18963", "authors": ["Fauna Robotics", ":", "Diego Aldarondo", "Ana Pervan", "Daniel Corbalan", "Dave Petrillo", "Bolun Dai", "Aadhithya Iyer", "Nina Mortensen", "Erik Pearson", "Sridhar Pandian Arunachalam", "Emma Reznick", "David Weis", "Jacob Davison", "Samuel Patterson", "Tess Carella", "Michael Suguitan", "David Ye", "Oswaldo Ferro", "Nilesh Suriyarachchi", "Spencer Ling", "Erik Su", "Daniel Giebisch", "Peter Traver", "Sam Fonseca", "Mack Mor", "Rohan Singh", "Sertac Guven", "Kangni Liu", "Yaswanth Kumar Orru", "Ashiq Rahman Anwar Batcha", "Shruthi Ravindranath", "Silky Arora", "Hugo Ponte", "Dez Hernandez", "Utsav Chaudhary", "Zack Walker", "Michael Kelberman", "Ivan Veloz", "Christina Santa Lucia", "Kat Casale", "Helen Han", "Michael Gromis", "Michael Mignatti", "Jason Reisman", "Kelleher Guerin", "Dario Narvaez", "Christopher Anderson", "Anthony Moschella", "Robert Cochran", "Josh Merel"], "title": "Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot", "comment": null, "summary": "Recent advances in learned control, large-scale simulation, and generative models have accelerated progress toward general-purpose robotic controllers, yet the field still lacks platforms suitable for safe, expressive, long-term deployment in human environments. Most existing humanoids are either closed industrial systems or academic prototypes that are difficult to deploy and operate around people, limiting progress in robotics. We introduce Sprout, a developer platform designed to address these limitations through an emphasis on safety, expressivity, and developer accessibility. Sprout adopts a lightweight form factor with compliant control, limited joint torques, and soft exteriors to support safe operation in shared human spaces. The platform integrates whole-body control, manipulation with integrated grippers, and virtual-reality-based teleoperation within a unified hardware-software stack. An expressive head further enables social interaction -- a domain that remains underexplored on most utilitarian humanoids. By lowering physical and technical barriers to deployment, Sprout expands access to capable humanoid platforms and provides a practical basis for developing embodied intelligence in real human environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aSprout\u7684\u5f00\u53d1\u8005\u5e73\u53f0\uff0c\u5b83\u901a\u8fc7\u5f3a\u8c03\u5b89\u5168\u6027\u3001\u8868\u73b0\u529b\u548c\u5f00\u53d1\u8005\u53ef\u8bbf\u95ee\u6027\u6765\u89e3\u51b3\u73b0\u6709\u7c7b\u4eba\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u90e8\u7f72\u548c\u64cd\u4f5c\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u7c7b\u4eba\u673a\u5668\u4eba\u8981\u4e48\u662f\u5c01\u95ed\u7684\u5de5\u4e1a\u7cfb\u7edf\uff0c\u8981\u4e48\u662f\u96be\u4ee5\u5728\u4eba\u4eec\u5468\u56f4\u90e8\u7f72\u548c\u64cd\u4f5c\u7684\u5b66\u672f\u539f\u578b\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u4eba\u6280\u672f\u7684\u8fdb\u6b65\u3002", "method": "Sprout\u91c7\u7528\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u3001\u987a\u4ece\u63a7\u5236\u3001\u6709\u9650\u5173\u8282\u626d\u77e9\u548c\u67d4\u8f6f\u5916\u8868\uff0c\u652f\u6301\u5728\u5171\u4eab\u4eba\u7c7b\u7a7a\u95f4\u4e2d\u7684\u5b89\u5168\u64cd\u4f5c\uff0c\u5e76\u96c6\u6210\u4e86\u5168\u8eab\u63a7\u5236\u3001\u5e26\u6709\u96c6\u6210\u5939\u722a\u7684\u64cd\u4f5c\u4ee5\u53ca\u57fa\u4e8e\u865a\u62df\u73b0\u5b9e\u7684\u8fdc\u7a0b\u64cd\u4f5c\u3002", "result": "\u901a\u8fc7\u964d\u4f4e\u7269\u7406\u548c\u6280\u672f\u4e0a\u7684\u90e8\u7f72\u969c\u788d\uff0cSprout\u6269\u5927\u4e86\u5bf9\u6709\u80fd\u529b\u7684\u4eba\u5f62\u5e73\u53f0\u7684\u8bbf\u95ee\uff0c\u5e76\u4e3a\u5728\u771f\u5b9e\u4eba\u7c7b\u73af\u5883\u4e2d\u5f00\u53d1\u4f53\u73b0\u667a\u80fd\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002", "conclusion": "Sprout\u4f5c\u4e3a\u4e00\u79cd\u5f00\u53d1\u8005\u53cb\u597d\u578b\u5e73\u53f0\uff0c\u65e8\u5728\u4fc3\u8fdb\u66f4\u5b89\u5168\u3001\u66f4\u5177\u8868\u73b0\u529b\u4e14\u6613\u4e8e\u90e8\u7f72\u4e8e\u4eba\u7c7b\u73af\u5883\u4e2d\u7684\u901a\u7528\u673a\u5668\u4eba\u63a7\u5236\u5668\u53d1\u5c55\u3002"}}
{"id": "2601.18971", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.18971", "abs": "https://arxiv.org/abs/2601.18971", "authors": ["Ioannis G. Polyzos", "Konstantinos J. Kyriakopoulos"], "title": "A Switching Nonlinear Model Predictive Control Strategy for Safe Collision Handling by an Underwater Vehicle-Manipulator System", "comment": "This work has been submitted to the 2026 Mediterranean Conference on Control and Automation (MED) to be considered for publication. Figures and animations are available at https://zenodo.org/records/18357280", "summary": "For active intervention tasks in underwater environments, the use of autonomous vehicles is just now emerging as an active area of research. During operation, for various reasons, the robot might find itself on a collision course with an obstacle in its environment. In this paper, a switching Nonlinear Model Predictive Control (NMPC) strategy is proposed to safely handle collisions for an Underwater Vehicle-Manipulator System (UVMS). When avoiding the collision is impossible, the control algorithm takes advantage of the manipulator, using it to push against the obstacle, and deflect away from the collision. Virtual experiments are performed to demonstrate the algorithm's capability to successfully detect collisions and either avoid them, or use the manipulator to handle them appropriately without damaging sensitive areas of the vehicle.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5207\u6362\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b56\u7565\uff0c\u4ee5\u5b89\u5168\u5904\u7406\u6c34\u4e0b\u8f66\u8f86-\u64cd\u7eb5\u5668\u7cfb\u7edf\uff08UVMS\uff09\u7684\u78b0\u649e\u95ee\u9898\u3002\u5f53\u65e0\u6cd5\u907f\u514d\u78b0\u649e\u65f6\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u64cd\u7eb5\u5668\u63a8\u5f00\u969c\u788d\u7269\uff0c\u4ece\u800c\u907f\u514d\u5bf9\u8f66\u8f86\u654f\u611f\u533a\u57df\u9020\u6210\u635f\u5bb3\u3002", "motivation": "\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u6267\u884c\u4e3b\u52a8\u5e72\u9884\u4efb\u52a1\u65f6\uff0c\u81ea\u4e3b\u8f66\u8f86\u7684\u5e94\u7528\u6b63\u5728\u6210\u4e3a\u4e00\u4e2a\u6d3b\u8dc3\u7684\u7814\u7a76\u9886\u57df\u3002\u7136\u800c\uff0c\u5728\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\uff0c\u673a\u5668\u4eba\u53ef\u80fd\u4f1a\u56e0\u4e3a\u5404\u79cd\u539f\u56e0\u4e0e\u73af\u5883\u4e2d\u7684\u969c\u788d\u7269\u53d1\u751f\u78b0\u649e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5b89\u5168\u5730\u5904\u7406\u8fd9\u4e9b\u6f5c\u5728\u7684\u78b0\u649e\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5207\u6362\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236(NMPC)\u7b56\u7565\uff0c\u4e13\u95e8\u7528\u4e8e\u89e3\u51b3UVMS\u53ef\u80fd\u9047\u5230\u7684\u78b0\u649e\u95ee\u9898\u3002\u8be5\u7b56\u7565\u80fd\u591f\u5728\u9884\u89c1\u5230\u4e0d\u53ef\u907f\u514d\u7684\u78b0\u649e\u65f6\uff0c\u91c7\u53d6\u63aa\u65bd\u8ba9\u64cd\u7eb5\u5668\u63a8\u538b\u969c\u788d\u7269\uff0c\u4ece\u800c\u4fdd\u62a4\u8f66\u8f86\u81ea\u8eab\u4e0d\u53d7\u635f\u3002", "result": "\u901a\u8fc7\u865a\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b97\u6cd5\u80fd\u591f\u6210\u529f\u68c0\u6d4b\u5230\u78b0\u649e\uff0c\u5e76\u4e14\u8981\u4e48\u907f\u5f00\u78b0\u649e\uff0c\u8981\u4e48\u6070\u5f53\u5730\u4f7f\u7528\u64cd\u7eb5\u5668\u5904\u7406\u78b0\u649e\uff0c\u786e\u4fdd\u4e0d\u635f\u574f\u8f66\u8f86\u4e0a\u7684\u654f\u611f\u90e8\u4ef6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684NMPC\u7b56\u7565\u5bf9\u4e8e\u63d0\u9ad8UVMS\u5728\u9762\u4e34\u78b0\u649e\u5a01\u80c1\u65f6\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\u5177\u6709\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2601.19079", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19079", "abs": "https://arxiv.org/abs/2601.19079", "authors": ["Naqash Afzal", "Niklas Funk", "Erik Helmut", "Jan Peters", "Benjamin Ward-Cherrier"], "title": "Neuromorphic BrailleNet: Accurate and Generalizable Braille Reading Beyond Single Characters through Event-Based Optical Tactile Sensing", "comment": null, "summary": "Conventional robotic Braille readers typically rely on discrete, character-by-character scanning, limiting reading speed and disrupting natural flow. Vision-based alternatives often require substantial computation, introduce latency, and degrade in real-world conditions. In this work, we present a high accuracy, real-time pipeline for continuous Braille recognition using Evetac, an open-source neuromorphic event-based tactile sensor. Unlike frame-based vision systems, the neuromorphic tactile modality directly encodes dynamic contact events during continuous sliding, closely emulating human finger-scanning strategies. Our approach combines spatiotemporal segmentation with a lightweight ResNet-based classifier to process sparse event streams, enabling robust character recognition across varying indentation depths and scanning speeds. The proposed system achieves near-perfect accuracy (>=98%) at standard depths, generalizes across multiple Braille board layouts, and maintains strong performance under fast scanning. On a physical Braille board containing daily-living vocabulary, the system attains over 90% word-level accuracy, demonstrating robustness to temporal compression effects that challenge conventional methods. These results position neuromorphic tactile sensing as a scalable, low latency solution for robotic Braille reading, with broader implications for tactile perception in assistive and robotic applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5f62\u6001\u4e8b\u4ef6\u89e6\u89c9\u4f20\u611f\u5668Evetac\u7684\u5b9e\u65f6\u8fde\u7eed\u76f2\u6587\u8bc6\u522b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u65f6\u7a7a\u5206\u5272\u548c\u8f7b\u91cf\u7ea7ResNet\u5206\u7c7b\u5668\u5904\u7406\u7a00\u758f\u4e8b\u4ef6\u6d41\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff08\u6807\u51c6\u6df1\u5ea6\u4e0b\u226598%\uff09\u7684\u5b57\u7b26\u8bc6\u522b\uff0c\u5e76\u5728\u7269\u7406\u76f2\u6587\u677f\u4e0a\u8fbe\u5230\u4e86\u8d85\u8fc790%\u7684\u8bcd\u7ea7\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u673a\u5668\u4eba\u76f2\u6587\u9605\u8bfb\u5668\u901a\u5e38\u4f9d\u8d56\u4e8e\u9010\u4e2a\u5b57\u7b26\u7684\u79bb\u7ebf\u626b\u63cf\uff0c\u9650\u5236\u4e86\u9605\u8bfb\u901f\u5ea6\u4e14\u7834\u574f\u4e86\u81ea\u7136\u6d41\u7545\u6027\uff1b\u800c\u57fa\u4e8e\u89c6\u89c9\u7684\u66ff\u4ee3\u65b9\u6848\u5f80\u5f80\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u5f15\u5165\u5ef6\u8fdf\uff0c\u5e76\u5728\u5b9e\u9645\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u3001\u5b9e\u65f6\u6027\u7684\u8fde\u7eed\u76f2\u6587\u8bc6\u522b\u7cfb\u7edf\uff0c\u4ee5\u6539\u5584\u73b0\u6709\u6280\u672f\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u540d\u4e3aEvetac\u7684\u5f00\u6e90\u795e\u7ecf\u5f62\u6001\u4e8b\u4ef6\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u8be5\u4f20\u611f\u5668\u80fd\u591f\u76f4\u63a5\u7f16\u7801\u8fde\u7eed\u6ed1\u52a8\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u63a5\u89e6\u4e8b\u4ef6\uff0c\u6a21\u4eff\u4eba\u7c7b\u624b\u6307\u626b\u63cf\u7b56\u7565\u3002\u7814\u7a76\u56e2\u961f\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u65f6\u7a7a\u5206\u5272\u4e0e\u8f7b\u91cf\u7ea7ResNet\u67b6\u6784\u5206\u7c7b\u5668\u7684\u65b9\u6cd5\u6765\u5904\u7406\u7531\u4f20\u611f\u5668\u4ea7\u751f\u7684\u7a00\u758f\u4e8b\u4ef6\u6d41\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u4e0d\u540c\u538b\u75d5\u6df1\u5ea6\u53ca\u626b\u63cf\u901f\u5ea6\u4e0b\u5b57\u7b26\u7684\u9c81\u68d2\u8bc6\u522b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u6807\u51c6\u538b\u75d5\u6df1\u5ea6\u4e0b\u8fbe\u5230\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u6027(\u226598%)\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u591a\u79cd\u76f2\u6587\u7248\u9762\u5e03\u5c40\u95f4\u901a\u7528\u5316\u8868\u73b0\u826f\u597d\uff0c\u5373\u4f7f\u662f\u5728\u5feb\u901f\u626b\u63cf\u6761\u4ef6\u4e0b\u4e5f\u80fd\u4fdd\u6301\u5f3a\u52b2\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5728\u5305\u542b\u65e5\u5e38\u751f\u6d3b\u8bcd\u6c47\u7684\u5b9e\u4f53\u76f2\u6587\u677f\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7cfb\u7edf\u8fbe\u5230\u4e86\u8d85\u8fc790%\u7684\u8bcd\u7ea7\u522b\u51c6\u786e\u5ea6\uff0c\u663e\u793a\u51fa\u5176\u5bf9\u4e8e\u65f6\u95f4\u538b\u7f29\u6548\u5e94\u5177\u6709\u5f88\u597d\u7684\u62b5\u6297\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u795e\u7ecf\u5f62\u6001\u89e6\u89c9\u611f\u77e5\u4e3a\u673a\u5668\u4eba\u76f2\u6587\u9605\u8bfb\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u4f4e\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u5bf9\u4e8e\u8f85\u52a9\u6280\u672f\u548c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u89e6\u89c9\u611f\u77e5\u4e5f\u6709\u66f4\u5e7f\u6cdb\u7684\u610f\u4e49\u3002"}}
{"id": "2601.19098", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19098", "abs": "https://arxiv.org/abs/2601.19098", "authors": ["Kurt Enkera", "Josh Pinskier", "Marcus Gallagher", "David Howard"], "title": "SimTO: A simulation-based topology optimization framework for bespoke soft robotic grippers", "comment": "12 pages, 8 figures. Submitted to Structural and Multidisciplinary Optimization", "summary": "Soft robotic grippers are essential for grasping delicate, geometrically complex objects in manufacturing, healthcare and agriculture. However, existing grippers struggle to grasp feature-rich objects with high topological variability, including gears with sharp tooth profiles on automotive assembly lines, corals with fragile protrusions, or vegetables with irregular branching structures like broccoli. Unlike simple geometric primitives such as cubes or spheres, feature-rich objects lack a clear \"optimal\" contact surface, making them both difficult to grasp and susceptible to damage when grasped by existing gripper designs. Safe handling of such objects therefore requires specialized soft grippers whose morphology is tailored to the object's features. Topology optimization offers a promising approach for producing specialized grippers, but its utility is limited by the requirement for pre-defined load cases. For soft grippers interacting with feature-rich objects, these loads arise from hundreds of unpredictable gripper-object contact forces during grasping and are unknown a priori. To address this problem, we introduce SimTO, a framework that enables high-resolution topology optimization by automatically extracting load cases from a contact-based physics simulator, eliminating the need for manual load specification. Given an arbitrary feature-rich object, SimTO produces highly customized soft grippers with fine-grained morphological features tailored to the object geometry. Numerical results show our designs are not only highly specialized to feature-rich objects, but also generalize to unseen objects.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSimTO\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u57fa\u4e8e\u63a5\u89e6\u7684\u7269\u7406\u6a21\u62df\u5668\u81ea\u52a8\u63d0\u53d6\u8d1f\u8f7d\u60c5\u51b5\u6765\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u62d3\u6251\u4f18\u5316\uff0c\u4ece\u800c\u751f\u6210\u9488\u5bf9\u7279\u5f81\u4e30\u5bcc\u7269\u4f53\u9ad8\u5ea6\u5b9a\u5236\u7684\u8f6f\u4f53\u6293\u624b\u3002", "motivation": "\u73b0\u6709\u7684\u8f6f\u4f53\u6293\u624b\u96be\u4ee5\u6293\u4f4f\u5177\u6709\u590d\u6742\u5f62\u72b6\u548c\u9ad8\u62d3\u6251\u53d8\u5316\u7684\u7269\u4f53\uff0c\u5982\u9f7f\u8f6e\u3001\u73ca\u745a\u6216\u82b1\u6930\u83dc\u7b49\u3002\u8fd9\u4e9b\u7269\u4f53\u7f3a\u4e4f\u660e\u786e\u7684\u6700\u4f73\u63a5\u89e6\u9762\uff0c\u4f7f\u5f97\u5b83\u4eec\u65e2\u96be\u4e8e\u88ab\u6293\u4f4f\u53c8\u5bb9\u6613\u5728\u88ab\u6293\u65f6\u53d7\u635f\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4e3a\u7279\u5b9a\u7269\u4f53\u7279\u6027\u5b9a\u5236\u5f62\u6001\u7684\u4e13\u95e8\u8f6f\u4f53\u6293\u624b\u3002", "method": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86SimTO\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u901a\u8fc7\u57fa\u4e8e\u63a5\u89e6\u7684\u7269\u7406\u6a21\u62df\u5668\u81ea\u52a8\u63d0\u53d6\u8d1f\u8377\u60c5\u51b5\uff0c\u800c\u4e0d\u9700\u8981\u9884\u5148\u5b9a\u4e49\u8d1f\u8377\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u4efb\u610f\u7279\u5f81\u4e30\u5bcc\u7269\u4f53\u7684\u9ad8\u5206\u8fa8\u7387\u62d3\u6251\u4f18\u5316\uff0c\u5e76\u751f\u4ea7\u51fa\u9ad8\u5ea6\u5b9a\u5236\u5316\u7684\u8f6f\u4f53\u6293\u624b\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528SimTO\u8bbe\u8ba1\u7684\u8f6f\u4f53\u6293\u624b\u4e0d\u4ec5\u5bf9\u7279\u5f81\u4e30\u5bcc\u7684\u7269\u4f53\u9ad8\u5ea6\u7279\u5316\uff0c\u800c\u4e14\u8fd8\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u4e0a\u3002", "conclusion": "SimTO\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u9014\u7684\u65b9\u6cd5\u6765\u751f\u6210\u9488\u5bf9\u7279\u5f81\u4e30\u5bcc\u7269\u4f53\u7684\u9ad8\u5ea6\u5b9a\u5236\u5316\u8f6f\u4f53\u6293\u624b\uff0c\u8fd9\u79cd\u65b9\u6cd5\u89e3\u51b3\u4e86\u4f20\u7edf\u62d3\u6251\u4f18\u5316\u4e2d\u9700\u8981\u9884\u5b9a\u4e49\u8d1f\u8377\u7684\u95ee\u9898\u3002"}}
{"id": "2601.19144", "categories": ["cs.RO", "cs.DS"], "pdf": "https://arxiv.org/pdf/2601.19144", "abs": "https://arxiv.org/abs/2601.19144", "authors": ["Tzvika Geft", "William Zhang", "Jingjin Yu", "Kostas Bekris"], "title": "Robust Out-of-Order Retrieval for Grid-Based Storage at Maximum Capacity", "comment": "AAAI 2026", "summary": "This paper proposes a framework for improving the operational efficiency of automated storage systems under uncertainty. It considers a 2D grid-based storage for uniform-sized loads (e.g., containers, pallets, or totes), which are moved by a robot (or other manipulator) along a collision-free path in the grid. The loads are labeled (i.e., unique) and must be stored in a given sequence, and later be retrieved in a different sequence -- an operational pattern that arises in logistics applications, such as last-mile distribution centers and shipyards. The objective is to minimize the load relocations to ensure efficient retrieval. A previous result guarantees a zero-relocation solution for known storage and retrieval sequences, even for storage at full capacity, provided that the side of the grid through which loads are stored/retrieved is at least 3 cells wide. However, in practice, the retrieval sequence can change after the storage phase. To address such uncertainty, this work investigates \\emph{$k$-bounded perturbations} during retrieval, under which any two loads may depart out of order if they are originally at most $k$ positions apart. We prove that a $\u0398(k)$ grid width is necessary and sufficient for eliminating relocations at maximum capacity. We also provide an efficient solver for computing a storage arrangement that is robust to such perturbations. To address the higher-uncertainty case where perturbations exceed $k$, a strategy is introduced to effectively minimize relocations. Extensive experiments show that, for $k$ up to half the grid width, the proposed storage-retrieval framework essentially eliminates relocations. For $k$ values up to the full grid width, relocations are reduced by $50\\%+$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u63d0\u9ad8\u81ea\u52a8\u5316\u5b58\u50a8\u7cfb\u7edf\u64cd\u4f5c\u6548\u7387\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u86512D\u7f51\u683c\u5b58\u50a8\u548ck-\u6709\u754c\u6270\u52a8\u6765\u6700\u5c0f\u5316\u8d1f\u8f7d\u91cd\u5b9a\u4f4d\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u51cf\u5c11\u751a\u81f3\u6d88\u9664\u91cd\u5b9a\u4f4d\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u81ea\u52a8\u5316\u5b58\u50a8\u7cfb\u7edf\u7684\u64cd\u4f5c\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u7269\u6d41\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u5df2\u77e5\u5b58\u50a8\u5e8f\u5217\u4f46\u68c0\u7d22\u5e8f\u5217\u53ef\u80fd\u53d1\u751f\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u6700\u5c0f\u5316\u8d1f\u8f7d\u91cd\u5b9a\u4f4d\u3002\u5148\u524d\u7684\u7814\u7a76\u4fdd\u8bc1\u4e86\u5bf9\u4e8e\u5df2\u77e5\u7684\u5b58\u53d6\u5e8f\u5217\u53ef\u4ee5\u5b9e\u73b0\u96f6\u91cd\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u8fd9\u5e76\u4e0d\u9002\u7528\u4e8e\u68c0\u7d22\u5e8f\u5217\u53d8\u5316\u7684\u60c5\u51b5\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u7814\u7a76\u8005\u4eec\u5f15\u5165\u4e86k-\u6709\u754c\u6270\u52a8\u7684\u6982\u5ff5\uff0c\u5728\u6b64\u6761\u4ef6\u4e0b\u4efb\u610f\u4e24\u4e2a\u8d1f\u8f7d\u5982\u679c\u539f\u672c\u81f3\u591a\u76f8\u9694k\u4e2a\u4f4d\u7f6e\u5219\u53ef\u4ee5\u4e0d\u540c\u987a\u5e8f\u79bb\u5f00\uff0c\u5e76\u8bc1\u660e\u4e86\u0398(k)\u7f51\u683c\u5bbd\u5ea6\u662f\u6ee1\u8f7d\u60c5\u51b5\u4e0b\u6d88\u9664\u91cd\u5b9a\u4f4d\u6240\u5fc5\u9700\u4e14\u5145\u5206\u7684\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u6c42\u89e3\u5668\u6765\u8ba1\u7b97\u5bf9\u8fd9\u79cd\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\u7684\u5b58\u50a8\u5b89\u6392\u7b56\u7565\u3002\u5bf9\u4e8e\u8d85\u51fak\u7684\u66f4\u9ad8\u4e0d\u786e\u5b9a\u6027\u60c5\u51b5\uff0c\u5219\u5f15\u5165\u4e86\u4e00\u79cd\u6709\u6548\u51cf\u5c11\u91cd\u5b9a\u4f4d\u7684\u7b56\u7565\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53k\u503c\u8fbe\u5230\u7f51\u683c\u5bbd\u5ea6\u7684\u4e00\u534a\u65f6\uff0c\u6240\u63d0\u51fa\u7684\u5b58\u50a8-\u68c0\u7d22\u6846\u67b6\u51e0\u4e4e\u53ef\u4ee5\u5b8c\u5168\u6d88\u9664\u91cd\u5b9a\u4f4d\uff1b\u800c\u5f53k\u503c\u8fbe\u5230\u6574\u4e2a\u7f51\u683c\u5bbd\u5ea6\u65f6\uff0c\u91cd\u5b9a\u4f4d\u51cf\u5c11\u4e8650%\u4ee5\u4e0a\u3002", "conclusion": "\u901a\u8fc7\u91c7\u7528\u672c\u6587\u63d0\u51fa\u7684\u8003\u8651k-\u6709\u754c\u6270\u52a8\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u786e\u5b9a\u6027\u589e\u52a0\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u6539\u5584\u81ea\u52a8\u5316\u5b58\u50a8\u7cfb\u7edf\u7684\u64cd\u4f5c\u6548\u7387\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u90a3\u4e9b\u8981\u6c42\u9ad8\u7075\u6d3b\u6027\u4e0e\u9002\u5e94\u6027\u7684\u5e94\u7528\u573a\u666f\u800c\u8a00\u3002"}}
{"id": "2601.19275", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19275", "abs": "https://arxiv.org/abs/2601.19275", "authors": ["Tatsuya Kamijo", "Mai Nishimura", "Cristian C. Beltran-Hernandez", "Nodoka Shibasaki", "Masashi Hamaya"], "title": "Tactile Memory with Soft Robot: Robust Object Insertion via Masked Encoding and Soft Wrist", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Tactile memory, the ability to store and retrieve touch-based experience, is critical for contact-rich tasks such as key insertion under uncertainty. To replicate this capability, we introduce Tactile Memory with Soft Robot (TaMeSo-bot), a system that integrates a soft wrist with tactile retrieval-based control to enable safe and robust manipulation. The soft wrist allows safe contact exploration during data collection, while tactile memory reuses past demonstrations via retrieval for flexible adaptation to unseen scenarios. The core of this system is the Masked Tactile Trajectory Transformer (MAT$^\\text{3}$), which jointly models spatiotemporal interactions between robot actions, distributed tactile feedback, force-torque measurements, and proprioceptive signals. Through masked-token prediction, MAT$^\\text{3}$ learns rich spatiotemporal representations by inferring missing sensory information from context, autonomously extracting task-relevant features without explicit subtask segmentation. We validate our approach on peg-in-hole tasks with diverse pegs and conditions in real-robot experiments. Our extensive evaluation demonstrates that MAT$^\\text{3}$ achieves higher success rates than the baselines over all conditions and shows remarkable capability to adapt to unseen pegs and conditions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aTaMeSo-bot\u7684\u7cfb\u7edf\uff0c\u5b83\u7ed3\u5408\u4e86\u67d4\u8f6f\u624b\u8155\u548c\u57fa\u4e8e\u89e6\u89c9\u68c0\u7d22\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u5b89\u5168\u4e14\u7a33\u5065\u7684\u64cd\u4f5c\u3002\u6838\u5fc3\u662fMasked Tactile Trajectory Transformer (MAT$^3$)\uff0c\u901a\u8fc7\u9884\u6d4b\u88ab\u906e\u853d\u7684\u6807\u8bb0\u6765\u5b66\u4e60\u4e30\u5bcc\u7684\u65f6\u7a7a\u8868\u793a\u3002\u5b9e\u9a8c\u8bc1\u660eMAT$^3$\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u90fd\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u60c5\u51b5\u3002", "motivation": "\u4e3a\u4e86\u590d\u5236\u5b58\u50a8\u548c\u68c0\u7d22\u57fa\u4e8e\u89e6\u6478\u7684\u7ecf\u9a8c\u7684\u80fd\u529b\uff0c\u8fd9\u5bf9\u4e8e\u8bf8\u5982\u5728\u4e0d\u786e\u5b9a\u60c5\u51b5\u4e0b\u63d2\u5165\u94a5\u5319\u7b49\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTaMeSo-bot\u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u4e00\u79cd\u8f6f\u624b\u8155\u4e0e\u57fa\u4e8e\u89e6\u89c9\u8bb0\u5fc6\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5141\u8bb8\u5728\u6570\u636e\u6536\u96c6\u671f\u95f4\u8fdb\u884c\u5b89\u5168\u63a5\u89e6\u63a2\u7d22\uff0c\u5e76\u901a\u8fc7\u68c0\u7d22\u91cd\u7528\u8fc7\u53bb\u6f14\u793a\u6765\u7075\u6d3b\u5730\u9002\u5e94\u672a\u89c1\u60c5\u666f\u3002\u7cfb\u7edf\u7684\u6838\u5fc3\u662fMasked Tactile Trajectory Transformer (MAT$^3$)\uff0c\u5b83\u8054\u5408\u5efa\u6a21\u673a\u5668\u4eba\u52a8\u4f5c\u3001\u5206\u5e03\u5f0f\u89e6\u89c9\u53cd\u9988\u3001\u529b\u77e9\u6d4b\u91cf\u548c\u672c\u4f53\u611f\u53d7\u4fe1\u53f7\u4e4b\u95f4\u7684\u65f6\u7a7a\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u5728\u5404\u79cd\u9500\u9489\u548c\u6761\u4ef6\u4e0b\u8fdb\u884c\u7684\u63d2\u9500\u4efb\u52a1\u4e2d\uff0cMAT$^3$\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u7684\u6210\u529f\u7387\u66f4\u9ad8\uff0c\u5e76\u663e\u793a\u51fa\u5bf9\u672a\u89c1\u9500\u9489\u548c\u6761\u4ef6\u7684\u663e\u8457\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528TaMeSo-bot\u7cfb\u7edf\u53ca\u5176\u6838\u5fc3\u7ec4\u4ef6MAT$^3$\uff0c\u7814\u7a76\u6210\u529f\u5730\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u6267\u884c\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u65f6\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u65b0\u60c5\u51b5\u65f6\u8868\u73b0\u51fa\u4e86\u5f88\u597d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2601.19354", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.19354", "abs": "https://arxiv.org/abs/2601.19354", "authors": ["Ziqian Wang", "Chenxi Fang", "Zhen Zhang"], "title": "Self-Supervised Path Planning in Unstructured Environments via Global-Guided Differentiable Hard Constraint Projection", "comment": null, "summary": "Deploying deep learning agents for autonomous navigation in unstructured environments faces critical challenges regarding safety, data scarcity, and limited computational resources. Traditional solvers often suffer from high latency, while emerging learning-based approaches struggle to ensure deterministic feasibility. To bridge the gap from embodied to embedded intelligence, we propose a self-supervised framework incorporating a differentiable hard constraint projection layer for runtime assurance. To mitigate data scarcity, we construct a Global-Guided Artificial Potential Field (G-APF), which provides dense supervision signals without manual labeling. To enforce actuator limitations and geometric constraints efficiently, we employ an adaptive neural projection layer, which iteratively rectifies the coarse network output onto the feasible manifold. Extensive benchmarks on a test set of 20,000 scenarios demonstrate an 88.75\\% success rate, substantiating the enhanced operational safety. Closed-loop experiments in CARLA further validate the physical realizability of the planned paths under dynamic constraints. Furthermore, deployment verification on an NVIDIA Jetson Orin NX confirms an inference latency of 94 ms, showing real-time feasibility on resource-constrained embedded hardware. This framework offers a generalized paradigm for embedding physical laws into neural architectures, providing a viable direction for solving constrained optimization in mechatronics. Source code is available at: https://github.com/wzq-13/SSHC.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u5fae\u786c\u7ea6\u675f\u6295\u5f71\u5c42\u6765\u4fdd\u8bc1\u8fd0\u884c\u65f6\u7684\u5b89\u5168\u6027\uff0c\u5e76\u901a\u8fc7\u5168\u5c40\u5f15\u5bfc\u7684\u4eba\u5de5\u52bf\u573a\u548c\u81ea\u9002\u5e94\u795e\u7ecf\u6295\u5f71\u5c42\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u6267\u884c\u5668\u9650\u5236\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u572820,000\u4e2a\u573a\u666f\u4e2d\u6210\u529f\u7387\u8fbe\u523088.75%\uff0c\u5e76\u5728CARLA\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5b9e\u73b0\u53ef\u80fd\u6027\u53ca\u5728NVIDIA Jetson Orin NX\u4e0a\u7684\u5b9e\u65f6\u53ef\u884c\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u667a\u80fd\u4f53\u8fdb\u884c\u81ea\u4e3b\u5bfc\u822a\u65f6\u9047\u5230\u7684\u5b89\u5168\u6027\u3001\u6570\u636e\u7a00\u7f3a\u4ee5\u53ca\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u4f20\u7edf\u6c42\u89e3\u5668\u7684\u9ad8\u5ef6\u8fdf\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u96be\u4ee5\u786e\u4fdd\u786e\u5b9a\u6027\u53ef\u884c\u6027\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u53ef\u5fae\u786c\u7ea6\u675f\u6295\u5f71\u5c42\u7684\u81ea\u76d1\u7763\u6846\u67b6\u4ee5\u63d0\u4f9b\u8fd0\u884c\u65f6\u4fdd\u969c\uff1b\u6784\u5efa\u4e86\u5168\u5c40\u5f15\u5bfc\u7684\u4eba\u5de5\u52bf\u573a\uff08G-APF\uff09\u751f\u6210\u5bc6\u96c6\u76d1\u7763\u4fe1\u53f7\u800c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u795e\u7ecf\u6295\u5f71\u5c42\u6709\u6548\u5b9e\u65bd\u6267\u884c\u5668\u9650\u5236\u4e0e\u51e0\u4f55\u7ea6\u675f\u3002", "result": "\u57282\u4e07\u6b21\u6d4b\u8bd5\u573a\u666f\u4e2d\u53d6\u5f97\u4e8688.75%\u7684\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u589e\u5f3a\u7684\u64cd\u4f5c\u5b89\u5168\u6027\uff1b\u5728CARLA\u4e2d\u7684\u95ed\u73af\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u7269\u7406\u5b9e\u73b0\u7684\u53ef\u80fd\u6027\uff1b\u5728NVIDIA Jetson Orin NX\u4e0a\u7684\u90e8\u7f72\u9a8c\u8bc1\u663e\u793a\u63a8\u7406\u5ef6\u8fdf\u4e3a94\u6beb\u79d2\uff0c\u5c55\u793a\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u5d4c\u5165\u5f0f\u786c\u4ef6\u4e0a\u7684\u5b9e\u65f6\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u6846\u67b6\u4e3a\u5c06\u7269\u7406\u5b9a\u5f8b\u5d4c\u5165\u795e\u7ecf\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u8303\u4f8b\uff0c\u4e3a\u89e3\u51b3\u673a\u7535\u4e00\u4f53\u5316\u4e2d\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u6307\u660e\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u65b9\u5411\u3002"}}
{"id": "2601.19406", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19406", "abs": "https://arxiv.org/abs/2601.19406", "authors": ["Kaipeng Fang", "Weiqing Liang", "Yuyang Li", "Ji Zhang", "Pengpeng Zeng", "Lianli Gao", "Jingkuan Song", "Heng Tao Shen"], "title": "Sim-and-Human Co-training for Data-Efficient and Generalizable Robotic Manipulation", "comment": null, "summary": "Synthetic simulation data and real-world human data provide scalable alternatives to circumvent the prohibitive costs of robot data collection. However, these sources suffer from the sim-to-real visual gap and the human-to-robot embodiment gap, respectively, which limits the policy's generalization to real-world scenarios. In this work, we identify a natural yet underexplored complementarity between these sources: simulation offers the robot action that human data lacks, while human data provides the real-world observation that simulation struggles to render. Motivated by this insight, we present SimHum, a co-training framework to simultaneously extract kinematic prior from simulated robot actions and visual prior from real-world human observations. Based on the two complementary priors, we achieve data-efficient and generalizable robotic manipulation in real-world tasks. Empirically, SimHum outperforms the baseline by up to $\\mathbf{40\\%}$ under the same data collection budget, and achieves a $\\mathbf{62.5\\%}$ OOD success with only 80 real data, outperforming the real only baseline by $7.1\\times$. Videos and additional information can be found at \\href{https://kaipengfang.github.io/sim-and-human}{project website}.", "AI": {"tldr": "\u63d0\u51fa\u4e86SimHum\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6a21\u62df\u673a\u5668\u4eba\u52a8\u4f5c\u548c\u771f\u5b9e\u4eba\u7c7b\u89c2\u5bdf\u7684\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u64cd\u4f5c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u540c\u6570\u636e\u6536\u96c6\u9884\u7b97\u4e0b\uff0cSimHum\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6700\u591a40%\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4ec5\u4f7f\u752880\u4e2a\u771f\u5b9e\u6570\u636e\u5c31\u8fbe\u5230\u4e8662.5%\u7684OOD\u6210\u529f\u7387\uff0c\u6bd4\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u7684\u65b9\u6cd5\u9ad8\u51fa7.1\u500d\u3002", "motivation": "\u5408\u6210\u4eff\u771f\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u7684\u4eba\u7c7b\u6570\u636e\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u6765\u89c4\u907f\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u7684\u9ad8\u6602\u6210\u672c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6765\u6e90\u5206\u522b\u53d7\u5230\u89c6\u89c9\u5dee\u8ddd\u548c\u8eab\u4f53\u5dee\u8ddd\u7684\u9650\u5236\uff0c\u5f71\u54cd\u4e86\u7b56\u7565\u5411\u73b0\u5b9e\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002\u672c\u7814\u7a76\u53d1\u73b0\u8fd9\u4e24\u79cd\u6570\u636e\u6e90\u4e4b\u95f4\u5b58\u5728\u4e00\u79cd\u81ea\u7136\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u4e92\u8865\u6027\uff1a\u4eff\u771f\u53ef\u4ee5\u63d0\u4f9b\u4eba\u7c7b\u6570\u636e\u6240\u7f3a\u4e4f\u7684\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u800c\u4eba\u7c7b\u6570\u636e\u5219\u80fd\u63d0\u4f9b\u4eff\u771f\u96be\u4ee5\u6e32\u67d3\u7684\u771f\u5b9e\u4e16\u754c\u89c2\u5bdf\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5171\u8bad\u7ec3\u6846\u67b6SimHum\uff0c\u65e8\u5728\u540c\u65f6\u4ece\u6a21\u62df\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u4e2d\u63d0\u53d6\u8fd0\u52a8\u5b66\u5148\u9a8c\uff0c\u4ee5\u53ca\u4ece\u771f\u5b9e\u4e16\u754c\u7684\u4eba\u7c7b\u89c2\u5bdf\u4e2d\u63d0\u53d6\u89c6\u89c9\u5148\u9a8c\u3002\u57fa\u4e8e\u8fd9\u4e24\u79cd\u4e92\u8865\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6570\u636e\u9ad8\u6548\u4e14\u5177\u6709\u6cdb\u5316\u80fd\u529b\u7684\u673a\u5668\u4eba\u64cd\u7eb5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u76f8\u540c\u7684\u91c7\u96c6\u9884\u7b97\u6761\u4ef6\u4e0b\uff0cSimHum\u76f8\u8f83\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u6700\u9ad8\u53ef\u63d0\u534740%\u7684\u8868\u73b0\uff1b\u5e76\u4e14\u4ec5\u752880\u4e2a\u771f\u5b9e\u6837\u672c\u5373\u80fd\u8fbe\u523062.5%\u7684OOD\u6210\u529f\u7387\uff0c\u76f8\u6bd4\u4ec5\u91c7\u7528\u771f\u5b9e\u6570\u636e\u7684\u57fa\u7ebf\u63d0\u5347\u4e867.1\u500d\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6a21\u62df\u4e0e\u771f\u5b9e\u4eba\u7c7b\u6570\u636e\u7684\u4f18\u52bf\uff0cSimHum\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u89c6\u89c9\u53ca\u5b9e\u4f53\u5dee\u8ddd\u95ee\u9898\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u6267\u884c\u65f6\u7684\u6570\u636e\u5229\u7528\u6548\u7387\u53ca\u5176\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.19411", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19411", "abs": "https://arxiv.org/abs/2601.19411", "authors": ["Ziang Zheng", "Kai Feng", "Yi Nie", "Shentao Qin"], "title": "Task-Centric Policy Optimization from Misaligned Motion Priors", "comment": null, "summary": "Humanoid control often leverages motion priors from human demonstrations to encourage natural behaviors. However, such demonstrations are frequently suboptimal or misaligned with robotic tasks due to embodiment differences, retargeting errors, and task-irrelevant variations, causing na\u00efve imitation to degrade task performance. Conversely, task-only reinforcement learning admits many task-optimal solutions, often resulting in unnatural or unstable motions. This exposes a fundamental limitation of linear reward mixing in adversarial imitation learning. We propose \\emph{Task-Centric Motion Priors} (TCMP), a task-priority adversarial imitation framework that treats imitation as a conditional regularizer rather than a co-equal objective. TCMP maximizes task improvement while incorporating imitation signals only when they are compatible with task progress, yielding an adaptive, geometry-aware update that preserves task-feasible descent and suppresses harmful imitation under misalignment. We provide theoretical analysis of gradient conflict and task-priority stationary points, and validate our claims through humanoid control experiments demonstrating robust task performance with consistent motion style under noisy demonstrations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u4f18\u5148\u7684\u5bf9\u6297\u6a21\u4eff\u6846\u67b6\uff0c\u79f0\u4e3a\u4efb\u52a1\u4e2d\u5fc3\u8fd0\u52a8\u5148\u9a8c(TCMP)\uff0c\u8be5\u6846\u67b6\u5c06\u6a21\u4eff\u89c6\u4e3a\u6761\u4ef6\u6b63\u5219\u5316\u5668\u800c\u975e\u5e73\u7b49\u76ee\u6807\uff0c\u4ee5\u63d0\u9ad8\u4efb\u52a1\u8868\u73b0\u540c\u65f6\u4fdd\u6301\u81ea\u7136\u8fd0\u52a8\u3002", "motivation": "\u4eba\u5f62\u63a7\u5236\u901a\u5e38\u5229\u7528\u4eba\u7c7b\u6f14\u793a\u7684\u52a8\u4f5c\u5148\u9a8c\u6765\u9f13\u52b1\u81ea\u7136\u884c\u4e3a\uff0c\u4f46\u8fd9\u4e9b\u6f14\u793a\u5f80\u5f80\u7531\u4e8e\u5b9e\u4f53\u5dee\u5f02\u3001\u91cd\u65b0\u5b9a\u5411\u9519\u8bef\u548c\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u53d8\u5316\u800c\u5bf9\u673a\u5668\u4eba\u4efb\u52a1\u6765\u8bf4\u662f\u6b21\u4f18\u6216\u4e0d\u4e00\u81f4\u7684\u3002\u7b80\u5355\u6a21\u4eff\u4f1a\u964d\u4f4e\u4efb\u52a1\u6027\u80fd\uff1b\u76f8\u53cd\uff0c\u4ec5\u57fa\u4e8e\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u53ef\u4ee5\u5b9e\u73b0\u591a\u79cd\u4efb\u52a1\u6700\u4f18\u89e3\uff0c\u4f46\u901a\u5e38\u4f1a\u5bfc\u81f4\u4e0d\u81ea\u7136\u6216\u4e0d\u7a33\u5b9a\u7684\u52a8\u4f5c\u3002\u8fd9\u63ed\u793a\u4e86\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u4e2d\u7ebf\u6027\u5956\u52b1\u6df7\u5408\u7684\u57fa\u672c\u5c40\u9650\u6027\u3002", "method": "\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u4efb\u52a1\u4e2d\u5fc3\u8fd0\u52a8\u5148\u9a8c\uff08TCMP\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4efb\u52a1\u4f18\u5148\u7684\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u628a\u6a21\u4eff\u5f53\u4f5c\u4e00\u79cd\u6761\u4ef6\u6027\u7684\u6b63\u5219\u9879\u800c\u4e0d\u662f\u540c\u7b49\u91cd\u8981\u7684\u76ee\u6807\u3002TCMP\u65e8\u5728\u6700\u5927\u5316\u4efb\u52a1\u6539\u8fdb\u7684\u540c\u65f6\uff0c\u53ea\u6709\u5f53\u6a21\u4eff\u4fe1\u53f7\u4e0e\u4efb\u52a1\u8fdb\u5c55\u517c\u5bb9\u65f6\u624d\u5c06\u5176\u7eb3\u5165\u8003\u91cf\uff0c\u4ece\u800c\u4ea7\u751f\u4e00\u4e2a\u81ea\u9002\u5e94\u7684\u3001\u51e0\u4f55\u611f\u77e5\u7684\u66f4\u65b0\u8fc7\u7a0b\uff0c\u4fdd\u6301\u4efb\u52a1\u53ef\u884c\u7684\u4e0b\u964d\u8def\u5f84\u5e76\u6291\u5236\u5728\u5b58\u5728\u504f\u5dee\u60c5\u51b5\u4e0b\u7684\u6709\u5bb3\u6a21\u4eff\u884c\u4e3a\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u68af\u5ea6\u51b2\u7a81\u548c\u4efb\u52a1\u4f18\u5148\u5e73\u7a33\u70b9\uff0c\u5e76\u901a\u8fc7\u4eba\u5f62\u63a7\u5236\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660eTCMP\u80fd\u591f\u5728\u566a\u58f0\u793a\u8303\u4e0b\u5b9e\u73b0\u7a33\u5065\u7684\u4efb\u52a1\u8868\u73b0\u548c\u4e00\u81f4\u7684\u8fd0\u52a8\u98ce\u683c\u3002", "conclusion": "TCMP\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e0e\u4efb\u52a1\u4f18\u5316\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u901a\u8fc7\u667a\u80fd\u5730\u7ed3\u5408\u6a21\u4eff\u4fe1\u53f7\u4e0e\u4efb\u52a1\u9700\u6c42\uff0c\u4f7f\u5f97\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u66f4\u81ea\u7136\u4e14\u6709\u6548\u7684\u52a8\u4f5c\u3002"}}
{"id": "2601.19496", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19496", "abs": "https://arxiv.org/abs/2601.19496", "authors": ["Jie Gu", "Hongrun Gao", "Zhihao Xia", "Yirun Sun", "Chunxu Tian", "Dan Zhang"], "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots", "comment": null, "summary": "For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u91cd\u6784\u89c4\u5212\u7b97\u6cd5\uff0c\u786e\u4fdd\u53ef\u53d8\u5f62\u56db\u8fb9\u5f62\u6a21\u5757\u5316\u81ea\u91cd\u6784\u673a\u5668\u4eba\u5728\u91cd\u6784\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u7a33\u5b9a\u8fde\u63a5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u4e3a\u4e86\u4fdd\u8bc1\u683c\u5b50\u578b\u6a21\u5757\u5316\u81ea\u91cd\u6784\u673a\u5668\u4eba\u5728\u91cd\u6784\u8fc7\u7a0b\u4e2d\u7684\u7269\u7406\u53ef\u884c\u6027\u548c\u90e8\u7f72\u80fd\u529b\uff0c\u9700\u8981\u7ef4\u6301\u7a33\u5b9a\u7684\u8fde\u63a5\u3002", "method": "\u9996\u5148\u4f7f\u7528\u865a\u62df\u56fe\u8868\u793a\u6cd5\u6784\u5efa\u53ef\u884c\u7684\u8fde\u63a5/\u65ad\u5f00\u52a8\u4f5c\uff0c\u7136\u540e\u901a\u8fc7\u4f9d\u8d56\u6027\u53cd\u5411\u6811\uff08DRTree\uff09\u7ec4\u7ec7\u8fd9\u4e9b\u52a8\u4f5c\u4e3a\u6709\u6548\u7684\u6267\u884c\u5e8f\u5217\u4ee5\u89e3\u51b3\u76f8\u4e92\u4f9d\u8d56\u95ee\u9898\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u4e8e\u4e03\u4e2a\u6216\u4ee5\u4e0a\u6a21\u5757\uff08\u6392\u9664\u7ebf\u6027\u62d3\u6251\uff09\u7684\u4efb\u610f\u914d\u7f6e\u5bf9\uff0c\u5b58\u5728\u6ee1\u8db3\u8fd0\u52a8\u7279\u6027\u7684\u91cd\u6784\u5e8f\u5217\uff1b\u4e0e\u4fee\u6539\u540e\u7684BiRRT\u7b97\u6cd5\u76f8\u6bd4\uff0c\u672c\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff1b\u5728\u5b9e\u9645\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u6210\u529f\u90e8\u7f72\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u81ea\u91cd\u6784\u89c4\u5212\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u5757\u5316\u81ea\u91cd\u6784\u673a\u5668\u4eba\u7684\u91cd\u6784\u6548\u7387\u4e0e\u7a33\u5b9a\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.19509", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19509", "abs": "https://arxiv.org/abs/2601.19509", "authors": ["Jin Huang", "Zichen Liu", "Haoda Li", "Zhikun Wang", "Ying Chen"], "title": "A DVL Aided Loosely Coupled Inertial Navigation Strategy for AUVs with Attitude Error Modeling and Variance Propagation", "comment": null, "summary": "In underwater navigation systems, strap-down inertial navigation system/Doppler velocity log (SINS/DVL)-based loosely coupled architectures are widely adopted. Conventional approaches project DVL velocities from the body coordinate system to the navigation coordinate system using SINS-derived attitude; however, accumulated attitude estimation errors introduce biases into velocity projection and degrade navigation performance during long-term operation. To address this issue, two complementary improvements are introduced. First, a vehicle attitude error-aware DVL velocity transformation model is formulated by incorporating attitude error terms into the observation equation to reduce projection-induced velocity bias. Second, a covariance matrix-based variance propagation method is developed to transform DVL measurement uncertainty across coordinate systems, introducing an expectation-based attitude error compensation term to achieve statistically consistent noise modeling. Simulation and field experiment results demonstrate that both improvements individually enhance navigation accuracy and confirm that accumulated attitude errors affect both projected velocity measurements and their associated uncertainty. When jointly applied, long-term error divergence is effectively suppressed. Field experimental results show that the proposed approach achieves a 78.3% improvement in 3D position RMSE and a 71.8% reduction in the maximum component-wise position error compared with the baseline IMU+DVL method, providing a robust solution for improving long-term SINS/DVL navigation performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684SINS/DVL\u6c34\u4e0b\u5bfc\u822a\u7cfb\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u59ff\u6001\u8bef\u5dee\u611f\u77e5\u7684DVL\u901f\u5ea6\u8f6c\u6362\u6a21\u578b\u548c\u57fa\u4e8e\u534f\u65b9\u5dee\u77e9\u9635\u7684\u65b9\u5dee\u4f20\u64ad\u65b9\u6cd5\u6765\u51cf\u5c11\u56e0\u59ff\u6001\u4f30\u8ba1\u7d2f\u79ef\u8bef\u5dee\u5bfc\u81f4\u7684\u901f\u5ea6\u6295\u5f71\u504f\u5dee\uff0c\u4ece\u800c\u63d0\u9ad8\u957f\u671f\u5bfc\u822a\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8fd9\u79cd\u65b9\u6cd5\u57283D\u4f4d\u7f6eRMSE\u4e0a\u63d0\u9ad8\u4e8678.3%\uff0c\u6700\u5927\u5206\u91cf\u4f4d\u7f6e\u8bef\u5dee\u51cf\u5c11\u4e8671.8%\u3002", "motivation": "\u4f20\u7edf\u7684SINS/DVL\u677e\u8026\u5408\u67b6\u6784\u4e2d\uff0c\u7531\u4e8e\u4f7f\u7528SINS\u63d0\u4f9b\u7684\u59ff\u6001\u4fe1\u606f\u5c06DVL\u901f\u5ea6\u4ece\u8f7d\u4f53\u5750\u6807\u7cfb\u8f6c\u6362\u5230\u5bfc\u822a\u5750\u6807\u7cfb\u65f6\u4f1a\u79ef\u7d2f\u59ff\u6001\u4f30\u8ba1\u8bef\u5dee\uff0c\u8fd9\u4e9b\u8bef\u5dee\u6700\u7ec8\u8f6c\u5316\u4e3a\u901f\u5ea6\u6295\u5f71\u504f\u5dee\uff0c\u964d\u4f4e\u4e86\u957f\u65f6\u95f4\u8fd0\u884c\u4e0b\u7684\u5bfc\u822a\u6027\u80fd\u3002", "method": "\u9996\u5148\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u8003\u8651\u8f66\u8f86\u59ff\u6001\u8bef\u5dee\u7684DVL\u901f\u5ea6\u53d8\u6362\u6a21\u578b\uff0c\u5c06\u59ff\u6001\u8bef\u5dee\u9879\u52a0\u5165\u89c2\u6d4b\u65b9\u7a0b\u4ee5\u51cf\u5c11\u7531\u59ff\u6001\u8bef\u5dee\u5f15\u8d77\u7684\u901f\u5ea6\u504f\u5dee\uff1b\u5176\u6b21\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u534f\u65b9\u5dee\u77e9\u9635\u7684\u65b9\u5dee\u4f20\u64ad\u65b9\u6cd5\uff0c\u7528\u4e8e\u8de8\u5750\u6807\u7cfb\u8f6c\u6362DVL\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u671f\u671b\u7684\u59ff\u6001\u8bef\u5dee\u8865\u507f\u9879\uff0c\u5b9e\u73b0\u7edf\u8ba1\u4e00\u81f4\u6027\u7684\u566a\u58f0\u5efa\u6a21\u3002", "result": "\u4eff\u771f\u4e0e\u5b9e\u5730\u6d4b\u8bd5\u8868\u660e\uff0c\u6240\u63d0\u4e24\u9879\u6539\u8fdb\u5355\u72ec\u5e94\u7528\u65f6\u5747\u80fd\u589e\u5f3a\u5bfc\u822a\u7cbe\u5ea6\uff0c\u5e76\u8bc1\u5b9e\u7d2f\u79ef\u59ff\u6001\u8bef\u5dee\u786e\u5b9e\u5f71\u54cd\u4e86\u901f\u5ea6\u6d4b\u91cf\u503c\u53ca\u5176\u76f8\u5173\u4e0d\u786e\u5b9a\u6027\u3002\u5f53\u4e24\u8005\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u80fd\u591f\u6709\u6548\u6291\u5236\u957f\u671f\u8bef\u5dee\u53d1\u6563\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u76f8\u6bd4\u57fa\u51c6IMU+DVL\u65b9\u6cd5\uff0c\u65b0\u65b9\u6cd5\u57283D\u4f4d\u7f6eRMSE\u4e0a\u63d0\u5347\u4e8678.3%\uff0c\u6700\u5927\u5206\u91cf\u4f4d\u7f6e\u8bef\u5dee\u964d\u4f4e\u4e8671.8%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8003\u8651\u59ff\u6001\u8bef\u5dee\u7684DVL\u901f\u5ea6\u8f6c\u6362\u6a21\u578b\u53ca\u57fa\u4e8e\u534f\u65b9\u5dee\u77e9\u9635\u7684\u65b9\u5dee\u4f20\u64ad\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u6539\u5584SINS/DVL\u7ec4\u5408\u5bfc\u822a\u7cfb\u7edf\u7684\u957f\u671f\u5b9a\u4f4d\u51c6\u786e\u6027\u4e0e\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.19514", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19514", "abs": "https://arxiv.org/abs/2601.19514", "authors": ["Ruiyu Wang", "Zheyu Zhuang", "Danica Kragic", "Florian T. Pokorny"], "title": "PALM: Enhanced Generalizability for Local Visuomotor Policies via Perception Alignment", "comment": null, "summary": "Generalizing beyond the training domain in image-based behavior cloning remains challenging. Existing methods address individual axes of generalization, workspace shifts, viewpoint changes, and cross-embodiment transfer, yet they are typically developed in isolation and often rely on complex pipelines. We introduce PALM (Perception Alignment for Local Manipulation), which leverages the invariance of local action distributions between out-of-distribution (OOD) and demonstrated domains to address these OOD shifts concurrently, without additional input modalities, model changes, or data collection. PALM modularizes the manipulation policy into coarse global components and a local policy for fine-grained actions. We reduce the discrepancy between in-domain and OOD inputs at the local policy level by enforcing local visual focus and consistent proprioceptive representation, allowing the policy to retrieve invariant local actions under OOD conditions. Experiments show that PALM limits OOD performance drops to 8% in simulation and 24% in the real world, compared to 45% and 77% for baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPALM\uff08Perception Alignment for Local Manipulation\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u5c40\u90e8\u7b56\u7565\u7ea7\u522b\u7684\u57df\u5185\u548c\u57df\u5916\u8f93\u5165\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u89e3\u51b3\u56fe\u50cf\u884c\u4e3a\u514b\u9686\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4ece\u800c\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u4e5f\u80fd\u6267\u884c\u4e00\u81f4\u7684\u5c40\u90e8\u52a8\u4f5c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0cPALM\u80fd\u591f\u663e\u8457\u964d\u4f4e\u57df\u5916\u6027\u80fd\u4e0b\u964d\u5e45\u5ea6\u3002", "motivation": "\u57fa\u4e8e\u56fe\u50cf\u7684\u884c\u4e3a\u514b\u9686\u5728\u8d85\u51fa\u8bad\u7ec3\u57df\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6cdb\u5316\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u867d\u7136\u73b0\u6709\u65b9\u6cd5\u9488\u5bf9\u5de5\u4f5c\u7a7a\u95f4\u8f6c\u6362\u3001\u89c6\u89d2\u53d8\u5316\u548c\u8de8\u5b9e\u4f53\u8fc1\u79fb\u7b49\u4e2a\u522b\u6cdb\u5316\u8f74\u8fdb\u884c\u4e86\u5904\u7406\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u662f\u5b64\u7acb\u5f00\u53d1\u7684\uff0c\u5e76\u4e14\u7ecf\u5e38\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u6d41\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86PALM\uff08Perception Alignment for Local Manipulation\uff09\uff0c\u5b83\u5229\u7528\u4e86\u5206\u5e03\u5916(OOD)\u4e0e\u6f14\u793a\u9886\u57df\u4e4b\u95f4\u5c40\u90e8\u52a8\u4f5c\u5206\u5e03\u7684\u4e0d\u53d8\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u8fd9\u4e9bOOD\u504f\u79fb\u95ee\u9898\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u8f93\u5165\u6a21\u5f0f\u3001\u6a21\u578b\u66f4\u6539\u6216\u6570\u636e\u6536\u96c6\u3002PALM\u5c06\u64cd\u4f5c\u7b56\u7565\u6a21\u5757\u5316\u4e3a\u7c97\u7565\u7684\u5168\u5c40\u7ec4\u4ef6\u548c\u7528\u4e8e\u7cbe\u7ec6\u52a8\u4f5c\u7684\u5c40\u90e8\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u52a0\u5f3a\u5c40\u90e8\u89c6\u89c9\u7126\u70b9\u548c\u4e00\u81f4\u7684\u672c\u4f53\u611f\u53d7\u8868\u793a\u6765\u51cf\u5c11\u5c40\u90e8\u7b56\u7565\u5c42\u9762\u7684\u57df\u5185\u5916\u8f93\u5165\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0cPALM\u5c06OOD\u6027\u80fd\u4e0b\u964d\u9650\u5236\u57288%\uff0c\u800c\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5219\u4e3a24%\uff0c\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u57fa\u7ebf\u65b9\u6cd5\u5206\u522b\u5bfc\u81f4\u4e8645%\u548c77%\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u901a\u8fc7PALM\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u51cf\u5c11\u56fe\u50cf\u884c\u4e3a\u514b\u9686\u6280\u672f\u9762\u5bf9\u672a\u89c1\u573a\u666f\u65f6\u6027\u80fd\u8870\u9000\u7684\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u5bf9\u4e8e\u63d0\u9ad8\u673a\u5668\u4eba\u5728\u65b0\u73af\u5883\u4e0b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.19529", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19529", "abs": "https://arxiv.org/abs/2601.19529", "authors": ["Jie Gu", "Yirui Sun", "Zhihao Xia", "Tin Lun Lam", "Chunxu Tian", "Dan Zhang"], "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion", "comment": null, "summary": "In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module's stable reconfiguration ability, as well as its positional and docking accuracy.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aRhombot\u7684\u65b0\u578b\u53ef\u53d8\u5f62\u5e73\u9762\u6676\u683c\u6a21\u5757\u5316\u81ea\u91cd\u6784\u673a\u5668\u4eba\uff0c\u5176\u8bbe\u8ba1\u7279\u70b9\u5728\u4e8e\u4f7f\u7528\u83f1\u5f62\u6a21\u5757\u548c\u5355\u4e2a\u4e2d\u5fc3\u5b89\u88c5\u6267\u884c\u5668\u5b9e\u73b0\u6298\u53e0\u4e0e\u5c55\u5f00\u3002\u8be5\u8bbe\u8ba1\u65e8\u5728\u4ee5\u6700\u5c0f\u7684\u63a7\u5236\u590d\u6742\u6027\u5b9e\u73b0\u5f62\u6001\u53d8\u6362\u3001\u5bf9\u63a5\u53ca\u79fb\u52a8\u7b49\u5173\u952e\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u4e00\u7cfb\u5217\u7269\u7406\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u7a33\u5b9a\u91cd\u7ec4\u80fd\u529b\u4ee5\u53ca\u5b9a\u4f4d\u548c\u5bf9\u63a5\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u4e00\u4e2a\u80fd\u591f\u72ec\u7acb\u4e8e\u5468\u56f4\u4ecb\u8d28\u8fdb\u884c\u8fde\u7eed\u4e14\u7a33\u5b9a\u7684\u91cd\u65b0\u914d\u7f6e\u8fc7\u7a0b\uff0c\u5e76\u80fd\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u53ef\u9760\u5730\u5f62\u6210\u5404\u79cd\u914d\u7f6e\u7684\u6a21\u5757\u5316\u81ea\u91cd\u6784\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u540c\u65f6\u51cf\u5c11\u63a7\u5236\u590d\u6742\u5ea6\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u5177\u6709\u83f1\u5f62\u6a21\u5757\u7684\u65b0\u578b\u53ef\u53d8\u5f62\u5e73\u9762\u6676\u683cMSRR\u2014\u2014Rhombot\uff0c\u6bcf\u4e2a\u6a21\u5757\u7531\u4e00\u4e2a\u88c5\u6709\u5355\u4e2a\u4e2d\u5fc3\u6267\u884c\u5668\u7684\u5e73\u884c\u56db\u8fb9\u5f62\u9aa8\u67b6\u7ec4\u6210\uff0c\u53ef\u4ee5\u6cbf\u7740\u5bf9\u89d2\u7ebf\u6298\u53e0\u548c\u5c55\u5f00\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8fd0\u52a8\u539f\u8bedmorphpivoting\u7528\u4e8e\u91cd\u65b0\u914d\u7f6e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b56\u7565\u6765\u4fdd\u8bc1\u5176\u8fde\u7eed\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684Rhombot\u6a21\u5757\u5177\u6709\u826f\u597d\u7684\u7a33\u5b9a\u91cd\u7ec4\u6027\u80fd\uff0c\u540c\u65f6\u4e5f\u8fbe\u5230\u4e86\u8f83\u9ad8\u7684\u4f4d\u7f6e\u548c\u5bf9\u63a5\u51c6\u786e\u6027\u3002", "conclusion": "Rhombot\u7684\u8bbe\u8ba1\u4e3a\u5b9e\u73b0\u4f4e\u63a7\u5236\u590d\u6742\u5ea6\u4e0b\u7684\u591a\u529f\u80fdMSRR\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5176\u72ec\u7279\u7684\u8fd0\u52a8\u673a\u5236\u548c\u53ef\u9760\u7684\u6027\u80fd\u4f7f\u5176\u6210\u4e3a\u672a\u6765\u591a\u53d8\u73af\u5883\u5e94\u7528\u4e2d\u6781\u5177\u6f5c\u529b\u7684\u9009\u62e9\u3002"}}
{"id": "2601.19536", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19536", "abs": "https://arxiv.org/abs/2601.19536", "authors": ["Hongji Liu", "Linwei Zheng", "Yongjian Li", "Mingkai Tang", "Xiaoyang Yan", "Ming Liu", "Jun Ma"], "title": "Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation", "comment": null, "summary": "In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u4e14\u7edf\u4e00\u7684\u77e2\u91cf\u5316\u9053\u8def\u5730\u56fe\u6846\u67b6\uff0c\u5229\u7528\u589e\u5f3a\u7684\u9006\u900f\u89c6\u6620\u5c04\uff08IPM\uff09\uff0c\u901a\u8fc7Catmull-Rom\u6837\u6761\u63cf\u7ed8\u8f66\u9053\u7ebf\uff0c\u4f7f\u7528\u591a\u8fb9\u5f62\u8868\u793a\u5176\u4ed6\u5730\u9762\u6807\u8bb0\u3002\u8be5\u6846\u67b6\u51cf\u5c11\u4e86IPM\u76f8\u5173\u7684\u5236\u56fe\u8bef\u5dee\uff0c\u63d0\u9ad8\u4e86\u521d\u59cbIPM\u5355\u5e94\u77e9\u9635\u548c\u8f66\u8f86\u59ff\u6001\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u514b\u670d\u4e86IPM\u5171\u9762\u5047\u8bbe\u7684\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u63a5\u8fd1\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u9ad8\u7cbe\u5ea6\u5730\u56fe\uff0c\u4f18\u5316\u540e\u7684IPM\u77e9\u9635\u7cbe\u5ea6\u4e0e\u624b\u52a8\u6821\u51c6\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8f66\u8f86\u59ff\u6001\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u9006\u900f\u89c6\u6620\u5c04\uff08IPM\uff09\u5728\u77e2\u91cf\u5316\u9053\u8def\u5730\u56fe\u7ed8\u5236\u4e2d\u5b58\u5728\u4e00\u4e9b\u9650\u5236\uff0c\u5982\u5236\u56fe\u8bef\u5dee\u8f83\u5927\u3001\u5bf9\u5171\u9762\u5047\u8bbe\u654f\u611f\u7b49\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u5e76\u63d0\u9ad8\u9053\u8def\u5730\u56fe\u7684\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u76ca\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\u6846\u67b6\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528Catmul-Rom\u6837\u6761\u6765\u523b\u753b\u8f66\u9053\u7ebf\uff0c\u5e76\u4ee5\u591a\u8fb9\u5f62\u5f62\u5f0f\u4e00\u81f4\u5730\u8868\u8fbe\u6240\u6709\u5176\u5b83\u5730\u9762\u6807\u8bb0\u3002\u5b9e\u4f8b\u5206\u5272\u7684\u7ed3\u679c\u88ab\u7528\u4f5c\u53c2\u8003\uff0c\u4ee5\u7cbe\u70bc\u6837\u6761\u63a7\u5236\u70b9\u53ca\u591a\u8fb9\u5f62\u89d2\u70b9\u7684\u4e09\u7ef4\u4f4d\u7f6e\u3002\u540c\u65f6\uff0c\u4f18\u5316\u4e86IPM\u7684\u5355\u5e94\u77e9\u9635\u4e0e\u8f66\u8f86\u59ff\u6001\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86IPM\u76f8\u5173\u5236\u56fe\u9519\u8bef\uff0c\u63d0\u9ad8\u4e86\u521d\u59cbIPM\u5355\u5e94\u77e9\u9635\u4ee5\u53ca\u9884\u6d4b\u8f66\u8f86\u59ff\u6001\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86IPM\u5171\u9762\u6027\u5047\u8bbe\u5e26\u6765\u7684\u9650\u5236\u3002\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u4ea7\u751f\u63a5\u8fd1\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u9ad8\u7cbe\u5ea6\u5730\u56fe\uff0c\u5176\u4e2d\u4f18\u5316\u540e\u7684IPM\u77e9\u9635\u8fbe\u5230\u4e86\u4e0e\u624b\u5de5\u6821\u51c6\u76f8\u5ab2\u7f8e\u7684\u7cbe\u5ea6\uff0c\u800c\u8f66\u8f86\u59ff\u6001\u7684\u51c6\u786e\u6027\u4e5f\u5f97\u5230\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u7ecf\u6d4e\u5b9e\u60e0\u7684\u77e2\u91cf\u5316\u9053\u8def\u5730\u56fe\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5730\u56fe\u7ed8\u5236\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u6269\u5c55\u4e86\u5bf9\u5e38\u89c1\u5730\u9762\u6807\u8bb0\u548c\u8f66\u9053\u7ebf\u7684\u4e00\u822c\u5316\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2601.19634", "categories": ["cs.RO", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.19634", "abs": "https://arxiv.org/abs/2601.19634", "authors": ["Wenda Yu", "Tianshi Wang", "Fengling Li", "Jingjing Li", "Lei Zhu"], "title": "AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAC^2-VLA\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u8003\u8651\u52a8\u4f5c\u4e0a\u4e0b\u6587\u6765\u63d0\u9ad8\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u5e76\u52a0\u5feb\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u867d\u7136\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u95ed\u73af\u90e8\u7f72\u53d7\u5230\u9ad8\u5ef6\u8fdf\u548c\u8ba1\u7b97\u6210\u672c\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u90fd\u9700\u8981\u8fd0\u884c\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u3002\u6b64\u5916\uff0c\u73b0\u6709\u63d0\u9ad8\u6548\u7387\u7684\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u4e86\u52a8\u4f5c\u4e0a\u4e0b\u6587\u7684\u4f5c\u7528\uff0c\u800c\u540e\u8005\u5bf9\u4e8e\u4f53\u73b0\u5f0f\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA)\uff0c\u8fd9\u662f\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u5b83\u6839\u636e\u5f53\u524d\u89c6\u89c9\u89c2\u5bdf\u3001\u8bed\u8a00\u6307\u4ee4\u4ee5\u53ca\u5148\u524d\u7684\u52a8\u4f5c\u72b6\u6001\u6765\u8c03\u6574\u8ba1\u7b97\u8fc7\u7a0b\u3002\u57fa\u4e8e\u4ee5\u52a8\u4f5c\u4e3a\u4e2d\u5fc3\u7684\u4e0a\u4e0b\u6587\uff0cAC^2-VLA\u80fd\u591f\u5728\u4e0d\u540c\u65f6\u95f4\u6b65\u4e4b\u95f4\u81ea\u9002\u5e94\u5730\u91cd\u7528\u8ba4\u77e5\uff0c\u5e76\u6267\u884c\u4ee4\u724c\u526a\u679d\u548c\u9009\u62e9\u6027\u5730\u6267\u884c\u6a21\u578b\u7ec4\u4ef6\u3002\u4e3a\u4e86\u8bad\u7ec3\u8fd9\u79cd\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u7814\u7a76\u8005\u5f15\u5165\u4e86\u4e00\u4e2a\u52a8\u4f5c\u5f15\u5bfc\u7684\u81ea\u6211\u84b8\u998f\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u65e2\u4fdd\u7559\u4e86\u5bc6\u96c6VLA\u7b56\u7565\u7684\u884c\u4e3a\u53c8\u5141\u8bb8\u7ed3\u6784\u5316\u7a00\u758f\u5316\u8de8\u4efb\u52a1\u548c\u8bbe\u7f6e\u8f6c\u79fb\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u63a7\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aAC^2-VLA\u80fd\u591f\u5b9e\u73b0\u9ad8\u8fbe1.79\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u5c06FLOPs\u964d\u81f3\u5bc6\u96c6\u57fa\u7ebf\u768429.4%\uff0c\u5e76\u4e14\u5728\u4efb\u52a1\u6210\u529f\u7387\u65b9\u9762\u4e0e\u4e4b\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165AC^2-VLA\u6846\u67b6\uff0c\u7814\u7a76\u4eba\u5458\u6210\u529f\u5f00\u53d1\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u6267\u884c\u673a\u5668\u4eba\u64cd\u4f5c\u65f6\u7684\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6c34\u5e73\u7684\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2601.19643", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19643", "abs": "https://arxiv.org/abs/2601.19643", "authors": ["Zoe Betta", "Davide Corongiu", "Carmine Tommaso Recchiuto", "Antonio Sgorbissa"], "title": "Enhancing Worker Safety in Harbors Using Quadruped Robots", "comment": null, "summary": "Infrastructure inspection is becoming increasingly relevant in the field of robotics due to its significant impact on ensuring workers' safety. The harbor environment presents various challenges in designing a robotic solution for inspection, given the complexity of daily operations. This work introduces an initial phase to identify critical areas within the port environment. Following this, a preliminary solution using a quadruped robot for inspecting these critical areas is analyzed.", "AI": {"tldr": "This paper presents an initial phase for identifying critical inspection areas in a harbor and analyzes the use of a quadruped robot for inspecting these areas.", "motivation": "The motivation is to improve workers' safety through the application of robotics in infrastructure inspection, particularly addressing the challenges posed by the complex operations in harbor environments.", "method": "The method involves first identifying critical areas within the port that require inspection. Then, it evaluates the feasibility and effectiveness of using a quadruped robot to conduct inspections in those identified areas.", "result": "The result is a preliminary solution that demonstrates the potential of using a quadruped robot for inspecting critical areas in harbors, which can contribute to enhancing safety and efficiency in such environments.", "conclusion": "The conclusion suggests that employing a quadruped robot for harbor inspections is a promising approach, with the potential to address the unique challenges of this environment and improve overall safety."}}
{"id": "2601.19742", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.19742", "abs": "https://arxiv.org/abs/2601.19742", "authors": ["Ali Jnadi", "Hadi Salloum", "Yaroslav Kholodov", "Alexander Gasnikov", "Karam Almaghout"], "title": "SCOPE: Smooth Convex Optimization for Planned Evolution of Deformable Linear Objects", "comment": "Proceedings of Machine Learning Research tbd:1_13, 2025 International Conference on Computational Optimization", "summary": "We present SCOPE, a fast and efficient framework for modeling and manipulating deformable linear objects (DLOs). Unlike conventional energy-based approaches, SCOPE leverages convex approximations to significantly reduce computational cost while maintaining smooth and physically plausible deformations. This trade-off between speed and accuracy makes the method particularly suitable for applications requiring real-time or near-real-time response. The effectiveness of the proposed framework is demonstrated through comprehensive simulation experiments, highlighting its ability to generate smooth shape trajectories under geometric and length constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86SCOPE\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u6a21\u62df\u548c\u64cd\u4f5c\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff08DLOs\uff09\uff0c\u901a\u8fc7\u4f7f\u7528\u51f8\u8fd1\u4f3c\u65b9\u6cd5\u6765\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u5e73\u6ed1\u4e14\u7269\u7406\u4e0a\u5408\u7406\u7684\u53d8\u5f62\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5b9e\u65f6\u6216\u63a5\u8fd1\u5b9e\u65f6\u54cd\u5e94\u7684\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6a21\u62df\u548c\u64cd\u4f5c\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u65f6\u7684\u6548\u7387\u4e0e\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u53d8\u5f62\u7684\u771f\u5b9e\u611f\uff0c\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6216\u63a5\u8fd1\u5b9e\u65f6\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u540d\u4e3aSCOPE\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u51f8\u8fd1\u4f3c\u800c\u975e\u4f20\u7edf\u7684\u57fa\u4e8e\u80fd\u91cf\u7684\u65b9\u6cd5\u6765\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u786e\u4fdd\u53d8\u5f62\u8fc7\u7a0b\u65e2\u5e73\u6ed1\u53c8\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5168\u9762\u7684\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u51e0\u4f55\u548c\u957f\u5ea6\u7ea6\u675f\u4e0b\u751f\u6210\u5e73\u6ed1\u5f62\u72b6\u8f68\u8ff9\u7684\u80fd\u529b\u3002", "conclusion": "SCOPE\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u4e14\u6709\u6548\u7684\u9014\u5f84\u6765\u5904\u7406\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff0c\u7279\u522b\u9002\u5408\u4e8e\u90a3\u4e9b\u5bf9\u54cd\u5e94\u65f6\u95f4\u6709\u4e25\u683c\u8981\u6c42\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.19761", "categories": ["cs.RO", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.19761", "abs": "https://arxiv.org/abs/2601.19761", "authors": ["Jin Huang", "Fethiye Irmak Do\u011fan", "Hatice Gunes"], "title": "Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications", "comment": "HRI 2026", "summary": "Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users' immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u63a8\u8350\u7cfb\u7edf\u6280\u672f\u6574\u5408\u5230\u793e\u4ea4\u673a\u5668\u4eba\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u7528\u6237\u504f\u597d\uff0c\u5e76\u63d0\u4f9b\u66f4\u52a0\u4e2a\u6027\u5316\u548c\u9053\u5fb7\u8d1f\u8d23\u7684\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u7684\u793e\u4ea4\u673a\u5668\u4eba\u4e2a\u6027\u5316\u65b9\u6cd5\u4e0d\u80fd\u5168\u9762\u6355\u6349\u7528\u6237\u7684\u957f\u671f\u3001\u77ed\u671f\u53ca\u7ec6\u7c92\u5ea6\u504f\u597d\uff0c\u4e5f\u4e0d\u80fd\u5f88\u597d\u5730\u7528\u4e8e\u884c\u52a8\u6392\u5e8f\u9009\u62e9\u3001\u4e3b\u52a8\u4e2a\u6027\u5316\u4ea4\u4e92\u4ee5\u53ca\u786e\u4fdd\u9053\u5fb7\u8d23\u4efb\u7684\u9002\u5e94\u6027\u3002", "method": "\u501f\u9274\u63a8\u8350\u7cfb\u7edf\uff08RSs\uff09\u5728\u5efa\u6a21\u7528\u6237\u504f\u597d\u548c\u63d0\u4f9b\u4e2a\u6027\u5316\u63a8\u8350\u65b9\u9762\u7684\u4e13\u957f\uff0c\u901a\u8fc7\u8c03\u6574\u793e\u4ea4\u673a\u5668\u4eba\u4e0eRSs\u7684\u57fa\u672c\u8303\u5f0f\uff0c\u8bc6\u522b\u53ef\u4ee5\u589e\u5f3a\u793e\u4ea4\u673a\u5668\u4eba\u4e2a\u6027\u5316\u7684\u5173\u952e\u6280\u672f\uff0c\u5e76\u8bbe\u8ba1\u6210\u6a21\u5757\u5316\u3001\u5373\u63d2\u5373\u7528\u7684\u7ec4\u4ef6\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5c06\u63a8\u8350\u7cfb\u7edf\u6280\u672f\u6574\u5408\u5165\u793e\u4ea4\u673a\u5668\u4eba\u4e2d\u7684\u6846\u67b6\uff0c\u4e3aRS\u4e0e\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u9886\u57df\u4e4b\u95f4\u7684\u6df1\u5ea6\u5408\u4f5c\u5f00\u8f9f\u4e86\u9053\u8def\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u63a8\u8350\u7cfb\u7edf\u7684\u4e13\u4e1a\u6280\u80fd\u6765\u6539\u8fdb\u793e\u4ea4\u673a\u5668\u4eba\u7684\u4e2a\u4eba\u5316\u4f53\u9a8c\uff0c\u4e0d\u4ec5\u80fd\u63d0\u9ad8\u7528\u6237\u4f53\u9a8c\u8fd8\u80fd\u4fc3\u8fdb\u4e24\u4e2a\u7814\u7a76\u9886\u57df\u7684\u521b\u65b0\u548c\u53d1\u5c55\u3002"}}
{"id": "2601.19826", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.19826", "abs": "https://arxiv.org/abs/2601.19826", "authors": ["Fan Yang", "Renkai Ma", "Yaxin Hu", "Lingyao Li"], "title": "Whether We Care, How We Reason: The Dual Role of Anthropomorphism and Moral Foundations in Robot Abuse", "comment": null, "summary": "As robots become increasingly integrated into daily life, understanding responses to robot mistreatment carries important ethical and design implications. This mixed-methods study (N = 201) examined how anthropomorphic levels and moral foundations shape reactions to robot abuse. Participants viewed videos depicting physical mistreatment of robots varying in humanness (Spider, Twofoot, Humanoid) and completed measures assessing moral foundations, anger, and social distance. Results revealed that anthropomorphism determines whether people extend moral consideration to robots, while moral foundations shape how they reason about such consideration. Qualitative analysis revealed distinct reasoning patterns: low-progressivism individuals employed character-based judgments, while high-progressivism individuals engaged in future-oriented moral deliberation. Findings offer implications for robot design and policy communication.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u4eec\u5bf9\u673a\u5668\u4eba\u8650\u5f85\u7684\u53cd\u5e94\uff0c\u53d1\u73b0\u62df\u4eba\u5316\u7a0b\u5ea6\u548c\u9053\u5fb7\u57fa\u7840\u5f71\u54cd\u4eba\u4eec\u5982\u4f55\u5bf9\u906d\u53d7\u8650\u5f85\u7684\u673a\u5668\u4eba\u4ea7\u751f\u540c\u60c5\u53ca\u9053\u5fb7\u5224\u65ad\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u8d8a\u6765\u8d8a\u591a\u5730\u878d\u5165\u65e5\u5e38\u751f\u6d3b\uff0c\u7406\u89e3\u4eba\u4eec\u5bf9\u673a\u5668\u4eba\u8650\u5f85\u7684\u53cd\u5e94\u5177\u6709\u91cd\u8981\u7684\u4f26\u7406\u548c\u8bbe\u8ba1\u610f\u4e49\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff08N = 201\uff09\uff0c\u53c2\u4e0e\u8005\u89c2\u770b\u4e86\u5c55\u793a\u4e0d\u540c\u4eba\u7c7b\u7279\u5f81\u7b49\u7ea7\uff08\u8718\u86db\u578b\u3001\u4e24\u8db3\u884c\u8d70\u578b\u3001\u7c7b\u4eba\u578b\uff09\u673a\u5668\u4eba\u53d7\u5230\u8eab\u4f53\u8650\u5f85\u7684\u89c6\u9891\uff0c\u5e76\u5b8c\u6210\u4e86\u8bc4\u4f30\u5176\u9053\u5fb7\u57fa\u7840\u3001\u6124\u6012\u60c5\u7eea\u548c\u793e\u4f1a\u8ddd\u79bb\u611f\u7684\u6d4b\u91cf\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u62df\u4eba\u5316\u51b3\u5b9a\u4e86\u4eba\u4eec\u662f\u5426\u4f1a\u5c06\u9053\u5fb7\u8003\u91cf\u6269\u5c55\u5230\u673a\u5668\u4eba\u8eab\u4e0a\uff1b\u800c\u4e2a\u4eba\u7684\u9053\u5fb7\u57fa\u7840\u5219\u5f71\u54cd\u4ed6\u4eec\u5bf9\u6b64\u7c7b\u8003\u91cf\u7684\u63a8\u7406\u65b9\u5f0f\u3002\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u7684\u63a8\u7406\u6a21\u5f0f\uff1a\u8fdb\u6b65\u4e3b\u4e49\u503e\u5411\u8f83\u4f4e\u8005\u503e\u5411\u4e8e\u57fa\u4e8e\u89d2\u8272\u8fdb\u884c\u8bc4\u5224\uff0c\u800c\u8fdb\u6b65\u4e3b\u4e49\u503e\u5411\u8f83\u9ad8\u8005\u5219\u53c2\u4e0e\u66f4\u9762\u5411\u672a\u6765\u7684\u9053\u5fb7\u601d\u8003\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u673a\u5668\u4eba\u8bbe\u8ba1\u548c\u653f\u7b56\u6c9f\u901a\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2601.19832", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19832", "abs": "https://arxiv.org/abs/2601.19832", "authors": ["Elena Merlo", "Marta Lagomarsino", "Arash Ajoudani"], "title": "Information-Theoretic Detection of Bimanual Interactions for Dual-Arm Robot Plan Generation", "comment": null, "summary": "Programming by demonstration is a strategy to simplify the robot programming process for non-experts via human demonstrations. However, its adoption for bimanual tasks is an underexplored problem due to the complexity of hand coordination, which also hinders data recording. This paper presents a novel one-shot method for processing a single RGB video of a bimanual task demonstration to generate an execution plan for a dual-arm robotic system. To detect hand coordination policies, we apply Shannon's information theory to analyze the information flow between scene elements and leverage scene graph properties. The generated plan is a modular behavior tree that assumes different structures based on the desired arms coordination. We validated the effectiveness of this framework through multiple subject video demonstrations, which we collected and made open-source, and exploiting data from an external, publicly available dataset. Comparisons with existing methods revealed significant improvements in generating a centralized execution plan for coordinating two-arm systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u6b21\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5355\u4e2aRGB\u89c6\u9891\u4e2d\u53cc\u624b\u4efb\u52a1\u6f14\u793a\u6765\u4e3a\u53cc\u81c2\u673a\u5668\u4eba\u7cfb\u7edf\u751f\u6210\u6267\u884c\u8ba1\u5212\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u9999\u519c\u4fe1\u606f\u8bba\u548c\u573a\u666f\u56fe\u5c5e\u6027\u6765\u68c0\u6d4b\u624b\u90e8\u534f\u8c03\u7b56\u7565\uff0c\u5e76\u751f\u6210\u6a21\u5757\u5316\u884c\u4e3a\u6811\u4f5c\u4e3a\u6267\u884c\u8ba1\u5212\u3002\u901a\u8fc7\u591a\u4e2a\u4e3b\u9898\u89c6\u9891\u6f14\u793a\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u751f\u6210\u53cc\u81c2\u7cfb\u7edf\u96c6\u4e2d\u6267\u884c\u8ba1\u5212\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u7f16\u7a0b\u6f14\u793a\u662f\u4e00\u79cd\u901a\u8fc7\u4eba\u7c7b\u6f14\u793a\u7b80\u5316\u975e\u4e13\u5bb6\u7684\u673a\u5668\u4eba\u7f16\u7a0b\u8fc7\u7a0b\u7684\u7b56\u7565\u3002\u7136\u800c\uff0c\u7531\u4e8e\u624b\u90e8\u534f\u8c03\u7684\u590d\u6742\u6027\uff0c\u5176\u5728\u53cc\u624b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4ecd\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\uff0c\u8fd9\u4e5f\u963b\u788d\u4e86\u6570\u636e\u8bb0\u5f55\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5904\u7406\u5355\u4e2aRGB\u89c6\u9891\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u751f\u6210\u9002\u7528\u4e8e\u53cc\u81c2\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6267\u884c\u8ba1\u5212\u3002", "method": "\u672c\u6587\u91c7\u7528\u7684\u65b9\u6cd5\u662f\u57fa\u4e8e\u9999\u519c\u4fe1\u606f\u8bba\u6765\u5206\u6790\u573a\u666f\u5143\u7d20\u4e4b\u95f4\u7684\u4fe1\u606f\u6d41\uff0c\u5e76\u5229\u7528\u573a\u666f\u56fe\u5c5e\u6027\u6765\u68c0\u6d4b\u624b\u90e8\u534f\u8c03\u7b56\u7565\u3002\u6839\u636e\u6240\u9700\u7684\u53cc\u81c2\u534f\u8c03\u60c5\u51b5\uff0c\u6240\u751f\u6210\u7684\u6267\u884c\u8ba1\u5212\u662f\u4e00\u79cd\u5177\u6709\u4e0d\u540c\u7ed3\u6784\u7684\u6a21\u5757\u5316\u884c\u4e3a\u6811\u3002", "result": "\u901a\u8fc7\u5bf9\u591a\u4e3b\u4f53\u89c6\u9891\u6f14\u793a\u7684\u9a8c\u8bc1\uff0c\u4ee5\u53ca\u4f7f\u7528\u5916\u90e8\u516c\u5f00\u6570\u636e\u96c6\u7684\u6570\u636e\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u751f\u6210\u53cc\u81c2\u7cfb\u7edf\u6267\u884c\u8ba1\u5212\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u65b0\u65b9\u6cd5\u5728\u4ea7\u751f\u53cc\u81c2\u7cfb\u7edf\u96c6\u4e2d\u6267\u884c\u8ba1\u5212\u4e0a\u663e\u793a\u51fa\u660e\u663e\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u5355\u6b21\u65b9\u6cd5\u6210\u529f\u5730\u4ece\u4e00\u4e2aRGB\u89c6\u9891\u4e2d\u63d0\u53d6\u4e86\u53cc\u81c2\u4efb\u52a1\u7684\u624b\u90e8\u534f\u8c03\u7b56\u7565\uff0c\u5e76\u4e3a\u53cc\u81c2\u673a\u5668\u4eba\u7cfb\u7edf\u751f\u6210\u4e86\u6709\u6548\u7684\u6267\u884c\u8ba1\u5212\u3002\u8fd9\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u514b\u670d\u4ee5\u5f80\u53cc\u624b\u4efb\u52a1\u7f16\u7a0b\u6f14\u793a\u4e2d\u7684\u6311\u6218\uff0c\u8fd8\u663e\u8457\u63d0\u9ad8\u4e86\u53cc\u81c2\u7cfb\u7edf\u534f\u8c03\u5de5\u4f5c\u7684\u80fd\u529b\u3002"}}
{"id": "2601.19839", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.19839", "abs": "https://arxiv.org/abs/2601.19839", "authors": ["Jeanne Mal\u00e9cot", "Hamed Rahimi", "Jeanne Cattoni", "Marie Samson", "Mouad Abrini", "Mahdi Khoramshahi", "Maribel Pino", "Mohamed Chetouani"], "title": "HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs", "comment": null, "summary": "Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHARMONI\u7684\u591a\u6a21\u6001\u4e2a\u6027\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u652f\u6301\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u5728\u591a\u7528\u6237\u73af\u5883\u4e2d\u8fdb\u884c\u957f\u671f\u4e92\u52a8\u3002\u901a\u8fc7\u56db\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u611f\u77e5\u3001\u4e16\u754c\u5efa\u6a21\u3001\u7528\u6237\u5efa\u6a21\u548c\u751f\u6210\uff0cHARMONI\u80fd\u591f\u5b9e\u73b0\u7a33\u5b9a\u7684\u8bf4\u8bdd\u4eba\u8bc6\u522b\u3001\u5728\u7ebf\u8bb0\u5fc6\u66f4\u65b0\u4ee5\u53ca\u7b26\u5408\u4f26\u7406\u7684\u4e2a\u6027\u5316\u670d\u52a1\uff0c\u5e76\u5728\u7528\u6237\u5efa\u6a21\u51c6\u786e\u6027\u3001\u4e2a\u6027\u5316\u8d28\u91cf\u53ca\u7528\u6237\u6ee1\u610f\u5ea6\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u901a\u5e38\u7f3a\u4e4f\u5728\u591a\u7528\u6237\u73af\u5883\u4e2d\u6301\u7eed\u4e2a\u6027\u5316\u548c\u52a8\u6001\u9002\u5e94\u7684\u673a\u5236\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86HARMONI\u6846\u67b6\u3002", "method": "HARMONI\u6846\u67b6\u5305\u62ec\u56db\u4e2a\u4e3b\u8981\u7ec4\u6210\u90e8\u5206\uff1a(i) \u611f\u77e5\u6a21\u5757\uff0c\u7528\u4e8e\u8bc6\u522b\u6d3b\u8dc3\u8bf4\u8bdd\u8005\u5e76\u63d0\u53d6\u591a\u6a21\u6001\u8f93\u5165\uff1b(ii) \u4e16\u754c\u5efa\u6a21\u6a21\u5757\uff0c\u8d1f\u8d23\u7ef4\u62a4\u73af\u5883\u8868\u793a\u4e0e\u77ed\u671f\u5bf9\u8bdd\u4e0a\u4e0b\u6587\uff1b(iii) \u7528\u6237\u5efa\u6a21\u6a21\u5757\uff0c\u7528\u6765\u66f4\u65b0\u9488\u5bf9\u6bcf\u4e2a\u8bf4\u8bdd\u8005\u7684\u957f\u671f\u4e2a\u4eba\u8d44\u6599\uff1b(iv) \u751f\u6210\u6a21\u5757\uff0c\u5219\u662f\u4ea7\u751f\u57fa\u4e8e\u60c5\u5883\u4e14\u9075\u5faa\u9053\u5fb7\u89c4\u8303\u7684\u56de\u5e94\u3002", "result": "\u901a\u8fc7\u5bf9\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u548c\u6d88\u878d\u5b9e\u9a8c\uff0c\u4ee5\u53ca\u5728\u4e00\u4e2a\u517b\u8001\u9662\u73af\u5883\u4e2d\u5f00\u5c55\u7684\u771f\u5b9e\u573a\u666f\u9a71\u52a8\u7684\u7528\u6237\u7814\u7a76\u663e\u793a\uff0cHARMONI\u6846\u67b6\u5728\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7a33\u5b9a\u6027\u3001\u5728\u7ebf\u8bb0\u5fc6\u66f4\u65b0\u80fd\u529b\u4ee5\u53ca\u7b26\u5408\u4f26\u7406\u7684\u4e2a\u6027\u5316\u670d\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4\u5176\u4ed6\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5728\u7528\u6237\u5efa\u6a21\u51c6\u786e\u5ea6\u3001\u4e2a\u6027\u5316\u54c1\u8d28\u53ca\u7528\u6237\u6ee1\u610f\u5ea6\u4e0a\u5747\u6709\u6240\u63d0\u5347\u3002", "conclusion": "HARMONI\u6846\u67b6\u6210\u529f\u5730\u89e3\u51b3\u4e86\u591a\u7528\u6237\u73af\u5883\u4e0b\u793e\u4f1a\u8f85\u52a9\u673a\u5668\u4eba\u9762\u4e34\u7684\u4e2a\u6027\u5316\u548c\u9002\u5e94\u6027\u6311\u6218\uff0c\u4e3a\u63d0\u9ad8\u6b64\u7c7b\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.19856", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19856", "abs": "https://arxiv.org/abs/2601.19856", "authors": ["Giulio Campagna", "Marta Lagomarsino", "Marta Lorenzini", "Dimitrios Chrysostomou", "Matthias Rehm", "Arash Ajoudani"], "title": "Estimating Trust in Human-Robot Collaboration through Behavioral Indicators and Explainability", "comment": null, "summary": "Industry 5.0 focuses on human-centric collaboration between humans and robots, prioritizing safety, comfort, and trust. This study introduces a data-driven framework to assess trust using behavioral indicators. The framework employs a Preference-Based Optimization algorithm to generate trust-enhancing trajectories based on operator feedback. This feedback serves as ground truth for training machine learning models to predict trust levels from behavioral indicators. The framework was tested in a chemical industry scenario where a robot assisted a human operator in mixing chemicals. Machine learning models classified trust with over 80\\% accuracy, with the Voting Classifier achieving 84.07\\% accuracy and an AUC-ROC score of 0.90. These findings underscore the effectiveness of data-driven methods in assessing trust within human-robot collaboration, emphasizing the valuable role behavioral indicators play in predicting the dynamics of human trust.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u884c\u4e3a\u6307\u6807\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u4fe1\u4efb\u5ea6\u3002\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u4fe1\u4efb\u6c34\u5e73\uff0c\u5e76\u5728\u5316\u5b66\u5de5\u4e1a\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u8d85\u8fc780%\u7684\u51c6\u786e\u7387\u5206\u7c7b\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u968f\u7740\u5de5\u4e1a5.0\u7684\u53d1\u5c55\uff0c\u5f3a\u8c03\u4e86\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5408\u4f5c\u5173\u7cfb\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5b89\u5168\u3001\u8212\u9002\u548c\u4fe1\u4efb\u65b9\u9762\u3002\u4e3a\u4e86\u4fc3\u8fdb\u8fd9\u79cd\u5408\u4f5c\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u5e76\u589e\u5f3a\u64cd\u4f5c\u5458\u5bf9\u534f\u4f5c\u673a\u5668\u4eba\u7684\u4fe1\u4efb\u3002", "method": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u4fe1\u4efb\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u504f\u597d\u4f18\u5316\u7b97\u6cd5\u751f\u6210\u80fd\u589e\u8fdb\u4fe1\u4efb\u7684\u8f68\u8ff9\uff0c\u5e76\u57fa\u4e8e\u64cd\u4f5c\u5458\u53cd\u9988\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u4ece\u884c\u4e3a\u6307\u6807\u9884\u6d4b\u4fe1\u4efb\u6c34\u5e73\u3002", "result": "\u5728\u5316\u5b66\u5de5\u4e1a\u573a\u666f\u4e0b\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e0b\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u4ee5\u8d85\u8fc780%\u7684\u51c6\u786e\u7387\u5bf9\u4fe1\u4efb\u8fdb\u884c\u5206\u7c7b\uff1b\u5176\u4e2d\u6295\u7968\u5206\u7c7b\u5668\u8fbe\u5230\u4e8684.07%\u7684\u51c6\u786e\u7387\u53ca0.90\u7684AUC-ROC\u5206\u6570\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u6570\u636e\u7684\u65b9\u6cd5\u5bf9\u4e8e\u8bc4\u4f30\u4eba\u673a\u534f\u4f5c\u73af\u5883\u4e2d\u7684\u4fe1\u4efb\u975e\u5e38\u6709\u6548\uff0c\u540c\u65f6\u4e5f\u5f3a\u8c03\u4e86\u884c\u4e3a\u6307\u6807\u5728\u9884\u6d4b\u4eba\u7c7b\u4fe1\u4efb\u52a8\u6001\u65b9\u9762\u7684\u4ef7\u503c\u3002"}}
