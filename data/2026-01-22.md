<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [RoboBrain 2.5: Depth in Sight, Time in Mind](https://arxiv.org/abs/2601.14352)
*Huajie Tan,Enshen Zhou,Zhiyu Li,Yijie Xu,Yuheng Ji,Xiansheng Chen,Cheng Chi,Pengwei Wang,Huizhu Jia,Yulong Ao,Mingyu Cao,Sixiang Chen,Zhe Li,Mengzhen Liu,Zixiao Wang,Shanyu Rong,Yaoxu Lyu,Zhongxia Zhao,Peterson Co,Yibo Li,Yi Han,Shaoxuan Xie,Guocai Yao,Songjing Wang,Leiduo Zhang,Xi Yang,Yance Jiao,Donghai Shi,Kunchang Xie,Shaokai Nie,Chunlei Men,Yonghua Lin,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboBrain 2.5是一个先进的AI基础模型，通过高质量的时空监督训练，增强了通用感知、空间推理和时间建模能力。它引入了精确3D空间推理和密集时间价值估计两大升级，推动了更物理化和执行感知的具身智能发展，适用于复杂精细的操作任务。


<details>
  <summary>Details</summary>
Motivation: 开发RoboBrain 2.5的主要动机是提升AI在理解物理世界以及执行复杂操作任务时的能力。通过引入对3D空间更加准确的理解与评估行动进展的新方法，旨在实现更为精准且具备自我反馈机制的机器人智能。

Method: RoboBrain 2.5通过对高质量时空数据进行广泛训练来改进其前身版本。该模型采用了从2D像素相对定位到深度感知坐标预测及绝对度量约束理解的方法，实现了精确3D空间推理；同时，还发展了能够提供密集步骤级进度预测和状态理解的密集时间价值估计技术。

Result: 这些改进使得RoboBrain 2.5能够生成满足物理约束条件下的完整3D操纵轨迹，并且能够在不同视角下提供稳定反馈信号以支持后续学习过程。

Conclusion: RoboBrain 2.5展示了在增强AI系统对于真实世界环境的理解力及其执行复杂精密操控任务方面取得的重大进步。

Abstract: We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io

</details>


### [2] [Robust Haptic Rendering Using a Nonlinear Impedance Matching Approach (NIMA) for Robotic Laparoscopic Surgery](https://arxiv.org/abs/2601.14445)
*Aiden Mazidi,Majid Roshanfar,Amir Sayadi,Javad Dargahi,Jake Barralet,Liane S. Feldman,Amir Hooshiar*

Main category: cs.RO

TL;DR: 本研究提出了一种非线性阻抗匹配方法（NIMA），用于改善机器人辅助微创手术中的力反馈精度。NIMA通过准确建模工具-组织交互的非线性动力学，显著提高了力反馈的准确性，并解决了触觉“回弹”问题，从而增强了患者安全性和用户舒适度。


<details>
  <summary>Details</summary>
Motivation: 在机器人辅助微创手术中，准确再现力反馈并保证系统安全性一直面临挑战。开发一个鲁棒且高保真的触觉系统对于提高远程操作外科工具的精确度和可靠性至关重要。

Method: 研究人员提出了非线性阻抗匹配方法（NIMA），它基于先前验证过的阻抗匹配方法（IMA）进行改进，通过引入非线性动力学来更有效地捕捉和再现工具与组织之间的相互作用力。

Result: NIMA将力反馈的平均绝对误差降至0.01牛顿（标准差0.02牛顿），相比IMA减少了95%的误差。此外，NIMA还消除了用户释放手柄时由触觉装置施加给用户的力，即解决了触觉‘回弹’现象，这不仅提升了用户体验也增加了安全性。

Conclusion: NIMA能够更好地考虑工具-组织交互过程中的非线性因素，从而在多种手术条件下提供更佳的力量保真度、响应速度及精确度。这些发现促进了机器人手术中触觉反馈系统的进步，为机器人辅助手术提供了更加现实可靠的界面。

Abstract: Background: The integration of haptic feedback into robot-assisted minimally invasive surgery (RAMIS) has long been limited by challenges in accurately rendering forces and ensuring system safety. The need for robust, high-fidelity haptic systems is critical for enhancing the precision and reliability of teleoperated surgical tools. Methods: In this study, we present a Nonlinear Impedance Matching Approach (NIMA) designed to improve force rendering by accurately modelling complex tool-tissue interactions. Based on our previously validated Impedance Matching Approach (IMA), our novel NIMA method includes nonlinear dynamics to capture and render tool-tissue forces effectively. Results: NIMA improves force feedback accuracy with a mean absolute error (MAE) of 0.01 (SD 0.02) N, achieving a 95% reduction in MAE compared to IMA. Furthermore, NIMA effectively eliminates haptic "kickback" by ensuring no force is applied by the haptic device to the user's hand when they release the handle, enhancing both patient safety and user comfort. Conclusion: NIMA's ability to account for nonlinearities in tool-tissue interactions provides an improvement in force fidelity, responsiveness, and precision across various surgical conditions. Our findings promote the advancement of haptic feedback systems for robotic surgery, offering a realistic and reliable interface for robot-assisted surgical procedures.

</details>


### [3] [UNCLE-Grasp: Uncertainty-Aware Grasping of Leaf-Occluded Strawberries](https://arxiv.org/abs/2601.14492)
*Malak Mansour,Ali Abouzeid,Zezhou Sun,Qinbo Sun,Dezhen Song,Abdalla Swikir*

Main category: cs.RO

TL;DR: 提出了一种针对部分遮挡草莓的不确定性感知抓取方法，通过点云补全和蒙特卡洛dropout来采样多个形状假设，并使用物理基础的力闭合度量评估抓取可行性。该方法在模拟和实际机器人实验中均优于确定性基线。


<details>
  <summary>Details</summary>
Motivation: 在部分遮挡情况下，基于单一确定性形状估计的抓取决策不可靠，因为从单次局部观察可能推导出多种不兼容的3D补全方案，导致某些看似可行的抓取动作实际上会失败。

Method: 采用点云补全结合蒙特卡洛dropout技术生成多个形状假设，为每个假设生成候选抓取姿态，并利用基于物理的力闭合指标评估抓取可行性。最终，通过综合各补全方案下的可行性并应用保守的下置信界（LCB）准则决定是否执行抓取或安全放弃。

Result: 所提方法在仿真环境及真实机器人上进行了测试，结果表明，在严重遮挡条件下，不确定性感知决策能够可靠地避免高风险抓取尝试，同时在几何信心充足时保持稳健的抓取执行效果，总体表现优于确定性基线方法。

Conclusion: 研究展示了一种有效处理部分遮挡场景下草莓采摘不确定性的方法，通过整合形状重建不确定性与抓取策略选择提高了抓取系统的鲁棒性和可靠性。

Abstract: Robotic strawberry harvesting is challenging under partial occlusion, where leaves induce significant geometric uncertainty and make grasp decisions based on a single deterministic shape estimate unreliable. From a single partial observation, multiple incompatible 3D completions may be plausible, causing grasps that appear feasible on one completion to fail on another. We propose an uncertainty-aware grasping pipeline for partially occluded strawberries that explicitly models completion uncertainty arising from both occlusion and learned shape reconstruction. Our approach uses point cloud completion with Monte Carlo dropout to sample multiple shape hypotheses, generates candidate grasps for each completion, and evaluates grasp feasibility using physically grounded force-closure-based metrics. Rather than selecting a grasp based on a single estimate, we aggregate feasibility across completions and apply a conservative lower confidence bound (LCB) criterion to decide whether a grasp should be attempted or safely abstained. We evaluate the proposed method in simulation and on a physical robot across increasing levels of synthetic and real leaf occlusion. Results show that uncertainty-aware decision making enables reliable abstention from high-risk grasp attempts under severe occlusion while maintaining robust grasp execution when geometric confidence is sufficient, outperforming deterministic baselines in both simulated and physical robot experiments.

</details>


### [4] [TacUMI: A Multi-Modal Universal Manipulation Interface for Contact-Rich Tasks](https://arxiv.org/abs/2601.14550)
*Tailai Cheng,Kejia Chen,Lingyun Chen,Liding Zhang,Yue Zhang,Yao Ling,Mahdi Hamad,Zhenshan Bing,Fan Wu,Karan Sharma,Alois Knoll*

Main category: cs.RO

TL;DR: 本文介绍了一种多模态数据收集系统TacUMI及其配套的多模态分割框架，用于提高复杂长周期操作任务的理解和学习。通过集成多种传感器，该系统能够同步获取人类演示中的多种信息，并通过时间模型识别操作序列中语义上有意义的事件边界。实验结果表明，在电缆安装任务上，其分割准确率超过90%，且随着模态数量的增加性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 针对仅依赖视觉观察与机器人本体感觉信息难以揭示复杂长周期操作任务中潜在事件转换的问题，提出需要高效收集高质量多模态数据以及开发稳健的分割方法来将演示分解为有意义的模块。

Method: 开发了名为TacUMI的多模态数据收集系统，它集成了ViTac传感器、力矩传感器及姿态追踪器于一个紧凑且兼容机器人的夹爪设计中；提出了一个多模态分割框架，利用时间模型在连续操作过程中检测语义上重要的事件边界。

Result: 对一项具有挑战性的电缆安装任务进行评估显示，所提出的解决方案分割准确率超过了90%。同时，随着使用模态数量的增加，系统的性能得到了显著改善。

Conclusion: TacUMI不仅为接触丰富任务中的多模态演示提供了可扩展的数据收集手段，还为其分割奠定了实用基础。

Abstract: Task decomposition is critical for understanding and learning complex long-horizon manipulation tasks. Especially for tasks involving rich physical interactions, relying solely on visual observations and robot proprioceptive information often fails to reveal the underlying event transitions. This raises the requirement for efficient collection of high-quality multi-modal data as well as robust segmentation method to decompose demonstrations into meaningful modules. Building on the idea of the handheld demonstration device Universal Manipulation Interface (UMI), we introduce TacUMI, a multi-modal data collection system that integrates additionally ViTac sensors, force-torque sensor, and pose tracker into a compact, robot-compatible gripper design, which enables synchronized acquisition of all these modalities during human demonstrations. We then propose a multi-modal segmentation framework that leverages temporal models to detect semantically meaningful event boundaries in sequential manipulations. Evaluation on a challenging cable mounting task shows more than 90 percent segmentation accuracy and highlights a remarkable improvement with more modalities, which validates that TacUMI establishes a practical foundation for both scalable collection and segmentation of multi-modal demonstrations in contact-rich tasks.

</details>


### [5] [UniCon: A Unified System for Efficient Robot Learning Transfers](https://arxiv.org/abs/2601.14617)
*Yunfeng Lin,Li Xu,Yong Yu,Jiangmiao Pang,Weinan Zhang*

Main category: cs.RO

TL;DR: UniCon, a lightweight and efficient framework, standardizes the deployment of learning-based controllers across different robot platforms, reducing code redundancy and improving inference efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to tackle the challenges faced when deploying learning-based controllers on various robots, such as platform differences, inconsistent interfaces, and inefficient middleware. These issues can lead to increased development time, higher costs, and reduced performance of robotic systems.

Method: The authors introduce UniCon, a novel framework designed to standardize states, control flow, and instrumentation across diverse robot platforms. It employs a modular, data-oriented design that separates system states from control logic, allowing for more flexible and efficient deployment. A key feature of UniCon is its use of batched, vectorized data flow, which minimizes communication overhead and enhances inference latency compared to traditional middleware solutions like ROS.

Result: The implementation of UniCon has shown significant improvements in terms of reducing code redundancy during workflow transfers between different robot models and achieving better inference efficiency than ROS-based systems. The framework has been successfully deployed on over 12 distinct robot models from 7 different manufacturers and has been integrated into multiple ongoing research projects, demonstrating its practical value and adaptability in real-world applications.

Conclusion: In conclusion, UniCon provides a promising solution for the seamless and efficient deployment of learning-based controllers across heterogeneous robot platforms. By addressing critical challenges related to platform compatibility and middleware inefficiency, it enables researchers and developers to focus more on advancing control algorithms rather than dealing with integration issues, thus accelerating progress in robotics.

Abstract: Deploying learning-based controllers across heterogeneous robots is challenging due to platform differences, inconsistent interfaces, and inefficient middleware. To address these issues, we present UniCon, a lightweight framework that standardizes states, control flow, and instrumentation across platforms. It decomposes workflows into execution graphs with reusable components, separating system states from control logic to enable plug-and-play deployment across various robot morphologies. Unlike traditional middleware, it prioritizes efficiency through batched, vectorized data flow, minimizing communication overhead and improving inference latency. This modular, data-oriented approach enables seamless sim-to-real transfer with minimal re-engineering. We demonstrate that UniCon reduces code redundancy when transferring workflows and achieves higher inference efficiency compared to ROS-based systems. Deployed on over 12 robot models from 7 manufacturers, it has been successfully integrated into ongoing research projects, proving its effectiveness in real-world scenarios.

</details>


### [6] [Probing Prompt Design for Socially Compliant Robot Navigation with Vision Language Models](https://arxiv.org/abs/2601.14622)
*Ling Xiao,Toshihiko Yamasaki*

Main category: cs.RO

TL;DR: 研究发现，对于未微调的GPT-4o，在与人类竞争时表现最佳；而对于微调过的模型，与过去的自己竞争产生最好的结果。系统提示设计对社会合规导航任务中的性能有显著影响，适当的提示设计可以比直接微调更有效地提高行动准确性。


<details>
  <summary>Details</summary>
Motivation: 当前社交机器人导航中使用的语言模型在很大程度上忽略了为了实现社会合规行为而精心设计的提示。特别是小型视觉语言模型（VLMs）相比大型语言模型表现出较弱的决策能力，使得有效的提示设计变得尤为重要。

Method: 研究通过两个维度探索了提示设计：系统指导（行动导向、推理导向和感知推理提示）和动机框架设定，其中让模型与人类、其他AI系统或它们的过去版本进行竞争。实验基于两个社会合规导航数据集展开。

Result: 研究表明，对于未经微调的GPT-4o而言，与人类的竞争能够获得最佳表现，而与其他AI系统的竞争则表现最差。对于经过微调的模型来说，与其自身历史版本的竞争能产生最强的结果。此外，不当的系统提示设计会显著降低性能。虽然直接微调大幅提高了语义层面如感知、预测和推理等指标，但对行动准确性的提升有限。相比之下，本研究提出的系统提示能够在行动准确性上带来不成比例的更大改进。

Conclusion: 该研究强调了恰当提示设计的重要性，特别是在使用小型视觉语言模型的社会合规导航场景下。合适的激励框架结合有效的系统提示设计，能够显著改善机器人的导航性能，尤其是其决策水平。

Abstract: Language models are increasingly used for social robot navigation, yet existing benchmarks largely overlook principled prompt design for socially compliant behavior. This limitation is particularly relevant in practice, as many systems rely on small vision language models (VLMs) for efficiency. Compared to large language models, small VLMs exhibit weaker decision-making capabilities, making effective prompt design critical for accurate navigation. Inspired by cognitive theories of human learning and motivation, we study prompt design along two dimensions: system guidance (action-focused, reasoning-oriented, and perception-reasoning prompts) and motivational framing, where models compete against humans, other AI systems, or their past selves. Experiments on two socially compliant navigation datasets reveal three key findings. First, for non-finetuned GPT-4o, competition against humans achieves the best performance, while competition against other AI systems performs worst. For finetuned models, competition against the model's past self yields the strongest results, followed by competition against humans, with performance further influenced by coupling effects among prompt design, model choice, and dataset characteristics. Second, inappropriate system prompt design can significantly degrade performance, even compared to direct finetuning. Third, while direct finetuning substantially improves semantic-level metrics such as perception, prediction, and reasoning, it yields limited gains in action accuracy. In contrast, our system prompts produce a disproportionately larger improvement in action accuracy, indicating that the proposed prompt design primarily acts as a decision-level constraint rather than a representational enhancement.

</details>


### [7] [A Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control](https://arxiv.org/abs/2601.14628)
*Weiyu Guo,He Zhang,Pengteng Li,Tiefu Cai,Ziyang Chen,Yandong Guo,Xiao He,Yongkui Yang,Ying Sun,Hui Xiong*

Main category: cs.RO

TL;DR: 本文介绍了一种模仿生物神经系统结构的Neuromorphic Vision-Language-Action (NeuroVLA)框架，实现了在物理机器人上的首次部署，并展现了生物运动特性，如减少机械臂抖动、节省能源、表现出时间记忆能力和快速触发安全反射。


<details>
  <summary>Details</summary>
Motivation: 当前的机器人策略难以复制生物运动中的动态稳定性、反射响应性和时间记忆能力。而生物系统却能从稀疏的经验中快速学习技能。

Method: 提出了Neuromorphic Vision-Language-Action (NeuroVLA)框架，该框架模拟了大脑皮层、小脑和脊髓之间的生物神经系统结构。采用了系统级的生物启发设计：高级模型规划目标，自适应小脑模块使用高频传感器反馈稳定动作，生物启发的脊柱层执行极速动作生成。

Result: NeuroVLA在物理机器人上实现了最新的性能表现，无需额外数据或特殊指导即可出现生物运动特征：减少了机械臂抖动、显著节能（仅需0.4瓦）、显示时间记忆能力并能在20毫秒内触发安全反射。

Conclusion: NeuroVLA代表了在物理机器人上神经形态VLA的首次部署，展示了与生物运动相似的重要特性，为未来机器人技术的发展提供了新的方向。

Abstract: Recent advances in embodied intelligence have leveraged massive scaling of data and model parameters to master natural-language command following and multi-task control. In contrast, biological systems demonstrate an innate ability to acquire skills rapidly from sparse experience. Crucially, current robotic policies struggle to replicate the dynamic stability, reflexive responsiveness, and temporal memory inherent in biological motion. Here we present Neuromorphic Vision-Language-Action (NeuroVLA), a framework that mimics the structural organization of the bio-nervous system between the cortex, cerebellum, and spinal cord. We adopt a system-level bio-inspired design: a high-level model plans goals, an adaptive cerebellum module stabilizes motion using high-frequency sensors feedback, and a bio-inspired spinal layer executes lightning-fast actions generation. NeuroVLA represents the first deployment of a neuromorphic VLA on physical robotics, achieving state-of-the-art performance. We observe the emergence of biological motor characteristics without additional data or special guidance: it stops the shaking in robotic arms, saves significant energy(only 0.4w on Neuromorphic Processor), shows temporal memory ability and triggers safety reflexes in less than 20 milliseconds.

</details>


### [8] [Landing-Induced Viscoelastic Changes in an Anthropomimetic Foot Joint Structure are Modulated by Foot Structure and Posture](https://arxiv.org/abs/2601.14634)
*Satoru Hashimoto,Yinlai Jiang,Hiroshi Yokoi,Shunta Togo*

Main category: cs.RO

TL;DR: 本研究开发了一种模仿人类足部骨骼几何结构的拟人化足关节结构，通过实验发现这种多关节的拱形结构能够比简单的平足或刚性足提供更高的阻尼比，并且通过调整姿态可以调节冲击衰减与反弹之间的平衡。这些结果表明，通过解剖学上的骨骼复制以及着陆时的姿态调整，在工程上具有实现类似人类的表观粘弹性行为的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管尸体研究为理解人脚弓和足底筋膜的力学提供了重要见解，但立即在生物样本中重复探索着陆冲击后的姿势依赖性粘弹性反应是困难的，这导致骨骼结构对落地动力学的贡献尚未被完全理解。

Method: 研究人员开发了一种旨在复制人脚骨骼几何结构的拟人化足关节结构，并使用模拟着陆的垂直下落装置和粘弹性系统识别模型来研究骨骼结构和姿态如何调节着陆后明显的粘弹性响应。

Result: 结果表明，多关节的拟人化结构显示出比简化后的平足和刚性足更高的阻尼比。此外，踝关节背屈和趾伸展系统地改变了所确定的参数，在测试条件下降低了阻尼比。

Conclusion: 这些发现表明，一种拱形、多关节的骨骼结构可以在拟人化的机械足中增强冲击衰减，并且形态和被动姿态本身就能调节衰减与反弹之间的权衡。观察到的姿态依赖趋势与报告的人类着陆策略差异定性一致，暗示了骨骼结构可能部分解释了这种调节。

Abstract: Cadaveric studies have provided important insights into the mechanics of the human foot arch and plantar fascia. However, repeatedly probing posture-dependent viscoelastic responses immediately after landing impact is difficult in biological specimens, leaving the contribution of skeletal architecture to landing dynamics incompletely understood. In this study, we developed an anthropomimetic foot joint structure aimed at replicating the skeletal geometry of the human foot. Using a vertical drop apparatus that simulates landing and a viscoelastic system-identification model, we investigated how skeletal structure and posture modulate the apparent post-impact viscoelastic response. The results show that the multi-jointed anthropomimetic structure exhibited a higher damping ratio than simplified flat and rigid feet. Moreover, ankle dorsiflexion and toe extension systematically shifted the identified parameters, reducing the damping ratio under the tested conditions. Taken together, these findings indicate that an arch-like, multi-jointed skeletal architecture can enhance impact attenuation in an anthropomimetic mechanical foot, and that morphology and passive posture alone can tune the trade-off between attenuation and rebound. The observed posture-dependent trends are qualitatively consistent with reported differences in human landing strategies, suggesting that skeletal architecture may partly account for the modulation. Furthermore, these results highlight the engineering advantage of anatomically informed skeletal replication for achieving human-like apparent viscoelastic behavior through postural adjustment during landing.

</details>


### [9] [FARE: Fast-Slow Agentic Robotic Exploration](https://arxiv.org/abs/2601.14681)
*Shuhao Liao,Xuxin Lv,Jeric Lew,Shizhe Zhang,Jingsong Liang,Peizhuo Li,Yuhong Cao,Wenjun Wu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 本研究提出了一种名为FARE的自主探索框架，结合了大语言模型(LLM)进行全局推理和强化学习(RL)策略以实现局部决策，从而提高了机器人在未知环境中的探索效率。通过将语义推理与几何决策分离，FARE能够在复杂环境中展现出显著优于现有方法的表现，并且在大规模建筑环境的实际部署中得到了验证。


<details>
  <summary>Details</summary>
Motivation: 为了提高自主机器人在未知环境中的探索能力，本研究旨在通过整合高级别的语义推理以及快速的局部控制来优化这一过程。

Method: 提出了一个分层自主探索框架FARE，它利用大语言模型（LLM）执行全局层面的推理任务，同时依靠强化学习（RL）策略处理基于局部观察的即时决策。该系统采用快慢思维模式，其中慢思维部分负责解析环境描述并生成探索策略；而快思维部分则根据此策略及当前感知信息作出反应。此外，还引入了一种基于模块性的剪枝机制以减少冗余结构，提高推理效率。

Result: 实验结果表明，在具有挑战性的模拟环境中，FARE相比最先进基线方法在探索效率上取得了显著提升。并且，当应用于实际硬件并在复杂的大型建筑环境中测试时，FARE也展示了其有效性。

Conclusion: FARE框架通过有效结合全局语义推理与局部即时响应，在多种复杂场景下实现了高效自主探索，为未来相关领域研究提供了新的思路和技术支持。

Abstract: This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale $200m\times130m$ building environment.

</details>


### [10] [Stochastic Decision-Making Framework for Human-Robot Collaboration in Industrial Applications](https://arxiv.org/abs/2601.14809)
*Muhammad Adel Yusuf,Ali Nasir,Zeeshan Hameed Khan*

Main category: cs.RO

TL;DR: 本文提出了一种在人机协作环境中利用随机建模进行决策的方法，旨在通过概率模型和控制策略预测人类的行为和情绪，使协作机器人能够相应地调整其行为，从而提高协作的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 为了实现有效的人机协作，机器人需要基于人类因素（如动机水平和攻击性水平）进行推理。当前大多数研究集中在检测人类同事的意图上，而本文则更进一步，关注于理解并预测人类的情绪和动作，以便协作机器人能够更好地适应这些变化。

Method: 本文采用了随机建模的方法来构建决策过程。通过使用概率模型和控制策略，该方法试图预测人类伙伴的动作及情感状态，进而让协作机器人能够根据这些信息调整自身的行为模式。

Result: 论文介绍了理论框架、实施策略、模拟结果以及双边协作方法在协作机器人安全与效率方面的潜在应用。虽然具体数据未被提及，但模拟结果显示了所提方法的有效性。

Conclusion: 研究表明，采用随机建模技术可以帮助协作机器人更好地理解和适应人类合作伙伴的行为与情感，这对于提升人机协同工作的安全性和效率具有重要意义。

Abstract: Collaborative robots, or cobots, are increasingly integrated into various industrial and service settings to work efficiently and safely alongside humans. However, for effective human-robot collaboration, robots must reason based on human factors such as motivation level and aggression level. This paper proposes an approach for decision-making in human-robot collaborative (HRC) environments utilizing stochastic modeling. By leveraging probabilistic models and control strategies, the proposed method aims to anticipate human actions and emotions, enabling cobots to adapt their behavior accordingly. So far, most of the research has been done to detect the intentions of human co-workers. This paper discusses the theoretical framework, implementation strategies, simulation results, and potential applications of the bilateral collaboration approach for safety and efficiency in collaborative robotics.

</details>


### [11] [Moving Beyond Compliance in Soft-Robotic Catheters Through Modularity for Precision Therapies](https://arxiv.org/abs/2601.14837)
*B. Calmé,N. J. Greenidge,A. Metcalf,A. Bacchetti,G. Loza,D. Kpeglo,P. Lloyd,V. Pensabene,J. H. Chandler,P. Valdastri*

Main category: cs.RO

TL;DR: 本文介绍了一种直径为1.47毫米的模块化软体机器人导管，该导管集成了感应、驱动和治疗功能，并保持了安全内腔导航所需的柔顺性。它在多种体内环境中得到验证，特别适用于ERCP程序，展示了在活猪模型中自主部署到胰管并进行长达7.5厘米的内镜导航的能力。此外，还开发了一个闭环自主/共享控制系统以提高插管准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的软体机器人工具在临床应用中受到限制，因为它们缺乏足够的尖端功能化以及在组织界面的实时反馈。很少有传感器和治疗模块既小巧又足够坚固且适应性强，可以在内腔手术过程中测量并响应微妙的生理信号。

Method: 研究人员设计了一种集成传感、驱动和治疗功能的1.47毫米直径模块化软体机器人导管，能够支持多达四个独立控制的功能单元。通过结合学习模型、磁力驱动、机载形状感应和视觉标记跟踪，实现了一个闭环保守/共享控制系统来提高插管精度。

Result: 在活体猪模型实验中，该设备展示了半自主进入胰管并在其中进行长达7.5厘米内镜导航的能力，这是当前标准导管无法到达的区域。同时，提出的闭环系统进一步提高了插管准确性。

Conclusion: 这些结果建立了一个可扩展的多功能软体机器人导管平台，为复杂的内腔干预提供了一个新的范式，具有减少辐射暴露、缩短培训时间及加速软体机器人技术临床转化的潜力。

Abstract: Soft robotic instruments could navigate delicate, tortuous anatomy more safely than rigid tools, but clinical adoption is limited by insufficient tip functionalization and real-time feedback at the tissue interface. Few sensing and therapeutic modules are compact, robust, and adaptable enough to measure, and respond to, subtle physiological cues during intraluminal procedures. We present a 1.47 mm diameter modular soft robotic catheter that integrates sensing, actuation, and therapy while retaining the compliance needed for safe endoluminal navigation. Validated across multiple in vivo settings, we emphasize its utility in endoscopic retrograde cholangiopancreatography (ERCP), a highly technical procedure and a key access route to the pancreas, an organ that is fragile, difficult to instrument, and central to diseases such as pancreatic cancer. Our architecture supports up to four independently controlled functional units, allowing customizable combinations of anchoring, manipulation, sensing, and targeted drug delivery. In a live porcine model, we demonstrate semi-autonomous deployment into the pancreatic duct and 7.5 cm of endoscopic navigation within it, a region currently inaccessible with standard catheters. A closed-loop autonomous/shared-control system that combines a learned model, magnetic actuation, onboard shape sensing, and visual marker tracking further improves cannulation accuracy. Together, these results establish a scalable platform for multifunctional soft robotic catheters and a new paradigm for complex endoluminal interventions, with potential to reduce radiation exposure, shorten training, and accelerate clinical translation of soft robotic technologies.

</details>


### [12] [On-the-fly hand-eye calibration for the da Vinci surgical robot](https://arxiv.org/abs/2601.14871)
*Zejian Cui,Ferdinando Rodriguez y Baena*

Main category: cs.RO

TL;DR: 本文提出了一种新的校准框架，用于在机器人辅助微创手术中实现实时准确的工具定位。通过两个相互关联的算法：特征关联块和手眼校准块，该框架能够在不同手术场景下提供鲁棒的关键点对应关系，并且不需要预训练。实验结果表明，在不同的光照条件和关键点测量精度下，该方法显著减少了工具定位误差，同时比其他现有方法更高效。


<details>
  <summary>Details</summary>
Motivation: 在机器人辅助微创手术中，精确的工具定位对于确保患者安全和任务成功执行至关重要。然而，对于电缆驱动的机器人（如达芬奇机器人）来说，由于编码器读数错误导致姿态估计误差，这仍然是一个挑战。

Method: 研究者们提出了一种新的校准框架，能够实时计算手眼变换矩阵以产生准确的工具定位结果。该框架包含两个相互关联的算法：特征关联模块和手眼校准模块。前者无需预先训练即可为单目图像上检测到的关键点提供稳健的对应关系；后者则通过采用一系列过滤方法来适应各种外科手术场景。

Result: 通过对公开可用视频数据集进行广泛测试，这些数据集涵盖了多种外科器械在体外和体内条件下执行任务的情况，结果显示所提出的校准框架下的工具定位误差显著减少。与其他最先进方法相比，该框架不仅具有可比较的准确性，而且更加时间高效。

Conclusion: 新提出的校准框架能有效提高机器人辅助微创手术中的工具定位精度，尤其适合于面对变化的照明条件及不同水平的关键点测量精度时。

Abstract: In Robot-Assisted Minimally Invasive Surgery (RMIS), accurate tool localization is crucial to ensure patient safety and successful task execution. However, this remains challenging for cable-driven robots, such as the da Vinci robot, because erroneous encoder readings lead to pose estimation errors. In this study, we propose a calibration framework to produce accurate tool localization results through computing the hand-eye transformation matrix on-the-fly. The framework consists of two interrelated algorithms: the feature association block and the hand-eye calibration block, which provide robust correspondences for key points detected on monocular images without pre-training, and offer the versatility to accommodate various surgical scenarios by adopting an array of filter approaches, respectively. To validate its efficacy, we test the framework extensively on publicly available video datasets that feature multiple surgical instruments conducting tasks in both in vitro and ex vivo scenarios, under varying illumination conditions and with different levels of key point measurement accuracy. The results show a significant reduction in tool localization errors under the proposed calibration framework, with accuracies comparable to other state-of-the-art methods while being more time-efficient.

</details>


### [13] [HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation](https://arxiv.org/abs/2601.14874)
*Yara Mahmoud,Yasheerah Yaqoot,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本文介绍了一种视觉-语言驱动的检索框架HumanoidVLM，使机器人能够根据第一人称视角图像自适应选择适合任务的阻抗参数和夹爪配置。通过与语义任务推理模型及基于FAISS的检索模块结合，该系统从定制数据库中检索经实验验证的刚度-阻尼对和对象特定抓握角度，并通过任务空间阻抗控制器执行，实现顺从性操作。


<details>
  <summary>Details</summary>
Motivation: 类人机器人在面对不同物体和任务时需要调整其接触行为，但现有控制器通常依赖于固定的手动调校阻抗增益和夹爪设置。为了解决这一局限性，提出了一个新的框架来增强机器人的适应性和灵活性。

Method: 提出的方法名为HumanoidVLM，它利用视觉-语言模型进行语义任务推断，并结合基于FAISS的检索增强生成(RAG)模块，从两个定制数据库中检索出实验验证过的刚度-阻尼配对以及针对特定对象的最佳抓握角度。随后，这些参数通过任务空间阻抗控制器被应用到实际操作中。

Result: 在14个视觉场景上评估了HumanoidVLM，达到了93%的检索准确率。真实世界实验表明，交互动态稳定，z轴跟踪误差通常保持在1-3.5厘米范围内，虚拟力与任务相关的阻抗设置一致。

Conclusion: 结果证明了将语义感知与基于检索的控制相结合的方法，在朝着更加灵活、可解释的人形机器人操控方向上是可行的。

Abstract: Humanoid robots must adapt their contact behavior to diverse objects and tasks, yet most controllers rely on fixed, hand-tuned impedance gains and gripper settings. This paper introduces HumanoidVLM, a vision-language driven retrieval framework that enables the Unitree G1 humanoid to select task-appropriate Cartesian impedance parameters and gripper configurations directly from an egocentric RGB image. The system couples a vision-language model for semantic task inference with a FAISS-based Retrieval-Augmented Generation (RAG) module that retrieves experimentally validated stiffness-damping pairs and object-specific grasp angles from two custom databases, and executes them through a task-space impedance controller for compliant manipulation. We evaluate HumanoidVLM on 14 visual scenarios and achieve a retrieval accuracy of 93%. Real-world experiments show stable interaction dynamics, with z-axis tracking errors typically within 1-3.5 cm and virtual forces consistent with task-dependent impedance settings. These results demonstrate the feasibility of linking semantic perception with retrieval-based control as an interpretable path toward adaptive humanoid manipulation.

</details>


### [14] [Vision-Language Models on the Edge for Real-Time Robotic Perception](https://arxiv.org/abs/2601.14921)
*Sarat Ahmad,Maryam Hafeez,Syed Ali Raza Zaidi*

Main category: cs.RO

TL;DR: 本文研究了在Open RAN和多接入边缘计算(MEC)基础设施上部署视觉-语言模型(VLMs)，以提高机器人感知与交互的实时性和隐私保护。实验结果表明，相比于云端部署，边缘部署能够保持接近云端的准确性同时减少5%的端到端延迟；此外，对于资源受限环境优化的小型模型Qwen2-VL-2B-Instruct，虽然其响应时间缩短超过一半，但牺牲了一定的准确性。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）为机器人感知与交互提供了多模态推理能力，但在实际应用中受到延迟、有限的板载资源以及云卸载带来的隐私风险等因素限制。通过将计算推向数据源附近，6G技术特别是开放无线接入网(ORAN)与多接入边缘计算(MEC)提供了解决上述问题的可能性。

Method: 采用Unitree G1人形机器人作为物理测试平台，设计了一个基于WebRTC的数据流传输管道，将多模态数据发送至边缘节点，并对比分析了位于边缘服务器上的LLaMA-3.2-11B-Vision-Instruct模型与云端版本之间的性能差异。同时，还评估了针对资源受限环境优化过的紧凑型模型Qwen2-VL-2B-Instruct的表现。

Result: 边缘部署方案能够在保证接近云端准确性的前提下，将端到端延迟降低5%。而对于小型化模型Qwen2-VL-2B-Instruct来说，它实现了亚秒级响应速度，使得延迟减少了超过一半，不过这是以牺牲部分准确性为代价换来的。

Conclusion: 利用ORAN/MEC架构部署VLMs可以有效缓解现有挑战，包括延迟改善及隐私保护增强等。尽管存在准确性与响应时间之间的权衡，但对于特定应用场景而言，选择合适的模型规模和部署方式是可行且有益的。

Abstract: Vision-Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5\%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.

</details>


### [15] [TIDAL: Temporally Interleaved Diffusion and Action Loop for High-Frequency VLA Control](https://arxiv.org/abs/2601.14945)
*Yuteng Sun,Haoran Wang,Ruofei Bai,Zhengguo Li,Jun Li,Meng Yee,Chuah,Wei Yun Yau*

Main category: cs.RO

TL;DR: 提出了TIDAL，一种用于视觉-语言-动作模型的双频架构，通过将语义推理与高频动作执行解耦来解决大延迟问题。在动态拦截任务中比开环基线性能提高2倍，并且反馈频率提高了4倍。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉-语言-动作（VLA）模型虽然提供了语义泛化能力，但高推理延迟限制了它们的应用场景，尤其在动态环境中会导致执行失败。

Method: TIDAL采用了双频架构，低频宏意图循环缓存语义嵌入，而高频微控制循环则结合单步流集成与执行。此外，还引入了一种时间不对齐的训练策略以及一个差分运动预测器以处理延迟移位和速度不敏感问题。

Result: 实验表明，在动态拦截任务中，相比开环基线方法，TIDAL表现出约2倍的性能提升，并且反馈频率提高了4倍。即使是在非暂停推理协议下，TIDAL也能保持稳健表现。

Conclusion: TIDAL框架通过重新分配计算预算有效地解决了视觉-语言-动作模型中的延迟问题，提升了动态环境下的执行效率与准确性。

Abstract: Large-scale Vision-Language-Action (VLA) models offer semantic generalization but suffer from high inference latency, limiting them to low-frequency batch-and-execute paradigm. This frequency mismatch creates an execution blind spot, causing failures in dynamic environments where targets move during the open-loop execution window. We propose TIDAL (Temporally Interleaved Diffusion and Action Loop), a hierarchical framework that decouples semantic reasoning from high-frequency actuation. TIDAL operates as a backbone-agnostic module for diffusion-based VLAs, using a dual-frequency architecture to redistribute the computational budget. Specifically, a low-frequency macro-intent loop caches semantic embeddings, while a high-frequency micro-control loop interleaves single-step flow integration with execution. This design enables approximately 9 Hz control updates on edge hardware (vs. approximately 2.4 Hz baselines) without increasing marginal overhead. To handle the resulting latency shift, we introduce a temporally misaligned training strategy where the policy learns predictive compensation using stale semantic intent alongside real-time proprioception. Additionally, we address the insensitivity of static vision encoders to velocity by incorporating a differential motion predictor. TIDAL is architectural, making it orthogonal to system-level optimizations. Experiments show a 2x performance gain over open-loop baselines in dynamic interception tasks. Despite a marginal regression in static success rates, our approach yields a 4x increase in feedback frequency and extends the effective horizon of semantic embeddings beyond the native action chunk size. Under non-paused inference protocols, TIDAL remains robust where standard baselines fail due to latency.

</details>


### [16] [HumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV](https://arxiv.org/abs/2601.14973)
*Faryal Batool,Iana Zhura,Valerii Serpiva,Roohan Ahmed Khan,Ivan Valuev,Issatay Tokmurziyev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本研究提出了一种名为HumanDiffusion的轻量级图像条件扩散规划器，它能直接从RGB图像生成人感知的导航轨迹，结合了基于YOLO-11的人体检测与扩散驱动的轨迹生成技术。该系统在模拟和实际室内灾难场景中进行了评估，显示了在紧急情况下实现人类-机器人协作的可能性。


<details>
  <summary>Details</summary>
Motivation: 可靠的紧急情况下的人员-机器人协作需要能够检测人类、推断导航目标并在动态环境中安全操作的自主系统。

Method: 通过结合基于YOLO-11的人体检测与扩散驱动的轨迹生成方法，开发出一种轻量级图像条件扩散规划器（HumanDiffusion），能够直接从RGB图像生成人感知的导航轨迹。

Result: 在300个样本测试集上，模型达到了像素空间轨迹重建均方误差为0.02的成绩；真实世界实验表明，在部分遮挡条件下执行事故响应和搜索定位任务的整体任务成功率达到了80%。

Conclusion: 基于人的条件扩散规划为人机交互无人机导航提供了一个实用且强大的解决方案，尤其是在时间紧迫的援助设置下。

Abstract: Reliable human--robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents HumanDiffusion, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11--based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans. We evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of 80% across accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.

</details>


### [17] [Graph-Based Adaptive Planning for Coordinated Dual-Arm Robotic Disassembly of Electronic Devices (eGRAP)](https://arxiv.org/abs/2601.14998)
*Adip Ranjan Das,Maria Koskinopoulou*

Main category: cs.RO

TL;DR: 提出了一种基于图的自适应规划方法eGRAP，通过整合视觉、动态规划和双臂执行来实现电子设备的自主拆解。实验表明该方法能够实时适应性地协调双臂任务，成功且高效地完成了3.5英寸硬盘驱动器的完全拆解。


<details>
  <summary>Details</summary>
Motivation: 鉴于电子废物的增长速度远超其回收率的问题，研究旨在开发一种能够提高电子废弃物处理效率的新方法。

Method: 引入了电子设备图基自适应规划（eGRAP）系统，该系统结合了视觉识别、动态规划以及双臂机器人操作以完成自动拆卸任务。其中一个机械臂配备螺丝刀及眼在手深度相机用于拆卸工作，另一臂则负责持握或处理组件。

Result: 实验证明，对于3.5英寸硬盘的拆解，eGRAP系统能够在线更新其图结构与计划，并保持较高的成功率和较短的操作周期时间。

Conclusion: eGRAP展示了其实时调整双臂任务的能力，为解决快速增长的电子废物问题提供了一个有效的解决方案。

Abstract: E-waste is growing rapidly while recycling rates remain low. We propose an electronic-device Graph-based Adaptive Planning (eGRAP) that integrates vision, dynamic planning, and dual-arm execution for autonomous disassembly. A camera-equipped arm identifies parts and estimates their poses, and a directed graph encodes which parts must be removed first. A scheduler uses topological ordering of this graph to select valid next steps and assign them to two robot arms, allowing independent tasks to run in parallel. One arm carries a screwdriver (with an eye-in-hand depth camera) and the other holds or handles components. We demonstrate eGRAP on 3.5in hard drives: as parts are unscrewed and removed, the system updates its graph and plan online. Experiments show consistent full disassembly of each HDD, with high success rates and efficient cycle times, illustrating the method's ability to adaptively coordinate dual-arm tasks in real time.

</details>


### [18] [DWPP: Dynamic Window Pure Pursuit Considering Velocity and Acceleration Constraints](https://arxiv.org/abs/2601.15006)
*Fumiya Ohnishi,Masaki Takahashi*

Main category: cs.RO

TL;DR: 本文提出了一种名为动态窗口纯追踪（DWPP）的方法，该方法在速度空间中重新定义了指令速度的计算过程，以明确考虑速度和加速度的约束。实验表明，DWPP相比传统纯追踪方法，在路径跟踪精度上有所提升，并且避免了违反约束的命令。


<details>
  <summary>Details</summary>
Motivation: 传统的纯追踪及其变体方法由于简单性和计算效率而被广泛用于移动机器人的路径跟踪。然而，许多传统方法没有明确考虑到速度和加速度的限制，导致指令速度与实际速度之间的差异，从而引起超调和跟踪性能下降的问题。

Method: 提出了动态窗口纯追踪（DWPP），通过在速度空间（v-ω平面）内重构指令速度计算流程，直接将速度与加速度约束纳入考量。具体来说，DWPP选择动态窗口内最接近直线ω=κv的点作为指令速度。

Result: 实验结果证明，DWPP能够避免产生违反约束条件的指令，并且相对于传统的纯追踪方法而言，实现了更优的路径跟踪准确性。

Conclusion: 通过引入动态窗口的概念来优化纯追踪算法，DWPP成功解决了因未充分考虑速度和加速度约束而导致的路径跟踪问题，提高了移动机器人路径跟踪的整体性能。

Abstract: Pure pursuit and its variants are widely used for mobile robot path tracking owing to their simplicity and computational efficiency. However, many conventional approaches do not explicitly account for velocity and acceleration constraints, resulting in discrepancies between commanded and actual velocities that result in overshoot and degraded tracking performance. To address this problem, this paper proposes dynamic window pure pursuit (DWPP), which fundamentally reformulates the command velocity computation process to explicitly incorporate velocity and acceleration constraints. Specifically, DWPP formulates command velocity computation in the velocity space (the $v$-$ω$ plane) and selects the command velocity as the point within the dynamic window that is closest to the line $ω= κv$. Experimental results demonstrate that DWPP avoids constraint-violating commands and achieves superior path-tracking accuracy compared with conventional pure pursuit methods. The proposed method has been integrated into the official Nav2 repository and is publicly available (https://github.com/ros-navigation/navigation2).

</details>


### [19] [Risk Estimation for Automated Driving](https://arxiv.org/abs/2601.15018)
*Leon Tolksdorf,Arturo Tejada,Jonas Bauernfeind,Christian Birkner,Nathan van de Wouw*

Main category: cs.RO

TL;DR: 本文提出了一种结合碰撞概率估计和碰撞严重性概念的一般方法，用于自动驾驶中准确的风险评估。该方法能够为不同类型的碰撞分配独立的严重性函数，并且计算效率高，适用于实时运动规划应用。


<details>
  <summary>Details</summary>
Motivation: 现有的风险计算方法要么依赖于经验建模，要么进行严重的近似处理，因此缺乏通用性和准确性。为了支持自动驾驶中的运动规划技术和安全评估，需要一种更加准确和通用的风险评估方法。

Method: 通过整合碰撞概率估计领域的最新进展与碰撞严重性的概念，开发了一种新的风险评估方法。该方法可以针对不同的碰撞情况（如正面或侧面碰撞）指定特定的严重性函数，并证明了其实时应用中的计算效率。

Result: 所提方法能够实现对不同类型碰撞事件的精确风险估计，并且在计算上足够高效，可应用于实时场景。此外，还提供了基于高斯不确定性的示例代码实现。

Conclusion: 本文介绍的方法为自动驾驶车辆提供了一种更准确、更通用的风险评估手段，有助于提高自动驾驶的安全性及其实时运动规划能力。

Abstract: Safety is a central requirement for automated vehicles. As such, the assessment of risk in automated driving is key in supporting both motion planning technologies and safety evaluation. In automated driving, risk is characterized by two aspects. The first aspect is the uncertainty on the state estimates of other road participants by an automated vehicle. The second aspect is the severity of a collision event with said traffic participants. Here, the uncertainty aspect typically causes the risk to be non-zero for near-collision events. This makes risk particularly useful for automated vehicle motion planning. Namely, constraining or minimizing risk naturally navigates the automated vehicle around traffic participants while keeping a safety distance based on the level of uncertainty and the potential severity of the impending collision. Existing approaches to calculate the risk either resort to empirical modeling or severe approximations, and, hence, lack generalizability and accuracy. In this paper, we combine recent advances in collision probability estimation with the concept of collision severity to develop a general method for accurate risk estimation. The proposed method allows us to assign individual severity functions for different collision constellations, such as, e.g., frontal or side collisions. Furthermore, we show that the proposed approach is computationally efficient, which is beneficial, e.g., in real-time motion planning applications. The programming code for an exemplary implementation of Gaussian uncertainties is also provided.

</details>


### [20] [CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes](https://arxiv.org/abs/2601.15039)
*Jiyao Zhang,Zhiyuan Ma,Tianhao Wu,Zeyuan Chen,Hao Dong*

Main category: cs.RO

TL;DR: 提出了一种名为CADGrasp的两阶段算法，用于处理杂乱环境中的灵巧抓取问题。通过单视角点云输入，该算法首先预测一个稀疏IBS作为优化目标，然后基于此进行优化以生成高质量的灵巧抓取姿态。实验证明了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在杂乱环境中实现灵巧抓取面临许多挑战，如手部自由度高、遮挡以及由不同物体形状和复杂布局引起的潜在碰撞。为解决这些问题，提出了新的算法。

Method: CADGrasp算法分为两个阶段：第一阶段，预测一种与场景解耦且考虑接触和碰撞的表示——稀疏IBS，并引入了体素级条件引导的占用扩散模型及力闭合得分过滤来改进高维表示的预测；第二阶段，基于稀疏IBS开发了几种能量函数和排名策略来进行优化，从而生成高质量的灵巧抓取姿态。

Result: 通过在模拟环境和真实世界设置中进行广泛的实验，证明了所提方法能够有效减少碰撞同时保持较高的抓取成功率。

Conclusion: 提出的CADGrasp算法能够在多样化物体和复杂场景下实现高效稳定的灵巧抓取。

Abstract: Dexterous grasping in cluttered environments presents substantial challenges due to the high degrees of freedom of dexterous hands, occlusion, and potential collisions arising from diverse object geometries and complex layouts. To address these challenges, we propose CADGrasp, a two-stage algorithm for general dexterous grasping using single-view point cloud inputs. In the first stage, we predict sparse IBS, a scene-decoupled, contact- and collision-aware representation, as the optimization target. Sparse IBS compactly encodes the geometric and contact relationships between the dexterous hand and the scene, enabling stable and collision-free dexterous grasp pose optimization. To enhance the prediction of this high-dimensional representation, we introduce an occupancy-diffusion model with voxel-level conditional guidance and force closure score filtering. In the second stage, we develop several energy functions and ranking strategies for optimization based on sparse IBS to generate high-quality dexterous grasp poses. Extensive experiments in both simulated and real-world settings validate the effectiveness of our approach, demonstrating its capability to mitigate collisions while maintaining a high grasp success rate across diverse objects and complex scenes.

</details>


### [21] [Systematic Evaluation of Hip Exoskeleton Assistance Parameters for Enhancing Gait Stability During Ground Slip Perturbations](https://arxiv.org/abs/2601.15056)
*Maria T. Tagliaferri,Inseung Kang*

Main category: cs.RO

TL;DR: 研究了双侧髋部外骨骼在滑动干扰期间对稳定性的影响，发现优化外骨骼辅助以提高能量效率不足以改善步态干扰时的反应稳定性。为提高老年人行走稳定性并减少跌倒风险，外骨骼控制应优先考虑时间辅助参数，并包括用户特定的个性化设置。


<details>
  <summary>Details</summary>
Motivation: 鉴于跌倒是导致老年人受伤住院和死亡的主要原因，本研究旨在通过调整外骨骼提供的扭矩大小与时长来探索其如何帮助维持步行过程中的稳定性，特别是针对滑动干扰情况下的整体身体角动量（WBAM）变化。

Method: 研究者系统地改变了双边髋关节外骨骼提供给八名健康成年人的扭矩幅度与持续时间，在滑动干扰下测量了他们的全身角动量(WBAM)作为稳定性的指标。

Result: 研究显示，外骨骼辅助的效果取决于扭矩大小与作用时间之间的相互作用；相较于不穿戴设备的情况，适当调整这些参数可以显著降低WBAM的变化范围达25.7%。此外，不同个体间对于最能减少WBAM波动的最佳参数组合存在较大差异。

Conclusion: 单独优化外骨骼以实现能源效率并不足以增强步态受到干扰时的响应稳定性。为了更好地支持老年人群，未来的外骨骼控制系统设计应该更加注重时间相关参数的选择，并结合每位用户的个人特点进行定制化调整。

Abstract: Falls are the leading cause of injury related hospitalization and mortality among older adults. Consequently, mitigating age-related declines in gait stability and reducing fall risk during walking is a critical goal for assistive devices. Lower-limb exoskeletons have the potential to support users in maintaining stability during walking. However, most exoskeleton controllers are optimized to reduce the energetic cost of walking rather than to improve stability. While some studies report stability benefits with assistance, the effects of specific parameters, such as assistance magnitude and duration, remain unexplored. To address this gap, we systematically modulated the magnitude and duration of torque provided by a bilateral hip exoskeleton during slip perturbations in eight healthy adults, quantifying stability using whole-body angular momentum (WBAM). WBAM responses were governed by a significant interaction between assistance magnitude and duration, with duration determining whether exoskeleton assistance was stabilizing or destabilizing relative to not wearing the exoskeleton device. Compared to an existing energy-optimized controller, experimentally identified stability-optimal parameters reduced WBAM range by 25.7% on average. Notably, substantial inter-subject variability was observed in the parameter combinations that minimized WBAM during perturbations. We found that optimizing exoskeleton assistance for energetic outcomes alone is insufficient for improving reactive stability during gait perturbations. Stability-focused exoskeleton control should prioritize temporal assistance parameters and include user-specific personalization. This study represents an important step toward personalized, stability-focused exoskeleton control, with direct implications for improving stability and reducing fall risk in older adults.

</details>


### [22] [Influence of Operator Expertise on Robot Supervision and Intervention](https://arxiv.org/abs/2601.15069)
*Yanran Jiang,Pavan Sikka,Leimin Tian,Dana Kuliic,Cecile Paris*

Main category: cs.RO

TL;DR: 本研究探索了不同专业水平的用户在监督远程机器人时如何感知信息并做出干预决策，通过用户研究发现了新手、中级和专家用户在干预时机和决策策略上的差异。


<details>
  <summary>Details</summary>
Motivation: 随着机器人自主性的提高，监督这些机器人的用户的专业水平也各不相同。为了理解这种多样性对人机团队表现的影响，需要研究不同经验水平的操作者在监督任务中如何处理信息以及决定何时干预。

Method: 进行了一个包含27名参与者的研究项目，让参与者在一个模拟器里监督一台自动探索四个未知隧道环境的机器人，并要求他们在认为机器人遇到困难时提供航点进行干预。通过分析互动数据及问卷调查回复来识别不同级别用户的行为模式。

Result: 研究发现，在干预时间的选择与决策策略上，新手、中级用户和专家之间存在显著差异。

Conclusion: 了解不同经验水平操作者之间的这些差异对于改进人机交互界面设计、培训材料以及未来机器人系统的开发具有重要意义。

Abstract: With increasing levels of robot autonomy, robots are increasingly being supervised by users with varying levels of robotics expertise. As the diversity of the user population increases, it is important to understand how users with different expertise levels approach the supervision task and how this impacts performance of the human-robot team. This exploratory study investigates how operators with varying expertise levels perceive information and make intervention decisions when supervising a remote robot. We conducted a user study (N=27) where participants supervised a robot autonomously exploring four unknown tunnel environments in a simulator, and provided waypoints to intervene when they believed the robot had encountered difficulties. By analyzing the interaction data and questionnaire responses, we identify differing patterns in intervention timing and decision-making strategies across novice, intermediate, and expert users.

</details>


### [23] [V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks](https://arxiv.org/abs/2601.15164)
*Yaru Liu,Ao-bo Wang,Nanyang Ye*

Main category: cs.RO

TL;DR: V-CAGE, a framework for generating high-quality, semantically aligned manipulation datasets, improves the physical and semantic fidelity of synthetic scenes, thereby enhancing the performance of downstream policies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges in learning long-horizon embodied behaviors from synthetic data, such as physically implausible scenes, lack of task semantic satisfaction, and the need to translate high-level instructions into executable actions.

Method: V-CAGE uses a context-aware instantiation mechanism for scene synthesis, ensuring geometric consistency, and a hierarchical instruction decomposition module that breaks down high-level goals into action primitives. It also incorporates a VLM-based verification loop to ensure semantic correctness by filtering out silent failures.

Result: Experiments show that V-CAGE generates datasets with higher physical and semantic accuracy, leading to improved success rates and better generalization in downstream policies compared to non-verified baseline methods.

Conclusion: V-CAGE effectively addresses the limitations of current synthetic data generation methods for embodied AI, providing a robust solution for creating semantically rich and physically plausible datasets.

Abstract: Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently "succeed" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., "get ready for work") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out "silent failures" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.

</details>
