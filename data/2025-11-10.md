<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [ScheduleStream: Temporal Planning with Samplers for GPU-Accelerated Multi-Arm Task and Motion Planning & Scheduling](https://arxiv.org/abs/2511.04758)
*Caelan Garrett,Fabio Ramos*

Main category: cs.RO

TL;DR: 本文提出了ScheduleStream，这是一个用于规划和调度采样操作的一般框架，它能够处理混合持续性动作，从而支持并行臂运动。通过使用GPU加速，ScheduleStream在任务和运动规划与调度（TAMPAS）中表现出色，并且在模拟实验中比其他方法产生了更高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 双臂和类人机器人因其类似人类的能力而受到青睐，能够利用多条手臂高效完成任务。然而，同时控制多条手臂在计算上是具有挑战性的，因为这会增加混合离散-连续动作空间的复杂度。尽管任务和运动规划（TAMP）算法能够在混合空间中有效规划，但它们通常只产生单臂依次移动的计划，而不是允许多个手臂同时移动的调度方案。

Method: 为了扩展TAMP以生成调度方案，研究者们提出了ScheduleStream，这是一个用于规划和调度采样操作的一般用途框架。ScheduleStream使用混合持续性动作来建模时间动态，这些动作可以异步启动并且根据其参数持续一定的时间长度。研究者还提出了不依赖于具体领域的算法，用来解决ScheduleStream问题。此外，在TAMPAS应用中，他们采用了GPU加速采样器以加快规划过程。

Result: 通过比较不同版本的ScheduleStream算法与其他几种简化版本在仿真中的表现，发现ScheduleStream算法产生的解决方案更加高效。

Conclusion: ScheduleStream为双臂机器人的任务和运动规划及调度提供了一种新方法，该方法能够支持并行臂运动，并且通过GPU加速提高了规划效率。

Abstract: Bimanual and humanoid robots are appealing because of their human-like
ability to leverage multiple arms to efficiently complete tasks. However,
controlling multiple arms at once is computationally challenging due to the
growth in the hybrid discrete-continuous action space. Task and Motion Planning
(TAMP) algorithms can efficiently plan in hybrid spaces but generally produce
plans, where only one arm is moving at a time, rather than schedules that allow
for parallel arm motion. In order to extend TAMP to produce schedules, we
present ScheduleStream, the first general-purpose framework for planning &
scheduling with sampling operations. ScheduleStream models temporal dynamics
using hybrid durative actions, which can be started asynchronously and persist
for a duration that's a function of their parameters. We propose
domain-independent algorithms that solve ScheduleStream problems without any
application-specific mechanisms. We apply ScheduleStream to Task and Motion
Planning & Scheduling (TAMPAS), where we use GPU acceleration within samplers
to expedite planning. We compare ScheduleStream algorithms to several ablations
in simulation and find that they produce more efficient solutions. We
demonstrate ScheduleStream on several real-world bimanual robot tasks at
https://schedulestream.github.io.

</details>


### [2] [ReGen: Generative Robot Simulation via Inverse Design](https://arxiv.org/abs/2511.04769)
*Phat Nguyen,Tsun-Hsuan Wang,Zhang-Wei Hong,Erfan Aasi,Andrew Silva,Guy Rosman,Sertac Karaman,Daniela Rus*

Main category: cs.RO

TL;DR: ReGen, a generative simulation framework, uses large language models to automatically design simulations based on robot behavior and textual descriptions, enhancing the diversity and complexity of simulated environments for better validation of robot policies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce the labor-intensive process of constructing simulations for robot learning and policy validation by automating the simulation design with an inverse design approach.

Method: ReGen employs large language models to create scenarios from a directed graph that represents cause-and-effect relationships. The graph is then converted into a symbolic program to set up and run a robot simulation environment. The method supports various features such as augmenting simulations, generating counterfactual scenarios, reasoning about agent cognition, and handling different sensing modalities.

Result: ReGen successfully generates more diverse and complex simulated environments in autonomous driving and robot manipulation tasks, with high success rates. It also enables the controllable generation of corner cases, which are useful for validating robot policies and augmenting data or simulations.

Conclusion: ReGen advances scalable robot learning by improving the generalization and robustness of robot policies through the creation of rich, varied, and controllable simulated environments.

Abstract: Simulation plays a key role in scaling robot learning and validating
policies, but constructing simulations remains a labor-intensive process. This
paper introduces ReGen, a generative simulation framework that automates
simulation design via inverse design. Given a robot's behavior -- such as a
motion trajectory or an objective function -- and its textual description,
ReGen infers plausible scenarios and environments that could have caused the
behavior. ReGen leverages large language models to synthesize scenarios by
expanding a directed graph that encodes cause-and-effect relationships,
relevant entities, and their properties. This structured graph is then
translated into a symbolic program, which configures and executes a robot
simulation environment. Our framework supports (i) augmenting simulations based
on ego-agent behaviors, (ii) controllable, counterfactual scenario generation,
(iii) reasoning about agent cognition and mental states, and (iv) reasoning
with distinct sensing modalities, such as braking due to faulty GPS signals. We
demonstrate ReGen in autonomous driving and robot manipulation tasks,
generating more diverse, complex simulated environments compared to existing
simulations with high success rates, and enabling controllable generation for
corner cases. This approach enhances the validation of robot policies and
supports data or simulation augmentation, advancing scalable robot learning for
improved generalization and robustness. We provide code and example videos at:
https://regen-sim.github.io/

</details>


### [3] [Unified Multimodal Diffusion Forcing for Forceful Manipulation](https://arxiv.org/abs/2511.04812)
*Zixuan Huang,Huaidian Hou,Dmitry Berenson*

Main category: cs.RO

TL;DR: 本文提出了一种名为多模态扩散强迫（MDF）的统一框架，用于从包含多种模式的机器人轨迹中学习，而不仅仅是生成动作。该方法通过随机部分遮罩和训练扩散模型来重建轨迹，从而学习时间与跨模态依赖性，并在模拟和现实环境中接触丰富、有力的操作任务上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 传统的模仿学习方法通常直接从观察到的数据（如RGB图像）学习到动作的映射，但这些方法往往忽略了不同模态（例如，感官输入、动作和奖励）之间复杂的相互作用，这对建模机器人行为及理解任务结果至关重要。

Method: 作者提出了一个多模态扩散强迫（Multimodal Diffusion Forcing, MDF）框架，该框架通过对轨迹进行随机部分遮蔽并训练一个扩散模型以重建整个轨迹的方式，来学习时间和跨模态之间的依赖关系。

Result: 实验结果显示，在涉及大量接触且需要施加力的操作任务上，无论是模拟环境还是真实世界条件下，MDF不仅提供了多样化的功能，而且表现出了出色的性能和对于噪声观测条件下的鲁棒性。

Conclusion: MDF作为一种新颖的学习框架，能够有效利用多模态数据间的复杂联系，支持更全面地理解与执行机器人任务。

Abstract: Given a dataset of expert trajectories, standard imitation learning
approaches typically learn a direct mapping from observations (e.g., RGB
images) to actions. However, such methods often overlook the rich interplay
between different modalities, i.e., sensory inputs, actions, and rewards, which
is crucial for modeling robot behavior and understanding task outcomes. In this
work, we propose Multimodal Diffusion Forcing, a unified framework for learning
from multimodal robot trajectories that extends beyond action generation.
Rather than modeling a fixed distribution, MDF applies random partial masking
and trains a diffusion model to reconstruct the trajectory. This training
objective encourages the model to learn temporal and cross-modal dependencies,
such as predicting the effects of actions on force signals or inferring states
from partial observations. We evaluate MDF on contact-rich, forceful
manipulation tasks in simulated and real-world environments. Our results show
that MDF not only delivers versatile functionalities, but also achieves strong
performance, and robustness under noisy observations. More visualizations can
be found on our website https://unified-df.github.io

</details>


### [4] [Pixi: Unified Software Development and Distribution for Robotics and AI](https://arxiv.org/abs/2511.04827)
*Tobias Fischer,Wolf Vollprecht,Bas Zalmstra,Ruben Arts,Tim de Jager,Alejandro Fontan,Adam D Hines,Michael Milford,Silvio Traversaro,Daniel Claes,Scarlett Raine*

Main category: cs.RO

TL;DR: 本文介绍了一种名为Pixi的统一包管理框架，它通过在项目级锁定文件中捕获确切的依赖状态来解决机器人研究中的可重复性危机，确保跨平台的一致性。Pixi不仅比同类工具快10倍地解析依赖关系，还整合了conda-forge和PyPI生态系统，从而减少从数小时到几分钟的设置时间，并自2023年以来已被超过5,300个项目采用。


<details>
  <summary>Details</summary>
Motivation: 科学研究计算中的再现性危机限制了机器人学研究的发展。据现有研究表明，高达70%的机器人算法无法被独立团队再现，而许多其他算法由于创建可共享软件环境过于复杂而未能投入使用。这些挑战源于碎片化、多语言以及硬件-软件工具链导致的依赖地狱问题。

Method: 提出了一种名为Pixi的统一包管理框架，该框架通过在项目级别的锁定文件中捕获确切的依赖状态来解决这些问题，确保跨平台的逐位可再现性。其高性能SAT求解器实现了比同类工具快达10倍的依赖解析速度，同时整合conda-forge和PyPI生态系统消除了对多个管理器的需求。

Result: 自2023年以来，Pixi已经被超过5,300个项目采纳使用，它将设置时间从几个小时缩短到了几分钟内，并为全球研究人员降低了技术障碍。

Conclusion: 通过提供可扩展、可再现的合作研究基础设施，Pixi加速了机器人技术和人工智能领域的进步。

Abstract: The reproducibility crisis in scientific computing constrains robotics
research. Existing studies reveal that up to 70% of robotics algorithms cannot
be reproduced by independent teams, while many others fail to reach deployment
because creating shareable software environments remains prohibitively complex.
These challenges stem from fragmented, multi-language, and hardware-software
toolchains that lead to dependency hell. We present Pixi, a unified
package-management framework that addresses these issues by capturing exact
dependency states in project-level lockfiles, ensuring bit-for-bit
reproducibility across platforms. Its high-performance SAT solver achieves up
to 10x faster dependency resolution than comparable tools, while integration of
the conda-forge and PyPI ecosystems removes the need for multiple managers.
Adopted in over 5,300 projects since 2023, Pixi reduces setup times from hours
to minutes and lowers technical barriers for researchers worldwide. By enabling
scalable, reproducible, collaborative research infrastructure, Pixi accelerates
progress in robotics and AI.

</details>


### [5] [Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning](https://arxiv.org/abs/2511.04831)
*NVIDIA,:,Mayank Mittal,Pascal Roth,James Tigue,Antoine Richard,Octi Zhang,Peter Du,Antonio Serrano-Muñoz,Xinjie Yao,René Zurbrügg,Nikita Rudin,Lukasz Wawrzyniak,Milad Rakhsha,Alain Denzler,Eric Heiden,Ales Borovicka,Ossama Ahmed,Iretiayo Akinola,Abrar Anwar,Mark T. Carlson,Ji Yuan Feng,Animesh Garg,Renato Gasoto,Lionel Gulich,Yijie Guo,M. Gussert,Alex Hansen,Mihir Kulkarni,Chenran Li,Wei Liu,Viktor Makoviychuk,Grzegorz Malczyk,Hammad Mazhar,Masoud Moghani,Adithyavairavan Murali,Michael Noseworthy,Alexander Poddubny,Nathan Ratliff,Welf Rehberg,Clemens Schwarke,Ritvik Singh,James Latham Smith,Bingjie Tang,Ruchik Thaker,Matthew Trepte,Karl Van Wyk,Fangzhou Yu,Alex Millane,Vikram Ramasamy,Remo Steiner,Sangeeta Subramanian,Clemens Volk,CY Chen,Neel Jawale,Ashwin Varghese Kuruttukulam,Michael A. Lin,Ajay Mandlekar,Karsten Patzwaldt,John Welsh,Huihua Zhao,Fatima Anes,Jean-Francois Lafleche,Nicolas Moënne-Loccoz,Soowan Park,Rob Stepinski,Dirk Van Gelder,Chris Amevor,Jan Carius,Jumyung Chang,Anka He Chen,Pablo de Heras Ciechomski,Gilles Daviet,Mohammad Mohajerani,Julia von Muralt,Viktor Reutskyy,Michael Sauter,Simon Schirm,Eric L. Shi,Pierre Terdiman,Kenny Vilella,Tobias Widmer,Gordon Yeoman,Tiffany Chen,Sergey Grizan,Cathy Li,Lotus Li,Connor Smith,Rafael Wiltz,Kostas Alexis,Yan Chang,David Chu,Linxi "Jim" Fan,Farbod Farshidian,Ankur Handa,Spencer Huang,Marco Hutter,Yashraj Narang,Soha Pouya,Shiwei Sheng,Yuke Zhu,Miles Macklin,Adam Moravanszky,Philipp Reist,Yunrong Guo,David Hoeller,Gavriel State*

Main category: cs.RO

TL;DR: Isaac Lab, 作为Isaac Gym的后续，是一个集成了高保真GPU并行物理计算、逼真渲染和模块化架构的平台，用于设计环境和训练机器人策略。它支持大规模多模态学习，并整合了执行器模型、多频率传感器模拟、数据收集管道和领域随机化工具，统一了强化学习和模仿学习的最佳实践。


<details>
  <summary>Details</summary>
Motivation: 为了推进机器人技术的发展，特别是通过结合高质量的仿真能力、丰富的感知能力和数据中心级别的执行规模来实现下一代机器人研究的突破。

Method: Isaac Lab 提供了一个扩展性强的平台，该平台结合了高保真度的GPU并行物理模拟、照片级真实感渲染以及用于设计环境和训练机器人策略的模块化可组合架构。此外，还融入了执行器模型、多频传感器模拟、数据收集流程及域随机化工具等特性。

Result: Isaac Lab 被成功应用于多种挑战中，包括全身控制、跨实体移动性、接触丰富且灵巧的操作任务，以及利用人类演示进行技能获取等方面。

Conclusion: Isaac Lab 的先进仿真能力、丰富的感觉信息处理功能及其在数据中心规模上的执行潜力有望推动机器人学研究中的新突破。

Abstract: We present Isaac Lab, the natural successor to Isaac Gym, which extends the
paradigm of GPU-native robotics simulation into the era of large-scale
multi-modal learning. Isaac Lab combines high-fidelity GPU parallel physics,
photorealistic rendering, and a modular, composable architecture for designing
environments and training robot policies. Beyond physics and rendering, the
framework integrates actuator models, multi-frequency sensor simulation, data
collection pipelines, and domain randomization tools, unifying best practices
for reinforcement and imitation learning at scale within a single extensible
platform. We highlight its application to a diverse set of challenges,
including whole-body control, cross-embodiment mobility, contact-rich and
dexterous manipulation, and the integration of human demonstrations for skill
acquisition. Finally, we discuss upcoming integration with the differentiable,
GPU-accelerated Newton physics engine, which promises new opportunities for
scalable, data-efficient, and gradient-based approaches to robot learning. We
believe Isaac Lab's combination of advanced simulation capabilities, rich
sensing, and data-center scale execution will help unlock the next generation
of breakthroughs in robotics research.

</details>


### [6] [Conformalized Non-uniform Sampling Strategies for Accelerated Sampling-based Motion Planning](https://arxiv.org/abs/2511.04835)
*Shubham Natraj,Bruno Sinopoli,Yiannis Kantaros*

Main category: cs.RO

TL;DR: 提出了一种新的非均匀采样策略，通过偏向于'认证'区域来提高基于采样的运动规划器(SBMPs)在复杂环境中的效率和速度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于采样的运动规划器(SBMPs)依赖于均匀采样，这往往导致在复杂环境中效率低下、规划缓慢。

Method: 引入了一种新的非均匀采样策略，该策略通过将采样偏向于'认证'区域而与现有SBMP集成。这些区域是通过使用任何启发式路径预测器（如A*或视觉-语言模型）生成初始可能不可行的路径，并应用一致性预测来量化预测器的不确定性来构建的。此过程产生围绕初始猜测路径的预测集，保证以用户指定的概率包含最优解。

Result: 广泛的评估表明，该方法比现有基线更快速地找到可行路径，并且对于未见环境有更好的泛化能力。

Conclusion: 这是首个为SBMPs提供关于采样区域概率正确性保证的非均匀采样方法，能够显著提升动态可行机器人路径计算的速度和可靠性。

Abstract: Sampling-based motion planners (SBMPs) are widely used to compute dynamically
feasible robot paths. However, their reliance on uniform sampling often leads
to poor efficiency and slow planning in complex environments. We introduce a
novel non-uniform sampling strategy that integrates into existing SBMPs by
biasing sampling toward `certified' regions. These regions are constructed by
(i) generating an initial, possibly infeasible, path using any heuristic path
predictor (e.g., A* or vision-language models) and (ii) applying conformal
prediction to quantify the predictor's uncertainty. This process yields
prediction sets around the initial-guess path that are guaranteed, with
user-specified probability, to contain the optimal solution. To our knowledge,
this is the first non-uniform sampling approach for SBMPs that provides such
probabilistically correct guarantees on the sampling regions. Extensive
evaluations demonstrate that our method consistently finds feasible paths
faster and generalizes better to unseen environments than existing baselines.

</details>


### [7] [Design Exploration for Protection and Cleaning of Solar Panels with Case Studies for Space Missions](https://arxiv.org/abs/2511.04837)
*Cameron Robinson,Ganghee Jang*

Main category: cs.RO

TL;DR: 设计并测试了太阳能板的清洁机制和保护材料，发现雨刷系统比轨道系统更高效，聚碳酸酯作为保护材料很有前景，但最重要的是在面板表面和硬质材料之间铺设一层软材料。


<details>
  <summary>Details</summary>
Motivation: 解决太阳能板因灰尘覆盖或太空碎片撞击而导致的操作受限或终止问题。

Method: 设计并比较了雨刷系统和轨道系统两种清洁机制；通过碰撞测试评估不同保护材料的效果。

Result: 雨刷系统在成本、清洁速度和总功耗方面优于轨道系统；聚碳酸酯作为保护材料表现出色，但在面板表面与硬质材料间增加一层软材料最为关键。

Conclusion: 本研究提出的清洁机制和保护材料方案能够有效提升太阳能板在恶劣环境下的工作性能与寿命。

Abstract: Solar energy is used for many mission-critical applications including space
exploration, sensor systems to monitor wildfires, etc. Their operation can be
limited or even terminated if solar panels are covered with dust or hit by
space debris. To address this issue, we designed panel cleaning mechanisms and
tested protective materials. For cleaning mechanisms, we designed and compared
a wiper system and a rail system. For protective materials, we found through
collision tests that polycarbonate was very promising, though the most
important factor was layering a soft material between the panel's surface and a
hard material. In the cleaning system comparisons, the wiper-based system was
more efficient than the rail-based system in terms of cost, cleaning speed, and
total power consumption.

</details>


### [8] [iFlyBot-VLM Technical Report](https://arxiv.org/abs/2511.04976)
*Xin Nie,Zhiyuan Cheng,Yuan Zhang,Chao Ji,Jiajia Wu,Yuhan Zhang,Jia Pan*

Main category: cs.RO

TL;DR: iFlyBot-VLM, a Vision-Language Model, aims to improve Embodied Intelligence by bridging the gap between high-dimensional perception and low-level robotic control. It abstracts information into an Operational Language, enabling coordination across various robotic platforms. The model is designed for four key capabilities: Spatial Understanding, Interactive Target Grounding, Action Abstraction, and Task Planning. It performed well on 10 benchmark datasets and will be publicly released to support further research.


<details>
  <summary>Details</summary>
Motivation: The motivation behind iFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional environmental perception and low-level robotic motion control, thus improving the domain of Embodied Intelligence. This advancement is expected to facilitate the progression from specialized task-oriented systems towards more generalist, cognitively capable agents.

Method: iFlyBot-VLM is designed with a systematic architecture that enables it to perform four key functions: 1) Spatial Understanding and Metric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and Control Parameter Generation; 4) Task Planning and Skill Sequencing. The model abstracts complex visual and spatial information into a body-agnostic and transferable Operational Language, which allows for seamless perception-action closed-loop coordination across different robotic platforms.

Result: iFlyBot-VLM achieved optimal performance when evaluated on 10 mainstream embodied intelligence-related VLM benchmark datasets, including Blink and Where2Place. The model maintained its general capabilities throughout these tests, demonstrating its effectiveness and adaptability in various scenarios.

Conclusion: iFlyBot-VLM represents a scalable and generalizable foundation model for embodied AI, with the potential to significantly advance the field of Embodied Intelligence. By facilitating the transition from specialized task-oriented systems to generalist, cognitively capable agents, iFlyBot-VLM opens up new possibilities for research and development. The public release of the training data and model weights will further promote progress in this area.

Abstract: We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) used
to improve the domain of Embodied Intelligence. The central objective of
iFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional
environmental perception and low-level robotic motion control. To this end, the
model abstracts complex visual and spatial information into a body-agnostic and
transferable Operational Language, thereby enabling seamless perception-action
closed-loop coordination across diverse robotic platforms. The architecture of
iFlyBot-VLM is systematically designed to realize four key functional
capabilities essential for embodied intelligence: 1) Spatial Understanding and
Metric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and
Control Parameter Generation; 4) Task Planning and Skill Sequencing. We
envision iFlyBot-VLM as a scalable and generalizable foundation model for
embodied AI, facilitating the progression from specialized task-oriented
systems toward generalist, cognitively capable agents. We conducted evaluations
on 10 current mainstream embodied intelligence-related VLM benchmark datasets,
such as Blink and Where2Place, and achieved optimal performance while
preserving the model's general capabilities. We will publicly release both the
training data and model weights to foster further research and development in
the field of Embodied Intelligence.

</details>


### [9] [A semi-analytical approach for computing the largest singularity-free spheres of a class of 6-6 Stewart-Gough platforms for specified orientation workspaces](https://arxiv.org/abs/2511.04992)
*Bibekananda Patra,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: 本文提出了一种计算6-6 Stewart-Gough平台操作器在指定姿态工作空间内最大无奇点球体的方法，并通过数值实验展示了该方法在分析和设计SGPM中的潜在应用价值。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是为了开发一种能够在特定的姿态工作空间中确定Stewart-Gough平台操作器（SGPM）的最大无奇点球体（SFS）的方法，以帮助分析和设计SGPM。

Method: 对于移动平台的固定姿态，通过解析方法计算SFS。这个过程会在姿态工作空间的一组样本上重复进行，其中最小的一个被指定为给定姿态工作空间所要求的SFS。

Result: 对四种不同的SGPM架构进行了数值实验，以了解它们在同一姿态工作空间中相对于SFS体积的表现。

Conclusion: 研究表明，所提出的计算方法在SGPM的分析和设计方面具有潜在的应用价值。

Abstract: This article presents a method for computing the largest singularity-free
sphere (SFS) of a 6-6 Stewart-Gough platform manipulator (SGPM) over a
specified orientation workspace. For a fixed orientation of the moving
platform, the SFS is computed analytically. This process is repeated over a set
of samples generated within the orientation workspace, and the smallest among
them is designated as the desired SFS for the given orientation workspace.
Numerical experiments are performed on four distinct architectures of the SGPM
to understand their relative performances w.r.t. SFS volumes over the same
orientation workspace. This study demonstrates the potential utility of the
proposed computational method both in analysis and design of SGPMs.

</details>


### [10] [MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery](https://arxiv.org/abs/2511.05007)
*Baiye Cheng,Tianhai Liang,Suning Huang,Maanping Shao,Feihong Zhang,Botian Xu,Zhengrong Xue,Huazhe Xu*

Main category: cs.RO

TL;DR: 本文提出了一种名为MoE-DP的方法，通过在视觉编码器和扩散模型之间插入一个专家混合层来提高机器人执行长时多阶段任务时从子任务失败中恢复的能力。实验表明该方法在受到干扰的情况下比标准基线有显著的鲁棒性改进，并且学到了可解释的任务技能分解。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略虽然强大，但在处理长时间、多阶段任务时缺乏从子任务失败中恢复的能力，并且其学习到的观察表示难以解释。为解决这些问题，提出了MoE-DP方法。

Method: 通过在视觉编码器与扩散模型之间引入一个Mixture of Experts (MoE) 层，将策略知识分解为一组专门化的专家，这些专家可以根据任务的不同阶段动态激活，从而增强了系统对干扰情况下的恢复能力。

Result: 在六个长时间模拟任务上的实验显示，在受干扰条件下，MoE-DP相比标准基线平均相对提高了36%的成功率；此外，它还展示了可解释性的技能分解，不同的专家对应着语义任务原语（例如接近、抓取）。

Conclusion: MoE-DP不仅提高了机器人执行复杂任务时面对扰动的鲁棒性，而且提供了一种更易于理解和调整的学习结构，这使得无需重新训练即可调整子任务顺序。

Abstract: Diffusion policies have emerged as a powerful framework for robotic
visuomotor control, yet they often lack the robustness to recover from subtask
failures in long-horizon, multi-stage tasks and their learned representations
of observations are often difficult to interpret. In this work, we propose the
Mixture of Experts-Enhanced Diffusion Policy (MoE-DP), where the core idea is
to insert a Mixture of Experts (MoE) layer between the visual encoder and the
diffusion model. This layer decomposes the policy's knowledge into a set of
specialized experts, which are dynamically activated to handle different phases
of a task. We demonstrate through extensive experiments that MoE-DP exhibits a
strong capability to recover from disturbances, significantly outperforming
standard baselines in robustness. On a suite of 6 long-horizon simulation
tasks, this leads to a 36% average relative improvement in success rate under
disturbed conditions. This enhanced robustness is further validated in the real
world, where MoE-DP also shows significant performance gains. We further show
that MoE-DP learns an interpretable skill decomposition, where distinct experts
correspond to semantic task primitives (e.g., approaching, grasping). This
learned structure can be leveraged for inference-time control, allowing for the
rearrangement of subtasks without any re-training.Our video and code are
available at the https://moe-dp-website.github.io/MoE-DP-Website/.

</details>


### [11] [Epically Powerful: An open-source software and mechatronics infrastructure for wearable robotic systems](https://arxiv.org/abs/2511.05033)
*Jennifer K. Leestma,Siddharth R. Nathella,Christoph P. O. Nuesslein,Snehil Mathur,Gregory S. Sawicki,Aaron J. Young*

Main category: cs.RO

TL;DR: Epically Powerful是一个开源机器人基础设施，旨在简化可穿戴机器人系统的底层框架管理，并提供从硬件选择到控制器实现的全面指南。它支持Python编程接口、与多种商用执行器和传感器无缝对接，并提供了实时可视化功能，以加速定制化可穿戴机器人系统的设计与发展过程。


<details>
  <summary>Details</summary>
Motivation: 降低开发和部署自定义可穿戴机器人系统的门槛，使研究人员能够快速有效地将原始硬件转化为模块化且坚固耐用的设备，同时该平台对其他使用类似组件进行闭环控制的机器人领域也具有广泛适用性。

Method: 通过提供一个代码库来简化用户端的实施，这个代码库可以通过Python轻松地与各种商业上最先进的准直驱（QDD）执行器、单板计算机以及常见传感器接口；提供示例控制器；并启用实时可视化。此外，还包含推荐零件列表和兼容性指南以及详细的软硬件实现文档。

Result: 创建了一个名为Epically Powerful的开放源码机器人基础设施，它不仅处理了可穿戴机器人系统的基础架构层面问题，如通信协议管理和数据记录等，而且还促进了从硬件选择到最终控制系统实现整个流程中的简便性和效率。

Conclusion: Epically Powerful成功地为想要开发自己的可穿戴机器人解决方案的研究者们提供了一个强大而灵活的起点，同时也为其他依赖于QDD执行器、单板计算机及传感器技术的机器人应用开辟了新的可能性。

Abstract: Epically Powerful is an open-source robotics infrastructure that streamlines
the underlying framework of wearable robotic systems - managing communication
protocols, clocking, actuator commands, visualization, sensor data acquisition,
data logging, and more - while also providing comprehensive guides for hardware
selection, system assembly, and controller implementation. Epically Powerful
contains a code base enabling simplified user implementation via Python that
seamlessly interfaces with various commercial state-of-the-art quasi-direct
drive (QDD) actuators, single-board computers, and common sensors, provides
example controllers, and enables real-time visualization. To further support
device development, the package also includes a recommended parts list and
compatibility guide and detailed documentation on hardware and software
implementation. The goal of Epically Powerful is to lower the barrier to
developing and deploying custom wearable robotic systems without a
pre-specified form factor, enabling researchers to go from raw hardware to
modular, robust devices quickly and effectively. Though originally designed
with wearable robotics in mind, Epically Powerful is broadly applicable to
other robotic domains that utilize QDD actuators, single-board computers, and
sensors for closed-loop control.

</details>


### [12] [TAPOM: Task-Space Topology-Guided Motion Planning for Manipulating Elongated Object in Cluttered Environments](https://arxiv.org/abs/2511.05052)
*Zihao Li,Yiming Zhu,Zhe Zhong,Qinyuan Ren,Yijiang Huang*

Main category: cs.RO

TL;DR: 本文提出了一种新的物体操作规划方法TAPOM，通过任务空间的拓扑分析来识别关键路径并生成引导关键帧，从而在狭窄通道中更高效地进行机器人操作。实验验证表明，该方法在低净空操作任务上比现有技术具有更高的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 在复杂的受限空间中执行机器人操作对于许多应用至关重要，但尤其在用细长物体穿过狭窄通道时极具挑战性。现有的规划方法由于采样困难或局部极小值问题，在这些低净空场景中往往失败。

Method: 本文提出了名为TAPOM的方法，它显式地结合了任务空间的拓扑分析以实现高效的规划。TAPOM采用高层次分析来识别关键路径并生成引导关键帧，这些关键帧被用于低层次规划器以找到可行的配置空间轨迹。

Result: 实验验证显示，与最先进方法相比，TAPOM在低净空操作任务上表现出显著更高的成功率和改进的效率。

Conclusion: 这项工作为提高复杂现实环境中机器人的操作能力提供了广泛的意义。

Abstract: Robotic manipulation in complex, constrained spaces is vital for widespread
applications but challenging, particularly when navigating narrow passages with
elongated objects. Existing planning methods often fail in these low-clearance
scenarios due to the sampling difficulties or the local minima. This work
proposes Topology-Aware Planning for Object Manipulation (TAPOM), which
explicitly incorporates task-space topological analysis to enable efficient
planning. TAPOM uses a high-level analysis to identify critical pathways and
generate guiding keyframes, which are utilized in a low-level planner to find
feasible configuration space trajectories. Experimental validation demonstrates
significantly high success rates and improved efficiency over state-of-the-art
methods on low-clearance manipulation tasks. This approach offers broad
implications for enhancing manipulation capabilities of robots in complex
real-world environments.

</details>


### [13] [Decomposed Object Manipulation via Dual-Actor Policy](https://arxiv.org/abs/2511.05129)
*Bin Fan,Jianjian Jiang,Zhuohao Li,Yixiang He,Xiaoming Wu,Yihan Yang,Shengbang Liu,Weishi Zheng*

Main category: cs.RO

TL;DR: 提出了一种新的Dual-Actor Policy（DAP），它通过利用不同的视觉先验来改进对象操作任务的不同阶段，包括接近阶段和操作阶段，并且创建了一个结合了这两种视觉先验的模拟数据集。实验表明，该方法在多个基准测试中都优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 以往的对象操作研究往往忽视了任务中的不同阶段特性，依赖单一策略直接学习整个操作过程。此外，现有的数据集缺乏支持训练所需的视觉先验。为了解决这些问题，提出了Dual-Actor Policy (DAP) 和一个新的模拟数据集。

Method: 开发了Dual-Actor Policy (DAP)，它包含基于功能性的执行者用于定位操作任务中的功能性部分以改善接近过程，以及基于运动流的执行者捕捉组件移动以促进操作过程。另外，还设计了一个决策制定者来判断当前阶段并选择相应的执行者。同时，构建了一个名为Dual-Prior Object Manipulation Dataset的新数据集，该数据集结合了两种视觉先验，并包含了七个任务，其中两个是具有挑战性的长期多阶段任务。

Result: 在自建的数据集、RoboTwin基准测试以及现实世界场景中的实验结果表明，与最先进(SOTA)的方法相比，所提出的方法分别平均提高了5.55%、14.7%和10.4%的表现。

Conclusion: 提出的Dual-Actor Policy (DAP) 通过明确考虑对象操作中的不同阶段，并利用异构视觉先验增强了每个阶段的表现。新构建的数据集也促进了这一领域的研究。实验结果显示了所提方法的有效性。

Abstract: Object manipulation, which focuses on learning to perform tasks on similar
parts across different types of objects, can be divided into an approaching
stage and a manipulation stage. However, previous works often ignore this
characteristic of the task and rely on a single policy to directly learn the
whole process of object manipulation. To address this problem, we propose a
novel Dual-Actor Policy, termed DAP, which explicitly considers different
stages and leverages heterogeneous visual priors to enhance each stage.
Specifically, we introduce an affordance-based actor to locate the functional
part in the manipulation task, thereby improving the approaching process.
Following this, we propose a motion flow-based actor to capture the movement of
the component, facilitating the manipulation process. Finally, we introduce a
decision maker to determine the current stage of DAP and select the
corresponding actor. Moreover, existing object manipulation datasets contain
few objects and lack the visual priors needed to support training. To address
this, we construct a simulated dataset, the Dual-Prior Object Manipulation
Dataset, which combines the two visual priors and includes seven tasks,
including two challenging long-term, multi-stage tasks. Experimental results on
our dataset, the RoboTwin benchmark and real-world scenarios illustrate that
our method consistently outperforms the SOTA method by 5.55%, 14.7% and 10.4%
on average respectively.

</details>


### [14] [Procedimiento de auditoría de ciberseguridad para sistemas autónomos: metodología, amenazas y mitigaciones](https://arxiv.org/abs/2511.05185)
*Adrián Campazas-Vega,Claudia Álvarez-Aparicio,David Sobrín-Hidalgo,Laura Inyesto-Alonso,Francisco Javier Rodríguez-Lera,Vicente Matellán-Olivera,Ángel Manuel Guerrero-Higueras*

Main category: cs.RO

TL;DR: 本文提出了一种针对自主系统的特定安全审计程序，该程序基于分层结构的方法论、适应于机器人环境的威胁分类以及一系列具体的缓解措施。通过四个实际案例研究证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统在工业、医学、物流和家庭环境等领域的集成日益增多，其面临的安全问题也愈加突出，尤其是那些在人类交互环境中运行的系统。技术进步和自主系统的高操作及架构复杂性导致了攻击面的扩大。

Method: 文章介绍了一种专为自主系统设计的安全审计程序，该程序依赖于一种分层结构的方法论，并且包含了特别调整以适用于机器人背景下的威胁分类法以及一组具体的缓解策略。

Result: 所提方法的有效性通过应用于四个代表性机器人平台的实际案例研究得到了验证：Ghost Robotics公司的Vision 60军用四足机器人、Unitree Robotics公司的A1机器人、Universal Robots公司的UR3协作臂以及Aldebaran Robotics公司的Pepper社交机器人。

Conclusion: 本研究提供了一套全面的安全审计框架来评估并增强不同应用场景下自主机器人的安全性。

Abstract: The deployment of autonomous systems has experienced remarkable growth in
recent years, driven by their integration into sectors such as industry,
medicine, logistics, and domestic environments. This expansion is accompanied
by a series of security issues that entail significant risks due to the
critical nature of autonomous systems, especially those operating in
human-interaction environments. Furthermore, technological advancement and the
high operational and architectural complexity of autonomous systems have
resulted in an increased attack surface. This article presents a specific
security auditing procedure for autonomous systems, based on a layer-structured
methodology, a threat taxonomy adapted to the robotic context, and a set of
concrete mitigation measures. The validity of the proposed approach is
demonstrated through four practical case studies applied to representative
robotic platforms: the Vision 60 military quadruped from Ghost Robotics, the A1
robot from Unitree Robotics, the UR3 collaborative arm from Universal Robots,
and the Pepper social robot from Aldebaran Robotics.

</details>


### [15] [Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation](https://arxiv.org/abs/2511.05199)
*Yichen Zhu,Feifei Feng*

Main category: cs.RO

TL;DR: 本文提出了一种通过从视频中检索（RfV）学习机器人策略的新方法，利用人类演示的类比来解决操作任务。系统构建了一个包含人类执行多样化日常任务的视频库，并从中提取中级信息作为额外输入以增强机器人模型的学习和泛化能力。该系统包括一个根据任务规格从外部视频库获取相关视频的视频检索器，以及一个将检索到的知识整合进学习周期的策略生成器。通过在多个模拟和真实世界环境中的严格测试，该系统相较于传统机器人系统展示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 针对复杂和不确定环境中操作的机器人面临的挑战，本文旨在开发一种能够通过观看视频演示来学习并完成新任务的机器人系统，模仿人类在面对不熟悉任务时的学习方式。

Method: 提出了一种名为Retrieving-from-Video (RfV) 的方法，其中包含了建立一个视频库、从视频中抽取物体可用性掩码和手部运动轨迹等中级信息的过程，以及设计了两部分系统：一个基于任务说明从外部视频库中检索任务相关视频的视频检索器，另一个是将检索到的知识融入学习循环的策略生成器。

Result: 通过在多种模拟与现实世界场景下的广泛测试，所提出的系统相比于传统的机器人系统展现出了显著的性能改进，表明了其对于处理超出训练数据范围的任务具有更强的适应性和泛化能力。

Conclusion: 本研究介绍的方法为机器人如何通过观察视频演示来学习新的操作任务提供了一个有效途径，代表了机器人学领域的一个重要突破。

Abstract: Robots operating in complex and uncertain environments face considerable
challenges. Advanced robotic systems often rely on extensive datasets to learn
manipulation tasks. In contrast, when humans are faced with unfamiliar tasks,
such as assembling a chair, a common approach is to learn by watching video
demonstrations. In this paper, we propose a novel method for learning robot
policies by Retrieving-from-Video (RfV), using analogies from human
demonstrations to address manipulation tasks. Our system constructs a video
bank comprising recordings of humans performing diverse daily tasks. To enrich
the knowledge from these videos, we extract mid-level information, such as
object affordance masks and hand motion trajectories, which serve as additional
inputs to enhance the robot model's learning and generalization capabilities.
We further feature a dual-component system: a video retriever that taps into an
external video bank to fetch task-relevant video based on task specification,
and a policy generator that integrates this retrieved knowledge into the
learning cycle. This approach enables robots to craft adaptive responses to
various scenarios and generalize to tasks beyond those in the training data.
Through rigorous testing in multiple simulated and real-world settings, our
system demonstrates a marked improvement in performance over conventional
robotic systems, showcasing a significant breakthrough in the field of
robotics.

</details>


### [16] [Beyond Master and Apprentice: Grounding Foundation Models for Symbiotic Interactive Learning in a Shared Latent Space](https://arxiv.org/abs/2511.05203)
*Linus Nwankwo,Björn Ellensohn,Christian Rauch,Elmar Rueckert*

Main category: cs.RO

TL;DR: 本文提出了一种共生交互学习（SIL）方法，旨在通过双向互动促进人类与机器人之间的共同适应。此方法利用预训练的基础模型进行空间感知和推理，并结合轻量级潜在编码器将模型输出转化为特定任务的表示。此外，为了保证任务演变过程中的稳定性，还引入了记忆架构来防止已学习的任务空间表示被遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有的人类-机器人交互方式主要遵循一种主从模式，在这种模式下，机器人作为学徒被动接收并执行人类（主人）的指令，而没有相互学习的过程。这种方式缺乏日常人际多轮对话中固有的共适应动态。

Method: 研究者提出了共生交互学习（SIL）的方法，将其形式化为共享潜在任务空间内的共同适应过程。在这个过程中，机器人和人保持基于交互历史发展的联合信念状态。该方法利用了预训练的基础模型用于空间感知与推理，并采用了一个轻量级潜变量编码器将模型输出转换成针对特定任务的表现形式。同时，为了维持随时间发展任务的稳定性，研究团队还增加了记忆结构以避免学习到的任务空间表现被遗忘。

Result: 实验验证表明，SIL不仅可以在模拟环境中有效运行，而且在真实世界的身体任务中也表现出色，包括但不限于指令跟随、信息检索、面向查询的推理以及互动对话等方面的应用。

Conclusion: 通过共生交互学习框架，本研究成功地促进了人类与机器人之间更自然、更具适应性的交互方式，超越了传统的响应式执行模式，实现了主动澄清、适应性建议及共享计划优化等新行为。

Abstract: Today's autonomous agents can understand free-form natural language
instructions and execute long-horizon tasks in a manner akin to human-level
reasoning. These capabilities are mostly driven by large-scale pre-trained
foundation models (FMs). However, the approaches with which these models are
grounded for human-robot interaction (HRI) perpetuate a master-apprentice
model, where the apprentice (embodied agent) passively receives and executes
the master's (human's) commands without reciprocal learning. This reactive
interaction approach does not capture the co-adaptive dynamics inherent in
everyday multi-turn human-human interactions. To address this, we propose a
Symbiotic Interactive Learning (SIL) approach that enables both the master and
the apprentice to co-adapt through mutual, bidirectional interactions. We
formalised SIL as a co-adaptation process within a shared latent task space,
where the agent and human maintain joint belief states that evolve based on
interaction history. This enables the agent to move beyond reactive execution
to proactive clarification, adaptive suggestions, and shared plan refinement.
To realise these novel behaviours, we leveraged pre-trained FMs for spatial
perception and reasoning, alongside a lightweight latent encoder that grounds
the models' outputs into task-specific representations. Furthermore, to ensure
stability as the tasks evolve, we augment SIL with a memory architecture that
prevents the forgetting of learned task-space representations. We validate SIL
on both simulated and real-world embodied tasks, including instruction
following, information retrieval, query-oriented reasoning, and interactive
dialogues. Demos and resources are public
at:~\href{https://linusnep.github.io/SIL/}{https://linusnep.github.io/SIL/}.

</details>


### [17] [TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models](https://arxiv.org/abs/2511.05275)
*Hokyun Im,Euijin Jeong,Jianlong Fu,Andrey Kolobov,Youngwoon Lee*

Main category: cs.RO

TL;DR: 本文介绍了一种名为TwinVLA的模块化框架，它将两个预训练的单臂视觉-语言-动作模型组合成一个协调的双臂模型，以提高数据效率和性能，在多种双臂任务中超越了同规模的整体模型，并且不需要任何双臂预训练。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型（VLAs）在处理双臂任务时需要大量的额外双臂数据及微调，因为大多数公开的数据集主要集中在单臂演示上。为了克服这一挑战，提出了TwinVLA框架。

Method: 通过引入TwinVLA框架，该方法利用两个预训练的单臂VLAs来构建一个协同工作的双臂VLA。这种方式与那些使用混合单臂和双臂数据训练而成的整体跨实体模型不同，旨在提升数据效率和表现力。

Result: 实验结果表明，无论是在现实世界还是模拟环境中执行多样的双臂任务时，TwinVLA都优于同样大小的整体RDT-1B模型，而且无需进行任何双臂预训练。此外，它也缩小了与依赖大量专有双臂数据和计算成本的最先进模型之间的差距。

Conclusion: 这些成果证明了我们的模块化组合方法是一条高效、可扩展的路径，能够利用公共单臂数据实现高性能的双臂操作。

Abstract: Vision-language-action models (VLAs) trained on large-scale robotic datasets
have demonstrated strong performance on manipulation tasks, including bimanual
tasks. However, because most public datasets focus on single-arm
demonstrations, adapting VLAs for bimanual tasks typically requires substantial
additional bimanual data and fine-tuning. To address this challenge, we
introduce TwinVLA, a modular framework that composes two copies of a pretrained
single-arm VLA into a coordinated bimanual VLA. Unlike monolithic
cross-embodiment models trained on mixtures of single-arm and bimanual data,
TwinVLA improves both data efficiency and performance by composing pretrained
single-arm policies. Across diverse bimanual tasks in real-world and simulation
settings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model
without requiring any bimanual pretraining. Furthermore, it narrows the gap to
state-of-the-art model, $\pi_0$ which rely on extensive proprietary bimanual
data and compute cost. These results establish our modular composition approach
as a data-efficient and scalable path toward high-performance bimanual
manipulation, leveraging public single-arm data.

</details>


### [18] [Force-Safe Environment Maps and Real-Time Detection for Soft Robot Manipulators](https://arxiv.org/abs/2511.05307)
*Akua K. Dickson,Juan C. Pacheco Garcia,Andrew P. Sabelhaus*

Main category: cs.RO

TL;DR: 本文提出了一种将任务空间中的力安全标准映射到配置空间的框架，以实现实时力安全检测，并通过软机器人操作器的实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的障碍物检测和避障方法没有考虑到操作器在接触脆弱障碍物时可能施加的力量限制。

Method: 通过将允许的环境接触力限制纳入给定的任务空间障碍物中，并通过操作器的正向运动学将其映射到配置空间（C-空间）。

Result: 结果表明，所提出的方法在与可变形障碍物交互过程中能够准确地检测力的安全性，为在精细、杂乱环境中软操纵器的实时安全规划奠定了基础。

Conclusion: 这种新的力安全检测框架为软体机器人操作器在复杂环境下的实时安全规划提供了有力支持。

Abstract: Soft robot manipulators have the potential for deployment in delicate
environments to perform complex manipulation tasks. However, existing obstacle
detection and avoidance methods do not consider limits on the forces that
manipulators may exert upon contact with delicate obstacles. This work
introduces a framework that maps force safety criteria from task space (i.e.
positions along the robot's body) to configuration space (i.e. the robot's
joint angles) and enables real-time force safety detection. We incorporate
limits on allowable environmental contact forces for given task-space
obstacles, and map them into configuration space (C-space) through the
manipulator's forward kinematics. This formulation ensures that configurations
classified as safe are provably below the maximum force thresholds, thereby
allowing us to determine force-safe configurations of the soft robot
manipulator in real-time. We validate our approach in simulation and hardware
experiments on a two-segment pneumatic soft robot manipulator. Results
demonstrate that the proposed method accurately detects force safety during
interactions with deformable obstacles, thereby laying the foundation for
real-time safe planning of soft manipulators in delicate, cluttered
environments.

</details>


### [19] [ETHOS: A Robotic Encountered-Type Haptic Display for Social Interaction in Virtual Reality](https://arxiv.org/abs/2511.05379)
*Eric Godden,Jacquie Groenewegen,Matthew K. X. J. Pan*

Main category: cs.RO

TL;DR: 本文介绍了一种名为ETHOS的动态触觉显示系统，它通过结合力矩控制的机器人操作器、可互换的被动道具、物理-虚拟注册和安全监控等技术，在虚拟现实中实现自然的身体接触，如交接物品、碰拳和击掌。实验结果表明该系统能够有效地重现社交互动中的触感体验。


<details>
  <summary>Details</summary>
Motivation: 为了在虚拟现实环境中实现更加自然且富有意义的社会互动，特别是涉及身体接触的场景，研究者们开发了ETHOS系统。

Method: ETHOS系统整合了力矩控制的机械臂与可替换的被动道具（硅胶手模和棒），并通过ChArUco板实现了基于标记的物理-虚拟对齐。此外，还引入了一个安全监控机制，根据用户的头部和手部姿态来控制机械臂的动作。文中提出了两种控制策略：一种是静态模式，另一种则是动态模式。

Result: 基准测试显示静态共位精度为5.09±0.94毫米；用户交互过程中，所有交互及控制条件下的平均接触延迟为28.53±31.21毫秒。这些结果显示了在虚拟现实中重现具有社会意义触觉的可能性。

Conclusion: 通过引入关键的安全性和控制机制，ETHOS为虚拟环境中高保真度的动态人际互动奠定了实用基础。

Abstract: We present ETHOS (Encountered-Type Haptics for On-demand Social Interaction),
a dynamic encountered-type haptic display (ETHD) that enables natural physical
contact in virtual reality (VR) during social interactions such as handovers,
fist bumps, and high-fives. The system integrates a torque-controlled robotic
manipulator with interchangeable passive props (silicone hand replicas and a
baton), marker-based physical-virtual registration via a ChArUco board, and a
safety monitor that gates motion based on the user's head and hand pose. We
introduce two control strategies: (i) a static mode that presents a stationary
prop aligned with its virtual counterpart, consistent with prior ETHD
baselines, and (ii) a dynamic mode that continuously updates prop position by
exponentially blending an initial mid-point trajectory with real-time hand
tracking, generating a unique contact point for each interaction. Bench tests
show static colocation accuracy of 5.09 +/- 0.94 mm, while user interactions
achieved temporal alignment with an average contact latency of 28.53 +/- 31.21
ms across all interaction and control conditions. These results demonstrate the
feasibility of recreating socially meaningful haptics in VR. By incorporating
essential safety and control mechanisms, ETHOS establishes a practical
foundation for high-fidelity, dynamic interpersonal interactions in virtual
environments.

</details>


### [20] [EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation](https://arxiv.org/abs/2511.05397)
*Samarth Chopra,Alex McMoil,Ben Carnovale,Evan Sokolson,Rajkumar Kubendran,Samuel Dickerson*

Main category: cs.RO

TL;DR: 介绍了EverydayVLA，一种低成本的6自由度操作器，能够执行由视觉-语言-动作模型指导的任务，具有与最先进模型相当的成功率，并在实际测试中优于先前的方法。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉-语言-动作（VLA）模型可以直接将视觉输入和语言指令映射到机器人动作上，但它们通常依赖于昂贵的硬件，并且难以应对新颖或杂乱的场景。

Method: 提出了EverydayVLA，这是一个成本低于300美元的6自由度操作器，能够处理适度的有效载荷和工作空间。该系统采用统一模型同时输出离散和连续动作，并通过自适应视界集合监控运动不确定性以触发即时重新规划来保证安全可靠的操作。

Result: 在LIBERO数据集上，EverydayVLA达到了最先进的成功率；在现实世界测试中，它比现有方法提高了49%的分布内表现以及34.9%的分布外表现。

Conclusion: 结合了最先进的VLA技术和成本效益高的硬件，EverydayVLA使更多人能够接触到机器人的基础模型，并为家庭和研究实验室中的经济使用铺平了道路。

Abstract: While Vision-Language-Action (VLA) models map visual inputs and language
instructions directly to robot actions, they often rely on costly hardware and
struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF
manipulator that can be assembled for under $300, capable of modest payloads
and workspace. A single unified model jointly outputs discrete and continuous
actions, and our adaptive-horizon ensemble monitors motion uncertainty to
trigger on-the-fly re-planning for safe, reliable operation. On LIBERO,
EverydayVLA matches state-of-the-art success rates, and in real-world tests it
outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution.
By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA
democratizes access to a robotic foundation model and paves the way for
economical use in homes and research labs alike. Experiment videos and details:
https://everydayvla.github.io/

</details>


### [21] [Stable and Robust SLIP Model Control via Energy Conservation-Based Feedback Cancellation for Quadrupedal Applications](https://arxiv.org/abs/2511.05402)
*Muhammad Saud Ul Hassan,Derek Vasquez,Hamza Asif,Christian Hubicki*

Main category: cs.RO

TL;DR: 本文提出了一种基于能量守恒的控制架构，用于四足机器人的稳定动态运动。通过将机器人建模为弹簧加载倒立摆（SLIP），并利用该模型在空中控制腿的方向、在支撑阶段控制腿长，结合能量守恒原则计算出稳定的抛物线样条路径进行跟踪。模拟实验表明该算法能够生成稳定的跳跃步态，并且在传感器测量误差高达10%的情况下仍能保持稳定。


<details>
  <summary>Details</summary>
Motivation: 受到自然四足动物行为以及生物启发式机器人系统的启发，旨在开发一种适用于四足机器人的控制方法，以实现如真实四足动物奔跑时所观察到的那种弹跳运动特征。

Method: 采用Spring-loaded Inverted Pendulum (SLIP)模型来表示四足机器人；允许在飞行过程中控制腿部方向，在支撑阶段控制腿部长度；根据能量守恒原理计算出一条稳定的抛物线样条路径供机器人跟踪。

Result: 通过基于Ghost Robotics Minitaur设计规格的仿真测试证明了所提控制算法能够产生稳定的跳跃步态；同时展示了控制器面对高达10%的传感器测量误差时依旧保持稳定跳跃的能力。

Conclusion: 提出的方法成功地让四足机器人模仿了自然界的四足动物的跑步步态，并且展现出了良好的鲁棒性。

Abstract: In this paper, we present an energy-conservation based control architecture
for stable dynamic motion in quadruped robots. We model the robot as a
Spring-loaded Inverted Pendulum (SLIP), a model well-suited to represent the
bouncing motion characteristic of running gaits observed in various biological
quadrupeds and bio-inspired robotic systems. The model permits leg-orientation
control during flight and leg-length control during stance, a design choice
inspired by natural quadruped behaviors and prevalent in robotic quadruped
systems. Our control algorithm uses the reduced-order SLIP dynamics of the
quadruped to track a stable parabolic spline during stance, which is calculated
using the principle of energy conservation. Through simulations based on the
design specifications of an actual quadruped robot, Ghost Robotics Minitaur, we
demonstrate that our control algorithm generates stable bouncing gaits.
Additionally, we illustrate the robustness of our controller by showcasing its
ability to maintain stable bouncing even when faced with up to a 10% error in
sensor measurements.

</details>


### [22] [Bioinspired Soft Quadrotors Jointly Unlock Agility, Squeezability, and Collision Resilience](https://arxiv.org/abs/2511.05426)
*Luca Girardi,Gabriel Maquignaz,Stefano Mintchev*

Main category: cs.RO

TL;DR: Inspired by the flexible and resilient wings of natural flyers, the FlexiQuad is a novel soft-frame quadrotor that offers high-speed maneuverability, the ability to withstand collisions, and can squeeze through tight spaces, outperforming traditional rigid-frame drones in complex environments.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to overcome the limitations of conventional quadrotor designs, which, due to their rigid frames, are not well-suited for flying in cluttered environments where collision resilience and the ability to pass through narrow spaces are crucial. The aim is to create a design that can offer both agile flight and enhanced flexibility and durability.

Method: The researchers developed FlexiQuad, a soft-frame quadrotor design that incorporates anisotropic stiffness and distributed mass-energy structures inspired by biological organisms. They created a prototype weighing 405 grams, which is much more compliant than conventional designs, and demonstrated its ability to perform acrobatic maneuvers with high speeds and accelerations while also being highly resilient to collisions and able to squeeze through narrow gaps.

Result: The FlexiQuad prototype proved to be capable of performing at high speeds and accelerations, similar to those of rigid frame quadrotors, while also demonstrating superior collision resilience and the ability to compress and fit through gaps as small as 70% of its nominal width. The optimal structural softness for achieving these characteristics was found to be in the range of 0.006 to 0.77 N/mm, which is comparable to the wings of natural flyers.

Conclusion: FlexiQuad expands the capabilities of hovering drones in complex environments, allowing for robust physical interactions without compromising on flight performance. It achieves a balance between agility, squeezability, and collision resilience, making it suitable for a wide range of drone sizes from 20 to 3000 grams.

Abstract: Natural flyers use soft wings to seamlessly enable a wide range of flight
behaviours, including agile manoeuvres, squeezing through narrow passageways,
and withstanding collisions. In contrast, conventional quadrotor designs rely
on rigid frames that support agile flight but inherently limit collision
resilience and squeezability, thereby constraining flight capabilities in
cluttered environments. Inspired by the anisotropic stiffness and distributed
mass-energy structures observed in biological organisms, we introduce
FlexiQuad, a soft-frame quadrotor design approach that limits this trade-off.
We demonstrate a 405-gram FlexiQuad prototype, three orders of magnitude more
compliant than conventional quadrotors, yet capable of acrobatic manoeuvres
with peak speeds above 80 km/h and linear and angular accelerations exceeding 3
g and 300 rad/s$^2$, respectively. Analysis demonstrates it can replicate
accelerations of rigid counterparts up to a thrust-to-weight ratio of 8.
Simultaneously, FlexiQuad exhibits fourfold higher collision resilience,
surviving frontal impacts at 5 m/s without damage and reducing destabilising
forces in glancing collisions by a factor of 39. Its frame can fully compress,
enabling flight through gaps as narrow as 70% of its nominal width. Our
analysis identifies an optimal structural softness range, from 0.006 to 0.77
N/mm, comparable to that of natural flyers' wings, whereby agility,
squeezability, and collision resilience are jointly achieved for FlexiQuad
models from 20 to 3000 grams. FlexiQuad expands hovering drone capabilities in
complex environments, enabling robust physical interactions without
compromising flight performance.

</details>
