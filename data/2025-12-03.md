<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 14]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Reinforcement Learning for Robotic Safe Control with Force Sensing](https://arxiv.org/abs/2512.02022)
*Nan Lin,Linrui Zhang,Yuxuan Chen,Zhenrui Chen,Yujun Zhu,Ruoxi Chen,Peichen Wu,Xiaoping Chen*

Main category: cs.RO

TL;DR: 本研究将力和触觉感知引入强化学习，以提高机器人在复杂非结构化环境中的安全性和可靠性。实验结果表明，在物体推动任务中，基于力的强化学习方法在模拟和现实环境中都更安全、更高效。


<details>
  <summary>Details</summary>
Motivation: 传统手工编码方法在处理非结构化环境中的复杂操作任务时效果不佳，而强化学习虽能提供更通用且有用的策略，但其稳定性和可靠性难以保证，可能带来安全隐患。此外，从模拟到现实世界的迁移也会导致不可预测的情况。因此，为了增强机器人的安全性和可靠性，研究者考虑将力和触觉感知结合进强化学习中。

Method: 本文提出了一种基于力反馈的强化学习方法，通过引入力和触觉信息来优化机器人在动态控制及人机交互中的表现。该方法特别针对从模拟环境向真实世界转移过程中遇到的问题进行了改进。

Result: 实验结果显示，在物体推动任务上，所提出的策略不仅能够有效适应环境变化，而且相比传统方法展现出更高的安全性和效率。这些优点在模拟测试和实际应用中均得到了验证。

Conclusion: 通过将力与触觉感知融入强化学习框架内，可以显著提升机器人面对复杂多变工作环境时的安全性与可靠性，为未来广泛领域的机器人技术应用开辟了新的前景。

Abstract: For the task with complicated manipulation in unstructured environments, traditional hand-coded methods are ineffective, while reinforcement learning can provide more general and useful policy. Although the reinforcement learning is able to obtain impressive results, its stability and reliability is hard to guarantee, which would cause the potential safety threats. Besides, the transfer from simulation to real world also will lead in unpredictable situations. To enhance the safety and reliability of robots, we introduce the force and haptic perception into reinforcement learning. Force and tactual sensation play key roles in robotic dynamic control and human-robot interaction. We demonstrate that the force-based reinforcement learning method can be more adaptive to environment, especially in sim-to-real transfer. Experimental results show in object pushing task, our strategy is safer and more efficient in both simulation and real world, thus it holds prospects for a wide variety of robotic applications.

</details>


### [2] [VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM](https://arxiv.org/abs/2512.02293)
*Zihan Zhu,Wei Zhang,Norbert Haala,Marc Pollefeys,Daniel Barath*

Main category: cs.RO

TL;DR: 介绍了VIGS-SLAM，一种视觉-惯性3D高斯点绘SLAM系统，它通过结合视觉和惯性信息，在统一的优化框架内共同优化相机姿态、深度和IMU状态，实现了鲁棒的实时跟踪和高保真重建。


<details>
  <summary>Details</summary>
Motivation: 尽管基于3DGS的SLAM方法能够实现密集且逼真的映射，但其纯视觉设计在运动模糊、低纹理和曝光变化的情况下性能下降。因此，需要一种更鲁棒的方法来处理这些挑战。

Method: VIGS-SLAM紧密耦合了视觉与惯性线索，采用统一的优化框架同时改进相机位置、深度以及IMU状态估计。该方法还具备可靠的IMU初始化、随时间变化的偏置建模及闭环检测时的一致性高斯更新功能。

Result: 在四个具有挑战性的数据集上的实验表明，所提出的方法相比最先进方法展现出优越性能。

Conclusion: VIGS-SLAM提供了一种有效增强现实世界中定位准确性与地图构建质量的新途径。

Abstract: We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction. Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations. Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states. It features robust IMU initialization, time-varying bias modeling, and loop closure with consistent Gaussian updates. Experiments on four challenging datasets demonstrate our superiority over state-of-the-art methods. Project page: https://vigs-slam.github.io

</details>


### [3] [Vehicle Dynamics Embedded World Models for Autonomous Driving](https://arxiv.org/abs/2512.02417)
*Huiqian Li,Wei Pan,Haodong Zhang,Jin Huang,Zhihua Zhong*

Main category: cs.RO

TL;DR: 提出了一种名为Vehicle Dynamics embedded Dreamer (VDD)的新方法，通过将自车动力学与环境转换动力学解耦，并结合策略调整和增强技术，提高了自动驾驶模型在不同车辆参数下的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的世界模型通常一起学习自车动力学和环境转换动力学，这导致效率低下并且对车辆动力学变化的鲁棒性不足。为了克服这些问题，研究引入了VDD方法，旨在提高模型对于不同车辆参数的适应性和鲁棒性。

Method: 本研究提出的方法包括：1. Vehicle Dynamics embedded Dreamer (VDD)，它将自车动力学建模从环境转换动力学中分离出来；2. 策略调整部署(PAD) 和 训练期间策略增强(PAT) 两种策略来进一步提升所学策略的鲁棒性。

Result: 在模拟环境中进行的全面实验表明，所提模型不仅提升了驾驶表现，而且对于车辆动力学变化也展现了更高的鲁棒性，超越了现有方法的表现。

Conclusion: 通过采用VDD方法以及PAD和PAT策略，可以有效提高自动驾驶系统在面对不同车辆动态特性时的性能及鲁棒性，为未来更加智能、安全的自动驾驶解决方案提供了新的思路。

Abstract: World models have gained significant attention as a promising approach for autonomous driving. By emulating human-like perception and decision-making processes, these models can predict and adapt to dynamic environments. Existing methods typically map high-dimensional observations into compact latent spaces and learn optimal policies within these latent representations. However, prior work usually jointly learns ego-vehicle dynamics and environmental transition dynamics from the image input, leading to inefficiencies and a lack of robustness to variations in vehicle dynamics. To address these issues, we propose the Vehicle Dynamics embedded Dreamer (VDD) method, which decouples the modeling of ego-vehicle dynamics from environmental transition dynamics. This separation allows the world model to generalize effectively across vehicles with diverse parameters. Additionally, we introduce two strategies to further enhance the robustness of the learned policy: Policy Adjustment during Deployment (PAD) and Policy Augmentation during Training (PAT). Comprehensive experiments in simulated environments demonstrate that the proposed model significantly improves both driving performance and robustness to variations in vehicle dynamics, outperforming existing approaches.

</details>


### [4] [AID: Agent Intent from Diffusion for Multi-Agent Informative Path Planning](https://arxiv.org/abs/2512.02535)
*Jeric Lew,Yuhong Cao,Derek Ming Siang Tan,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 提出了一种基于扩散模型的非自回归多智能体信息路径规划框架AID，通过行为克隆和强化学习优化策略，实现更快速执行和更高信息增益。


<details>
  <summary>Details</summary>
Motivation: 在大规模或时间紧迫的信息收集场景中，如环境监测、搜救等，需要在有限时间内实现广泛的覆盖范围，这促使了多智能体系统的应用。现有方法虽然使用未来位置分布作为‘意图’来支持协调，但这些自回归意图预测器计算成本高且容易累积误差。

Method: 受到扩散模型作为表达力强、长视野策略的有效性的启发，研究者提出了AID——一种完全去中心化的MAIPP框架，该框架利用扩散模型以非自回归方式生成长期轨迹。AID首先对现有MAIPP规划器产生的轨迹进行行为克隆，然后通过Diffusion Policy Policy Optimization (DPPO) 使用强化学习微调策略。

Result: 实验表明，与训练所基于的MAIPP规划器相比，AID始终表现出色，实现了高达4倍的更快执行速度和17%的信息增益增加，并且能够有效地扩展到更多数量的智能体。

Conclusion: AID框架通过结合行为克隆与强化学习，在多智能体信息路径规划问题上提供了显著改进，特别是在提高执行效率和信息获取方面。此外，其设计允许良好地适应更大规模的智能体群体。

Abstract: Information gathering in large-scale or time-critical scenarios (e.g., environmental monitoring, search and rescue) requires broad coverage within limited time budgets, motivating the use of multi-agent systems. These scenarios are commonly formulated as multi-agent informative path planning (MAIPP), where multiple agents must coordinate to maximize information gain while operating under budget constraints. A central challenge in MAIPP is ensuring effective coordination while the belief over the environment evolves with incoming measurements. Recent learning-based approaches address this by using distributions over future positions as "intent" to support coordination. However, these autoregressive intent predictors are computationally expensive and prone to compounding errors. Inspired by the effectiveness of diffusion models as expressive, long-horizon policies, we propose AID, a fully decentralized MAIPP framework that leverages diffusion models to generate long-term trajectories in a non-autoregressive manner. AID first performs behavior cloning on trajectories produced by existing MAIPP planners and then fine-tunes the policy using reinforcement learning via Diffusion Policy Policy Optimization (DPPO). This two-stage pipeline enables the policy to inherit expert behavior while learning improved coordination through online reward feedback. Experiments demonstrate that AID consistently improves upon the MAIPP planners it is trained from, achieving up to 4x faster execution and 17% increased information gain, while scaling effectively to larger numbers of agents. Our implementation is publicly available at https://github.com/marmotlab/AID.

</details>


### [5] [Robotic capabilities framework: A boundary object and intermediate-level knowledge artifact for co-designing robotic processes](https://arxiv.org/abs/2512.02549)
*Alessandro Ianniello,Dave Murray-Rust,Sara Muscolo,Olger Siebinga,Nicky Mol,Denis Zatyagov,Eva Verhoef,Deborah Forster,David Abbink*

Main category: cs.RO

TL;DR: 本文提出了一种机器人能力框架，旨在通过跨学科合作来有效地设计人机协作的未来工作模式。该框架侧重于讨论高级别的能力，而不是机器人的内部运作，并通过与机器人专家和学生的互动中得到了应用和发展。


<details>
  <summary>Details</summary>
Motivation: 随着机器人变得更加适应性强、响应迅速且能够与人类互动，有效的人机协作设计变得至关重要。然而，这一设计过程通常由单一学科的方法主导，往往忽视了跨学科知识以及最终将与这些系统共享任务的工人的经验性知识。为了解决这一差距，提出了机器人能力框架。

Method: 通过反思性和迭代过程开发了该框架，并在两个不同的场景中进行了应用：一是让机器人专家使用其词汇描述现有的商用机器人；二是通过与从事机器人相关项目的学生进行设计活动。

Result: 框架作为中级知识工具和边界对象出现，它连接了技术领域和经验领域，指导设计师，赋能工人，并有助于创造更加公正和协作的工作未来。

Conclusion: 机器人能力框架促进了跨学科合作，支持围绕任务应由人类还是机器人执行的对话，从而对工作未来的设计产生积极影响。

Abstract: As robots become more adaptable, responsive, and capable of interacting with humans, the design of effective human-robot collaboration becomes critical. Yet, this design process is typically led by monodisciplinary approaches, often overlooking interdisciplinary knowledge and the experiential knowledge of workers who will ultimately share tasks with these systems. To address this gap, we introduce the robotic capabilities framework, a vocabulary that enables transdisciplinary collaborations to meaningfully shape the future of work when robotic systems are integrated into the workplace. Rather than focusing on the internal workings of robots, the framework centers discussion on high-level capabilities, supporting dialogue around which elements of a task should remain human-led and which can be delegated to robots. We developed the framework through reflexive and iterative processes, and applied it in two distinct settings: by engaging roboticists in describing existing commercial robots using its vocabulary, and through a design activity with students working on robotics-related projects. The framework emerges as an intermediate-level knowledge artifact and a boundary object that bridges technical and experiential domains, guiding designers, empowering workers, and contributing to more just and collaborative futures of work.

</details>


### [6] [SAM2Grasp: Resolve Multi-modal Grasping via Prompt-conditioned Temporal Action Prediction](https://arxiv.org/abs/2512.02609)
*Shengkai Wu,Jinrong Yang,Wenqiu Luo,Linfeng Gao,Chaohui Shang,Meiyu Zhi,Mingshan Sun,Fangping Yang,Liangliang Ren,Yong Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种新的框架SAM2Grasp，通过将任务重新定义为单一模式、基于提示条件的预测问题来解决机器人抓取模仿学习中的多模态问题。该方法利用了冻结的SAM2模型的强大视觉时间跟踪能力，并引入了一个轻量级、可训练的动作头部，与原生分割头部并行操作。实验表明，SAM2Grasp在杂乱多目标抓取任务中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在机器人抓取的模仿学习中，当场景包含多个有效目标时，对不同物体进行抓取的演示会产生相互冲突的学习信号。标准的模仿学习策略会将这些不同的动作平均成一个单独的无效动作。为了解决这个问题，提出了一个新的框架SAM2Grasp。

Method: 提出的SAM2Grasp框架通过将任务转化为单模态、基于提示条件的预测问题来解决上述挑战。它使用了冻结状态下的SAM2模型以利用其强大的视觉-时间追踪能力，并添加了一个轻量级且可训练的动作头，与原有的分割头并行工作。此设计使得仅需针对预计算好的时间-视觉特征训练小规模的动作头即可。在推理过程中，通过上游对象检测模型提供的初始提示（如边界框）指定要抓取的具体对象，这促使动作头预测出一条独特而明确的抓取轨迹。之后，在所有后续视频帧中，SAM2内置的时间追踪功能自动保持选定对象的稳定追踪，从而允许模型从视频流中连续预测抓取轨迹而无需进一步的外部指导。

Result: 广泛的实验显示，SAM2Grasp在杂乱环境中执行多对象抓取任务时能够达到最先进水平的表现。

Conclusion: 通过将机器人抓取任务重新定义为基于提示条件的单模态预测问题，SAM2Grasp成功解决了传统模仿学习中存在的多模态问题。该方法不仅提高了在复杂多物体环境下的抓取精度，还展示了如何有效地利用现有强大视觉模型的能力来增强机器人的自主性。

Abstract: Imitation learning for robotic grasping is often plagued by the multimodal problem: when a scene contains multiple valid targets, demonstrations of grasping different objects create conflicting training signals. Standard imitation learning policies fail by averaging these distinct actions into a single, invalid action. In this paper, we introduce SAM2Grasp, a novel framework that resolves this issue by reformulating the task as a uni-modal, prompt-conditioned prediction problem. Our method leverages the frozen SAM2 model to use its powerful visual temporal tracking capability and introduces a lightweight, trainable action head that operates in parallel with its native segmentation head. This design allows for training only the small action head on pre-computed temporal-visual features from SAM2. During inference, an initial prompt, such as a bounding box provided by an upstream object detection model, designates the specific object to be grasped. This prompt conditions the action head to predict a unique, unambiguous grasp trajectory for that object alone. In all subsequent video frames, SAM2's built-in temporal tracking capability automatically maintains stable tracking of the selected object, enabling our model to continuously predict the grasp trajectory from the video stream without further external guidance. This temporal-prompted approach effectively eliminates ambiguity from the visuomotor policy. We demonstrate through extensive experiments that SAM2Grasp achieves state-of-the-art performance in cluttered, multi-object grasping tasks.

</details>


### [7] [RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning](https://arxiv.org/abs/2512.02729)
*Yuhong Zhang,Zihan Gao,Shengpeng Li,Ling-Hao Chen,Kaisheng Liu,Runqing Cheng,Xiao Lin,Junjia Liu,Zhuoheng Li,Jingyi Feng,Ziyan He,Jintian Lin,Zheyan Huang,Zhifang Liu,Haoqian Wang*

Main category: cs.RO

TL;DR: Robowheel是一个数据引擎，能够将人类手部与物体互动的视频转换为跨形态机器人学习的训练准备监督。该系统从单目RGB或RGB-D输入开始，通过高精度重建和物理合理性强化学习优化器来生成接触丰富的轨迹，并将其重定向到不同形态的机器人上。整个数据处理流程包括从视频采集、重建、重定向到数据增强。在主流视觉语言动作和模仿学习架构上的验证表明，该系统的轨迹稳定度可媲美远程操作，并且能产生相当的持续性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了提高跨形态机器人学习的数据可用性和多样性，同时降低数据收集的成本和复杂性。

Method: 采用高精度的人类手部与物体互动（HOI）重建技术，并通过强化学习优化器确保物理合理性；然后将这些轨迹重定向至不同类型的机器人平台；最后利用Isaac Sim模拟器进行数据增强以扩展覆盖范围。

Result: 证明了通过Robowheel产生的轨迹不仅稳定性接近于通过远程操作获得的结果，而且还能带来类似的持续性能改进。此外，还构建了一个大规模多模态数据集用于训练和评估实体模型。

Conclusion: Robowheel展示了如何有效地使用人类手部-物体交互作为机器人学习的有效监督形式，相比传统远程操作方法更为轻便灵活。

Abstract: We introduce Robowheel, a data engine that converts human hand object interaction (HOI) videos into training-ready supervision for cross morphology robotic learning. From monocular RGB or RGB-D inputs, we perform high precision HOI reconstruction and enforce physical plausibility via a reinforcement learning (RL) optimizer that refines hand object relative poses under contact and penetration constraints. The reconstructed, contact rich trajectories are then retargeted to cross-embodiments, robot arms with simple end effectors, dexterous hands, and humanoids, yielding executable actions and rollouts. To scale coverage, we build a simulation-augmented framework on Isaac Sim with diverse domain randomization (embodiments, trajectories, object retrieval, background textures, hand motion mirroring), which enriches the distributions of trajectories and observations while preserving spatial relationships and physical plausibility. The entire data pipeline forms an end to end pipeline from video,reconstruction,retargeting,augmentation data acquisition. We validate the data on mainstream vision language action (VLA) and imitation learning architectures, demonstrating that trajectories produced by our pipeline are as stable as those from teleoperation and yield comparable continual performance gains. To our knowledge, this provides the first quantitative evidence that HOI modalities can serve as effective supervision for robotic learning. Compared with teleoperation, Robowheel is lightweight, a single monocular RGB(D) camera is sufficient to extract a universal, embodiment agnostic motion representation that could be flexibly retargeted across embodiments. We further assemble a large scale multimodal dataset combining multi-camera captures, monocular videos, and public HOI corpora for training and evaluating embodied models.

</details>


### [8] [CogDrive: Cognition-Driven Multimodal Prediction-Planning Fusion for Safe Autonomy](https://arxiv.org/abs/2512.02777)
*Heye Huang,Yibin Yang,Mingfeng Fan,Haoran Wang,Xiaocong Zhao,Jianqiang Wang*

Main category: cs.RO

TL;DR: CogDrive提出了一个认知驱动的多模态预测和规划框架，通过结合显式的模式推理与安全意识轨迹优化，改善了在混合交通中自动驾驶的安全性和适应性。


<details>
  <summary>Details</summary>
Motivation: 为了克服基于学习的方法难以捕捉罕见但关键的安全行为以及基于规则的系统缺乏在复杂交互中的适应性的局限性。

Method: 采用基于拓扑运动语义和最近邻关系编码的认知表示方法来预测模块；使用可微分模式损失和多模态高斯解码学习稀疏且不平衡的交互行为；规划模块引入紧急响应概念，优化安全稳定轨迹。

Result: 在Argoverse2和INTERACTION数据集上的实验表明，CogDrive在轨迹准确性和漏检率方面表现出色，并且闭环仿真确认了其在合并和交叉口场景中的自适应行为。

Conclusion: 通过结合认知多模态预测与面向安全的规划，CogDrive为复杂交通中的安全自主提供了一个可解释且可靠的范式。

Abstract: Safe autonomous driving in mixed traffic requires a unified understanding of multimodal interactions and dynamic planning under uncertainty. Existing learning based approaches struggle to capture rare but safety critical behaviors, while rule based systems often lack adaptability in complex interactions. To address these limitations, CogDrive introduces a cognition driven multimodal prediction and planning framework that integrates explicit modal reasoning with safety aware trajectory optimization. The prediction module adopts cognitive representations of interaction modes based on topological motion semantics and nearest neighbor relational encoding. With a differentiable modal loss and multimodal Gaussian decoding, CogDrive learns sparse and unbalanced interaction behaviors and improves long horizon trajectory prediction. The planning module incorporates an emergency response concept and optimizes safety stabilized trajectories, where short term consistent branches ensure safety during replanning cycles and long term branches support smooth and collision free motion under low probability switching modes. Experiments on Argoverse2 and INTERACTION datasets show that CogDrive achieves strong performance in trajectory accuracy and miss rate, while closed loop simulations confirm adaptive behavior in merge and intersection scenarios. By combining cognitive multimodal prediction with safety oriented planning, CogDrive offers an interpretable and reliable paradigm for safe autonomy in complex traffic.

</details>


### [9] [Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols](https://arxiv.org/abs/2512.02787)
*Xianchao Zeng,Xinyu Zhou,Youcheng Li,Jiayou Shi,Tianle Li,Liangming Chen,Lei Ren,Yong-Lu Li*

Main category: cs.RO

TL;DR: ViFailback, a new framework for diagnosing robotic manipulation failures and providing correction guidance, comes with an extensive dataset and benchmark. It enhances the performance of Vision-Language Models (VLMs) in failure diagnosis and correction, as evidenced by real-world experiments.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language-Action (VLA) models have limitations in diagnosing failures and learning from them, and existing failure datasets are mainly generated in simulation, which hinders their applicability to real-world scenarios.

Method: The introduction of ViFailback, a framework that diagnoses robotic manipulation failures and provides textual and visual correction advice, enhanced by explicit visual symbols for more efficient annotation. The release of the ViFailback dataset, featuring 58,126 VQA pairs and 5,202 real-world manipulation trajectories, along with the establishment of ViFailback-Bench, a benchmark for evaluating VLMs' failure diagnosis and correction capabilities.

Result: ViFailback-8B VLM demonstrates significant overall performance improvement on ViFailback-Bench, capable of generating visual symbols for corrective action. Real-world robotic experiments show its effectiveness in assisting VLA models to recover from failures.

Conclusion: ViFailback offers a comprehensive solution for enhancing the failure diagnosis and recovery capabilities of VLMs in the context of robotic manipulation, supported by a large-scale, real-world oriented dataset and a dedicated evaluation benchmark.

Abstract: Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic manipulation, yet they remain limited in failure diagnosis and learning from failures. Additionally, existing failure datasets are mostly generated programmatically in simulation, which limits their generalization to the real world. In light of these, we introduce ViFailback, a framework designed to diagnose robotic manipulation failures and provide both textual and visual correction guidance. Our framework utilizes explicit visual symbols to enhance annotation efficiency. We further release the ViFailback dataset, a large-scale collection of 58,126 Visual Question Answering (VQA) pairs along with their corresponding 5,202 real-world manipulation trajectories. Based on the dataset, we establish ViFailback-Bench, a benchmark of 11 fine-grained VQA tasks designed to assess the failure diagnosis and correction abilities of Vision-Language Models (VLMs), featuring ViFailback-Bench Lite for closed-ended and ViFailback-Bench Hard for open-ended evaluation. To demonstrate the effectiveness of our framework, we built the ViFailback-8B VLM, which not only achieves significant overall performance improvement on ViFailback-Bench but also generates visual symbols for corrective action guidance. Finally, by integrating ViFailback-8B with a VLA model, we conduct real-world robotic experiments demonstrating its ability to assist the VLA model in recovering from failures. Project Website: https://x1nyuzhou.github.io/vifailback.github.io/

</details>


### [10] [Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach](https://arxiv.org/abs/2512.02834)
*Siyuan Yang,Yang Zhang,Haoran He,Ling Pan,Xiu Li,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 本文提出了一种名为TACO的测试时缩放框架，该框架通过应用轻量级伪计数估计器作为动作块的高保真验证器，从而提高视觉-语言-动作模型在下游任务中的推理稳定性和成功率。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作（VLA）模型在从大规模多模态数据集中学习复杂行为方面表现出色，但在针对特定任务进行微调后，由于预训练阶段纳入了多样化的数据模式以及微调数据集通常包含以运动学上不理想或不希望的方式收集的演示数据，导致存在与下游任务成功无关的多余动作模式。这使得VLA模型在监督微调后对于各种采样噪声表现出关键的推理时间脆弱性。

Method: 提出了TACO（Test-time-scaling），一种测试时缩放框架，它使用轻量级伪计数估计器来验证动作块，并允许VLA模型执行所有采样动作块中具有最大伪计数的动作，以此防止分布偏移同时保持VLA模型的泛化能力。此方法类似于离线强化学习中的经典反探索原则，并且无需梯度更新，因此相比RL更新提供了显著的计算优势。

Result: 广泛的实验表明，在四个模拟基准（RoboTwin2.0, Robotwin, LIBERO, SimplerEnv）和一个双臂平台上，所提方法能够显著改善下游任务适应过程中的推理稳定性和成功率。

Conclusion: 通过引入TACO框架，可以有效解决VLA模型因分布偏移引起的问题，增强其对下游任务的适应性及稳定性。

Abstract: Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.

</details>


### [11] [VLM as Strategist: Adaptive Generation of Safety-critical Testing Scenarios via Guided Diffusion](https://arxiv.org/abs/2512.02844)
*Xinzheng Wu,Junyi Chen,Naiting Zhong,Yong Shen*

Main category: cs.RO

TL;DR: 本文提出了一种结合视觉语言模型和自适应引导扩散模型的安全关键测试场景生成框架，以解决现有方法在构建长尾场景时面临的挑战。该框架能够高效生成真实、多样且高度互动的安全关键测试场景。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶系统测试场景生成方法难以有效地构建确保逼真性、关键性和交互性的长尾场景，并且缺乏对被测车辆的实时动态响应能力。

Method: 提出了一个三层架构的安全关键测试场景生成框架，包括由视觉语言模型指导的目标确定策略层、设计导向函数的战术层以及执行引导扩散的操作层。首先建立了一个学习真实驾驶场景数据分布的基础扩散模型，接着设计了允许闭环模拟中背景车辆实时精确控制的自适应引导扩散方法，并通过深度场景理解和风险推理来自动生成场景生成目标和导向函数。

Result: 实验结果表明，所提出的方法可以有效生成现实、多样化且高度互动的安全关键测试场景。案例研究也验证了该方法的适应性和由视觉语言模型指导的生成性能。

Conclusion: 本研究提出的框架成功解决了安全关键测试场景生成中的主要挑战，为自动驾驶系统的综合测试与评估提供了新的解决方案。

Abstract: The safe deployment of autonomous driving systems (ADSs) relies on comprehensive testing and evaluation. However, safety-critical scenarios that can effectively expose system vulnerabilities are extremely sparse in the real world. Existing scenario generation methods face challenges in efficiently constructing long-tail scenarios that ensure fidelity, criticality, and interactivity, while particularly lacking real-time dynamic response capabilities to the vehicle under test (VUT). To address these challenges, this paper proposes a safety-critical testing scenario generation framework that integrates the high-level semantic understanding capabilities of Vision Language Models (VLMs) with the fine-grained generation capabilities of adaptive guided diffusion models. The framework establishes a three-layer hierarchical architecture comprising a strategic layer for VLM-directed scenario generation objective determination, a tactical layer for guidance function formulation, and an operational layer for guided diffusion execution. We first establish a high-quality fundamental diffusion model that learns the data distribution of real driving scenarios. Next, we design an adaptive guided diffusion method that enables real-time, precise control of background vehicles (BVs) in closed-loop simulation. The VLM is then incorporated to autonomously generate scenario generation objectives and guidance functions through deep scenario understanding and risk reasoning, ultimately guiding the diffusion model to achieve VLM-directed scenario generation. Experimental results demonstrate that the proposed method can efficiently generate realistic, diverse, and highly interactive safety-critical testing scenarios. Furthermore, case studies validate the adaptability and VLM-directed generation performance of the proposed method.

</details>


### [12] [SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots](https://arxiv.org/abs/2512.02851)
*Iana Zhura,Sausar Karaf,Faryal Batool,Nipun Dhananjaya Weerakkodi Mudalige,Valerii Serpiva,Ali Alridha Abdulkarim,Aleksey Fedoseev,Didar Seyidov,Amjad Hajira,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 提出了一种名为SwarmDiffusion的轻量级端到端扩散模型，它可以从单个RGB图像中同时预测可穿越性和生成可行轨迹。该方法不需要人工标注或规划器生成路径，并且能够跨不同机器人平台提供物理一致的可穿越路径。


<details>
  <summary>Details</summary>
Motivation: 现有的基于VLM的方法依赖于手工制作的提示，在不同实体间泛化能力差，只能输出可穿越性地图，而将轨迹生成留给外部规划器处理。为了解决这些问题，提出了一个新的方法。

Method: 引入了一个无规划器的轨迹构建流程，基于随机航点采样、贝塞尔平滑和正则化强制连接性、安全性、方向性和路径细度。SwarmDiffusion利用VLM衍生的监督，无需提示工程，并将扩散过程条件化在一个紧凑的实体状态上。

Result: 在室内环境和两种实体（四足和空中）上，该方法达到了80-100%的导航成功率和0.09秒的推理时间。仅使用额外500个视觉样本即可适应新机器人。在模拟和实际试验中均可靠地推广到了未见过的环境。

Conclusion: 提供了一种可扩展的、无需提示的方法来统一进行可穿越性推理和轨迹生成。

Abstract: Visual traversability estimation is critical for autonomous navigation, but existing VLM-based methods rely on hand-crafted prompts, generalize poorly across embodiments, and output only traversability maps, leaving trajectory generation to slow external planners. We propose SwarmDiffusion, a lightweight end-to-end diffusion model that jointly predicts traversability and generates a feasible trajectory from a single RGB image. To remove the need for annotated or planner-produced paths, we introduce a planner-free trajectory construction pipeline based on randomized waypoint sampling, Bezier smoothing, and regularization enforcing connectivity, safety, directionality, and path thinness. This enables learning stable motion priors without demonstrations. SwarmDiffusion leverages VLM-derived supervision without prompt engineering and conditions the diffusion process on a compact embodiment state, producing physically consistent, traversable paths that transfer across different robot platforms. Across indoor environments and two embodiments (quadruped and aerial), the method achieves 80-100\% navigation success and 0.09 s inference, and adapts to a new robot using only-500 additional visual samples. It generalizes reliably to unseen environments in simulation and real-world trials, offering a scalable, prompt-free approach to unified traversability reasoning and trajectory generation.

</details>


### [13] [VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling](https://arxiv.org/abs/2512.02902)
*Weiqi Li,Quande Zhang,Ruifeng Zhai,Liang Lin,Guangrun Wang*

Main category: cs.RO

TL;DR: 本文针对视觉-语言-动作(VLA)模型在新摄像头视角和视觉干扰下的性能下降问题，提出了一种单次适应框架，通过轻量级可学习更新来重新校准视觉表示。提出的两种方法（特征令牌调制FTM和特征线性适应FLA）有效提升了模型的鲁棒性和视角泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作(VLA)模型虽然在已知分布上表现良好，但在面对新的摄像头视角或视觉干扰时性能显著下降。这种脆弱性主要源于空间建模中的不匹配问题而非物理建模。为解决这一挑战，作者探索了如何通过最小化的视觉适应手段恢复模型对不同视角的泛化能力。

Method: 提出了一个单次适应框架，包括两种方法：1. 特征令牌调制(FTM)，通过对视觉令牌施加全局仿射变换以改善视角准确性；2. 在此基础上，特征线性适应(FLA)引入了ViT编码器的低秩更新，进一步提高了性能同时保持较低的成本。

Result: 实验表明，使用仅4K参数的FTM方法能够将Libero视角准确率从48.5%提升至87.1%，而采用4.7M参数的FLA则达到了90.8%的成功率，几乎与LoRA规模微调效果相当但成本更低。

Conclusion: 研究揭示了预训练VLA模型中存在大量未被充分利用的鲁棒性，并证明了通过针对性的小范围视觉调整足以恢复模型对于不同视角的泛化能力。

Abstract: Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5% to 87.1% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.

</details>


### [14] [Experimental Characterization of Fingertip Trajectory following for a 3-DoF Series-Parallel Hybrid Robotic Finger](https://arxiv.org/abs/2512.02951)
*Nicholas Baiata,Nilanjan Chakraborty*

Main category: cs.RO

TL;DR: 本文介绍了具有解析正向运动学和闭式雅可比矩阵的三自由度连杆驱动串联-并联机器人手指的物理原型设计及其实验表征，并通过实验评估了指尖在各种轨迹上的跟踪性能，实现了毫米级精度的任务空间轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 任务空间控制对于灵巧操作至关重要，因为操纵目标最自然地是以指尖运动和施加力来指定，而不是单个关节角度。尽管对于较大的臂级操纵器来说，任务空间规划和控制已经得到了广泛研究，但在紧凑型多自由度机器人手指中实现精确的任务空间轨迹跟踪仍较为少见。

Method: 本文提出了一种三自由度、连杆驱动、串联-并联结构的机器人手指，该手指具备分析性的正向运动学解和闭式雅克比矩阵。为了达成闭环任务空间轨迹跟踪的目标，实施了解决运动速率控制（RMRC）方案。

Result: 通过对直线、圆以及更复杂的曲线等多种轨迹进行实验评估，证明了该机器人手指能够达到毫米级别的准确度。

Conclusion: 据我们所知，这项工作是首批系统性实验演示之一，展示了连杆驱动的机器人手指中精确的任务空间轨迹跟踪能力，为未来旨在提高手内灵巧操控的设计树立了基准。

Abstract: Task-space control of robotic fingers is a critical enabler of dexterous manipulation, as manipulation objectives are most naturally specified in terms of fingertip motions and applied forces rather than individual joint angles. While task-space planning and control have been extensively studied for larger, arm-scale manipulators, demonstrations of precise task-space trajectory tracking in compact, multi-DoF robotic fingers remain scarce. In this paper, we present the physical prototyping and experimental characterization of a three-degree-of-freedom, linkage-driven, series-parallel robotic finger with analytic forward kinematics and a closed-form Jacobian. A resolved motion rate control (RMRC) scheme is implemented to achieve closed-loop task-space trajectory tracking. We experimentally evaluate the fingertip tracking performance across a variety of trajectories, including straight lines, circles, and more complex curves, and report millimeter-level accuracy. To the best of our knowledge, this work provides one of the first systematic experimental demonstrations of precise task-space trajectory tracking in a linkage-driven robotic finger, thereby establishing a benchmark for future designs aimed at dexterous in-hand manipulation.

</details>
