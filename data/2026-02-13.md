<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 28]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Mitigating Error Accumulation in Continuous Navigation via Memory-Augmented Kalman Filtering](https://arxiv.org/abs/2602.11183)
*Yin Tang,Jiawei Ma,Jinrui Zhang,Alex Jinpeng Wang,Deyu Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为NeuroKalman的新框架，通过将导航解耦为基于运动动力学的先验预测和基于历史观测的可能性校正两个互补过程来解决无人机在复杂环境连续导航中出现的状态漂移问题。实验表明，仅用10%的训练数据微调，该方法就能明显优于强大的基线模型，并有效控制了漂移累积。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言导航（VLN）模型采用航位推算的方式逐步更新位置以预测下一个路点，这种方法随着时间推移会导致位置误差累积，造成内部估计与实际坐标之间错位的问题，即“状态漂移”，进而影响整个轨迹预测准确性。

Method: 受经典控制理论启发，作者们将这一序列预测问题形式化为一个递归贝叶斯状态估计问题，并设计了一个名为NeuroKalman的新框架。它将导航分解成两部分：基于运动动态的先验预测和基于历史观察的可能性修正。其中，利用基于注意力机制的检索机制数学上关联测量似然性的核密度估计，允许系统在没有梯度更新的情况下使用检索到的历史锚点来修正潜在表示。

Result: 在TravelUAV基准测试中的全面实验证明，即使只对10%的训练数据进行微调，所提出的方法也显著优于强大的基线，并能有效地管理漂移累积现象。

Conclusion: NeuroKalman提供了一种创新的方法来解决无人机导航中的状态漂移问题，通过结合运动模型预测与历史信息校正两种机制，成功地提高了导航精度并减少了累积误差。

Abstract: Continuous navigation in complex environments is critical for Unmanned Aerial Vehicle (UAV). However, the existing Vision-Language Navigation (VLN) models follow the dead-reckoning, which iteratively updates its position for the next waypoint prediction, and subsequently construct the complete trajectory. Then, such stepwise manner will inevitably lead to accumulated errors of position over time, resulting in misalignment between internal belief and objective coordinates, which is known as "state drift" and ultimately compromises the full trajectory prediction. Drawing inspiration from classical control theory, we propose to correct for errors by formulating such sequential prediction as a recursive Bayesian state estimation problem. In this paper, we design NeuroKalman, a novel framework that decouples navigation into two complementary processes: a Prior Prediction, based on motion dynamics and a Likelihood Correction, from historical observation. We first mathematically associate Kernel Density Estimation of the measurement likelihood with the attention-based retrieval mechanism, which then allows the system to rectify the latent representation using retrieved historical anchors without gradient updates. Comprehensive experiments on TravelUAV benchmark demonstrate that, with only 10% of the training data fine-tuning, our method clearly outperforms strong baselines and regulates drift accumulation.

</details>


### [2] [H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model](https://arxiv.org/abs/2602.11291)
*Wenyuan Chen,Jinbang Huang,Oscar Pang,Zhiyuan Li,Xiao Hu,Lingfeng Zhang,Zhanguang Zhang,Mark Coates,Tongtong Cao,Xingyue Quan,Yingxue Zhang*

Main category: cs.RO

TL;DR: 提出了一种层次化世界模型（H-WM），该模型在一个统一的双层框架内联合预测逻辑和视觉状态转换，结合了高层逻辑世界模型与低层视觉世界模型，从而将符号推理的机器人可执行性和长时域鲁棒性与来自视觉观察的感知基础相结合。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常侧重于视频生成或自然语言预测，这些难以直接与机器人动作联系起来，并且在长时间范围内会遇到错误累积的问题。传统任务和运动规划依赖于符号逻辑世界模型，但这些方法通常独立于视觉感知操作，无法实现符号和感知状态预测的同步。

Method: 通过引入一个使机器人动作与符号状态、动作和视觉观察对齐的数据集来训练H-WM。H-WM将高层次逻辑世界模型与低层次视觉世界模型相结合，在统一的双层框架内共同预测逻辑和视觉状态转换。

Result: 实验表明，这种方法能够有效地为长期任务提供稳定一致的中间指导，减少错误累积，并允许在扩展的任务序列中稳健执行。

Conclusion: H-WM通过整合符号逻辑与视觉感知的优点，为机器人提供了更加强大和通用的世界模型解决方案，特别适用于需要长时间范围推理的任务。

Abstract: World models are becoming central to robotic planning and control, as they enable prediction of future state transitions. Existing approaches often emphasize video generation or natural language prediction, which are difficult to directly ground in robot actions and suffer from compounding errors over long horizons. Traditional task and motion planning relies on symbolic logic world models, such as planning domains, that are robot-executable and robust for long-horizon reasoning. However, these methods typically operate independently of visual perception, preventing synchronized symbolic and perceptual state prediction. We propose a Hierarchical World Model (H-WM) that jointly predicts logical and visual state transitions within a unified bilevel framework. H-WM combines a high-level logical world model with a low-level visual world model, integrating the robot-executable, long-horizon robustness of symbolic reasoning with perceptual grounding from visual observations. The hierarchical outputs provide stable and consistent intermediate guidance for long-horizon tasks, mitigating error accumulation and enabling robust execution across extended task sequences. To train H-WM, we introduce a robotic dataset that aligns robot motion with symbolic states, actions, and visual observations. Experiments across vision-language-action (VLA) control policies demonstrate the effectiveness and generality of the approach.

</details>


### [3] [ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control](https://arxiv.org/abs/2602.11321)
*Ziyan Xiong,Lixing Fang,Junyun Huang,Kashu Yamazaki,Hao Zhang,Chuang Gan*

Main category: cs.RO

TL;DR: 提出了一种低延迟全身控制框架ExtremControl，通过直接操作选定刚性环节的SE(3)姿态、采用笛卡尔空间映射和引入速度前馈控制来减少延迟，实现了最低50ms的端到端延迟，支持光学动作捕捉与VR运动追踪，使得机器人能够执行如乒乓球平衡、杂耍等需要快速反馈的任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于高度预处理的人类到人形机器人的运动重定向及仅基于位置的PD控制，导致了显著的延迟问题，这严重限制了响应性和防止了对快速反馈与迅速反应的需求。为了克服这一挑战，研究者们提出了一个旨在减少延迟的新系统。

Method: 1. 直接操控选定刚体部件（主要为人形机器人四肢）的SE(3)位姿，避免全身体运动适配；2. 利用笛卡尔空间映射将人类运动直接转换成人形机器人链接目标；3. 在底层控制中加入速度前馈控制以支持在快速变化控制界面下的高响应行为。

Result: 通过仿真与真实环境中的实验验证了ExtremControl的有效性。基于此框架构建了一个低延迟的人形远程操作系统，支持光学动作捕捉和基于VR的动作跟踪，实现了低至50毫秒的端到端延迟，并且能够完成像乒乓球平衡、抛接球以及实时返回这样需要极高响应性的任务。

Conclusion: ExtremControl作为一种创新的低延迟控制策略，不仅显著降低了人形机器人遥操作系统的延迟，还展示了其在实现复杂动态任务方面的潜力。

Abstract: Building a low-latency humanoid teleoperation system is essential for collecting diverse reactive and dynamic demonstrations. However, existing approaches rely on heavily pre-processed human-to-humanoid motion retargeting and position-only PD control, resulting in substantial latency that severely limits responsiveness and prevents tasks requiring rapid feedback and fast reactions. To address this problem, we propose ExtremControl, a low latency whole-body control framework that: (1) operates directly on SE(3) poses of selected rigid links, primarily humanoid extremities, to avoid full-body retargeting; (2) utilizes a Cartesian-space mapping to directly convert human motion to humanoid link targets; and (3) incorporates velocity feedforward control at low level to support highly responsive behavior under rapidly changing control interfaces. We further provide a unified theoretical formulation of ExtremControl and systematically validate its effectiveness through experiments in both simulation and real-world environments. Building on ExtremControl, we implement a low-latency humanoid teleoperation system that supports both optical motion capture and VR-based motion tracking, achieving end-to-end latency as low as 50ms and enabling highly responsive behaviors such as ping-pong ball balancing, juggling, and real-time return, thereby substantially surpassing the 200ms latency limit observed in prior work.

</details>


### [4] [MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation](https://arxiv.org/abs/2602.11337)
*Yejin Kim,Wilbert Pumacay,Omar Rayyan,Max Argus,Winson Han,Eli VanderBilt,Jordi Salvador,Abhay Deshpande,Rose Hendrix,Snehal Jauhri,Shuo Liu,Nur Muhammad Mahi Shafiullah,Maya Guru,Ainaz Eftekhar,Karen Farley,Donovan Clay,Jiafei Duan,Arjun Guru,Piper Wolters,Alvaro Herrasti,Ying-Chun Lee,Georgia Chalvatzaki,Yuchen Cui,Ali Farhadi,Dieter Fox,Ranjay Krishna*

Main category: cs.RO

TL;DR: MolmoSpaces是一个开放的生态系统，旨在支持大规模机器人策略基准测试。它包含超过23万个多样化的室内环境和13万个带有丰富注释的对象资产，并且是模拟器无关的。通过MolmoSpaces-Bench这一基准测试套件，研究者能够评估机器人的性能，实验显示该基准具有强仿真到现实的相关性。


<details>
  <summary>Details</summary>
Motivation: 部署大规模机器人需要应对日常环境中各种各样的情况变化，而现有机器人基准未能充分代表这些变化。为了测量这种级别的泛化能力，需要比纯物理评估所能提供的更大规模和更多样性的基础设施。

Method: 开发了MolmoSpaces，一个包括大量多样化室内环境及富含注解对象资产的开放生态系统；设计了MolmoSpaces-Bench作为一套8项任务的基准测试集来让机器人与这些多样的场景和对象互动。

Result: 实验证明MolmoSpaces-Bench展示出很强的从模拟到真实的关联性（R=0.96, ρ=0.98），确认了新零样本策略在基准测试中优于早期版本，并揭示了对提示措辞、初始关节位置以及摄像机遮挡的关键敏感度。

Conclusion: 通过MolmoSpaces及其开源资产和工具，为机器人学习研究提供了可扩展的数据生成、策略训练和基准创建的基础。

Abstract: Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies. MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks: static and mobile manipulation, navigation, and multiroom long-horizon tasks requiring coordinated perception, planning, and interaction across entire indoor environments. We also design MolmoSpaces-Bench, a benchmark suite of 8 tasks in which robots interact with our diverse scenes and richly annotated objects. Our experiments show MolmoSpaces-Bench exhibits strong sim-to-real correlation (R = 0.96, \r{ho} = 0.98), confirm newer and stronger zero-shot policies outperform earlier versions in our benchmarks, and identify key sensitivities to prompt phrasing, initial joint positions, and camera occlusion. Through MolmoSpaces and its open-source assets and tooling, we provide a foundation for scalable data generation, policy training, and benchmark creation for robot learning research.

</details>


### [5] [Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video](https://arxiv.org/abs/2602.11393)
*Mrinal Verghese,Christopher G. Atkeson*

Main category: cs.RO

TL;DR: 本文提出了一种从以自我为中心的人类视频中学习机器人行为的方法，通过预测连续图像之间的跟踪点运动来建模人类偏好，并定义奖励函数为预测与观察到的对象运动之间的一致性。使用改进的SAC算法初始化10个在机器人上的演示，以估计价值函数并优化最大化该价值函数的策略。实验表明，所学策略在仿真和真实机器人上多个任务中的表现优于或等同于先前的工作。


<details>
  <summary>Details</summary>
Motivation: 现有从人类视频中学得奖励的方法存在一些限制其性能的假设，并且需要跨越实体和环境差距转移学习的价值函数。为了克服这些限制，作者提出了一种新的方法，直接在机器人上根据视频数据学习人类偏好并通过最大化基于此偏好的奖励来优化机器人行为。

Method: 通过学习预测连续图像之间跟踪点的移动来建模人类偏好，并以此为基础定义一个奖励函数。然后采用一种改良版的软演员评论家(SAC)算法，在机器人上利用10次实际演示初始化，基于这个奖励函数估计一个价值函数，并优化旨在最大化此价值函数的行为策略。

Result: 实验结果表明，在模拟和真实机器人环境中，利用该奖励模型学到的策略在多个任务上达到或超过了之前工作的表现。

Conclusion: 本研究提供了一种有效的方法，允许机器人直接从以自我为中心的人类视频中学习，而无需跨域转移学习的价值函数。这种方法能够产生与甚至优于先前技术的结果。

Abstract: We present an approach to robot learning from egocentric human videos by modeling human preferences in a reward function and optimizing robot behavior to maximize this reward. Prior work on reward learning from human videos attempts to measure the long-term value of a visual state as the temporal distance between it and the terminal state in a demonstration video. These approaches make assumptions that limit performance when learning from video. They must also transfer the learned value function across the embodiment and environment gap. Our method models human preferences by learning to predict the motion of tracked points between subsequent images and defines a reward function as the agreement between predicted and observed object motion in a robot's behavior at each step. We then use a modified Soft Actor Critic (SAC) algorithm initialized with 10 on-robot demonstrations to estimate a value function from this reward and optimize a policy that maximizes this value function, all on the robot. Our approach is capable of learning on a real robot, and we show that policies learned with our reward model match or outperform prior work across multiple tasks in both simulation and on the real robot.

</details>


### [6] [EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos](https://arxiv.org/abs/2602.11464)
*Tao Zhang,Song Xia,Ye Wang,Qin Jin*

Main category: cs.RO

TL;DR: 提出了一种低成本且可复制的EasyMimic框架，通过从标准RGB相机捕获的人类视频演示中学习操作策略，减少了对昂贵机器人数据收集的依赖，为智能机器人进入家庭提供了一条实用路径。


<details>
  <summary>Details</summary>
Motivation: 机器人模仿学习通常受到收集大规模真实世界数据成本高昂的限制，特别是对于设计用于家庭使用的低成本机器人来说，这一挑战更为显著。因此，需要一种解决方案来降低成本并提高用户友好性。

Method: 首先从视频中提取3D手部轨迹，然后通过动作对齐模块将这些轨迹映射到低成本机器人的夹爪控制空间。为了弥合人与机器人之间的领域差距，引入了一种简单易用的手部视觉增强策略。接着使用协同训练方法，在处理过的人类数据和少量机器人数据上微调模型，实现快速适应新任务。

Result: 在低成本LeRobot平台上的实验表明，EasyMimic在各种操作任务中实现了高性能，并显著降低了对昂贵机器人数据收集的依赖。

Conclusion: EasyMimic框架为低成本机器人提供了快速学习人类演示的能力，同时减少了对大量昂贵数据的需求，推动了智能机器人在家用环境中的应用可能性。

Abstract: Robot imitation learning is often hindered by the high cost of collecting large-scale, real-world data. This challenge is especially significant for low-cost robots designed for home use, as they must be both user-friendly and affordable. To address this, we propose the EasyMimic framework, a low-cost and replicable solution that enables robots to quickly learn manipulation policies from human video demonstrations captured with standard RGB cameras. Our method first extracts 3D hand trajectories from the videos. An action alignment module then maps these trajectories to the gripper control space of a low-cost robot. To bridge the human-to-robot domain gap, we introduce a simple and user-friendly hand visual augmentation strategy. We then use a co-training method, fine-tuning a model on both the processed human data and a small amount of robot data, enabling rapid adaptation to new tasks. Experiments on the low-cost LeRobot platform demonstrate that EasyMimic achieves high performance across various manipulation tasks. It significantly reduces the reliance on expensive robot data collection, offering a practical path for bringing intelligent robots into homes. Project website: https://zt375356.github.io/EasyMimic-Project/.

</details>


### [7] [Effective Task Planning with Missing Objects using Learning-Informed Object Search](https://arxiv.org/abs/2602.11468)
*Raihan Islam Arnob,Max Merlin,Abhishek Paudel,Benned Hedegaard,George Konidaris,Gregory Stein*

Main category: cs.RO

TL;DR: 本文提出了一种新的基于模型的LIOS动作规划框架，能够处理任务关键对象位置未知的情况，并在模拟和真实环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的任务规划方法通常假设环境信息完全已知，这使得当任务关键对象的位置未知时，这些方法（如PDDL）无法有效进行规划。此外，虽然最近基于学习的对象搜索方法表现良好，但它们作为独立工具存在，难以直接集成到需要同时确定必要对象及其查找时机的整体任务规划器中。

Method: 为解决上述限制，作者们开发了一种以新颖的基于模型的LIOS动作为核心的规划框架。每个LIOS动作都是一个旨在找到并取回单个物体的策略。高层次规划将LIOS动作视为确定性的——基于对每个动作预期成本的建模计算来生成计划——从而在不确定性条件下实现有效的、合理且完整的结合学习的任务规划。

Result: 实验结果表明，在ProcTHOR家庭模拟环境以及现实世界中，该方法在包括检索和餐前准备在内的任务上优于非学习基线方法及仅使用学习的方法。

Conclusion: 这项工作有效地解决了不确定性问题，同时保持了与现有全知识求解器的兼容性，展示了其在面对部分未知环境时执行复杂任务的强大能力。

Abstract: Task planning for mobile robots often assumes full environment knowledge and so popular approaches, like planning via the PDDL, cannot plan when the locations of task-critical objects are unknown. Recent learning-driven object search approaches are effective, but operate as standalone tools and so are not straightforwardly incorporated into full task planners, which must additionally determine both what objects are necessary and when in the plan they should be sought out. To address this limitation, we develop a planning framework centered around novel model-based LIOS actions: each a policy that aims to find and retrieve a single object. High-level planning treats LIOS actions as deterministic and so -- informed by model-based calculations of the expected cost of each -- generates plans that interleave search and execution for effective, sound, and complete learning-informed task planning despite uncertainty. Our work effectively reasons about uncertainty while maintaining compatibility with existing full-knowledge solvers. In simulated ProcTHOR homes and in the real world, our approach outperforms non-learned and learned baselines on tasks including retrieval and meal prep.

</details>


### [8] [HyperDet: 3D Object Detection with Hyper 4D Radar Point Clouds](https://arxiv.org/abs/2602.11554)
*Yichun Xiao,Runwei Guan,Fangqiang Ding*

Main category: cs.RO

TL;DR: HyperDet, a novel 4D radar point cloud processing framework, improves 3D detection accuracy by enhancing radar data quality and integrating it with LiDAR-oriented detectors, thereby narrowing the performance gap between radar and LiDAR systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the 3D detection capability of 4D mmWave radar, which is more cost-effective and weather-robust than LiDAR, but currently lags behind in terms of detection accuracy due to sparse, irregular, and noisy radar point clouds.

Method: HyperDet constructs a hyper 4D radar point cloud that is optimized for standard LiDAR-based detectors. It aggregates radar returns from multiple frames and sensors, applies a cross-sensor validation to remove inconsistent points, and uses a diffusion module to densify object structures while incorporating radar-specific attributes. The model is trained with mixed radar-LiDAR supervision and distilled into a consistency model for efficient inference.

Result: The results show that HyperDet can consistently enhance the performance of 3D detection when using radar-only inputs, as evidenced by improved detection on the MAN TruckScenes dataset with two different detector models (VoxelNeXt and CenterPoint).

Conclusion: The study concludes that by refining the input-level radar data, HyperDet can effectively leverage existing LiDAR-oriented detectors, improving radar's 3D detection capabilities without requiring changes to the detector architecture.

Abstract: 4D mmWave radar provides weather-robust, velocity-aware measurements and is more cost-effective than LiDAR. However, radar-only 3D detection still trails LiDAR-based systems because radar point clouds are sparse, irregular, and often corrupted by multipath noise, yielding weak and unstable geometry. We present HyperDet, a detector-agnostic radar-only 3D detection framework that constructs a task-aware hyper 4D radar point cloud for standard LiDAR-oriented detectors. HyperDet aggregates returns from multiple surround-view 4D radars over consecutive frames to improve coverage and density, then applies geometry-aware cross-sensor consensus validation with a lightweight self-consistency check outside overlap regions to suppress inconsistent returns. It further integrates a foreground-focused diffusion module with training-time mixed radar-LiDAR supervision to densify object structures while lifting radar attributes (e.g., Doppler, RCS); the model is distilled into a consistency model for single-step inference. On MAN TruckScenes, HyperDet consistently improves over raw radar inputs with VoxelNeXt and CenterPoint, partially narrowing the radar-LiDAR gap. These results show that input-level refinement enables radar to better leverage LiDAR-oriented detectors without architectural modifications.

</details>


### [9] [ReaDy-Go: Real-to-Sim Dynamic 3D Gaussian Splatting Simulation for Environment-Specific Visual Navigation with Moving Obstacles](https://arxiv.org/abs/2602.11575)
*Seungyeon Yoo,Youngseok Jang,Dabin Kim,Youngsoo Han,Seungwoo Jung,H. Jin Kim*

Main category: cs.RO

TL;DR: 提出了一种新的真实到模拟的仿真管道ReaDy-Go，能够生成针对目标环境的逼真动态场景，用于训练能够在存在移动障碍物的情况下也能表现良好的导航策略。


<details>
  <summary>Details</summary>
Motivation: 视觉导航模型在现实世界动态环境中经常遇到鲁棒性不足的问题，主要是由于模拟与现实之间的差距以及难以训练适合特定部署环境（如家庭、餐馆和工厂）的策略。尽管使用3D高斯散点技术的真实到模拟导航仿真可以缩小这种差距，但先前的工作通常只考虑静态场景或不真实的动态障碍物。

Method: ReaDy-Go结合了重建的静态GS场景与动态人类GS障碍物，通过三个主要组件实现：1) 动态GS模拟器将场景GS与人类动画模块集成起来；2) 利用该模拟器、为动态GS表示设计的机器人专家规划器及一个人类规划器来生成适用于动态环境的导航数据集；3) 使用生成的数据集进行策略学习。

Result: ReaDy-Go在多个目标环境下表现出优于基线的结果，在模拟和实际实验中均展示了改进的导航性能，即使是在模拟到现实转移后以及存在移动障碍物的情况下也是如此。此外，它还在未见过的环境中实现了零样本的模拟到现实部署，表明其具有泛化潜力。

Conclusion: ReaDy-Go提供了一个有效的方法来克服视觉导航中的sim-to-real问题，并且增强了系统对于动态障碍物的适应能力，从而提高了在不同环境下的导航性能。

Abstract: Visual navigation models often struggle in real-world dynamic environments due to limited robustness to the sim-to-real gap and the difficulty of training policies tailored to target deployment environments (e.g., households, restaurants, and factories). Although real-to-sim navigation simulation using 3D Gaussian Splatting (GS) can mitigate this gap, prior works have assumed only static scenes or unrealistic dynamic obstacles, despite the importance of safe navigation in dynamic environments. To address these issues, we propose ReaDy-Go, a novel real-to-sim simulation pipeline that synthesizes photorealistic dynamic scenarios for target environments. ReaDy-Go generates photorealistic navigation datasets for dynamic environments by combining a reconstructed static GS scene with dynamic human GS obstacles, and trains policies robust to both the sim-to-real gap and moving obstacles. The pipeline consists of three components: (1) a dynamic GS simulator that integrates scene GS with a human animation module, enabling the insertion of animatable human GS avatars and the synthesis of plausible human motions from 2D trajectories, (2) navigation dataset generation for dynamic environments that leverages the simulator, a robot expert planner designed for dynamic GS representations, and a human planner, and (3) policy learning using the generated datasets. ReaDy-Go outperforms baselines across target environments in both simulation and real-world experiments, demonstrating improved navigation performance even after sim-to-real transfer and in the presence of moving obstacles. Moreover, zero-shot sim-to-real deployment in an unseen environment indicates its generalization potential. Project page: https://syeon-yoo.github.io/ready-go-site/.

</details>


### [10] [AC-MASAC: An Attentive Curriculum Learning Framework for Heterogeneous UAV Swarm Coordination](https://arxiv.org/abs/2602.11735)
*Wanhao Liu,Junhong Dai,Yixuan Zhang,Shengyun Yin,Panshuo Li*

Main category: cs.RO

TL;DR: 本文提出了一种基于注意力机制的课程学习框架AC-MASAC，用于解决异构无人机群协同路径规划中的多智能体强化学习问题，特别是在处理不对称的智能体间依赖关系以及训练过程中稀疏奖励和灾难性遗忘的风险方面。通过自定义的多智能体仿真平台验证，该方法在成功率、编队保持率和加权任务时间等方面优于其他先进方法。


<details>
  <summary>Details</summary>
Motivation: 针对异构无人机群协同路径规划中存在的多智能体强化学习挑战，特别是智能体间不对称依赖性的管理，以及训练期间稀疏奖励与灾难性遗忘的问题，需要一种有效的方法来提高系统性能。

Method: 提出了一个注意机制课程学习框架（AC-MASAC），它引入了角色感知的异质注意机制以明确建模不对称依赖性，并设计了一个结构化的课程策略，结合层次知识转移与阶段比例经验回放技术来应对稀疏奖励及灾难性遗忘的问题。

Result: 所提出的AC-MASAC框架在一个定制的多智能体模拟平台上进行了验证，结果表明，在成功率、编队保持率和成功加权的任务时间指标上，该方法相比其他先进方法具有明显优势。

Conclusion: AC-MASAC提供了一种有效的解决方案来改善异构无人机群协同路径规划中遇到的MARL难题，特别是在处理不对称依赖性以及缓解稀疏奖励与灾难性遗忘方面表现出色。

Abstract: Cooperative path planning for heterogeneous UAV swarms poses significant challenges for Multi-Agent Reinforcement Learning (MARL), particularly in handling asymmetric inter-agent dependencies and addressing the risks of sparse rewards and catastrophic forgetting during training. To address these issues, this paper proposes an attentive curriculum learning framework (AC-MASAC). The framework introduces a role-aware heterogeneous attention mechanism to explicitly model asymmetric dependencies. Moreover, a structured curriculum strategy is designed, integrating hierarchical knowledge transfer and stage-proportional experience replay to address the issues of sparse rewards and catastrophic forgetting. The proposed framework is validated on a custom multi-agent simulation platform, and the results show that our method has significant advantages over other advanced methods in terms of Success Rate, Formation Keeping Rate, and Success-weighted Mission Time. The code is available at \textcolor{red}{https://github.com/Wanhao-Liu/AC-MASAC}.

</details>


### [11] [HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model](https://arxiv.org/abs/2602.11758)
*Dongting Li,Xingyu Chen,Qianyang Wu,Bo Chen,Sikai Wu,Hanyu Wu,Guoyao Zhang,Liang Li,Mingliang Zhou,Diyun Xiang,Jianzhu Ma,Qiang Zhang,Renjing Xu*

Main category: cs.RO

TL;DR: 提出了HAIC框架，用于处理人形机器人与欠驱动物体的交互问题，通过动态预测器估计物体状态，并使用非对称微调确保在分布变化下稳健的状态估计。实验表明该方法能有效应对滑板、推拉车等敏捷任务以及跨多种地形搬运箱子等多物体长时任务。


<details>
  <summary>Details</summary>
Motivation: 当前的人-物交互研究主要集中在完全驱动且刚性连接到机器人的物体上，而忽略了具有独立动力学特性和非完整约束的欠驱动物体。这些物体给控制带来了额外挑战，如耦合力和遮挡问题。

Method: HAIC框架包含一个关键组件——动态预测器，它仅基于本体感受历史来估计高阶物体状态（速度、加速度）。这些预测被投射到静态几何先验上，形成空间定位的动态占据地图，使策略能够推断出盲点处的碰撞边界和接触可能性。此外，采用非对称微调技术，让世界模型根据学生策略的探索持续适应，从而在分布变化的情况下也能保证稳定的状态估计。

Result: 实验结果表明，在敏捷任务（如滑板运动、不同负载下的推拉车）中，HAIC通过主动补偿惯性扰动达到了较高的成功率；同时，对于跨多种地形搬运箱子这样的多物体长时任务，HAIC也表现出了良好的性能。

Conclusion: HAIC提供了一个统一的框架，使得人形机器人能够在不依赖外部状态估计的情况下，有效地与各种动力学特性不同的物体进行交互。这不仅解决了欠驱动物体带来的控制难题，还提高了机器人执行复杂全身任务的能力。

Abstract: Humanoid robots show promise for complex whole-body tasks in unstructured environments. Although Human-Object Interaction (HOI) has advanced, most methods focus on fully actuated objects rigidly coupled to the robot, ignoring underactuated objects with independent dynamics and non-holonomic constraints. These introduce control challenges from coupling forces and occlusions. We present HAIC, a unified framework for robust interaction across diverse object dynamics without external state estimation. Our key contribution is a dynamics predictor that estimates high-order object states (velocity, acceleration) solely from proprioceptive history. These predictions are projected onto static geometric priors to form a spatially grounded dynamic occupancy map, enabling the policy to infer collision boundaries and contact affordances in blind spots. We use asymmetric fine-tuning, where a world model continuously adapts to the student policy's exploration, ensuring robust state estimation under distribution shifts. Experiments on a humanoid robot show HAIC achieves high success rates in agile tasks (skateboarding, cart pushing/pulling under various loads) by proactively compensating for inertial perturbations, and also masters multi-object long-horizon tasks like carrying a box across varied terrain by predicting the dynamics of multiple objects.

</details>


### [12] [LAMP: Implicit Language Map for Robot Navigation](https://arxiv.org/abs/2602.11862)
*Sibaek Lee,Hyeonwoo Yu,Giseop Kim,Sunwook Choi*

Main category: cs.RO

TL;DR: 本文提出了一种名为LAMP（语言地图）的新型导航框架，它通过学习连续的语言驱动地图并直接利用其进行精细路径生成，解决了现有方法在大规模环境中内存需求过高和分辨率有限的问题。实验结果表明，LAMP在内存效率和目标定位精度上优于现有的显式方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉-语言模型实现零样本导航的方法存在扩展性问题，尤其是在大环境中的应用受到极大限制，因为它们需要大量的内存来存储每个位置的语言向量，并且难以支持细粒度规划所需的高分辨率。

Method: LAMP采用一种新的神经语言场表示法，不是将语言特征明确地存储于每个位置，而是编码为隐式神经场。该方法结合了稀疏图以支持高效的粗略路径规划，并通过在学习到的场中执行基于梯度的优化来细化接近目标点的姿态。此外，LAMP还引入了一个贝叶斯框架来处理嵌入不确定性，并使用图采样策略优先考虑空间覆盖和嵌入置信度，从而减少计算开销。

Result: 实验结果显示，在NVIDIA Isaac Sim模拟器以及真实的多层建筑测试中，LAMP相比现有显式方法不仅提高了内存使用效率，而且在达到目标的准确性方面表现更佳。

Conclusion: LAMP提供了一种新颖而有效的解决方案，克服了传统方法在处理大规模环境时面临的挑战，通过采用隐式语言映射与基于梯度优化相结合的方式，实现了高效准确的导航。

Abstract: Recent advances in vision-language models have made zero-shot navigation feasible, enabling robots to follow natural language instructions without requiring labeling. However, existing methods that explicitly store language vectors in grid or node-based maps struggle to scale to large environments due to excessive memory requirements and limited resolution for fine-grained planning. We introduce LAMP (Language Map), a novel neural language field-based navigation framework that learns a continuous, language-driven map and directly leverages it for fine-grained path generation. Unlike prior approaches, our method encodes language features as an implicit neural field rather than storing them explicitly at every location. By combining this implicit representation with a sparse graph, LAMP supports efficient coarse path planning and then performs gradient-based optimization in the learned field to refine poses near the goal. This coarse-to-fine pipeline, language-driven, gradient-guided optimization is the first application of an implicit language map for precise path generation. This refinement is particularly effective at selecting goal regions not directly observed by leveraging semantic similarities in the learned feature space. To further enhance robustness, we adopt a Bayesian framework that models embedding uncertainty via the von Mises-Fisher distribution, thereby improving generalization to unobserved regions. To scale to large environments, LAMP employs a graph sampling strategy that prioritizes spatial coverage and embedding confidence, retaining only the most informative nodes and substantially reducing computational overhead. Our experimental results, both in NVIDIA Isaac Sim and on a real multi-floor building, demonstrate that LAMP outperforms existing explicit methods in both memory efficiency and fine-grained goal-reaching accuracy.

</details>


### [13] [Learning to Manipulate Anything: Revealing Data Scaling Laws in Bounding-Box Guided Policies](https://arxiv.org/abs/2602.11885)
*Yihao Wu,Jinming Ma,Junbo Tan,Yanzhao Yu,Shoujie Li,Mingliang Zhou,Diyun Xiang,Xueqian Wang*

Main category: cs.RO

TL;DR: 研究提出了一种基于边界框指令的方法来提高语义操作任务中扩散策略的泛化能力，并通过实验证明了数据量与泛化性能之间的幂律关系，同时介绍了一个高效的数据收集策略Label-UMI用于语义标注。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的策略在语义操作中的泛化能力有限，仅依赖文本指令不足以引导策略关注复杂动态环境下的目标物体。

Method: 设计了一个手持分割设备Label-UMI，能够高效地收集带有语义标签的演示数据；提出了一个语义-动作解耦框架，该框架结合了物体检测和基于边界框的扩散策略以改进泛化性和适应性。

Result: 通过大规模数据集上的广泛实验验证了方法的有效性，揭示了泛化性能与边界框对象数量间的幂律关系；总结出一种有效的数据收集策略，在四个任务上对已见和未见物体均能达到85%的成功率。

Conclusion: 研究表明，利用边界框指令可以直接指定目标物体并有效提升语义操作任务中模型的泛化能力，而所提出的Label-UMI及语义-动作解耦框架为此提供了强有力的支持。

Abstract: Diffusion-based policies show limited generalization in semantic manipulation, posing a key obstacle to the deployment of real-world robots. This limitation arises because relying solely on text instructions is inadequate to direct the policy's attention toward the target object in complex and dynamic environments. To solve this problem, we propose leveraging bounding-box instruction to directly specify target object, and further investigate whether data scaling laws exist in semantic manipulation tasks. Specifically, we design a handheld segmentation device with an automated annotation pipeline, Label-UMI, which enables the efficient collection of demonstration data with semantic labels. We further propose a semantic-motion-decoupled framework that integrates object detection and bounding-box guided diffusion policy to improve generalization and adaptability in semantic manipulation. Throughout extensive real-world experiments on large-scale datasets, we validate the effectiveness of the approach, and reveal a power-law relationship between generalization performance and the number of bounding-box objects. Finally, we summarize an effective data collection strategy for semantic manipulation, which can achieve 85\% success rates across four tasks on both seen and unseen objects. All datasets and code will be released to the community.

</details>


### [14] [General Humanoid Whole-Body Control via Pretraining and Fast Adaptation](https://arxiv.org/abs/2602.11929)
*Zepeng Wang,Jiangxing Wang,Shiqing Yao,Yu Zhang,Ziluo Ding,Ming Yang,Yuxuan Wang,Haobin Jiang,Chao Ma,Xiaochuan Shi,Zongqing Lu*

Main category: cs.RO

TL;DR: 本文提出了FAST，一种用于人形机器人全身控制的框架，通过Parseval-Guided残差策略适应和质心感知控制来实现快速适应与稳定的运动跟踪。实验表明FAST在鲁棒性、适应效率和泛化能力上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常需要特定任务的训练，在适应新动作时性能下降。因此，研究旨在开发一个能够快速适应新动作并且保持稳定平衡的一般性人形机器人全身控制器。

Method: FAST框架包含两部分：一是Parseval-Guided残差策略适应，学习了在正交性和KL约束下的轻量级delta行动策略，以有效适应分布外的动作同时减少灾难性遗忘；二是质心感知控制，通过整合与质心相关的观察和目标来增强追踪具有挑战性的参考动作时的平衡能力。

Result: 通过仿真及实际部署中的大量实验证明，FAST在鲁棒性、适应效率以及泛化能力方面始终优于最先进基准方法。

Conclusion: 提出的方法为解决人形机器人在多样运动分布下快速适应并保持稳定平衡提供了新的解决方案。

Abstract: Learning a general whole-body controller for humanoid robots remains challenging due to the diversity of motion distributions, the difficulty of fast adaptation, and the need for robust balance in high-dynamic scenarios. Existing approaches often require task-specific training or suffer from performance degradation when adapting to new motions. In this paper, we present FAST, a general humanoid whole-body control framework that enables Fast Adaptation and Stable Motion Tracking. FAST introduces Parseval-Guided Residual Policy Adaptation, which learns a lightweight delta action policy under orthogonality and KL constraints, enabling efficient adaptation to out-of-distribution motions while mitigating catastrophic forgetting. To further improve physical robustness, we propose Center-of-Mass-Aware Control, which incorporates CoM-related observations and objectives to enhance balance when tracking challenging reference motions. Extensive experiments in simulation and real-world deployment demonstrate that FAST consistently outperforms state-of-the-art baselines in robustness, adaptation efficiency, and generalization.

</details>


### [15] [Decentralized Multi-Robot Obstacle Detection and Tracking in a Maritime Scenario](https://arxiv.org/abs/2602.12012)
*Muhammad Farhan Ahmed,Vincent Frémont*

Main category: cs.RO

TL;DR: 该论文提出了一种用于检测和跟踪漂浮容器的分散式多机器人框架，结合了多个无人机与一个自主水面船只的合作。通过使用YOLOv8和立体视差视觉检测、基于不确定性感知的数据关联进行目标追踪，并通过协方差交叉保守地交换和融合紧凑轨迹摘要，确保在未知相关性下的一致性。信息驱动的任务分配模块则根据预期不确定性减少与旅行努力及安全距离之间的权衡来分配目标并选择无人机悬停视角。


<details>
  <summary>Details</summary>
Motivation: 为了解决在反射性水面上实现可靠感知以及在通信受限条件下的可扩展协调问题，从而促进海上监测中自主空-面机器人团队的有效部署。

Method: 采用了YOLOv8和立体视差视觉检测技术进行目标识别；利用每个对象的EKF（扩展卡尔曼滤波器）进行目标跟踪，并采用不确定性感知的数据关联方法；通过协方差交叉技术来保守地交换和融合来自不同机器人的轨迹摘要；设计了一个信息驱动的任务分配模块，以平衡预期不确定性降低与移动成本和安全间隔的需求。

Result: 模拟实验表明，在海事场景中应用该框架能够提高覆盖范围、定位精度和跟踪一致性，同时保持适度的通信需求。

Conclusion: 所提出的分散式多机器人系统框架能够在有限通信条件下有效增强对浮动物体的检测与跟踪能力，为海上监控提供了一种新的解决方案。

Abstract: Autonomous aerial-surface robot teams are promising for maritime monitoring. Robust deployment requires reliable perception over reflective water and scalable coordination under limited communication. We present a decentralized multi-robot framework for detecting and tracking floating containers using multiple UAVs cooperating with an autonomous surface vessel. Each UAV performs YOLOv8 and stereo-disparity-based visual detection, then tracks targets with per-object EKFs using uncertainty-aware data association. Compact track summaries are exchanged and fused conservatively via covariance intersection, ensuring consistency under unknown correlations. An information-driven assignment module allocates targets and selects UAV hover viewpoints by trading expected uncertainty reduction against travel effort and safety separation. Simulation results in a maritime scenario demonstrate improved coverage, localization accuracy, and tracking consistency while maintaining modest communication requirements.

</details>


### [16] [Adaptive-Horizon Conflict-Based Search for Closed-Loop Multi-Agent Path Finding](https://arxiv.org/abs/2602.12024)
*Jiarui Li,Federico Pecora,Runyu Zhang,Gioele Zardini*

Main category: cs.RO

TL;DR: 本文提出了一种名为ACCBS的闭环算法，该算法基于有限时间范围内的CBS，并通过一种受MPC中迭代加深机制启发的时间范围调整机制动态调整规划时间范围。ACCBS能够快速生成高质量可行解，同时随着计算预算增加而渐近最优，结合了对扰动的灵活性和强大的性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有的多机器人路径规划(MAPF)方法要么是难以处理干扰的开环规划器，要么是没有可靠性能保证的闭环启发式方法，限制了它们在安全关键部署中的使用。因此，需要一种既能有效应对干扰又能提供强性能保障的方法。

Method: 本文提出了ACCBS算法，这是一种基于有限时间范围CBS（Conflict-Based Search）的闭环算法，包含一个受到模型预测控制(MPC)中迭代加深启发的时间范围变化机制。该算法根据可用计算预算动态调整规划时间范围，并重用单一约束树以实现不同时间范围间的无缝过渡。

Result: 广泛案例研究表明，ACCBS不仅能够灵活应对干扰，还提供了强大的性能保证，有效地弥合了理论最优性和实际鲁棒性之间的差距，适用于大规模机器人部署。

Conclusion: ACCBS作为一种新颖的闭环MAPF解决方案，为自动化仓库和物流中的大型机器人舰队协调问题提供了既灵活又可靠的途径，能够在确保安全的同时优化性能。

Abstract: MAPF is a core coordination problem for large robot fleets in automated warehouses and logistics. Existing approaches are typically either open-loop planners, which generate fixed trajectories and struggle to handle disturbances, or closed-loop heuristics without reliable performance guarantees, limiting their use in safety-critical deployments. This paper presents ACCBS, a closed-loop algorithm built on a finite-horizon variant of CBS with a horizon-changing mechanism inspired by iterative deepening in MPC. ACCBS dynamically adjusts the planning horizon based on the available computational budget, and reuses a single constraint tree to enable seamless transitions between horizons. As a result, it produces high-quality feasible solutions quickly while being asymptotically optimal as the budget increases, exhibiting anytime behavior. Extensive case studies demonstrate that ACCBS combines flexibility to disturbances with strong performance guarantees, effectively bridging the gap between theoretical optimality and practical robustness for large-scale robot deployment.

</details>


### [17] [When would Vision-Proprioception Policies Fail in Robotic Manipulation?](https://arxiv.org/abs/2602.12032)
*Jingxian Lu,Wenke Xia,Yuxuan Wu,Zhiwu Lu,Di Hu*

Main category: cs.RO

TL;DR: 本研究探讨了在视觉-本体感觉策略中，当机器人动作转换时，视觉模态作用有限的问题。为解决这一问题，提出了一种基于阶段引导的梯度调整(GAP)算法，以动态调整本体感觉和视觉模态之间的协作，从而实现更加鲁棒且可泛化的策略。实验表明GAP算法在模拟与真实环境、单臂与双臂设置以及传统模型和视觉-语言-行动模型中均适用。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明，在复杂任务中视觉与本体感觉信息的结合可以显著提高操作策略的表现；然而，对于这种结合方式在不同场景下的通用性存在争议。特别是，当机器人需要通过定位目标来过渡其动作状态时，视觉信息似乎并未发挥预期的作用。因此，这项工作旨在深入理解这一现象背后的原因，并探索如何改进视觉-本体感觉融合策略以提升其性能与泛化能力。

Method: 研究者们设计了一系列时间控制实验来观察视觉-本体感觉策略在不同任务子阶段中的表现差异。发现训练过程中简洁的本体感觉信号由于能够更快地减少损失而主导了优化过程，这抑制了视觉模态的学习特别是在动作转换期间。为了解决这个问题，提出了一个名为GAP的新算法，该算法能够根据估计出的动作转换概率自适应地调节本体感觉梯度的大小，进而促进视觉-本体感觉策略间的动态合作。

Result: 实验结果证明了GAP算法的有效性，不仅能够在仿真环境中也适用于现实世界的应用场景下，同时支持单臂或双臂机器人配置。此外，它还展示了与包括传统方法及新兴的视觉-语言-行动模型在内的多种模型的良好兼容性。

Conclusion: 通过对视觉-本体感觉策略在机器人操作任务中面临挑战的研究，本文揭示了现有方法中存在的一个问题：即在某些关键阶段过度依赖于本体感觉输入可能会阻碍视觉信息的有效利用。所提出的GAP算法提供了一种新颖的方法来平衡这两种感知渠道，有助于开发出更加强大且灵活的操作策略。

Abstract: Proprioceptive information is critical for precise servo control by providing real-time robotic states. Its collaboration with vision is highly expected to enhance performances of the manipulation policy in complex tasks. However, recent studies have reported inconsistent observations on the generalization of vision-proprioception policies. In this work, we investigate this by conducting temporally controlled experiments. We found that during task sub-phases that robot's motion transitions, which require target localization, the vision modality of the vision-proprioception policy plays a limited role. Further analysis reveals that the policy naturally gravitates toward concise proprioceptive signals that offer faster loss reduction when training, thereby dominating the optimization and suppressing the learning of the visual modality during motion-transition phases. To alleviate this, we propose the Gradient Adjustment with Phase-guidance (GAP) algorithm that adaptively modulates the optimization of proprioception, enabling dynamic collaboration within the vision-proprioception policy. Specifically, we leverage proprioception to capture robotic states and estimate the probability of each timestep in the trajectory belonging to motion-transition phases. During policy learning, we apply fine-grained adjustment that reduces the magnitude of proprioception's gradient based on estimated probabilities, leading to robust and generalizable vision-proprioception policies. The comprehensive experiments demonstrate GAP is applicable in both simulated and real-world environments, across one-arm and dual-arm setups, and compatible with both conventional and Vision-Language-Action models. We believe this work can offer valuable insights into the development of vision-proprioception policies in robotic manipulation.

</details>


### [18] [Safety Beyond the Training Data: Robust Out-of-Distribution MPC via Conformalized System Level Synthesis](https://arxiv.org/abs/2602.12047)
*Anutam Srinivasan,Antoine Leeman,Glen Chou*

Main category: cs.RO

TL;DR: 本文提出了一种新的框架，使用共形预测和系统级合成来解决超出训练数据分布时的鲁棒性和安全性问题。通过加权共形预测导出高置信度模型误差界，并将其整合到基于SLS的鲁棒非线性模型预测控制中，通过体积优化前向可达集实现约束收紧。实证研究显示了该方法在复杂非线性系统中的优越性，特别是在数据分布之外的情况。


<details>
  <summary>Details</summary>
Motivation: 当使用学习到的动力学模型超出其训练数据分布时，确保安全性和鲁棒性是一个挑战。为了应对这一挑战，作者开发了一个新框架，旨在提高规划和控制的鲁棒性。

Method: 首先，利用带有学习状态-控制依赖协方差模型的加权共形预测（CP）推导出高置信度模型误差界限。接着，将这些界限集成到基于系统级合成（SLS）的鲁棒非线性模型预测控制(MPC)公式中，通过对预测范围内的体积优化前向可达集来进行约束收紧。

Result: 理论分析提供了关于覆盖范围及在分布漂移下的鲁棒性的保证，并探讨了数据密度和轨迹管大小对预测覆盖的影响。实验上，在越来越复杂的非线性系统中展示了该方法的有效性，包括4D汽车和12D四旋翼无人机的例子，与固定边界和非鲁棒基线相比，在数据分布之外的情况下提高了安全性和鲁棒性。

Conclusion: 所提出的方法能够有效增强非线性系统的鲁棒性和安全性，特别是当面对超出原始训练数据分布的新情况时。

Abstract: We present a novel framework for robust out-of-distribution planning and control using conformal prediction (CP) and system level synthesis (SLS), addressing the challenge of ensuring safety and robustness when using learned dynamics models beyond the training data distribution. We first derive high-confidence model error bounds using weighted CP with a learned, state-control-dependent covariance model. These bounds are integrated into an SLS-based robust nonlinear model predictive control (MPC) formulation, which performs constraint tightening over the prediction horizon via volume-optimized forward reachable sets. We provide theoretical guarantees on coverage and robustness under distributional drift, and analyze the impact of data density and trajectory tube size on prediction coverage. Empirically, we demonstrate our method on nonlinear systems of increasing complexity, including a 4D car and a {12D} quadcopter, improving safety and robustness compared to fixed-bound and non-robust baselines, especially outside of the data distribution.

</details>


### [19] [HoloBrain-0 Technical Report](https://arxiv.org/abs/2602.12062)
*Xuewu Lin,Tianwei Lin,Yun Du,Hongyu Xie,Yiwei Jin,Jiawei Li,Shijie Wu,Qingze Wang,Mengdi Li,Mengao Zhao,Ziang Li,Chaodong Huang,Hongzhe Bi,Lichao Huang,Zhizhong Su*

Main category: cs.RO

TL;DR: 本研究提出了HoloBrain-0，一个综合的视觉-语言-动作(Vision-Language-Action, VLA)框架，旨在连接基础模型研究与实际机器人部署。通过新颖的VLA架构和"预训练后训练"范式，在模拟基准测试及真实世界操作任务中取得了领先成果。此外，还开源了整个HoloBrain生态系统以加速研究进展和实践应用。


<details>
  <summary>Details</summary>
Motivation: 为了弥合基础模型研究与可靠现实世界机器人部署之间的差距，并增强3D空间推理能力以及支持多种形态的机器人。

Method: 开发了一个新的VLA架构，该架构明确地整合了机器人体现先验知识（包括多视图相机参数和运动学描述），并通过可扩展的“预训练然后后训练”模式进行了验证。

Result: 在RoboTwin 2.0、LIBERO和GenieSim等模拟基准上达到了最先进水平，并且在具有挑战性的长期真实世界操控任务中也表现优异。特别是其0.2B参数变体能够实现低延迟设备端部署。

Conclusion: HoloBrain-0不仅展示了在提高机器人性能方面的潜力，而且通过全面开源生态系统为高性能机器人操控提供了完整的、可重复的研究路径。

Abstract: In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post-train" paradigm, achieving state-of-the-art results on simulation benchmarks such as RoboTwin 2.0, LIBERO, and GenieSim, as well as strong results on challenging long-horizon real-world manipulation tasks. Notably, our efficient 0.2B-parameter variant rivals significantly larger baselines, enabling low-latency on-device deployment. To further accelerate research and practical adoption, we fully open-source the entire HoloBrain ecosystem, which includes: (1) powerful pre-trained VLA foundations; (2) post-trained checkpoints for multiple simulation suites and real-world tasks; and (3) RoboOrchard, a full-stack VLA infrastructure for data curation, model training and deployment. Together with standardized data collection protocols, this release provides the community with a complete, reproducible path toward high-performance robotic manipulation.

</details>


### [20] [VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model](https://arxiv.org/abs/2602.12063)
*Yanjiang Guo,Tony Lee,Lucy Xiaoyang Shi,Jianyu Chen,Percy Liang,Chelsea Finn*

Main category: cs.RO

TL;DR: 本文提出了一种通过迭代在线互动来提高视觉-语言-动作（VLA）模型性能和可靠性的方法。该方法利用从真实世界收集的数据改进了世界模型的真实性，进而生成补充的合成数据以进一步优化VLA模型，在实际机器人实验中展示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了提高视觉-语言-动作(VLA)模型在现实世界中的表现和可靠性，同时解决收集策略执行数据成本高昂的问题，研究者们探索了使用学习到的模拟器来生成额外的执行数据的可能性。然而，现有的世界模型缺乏足够的物理真实性，难以准确地模拟接触密集型物体操纵中的细微却关键的物理细节。

Method: 本文提出了一个简单的迭代改进算法，该算法利用来自真实世界的执行数据来增强世界模型的真实度，后者接着被用来产生补充性的合成数据，从而进一步改善VLA模型。

Result: 实验结果表明，在真实的机器人上应用此方法后，与基础策略相比，所提出的方案能够使最先进的VLA模型在多个下游任务上的绝对成功率提高39.2%，而通过训练由生成的合成执行序列带来的改进则达到了11.6%。

Conclusion: 本研究表明，通过结合真实世界的数据反馈循环与合成数据生成过程，可以有效地提高VLA模型在处理复杂交互任务时的表现。

Abstract: The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w

</details>


### [21] [Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning](https://arxiv.org/abs/2602.12065)
*Xiang Liu,Sen Cui,Guocai Yao,Zhong Cao,Jingheng Ma,Min Zhang,Changshui Zhang*

Main category: cs.RO

TL;DR: 提出了一种名为Affordance-Graphed Task Worlds (AGT-World)的新框架，该框架基于现实世界的观察自动构建交互式模拟环境及相应的机器人任务策略。通过将任务空间形式化为结构图，并引入结合视觉-语言模型推理和几何验证的自我进化机制，实现了对策略的自主精炼，从而在成功率和泛化能力上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 直接在现实世界中训练机器人策略成本高昂且难以扩展。尽管生成性仿真能够大规模合成数据，但当前的方法往往无法生成逻辑连贯的长期任务，并且由于开环执行而难以应对动态物理不确定性。

Method: 提出了Affordance-Graphed Task Worlds (AGT-World)框架，它能基于真实世界的观测自动构建交互式的模拟环境以及对应的机器人任务策略。不同于依赖随机提议或静态复制的方法，AGT-World将任务空间形式化为一个结构化的图表，支持将复杂目标精确地分解成理论基础的原子原语。此外，还引入了带有混合反馈的自我进化机制，以自主改进策略，这结合了视觉-语言模型推理与几何验证。

Result: 广泛的实验表明，所提出的方法在成功率和泛化方面显著优于其他方法，实现了一个包括提案、执行和修正在内的自我提升循环，促进了可扩展的机器人学习。

Conclusion: 通过AGT-World框架和自我进化机制，研究人员能够有效解决现有方法在处理长时序任务和动态不确定性上的不足，为机器人学习提供了一个更加高效、可扩展的解决方案。

Abstract: Training robotic policies directly in the real world is expensive and unscalable. Although generative simulation enables large-scale data synthesis, current approaches often fail to generate logically coherent long-horizon tasks and struggle with dynamic physical uncertainties due to open-loop execution. To address these challenges, we propose Affordance-Graphed Task Worlds (AGT-World), a unified framework that autonomously constructs interactive simulated environments and corresponding robot task policies based on real-world observations. Unlike methods relying on random proposals or static replication, AGT-World formalizes the task space as a structured graph, enabling the precise, hierarchical decomposition of complex goals into theoretically grounded atomic primitives. Furthermore, we introduce a Self-Evolution mechanism with hybrid feedback to autonomously refine policies, combining Vision-Language Model reasoning and geometric verification. Extensive experiments demonstrate that our method significantly outperforms in success rates and generalization, achieving a self-improving cycle of proposal, execution, and correction for scalable robot learning.

</details>


### [22] [RF-Modulated Adaptive Communication Improves Multi-Agent Robotic Exploration](https://arxiv.org/abs/2602.12074)
*Lorin Achey,Breanne Crockett,Christoffer Heckman,Bradley Hayes*

Main category: cs.RO

TL;DR: 本文提出了一种新的通信感知规划算法ART，该算法能够根据信号强度和数据负载大小动态调整传输位置，从而在通信受限的环境中提高多机器人团队的信息共享效率。此外，还提出了ART-SST方法，设定了信号强度阈值以确保高质量的数据传输。通过480多次模拟实验表明，ART方法比现有策略更优，减少了最多58%的行进距离，并将探索时间加快了最多52%。


<details>
  <summary>Details</summary>
Motivation: 在通信受限的环境中实现多机器人探索任务时，需要解决可靠协调与高效通信两大难题。为此，开发出一种新算法来优化信息共享过程，减少不必要的回溯路径，提高任务执行效率。

Method: 提出了Adaptive-RF Transmission (ART)算法，该算法可根据当前信号状况及待传输数据量自适应地选择最佳通讯点；同时研究了ART的一个扩展版本——ART-SST，它为保证高保真度的数据传输设置了最低信号强度要求。

Result: 经过三个洞穴式环境中的480多次仿真测试，结果显示ART相比完全会合以及基于最小信号强度启发式的传统方法，在减少移动距离（最高达58%）和加速探索进程（最高达52%）方面表现出色。

Conclusion: 研究证明，采用自适应且考虑数据负载的通信方式可以显著提升复杂、通信受限条件下的覆盖效率和任务速度，为未来行星探测及搜救行动奠定了良好基础。

Abstract: Reliable coordination and efficient communication are critical challenges for multi-agent robotic exploration of environments where communication is limited. This work introduces Adaptive-RF Transmission (ART), a novel communication-aware planning algorithm that dynamically modulates transmission location based on signal strength and data payload size, enabling heterogeneous robot teams to share information efficiently without unnecessary backtracking. We further explore an extension to this approach called ART-SST, which enforces signal strength thresholds for high-fidelity data delivery. Through over 480 simulations across three cave-inspired environments, ART consistently outperforms existing strategies, including full rendezvous and minimum-signal heuristic approaches, achieving up to a 58% reduction in distance traveled and up to 52% faster exploration times compared to baseline methods. These results demonstrate that adaptive, payload-aware communication significantly improves coverage efficiency and mission speed in complex, communication-constrained environments, offering a promising foundation for future planetary exploration and search-and-rescue missions.

</details>


### [23] [Pack it in: Packing into Partially Filled Containers Through Contact](https://arxiv.org/abs/2602.12095)
*David Russell,Zisong Xu,Maximo A. Roa,Mehmet Dogar*

Main category: cs.RO

TL;DR: 本文提出了一种接触感知的包装方法，该方法通过与先前放置的物体进行有目的的交互来创造自由空间，并成功地放置新物品。


<details>
  <summary>Details</summary>
Motivation: 现有的仓库操作自动化研究主要集中在将物品放入空容器中，并采用了无碰撞策略。然而，在实际情况下，容器通常已经部分填充了物品，这些物品由于在仓库中的运输而常常以次优方式排列。因此，需要一种能够处理这种情况的新方法。

Method: 本文的方法包括使用基于接触的多对象轨迹优化器在一个模型预测控制器内，结合一个物理感知的感知系统，即使在不可避免的遮挡期间也能估计物体的位置，并且有一种方法可以建议在容器内部放置物体的实际可行位置。

Result: 所提出的方法能够在已有物品的情况下找到新的物品放置位置，从而有效利用容器空间。

Conclusion: 本研究展示了一种新颖的接触感知包装方法，它能够利用与已放置物体的有意互动来创建可用空间，进而实现新物品的成功放置，这对于提高仓库自动化水平和效率具有重要意义。

Abstract: The automation of warehouse operations is crucial for improving productivity and reducing human exposure to hazardous environments. One operation frequently performed in warehouses is bin-packing where items need to be placed into containers, either for delivery to a customer, or for temporary storage in the warehouse. Whilst prior bin-packing works have largely been focused on packing items into empty containers and have adopted collision-free strategies, it is often the case that containers will already be partially filled with items, often in suboptimal arrangements due to transportation about a warehouse. This paper presents a contact-aware packing approach that exploits purposeful interactions with previously placed objects to create free space and enable successful placement of new items. This is achieved by using a contact-based multi-object trajectory optimizer within a model predictive controller, integrated with a physics-aware perception system that estimates object poses even during inevitable occlusions, and a method that suggests physically-feasible locations to place the object inside the container.

</details>


### [24] [Multi Graph Search for High-Dimensional Robot Motion Planning](https://arxiv.org/abs/2602.12096)
*Itamar Mishani,Maxim Likhachev*

Main category: cs.RO

TL;DR: 本文提出了一种名为多图搜索（MGS）的基于搜索的运动规划算法，该算法能够在高维状态空间中有效运作，同时保持完整性和有界次优性。通过在多种操作和移动操作任务上进行实证演示，证明了MGS的有效性。


<details>
  <summary>Details</summary>
Motivation: 高效的运动规划对于高维度机器人系统来说至关重要，但现有的方法往往导致不可预测、不一致的动作或需要过多的计算资源和内存。

Method: 提出了多图搜索（MGS）算法，它将经典单向和双向搜索推广到多图设置中，并且在搜索过程中允许初始不连接的子图通过可行转换合并。

Result: MGS被证明是完整的并且具有有界次优性，在一系列操作和移动操作任务上展示了其实效性。

Conclusion: MGS为高维度机器人系统的实时操作和可靠部署提供了一个有效的解决方案。

Abstract: Efficient motion planning for high-dimensional robotic systems, such as manipulators and mobile manipulators, is critical for real-time operation and reliable deployment. Although advances in planning algorithms have enhanced scalability to high-dimensional state spaces, these improvements often come at the cost of generating unpredictable, inconsistent motions or requiring excessive computational resources and memory. In this work, we introduce Multi-Graph Search (MGS), a search-based motion planning algorithm that generalizes classical unidirectional and bidirectional search to a multi-graph setting. MGS maintains and incrementally expands multiple implicit graphs over the state space, focusing exploration on high-potential regions while allowing initially disconnected subgraphs to be merged through feasible transitions as the search progresses. We prove that MGS is complete and bounded-suboptimal, and empirically demonstrate its effectiveness on a range of manipulation and mobile manipulation tasks. Demonstrations, benchmarks and code are available at https://multi-graph-search.github.io/.

</details>


### [25] [3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting](https://arxiv.org/abs/2602.12159)
*Wancai Zheng,Hao Chen,Xianlong Lu,Linlin Ou,Xinyi Yu*

Main category: cs.RO

TL;DR: 提出了一种新的零样本物体导航框架3DGSNav，该框架利用3D高斯点云作为视觉-语言模型的持久内存来增强空间推理能力，并通过主动感知逐步构建环境的3D表示。结合结构化视觉提示和链式思维提示进一步提高了VLM的推理能力。在实际测试中，该方法显示了稳健且具有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本物体导航方法依赖于场景抽象，这限制了高级决策的能力，因为它们受限于低级感知的准确性。

Method: 3DGSNav框架使用3D高斯点云作为持续记忆体以加强视觉-语言模型的空间推理能力；通过主动感知过程逐步建立环境的3D高斯点云表达；设计并整合了结构化的视觉提示与链式思维提示以增进VLM的推理性能；运用实时物体检测器筛选可能的目标对象，并通过VLM驱动的主动视角切换实现目标再验证。

Result: 在多个基准测试以及四足机器人上的现实世界实验表明，所提方法相比最先进方法能够取得更加稳健且具备竞争力的结果。

Conclusion: 3DGSNav提供了一种有效的方法来提高零样本物体导航任务中的空间推理能力和目标识别效率，展示了在未知环境中定位目标物体的强大能力。

Abstract: Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/

</details>


### [26] [LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion](https://arxiv.org/abs/2602.12215)
*Jiangran Lyu,Kai Liu,Xuheng Zhang,Haoran Liao,Yusen Feng,Wenxuan Zhu,Tingrui Shen,Jiayi Chen,Jiazhao Zhang,Yifei Dong,Wenbo Cui,Senmao Qi,Shuo Wang,Yixin Zheng,Mi Yan,Xuesong Shi,Haoran Li,Dongbin Zhao,Ming-Yu Liu,Zhizheng Zhang,Li Yi,Yizhou Wang,He Wang*

Main category: cs.RO

TL;DR: 本文提出了一种名为LDA-1B的机器人基础模型，通过同时学习动态、策略和视觉预测来处理多样化的数据，并利用一个大规模的数据集EI-30k进行训练。该模型在模拟和现实世界中的实验显示，在接触丰富、灵巧性和长时任务上分别比先前方法提高了21%、48%和23%，并且能够有效利用通常被丢弃的低质量轨迹数据来提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于行为克隆的机器人基础模型忽略了可以从异构体现数据中获取的可转移动力学知识，而统一世界模型（UWM）虽然有潜力利用这些多样化数据，但其实际应用受限于粗放的数据使用方式和碎片化数据集的问题。因此，需要一种新的方法来更有效地整合和利用这些数据。

Method: 研究者们提出了LDA-1B，这是一个可以处理普遍体现数据摄入的基础模型，它通过联合学习动态、策略与视觉预测的方式给不同质量的数据分配了不同的角色。为了支持这种规模下的运作，他们还构建并标准化了一个名为EI-30k的大规模体现交互数据集。此外，LDA-1B采用了多模态扩散变压器来处理异步的视觉与动作流，并且能够在DINO潜在空间内进行结构化预测以避免冗余的像素级外观建模。

Result: 在仿真环境及真实世界的测试中，LDA-1B相较于之前的方法（如$π_{0.5}$）在涉及高接触需求、灵巧操作以及长期规划的任务上分别提升了21%、48%和23%的表现。值得注意的是，LDA-1B还展现了高效的数据利用能力，即使只利用那些通常被认为有害而被舍弃的30%低质量轨迹也能够带来额外10%的性能提升。

Conclusion: LDA-1B作为一款新型的机器人基础模型，成功地实现了对不同类型体现数据的有效利用，并在多种复杂任务上取得了显著优于现有技术的结果。这表明，通过对动态、策略与感知进行综合学习，以及合理利用所有可用数据，即使是质量较低的数据也能为模型训练提供帮助。

Abstract: Recent robot foundation models largely rely on large-scale behavior cloning, which imitates expert actions but discards transferable dynamics knowledge embedded in heterogeneous embodied data. While the Unified World Model (UWM) formulation has the potential to leverage such diverse data, existing instantiations struggle to scale to foundation-level due to coarse data usage and fragmented datasets. We introduce LDA-1B, a robot foundation model that scales through universal embodied data ingestion by jointly learning dynamics, policy, and visual forecasting, assigning distinct roles to data of varying quality. To support this regime at scale, we assemble and standardize EI-30k, an embodied interaction dataset comprising over 30k hours of human and robot trajectories in a unified format. Scalable dynamics learning over such heterogeneous data is enabled by prediction in a structured DINO latent space, which avoids redundant pixel-space appearance modeling. Complementing this representation, LDA-1B employs a multi-modal diffusion transformer to handle asynchronous vision and action streams, enabling stable training at the 1B-parameter scale. Experiments in simulation and the real world show LDA-1B outperforms prior methods (e.g., $π_{0.5}$) by up to 21\%, 48\%, and 23\% on contact-rich, dexterous, and long-horizon tasks, respectively. Notably, LDA-1B enables data-efficient fine-tuning, gaining 10\% by leveraging 30\% low-quality trajectories typically harmful and discarded.

</details>


### [27] [Any House Any Task: Scalable Long-Horizon Planning for Abstract Human Tasks](https://arxiv.org/abs/2602.12244)
*Zhihong Liu,Yang Li,Rengming Huang,Cewu Lu,Panpan Cai*

Main category: cs.RO

TL;DR: 本文提出了一种名为Any House Any Task (AHAT)的家庭任务规划器，它利用大型语言模型将任务指令和文本场景图转换为PDDL定义的子目标，并通过显式符号推理生成可行且最优的长期计划。此外，还引入了TGPO这一新的强化学习算法来增强模型处理复杂模糊意图的能力。实验表明，AHAT在需要复杂执行计划但指令简短的人类风格家庭任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对开放世界语言条件下的任务规划问题，尤其是在大规模家庭环境中机器人操作时面临的可扩展性挑战，包括环境规模增大、计划长度增加、指令模糊性和约束复杂度提高导致性能快速下降的问题。

Method: 开发了Any House Any Task (AHAT)，一个专为处理大规模环境下基于模糊人类指令进行长时间范围规划而优化的家庭任务规划器。该系统的核心是使用训练过的大型语言模型（LLM）将任务指令与文本场景图映射到以规划领域定义语言（PDDL）表示的具体子目标上。同时提出了TGPO，一种新的强化学习算法，它结合了外部校正中间推理轨迹的方法于组相对策略优化（GRPO）之中。

Result: 实验结果表明，在面对由简短指令指导但却需要复杂执行步骤的人类风格家庭任务时，AHAT相比最前沿的提示、规划及学习方法展现了显著的性能提升。

Conclusion: AHAT通过结合大型语言模型和强化学习技术有效地解决了开放世界条件下根据模糊人类指令完成长时间范围内的家庭任务规划问题，特别是在应对具有高复杂度执行需求的任务方面显示出了优越性。

Abstract: Open world language conditioned task planning is crucial for robots operating in large-scale household environments. While many recent works attempt to address this problem using Large Language Models (LLMs) via prompting or training, a key challenge remains scalability. Performance often degrades rapidly with increasing environment size, plan length, instruction ambiguity, and constraint complexity. In this work, we propose Any House Any Task (AHAT), a household task planner optimized for long-horizon planning in large environments given ambiguous human instructions. At its core, AHAT utilizes an LLM trained to map task instructions and textual scene graphs into grounded subgoals defined in the Planning Domain Definition Language (PDDL). These subgoals are subsequently solved to generate feasible and optimal long-horizon plans through explicit symbolic reasoning. To enhance the model's ability to decompose complex and ambiguous intentions, we introduce TGPO, a novel reinforcement learning algorithm that integrates external correction of intermediate reasoning traces into Group Relative Policy Optimization (GRPO). Experiments demonstrate that AHAT achieves significant performance gains over state-of-the-art prompting, planning, and learning methods, particularly in human-style household tasks characterized by brief instructions but requiring complex execution plans.

</details>


### [28] [Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment](https://arxiv.org/abs/2602.12281)
*Jacky Kwok,Xilun Zhang,Mengdi Xu,Yuejiang Liu,Azalia Mirhoseini,Chelsea Finn,Marco Pavone*

Main category: cs.RO

TL;DR: 本文提出了一种名为CoVer的对比验证器，用于提高视觉-语言-动作模型在执行自然语言指令时的一致性。通过测试时间验证、扩展法则以及引入"启动时计算"和分层验证推理管道，该方法在SIMPLER基准测试中实现了显著的性能提升，并在真实世界实验中进一步提高了45%的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型在遵循自然语言指令方面取得了显著进展，但生成的动作仍可能与给定指令不一致。为了解决这一问题，研究者探索了测试时间验证作为缩小“意图-行动差距”的一种手段。

Method: 首先确定了体现指令跟随的测试时间缩放定律，展示了同时增加重述指令数量和生成动作可以大大提高测试时间样本多样性。基于这些发现，提出了一个名为CoVer的对比验证器来检查视觉-语言-动作对齐情况。此外，还介绍了“启动时计算”概念及一个针对VLA的层次化验证推理流程。

Result: 相比于在同一数据集上扩大策略预训练规模，所提出的验证方法在SIMPLER基准上的分布内表现提高了22%，分布外提高了13%，而在实际实验中更是达到了45%的进步。对于PolaRiS基准而言，CoVer在任务进度上获得了14%的增长，在成功率上也有9%的提升。

Conclusion: 通过采用测试时间验证和专门设计的对比验证器CoVer，本研究成功地提高了机器人理解和执行自然语言指令的能力，特别是在多样化场景下的表现。

Abstract: The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the "intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce "boot-time compute" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.

</details>
