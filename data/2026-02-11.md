<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 36]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Feasible Static Workspace Optimization of Tendon Driven Continuum Robot based on Euclidean norm](https://arxiv.org/abs/2602.09046)
*Mohammad Jabari,Carmen Visconte,Giuseppe Quaglia,Med Amine Laribi*

Main category: cs.RO

TL;DR: 本文提出了一种基于可行静态工作空间(FSW)的肌腱驱动连续体机器人(TDCR)最优设计方法。通过遗传算法优化来最大化FSW，即使在外部力和扭矩的影响下，该方法也能有效识别出使FSW最大化的最佳肌腱力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过优化肌腱驱动连续体机器人的肌腱力配置，以扩大其可行静态工作空间（FSW），同时考虑了外部负载对机器人性能的影响。

Method: 采用遗传算法作为优化工具，针对两段式八根肌腱驱动的连续体机器人进行设计。将肌腱力设定为设计变量，并通过最大化末端位置欧几里得范数来确定机器人的可行静态工作空间。模拟过程中还考虑了包括扭矩和力在内的外部载荷。

Result: 结果显示，所提方法能够有效地找到一组最佳肌腱力设置，使得即使在受到外力及扭矩作用的情况下，TDCR的可行静态工作空间也得以最大化。

Conclusion: 本研究提出的基于遗传算法的优化策略成功地应用于肌腱驱动连续体机器人的设计中，证明了对于提高此类机器人在实际应用场景中的适应性和灵活性具有重要意义。

Abstract: This paper focuses on the optimal design of a tendon-driven continuum robot (TDCR) based on its feasible static workspace (FSW). The TDCR under consideration is a two-segment robot driven by eight tendons, with four tendon actuators per segment. Tendon forces are treated as design variables, while the feasible static workspace (FSW) serves as the optimization objective. To determine the robot's feasible static workspace, a genetic algorithm optimization approach is employed to maximize a Euclidian norm of the TDCR's tip position over the workspace. During the simulations, the robot is subjected to external loads, including torques and forces. The results demonstrate the effectiveness of the proposed method in identifying optimal tendon forces to maximize the feasible static workspace, even under the influence of external forces and torques.

</details>


### [2] [Legs Over Arms: On the Predictive Value of Lower-Body Pose for Human Trajectory Prediction from Egocentric Robot Perception](https://arxiv.org/abs/2602.09076)
*Nhat Le,Daeun Song,Xuesu Xiao*

Main category: cs.RO

TL;DR: 该研究通过利用2D和3D骨骼关键点以及生物力学线索作为额外输入，来改进多智能体轨迹预测的准确性。发现关注下身3D关键点可以减少平均位移误差13%，而结合3D关键点与相应的生物力学线索则能进一步提高1-4%的表现。此外，即使使用全景图像中的2D关键点输入也能保持性能提升，表明单目环绕视觉能够捕捉到有用的运动预测信息。


<details>
  <summary>Details</summary>
Motivation: 为了在拥挤环境中实现社交机器人导航，准确预测人类轨迹至关重要。现有的方法大多将人视为质点，但本研究旨在通过利用不同的人类骨骼特征来提高预测精度。

Method: 系统地评估了2D和3D骨骼关键点以及从这些关键点衍生出的生物力学线索作为附加输入对预测效用的影响。研究基于JRDB数据集及另一个用于社会导航的新数据集（包含360度全景视频）进行。

Result: 研究表明，专注于下身3D关键点可以使平均位移误差减少13%，同时，将3D关键点输入与相应的生物力学线索相结合可带来额外1-4%的改善。重要的是，即便是在使用从等距柱状全景图中提取的2D关键点输入时，这种性能增益仍然存在。

Conclusion: 观察人的腿部可以帮助机器人有效预测人类动作，为设计社交机器人导航的感知能力提供了实用见解。

Abstract: Predicting human trajectory is crucial for social robot navigation in crowded environments. While most existing approaches treat human as point mass, we present a study on multi-agent trajectory prediction that leverages different human skeletal features for improved forecast accuracy. In particular, we systematically evaluate the predictive utility of 2D and 3D skeletal keypoints and derived biomechanical cues as additional inputs. Through a comprehensive study on the JRDB dataset and another new dataset for social navigation with 360-degree panoramic videos, we find that focusing on lower-body 3D keypoints yields a 13% reduction in Average Displacement Error and augmenting 3D keypoint inputs with corresponding biomechanical cues provides a further 1-4% improvement. Notably, the performance gain persists when using 2D keypoint inputs extracted from equirectangular panoramic images, indicating that monocular surround vision can capture informative cues for motion forecasting. Our finding that robots can forecast human movement efficiently by watching their legs provides actionable insights for designing sensing capabilities for social robot navigation.

</details>


### [3] [Agile asymmetric multi-legged locomotion: contact planning via geometric mechanics and spin model duality](https://arxiv.org/abs/2602.09123)
*Jackson Habala,Gabriel B. Margolis,Tianyu Wang,Pratyush Bhatt,Juntao He,Naheel Naeem,Zhaochen Xu,Pulkit Agrawal,Daniel I. Goldman,Di Luo,Baxi Chong*

Main category: cs.RO

TL;DR: 本文提出了一种基于几何力学和统计力学中的自旋模型对偶框架的方法，用于发现多足机器人运动控制的新结构。该方法在六足机器人上实现了比传统步态快50%的前进速度，并且在控制和硬件层面上都展示了不对称性特征。


<details>
  <summary>Details</summary>
Motivation: 尽管可以构建更多腿的机器人以提高运动性能，但目前的研究主要集中在双足或四足机器人上。这种不平衡并非由于硬件限制，而是缺乏能够解释何时以及如何通过增加腿来改善运动表现的原则性控制框架。现有的多足系统控制策略通常使用为双足或四足设计的低维步态，未能充分利用高维系统中出现的新对称性和控制机会。

Method: 采用几何力学将接触丰富的运动规划简化为图优化问题，并提出了一个源自统计力学的自旋模型对偶框架来利用对称性破坏并指导最优步态重组。

Result: 对于六足机器人而言，新识别出的一种非对称运动策略达到了每周期0.61个身长的前向速度（相比传统步态提高了50%）。此外，在控制层面观察到了身体方向在快速顺时针与慢速逆时针转向阶段之间的非对称振荡；而在硬件层面，同一侧的两条腿可以不被驱动甚至替换为刚性部件而不影响性能。

Conclusion: 本研究开发了一个原则性的框架来探索多足机器人运动控制中的新结构。通过数值模拟和物理实验验证了该框架的有效性，并揭示了从高维具身系统中对称性重构所涌现出来的新型运动行为。

Abstract: Legged robot research is presently focused on bipedal or quadrupedal robots, despite capabilities to build robots with many more legs to potentially improve locomotion performance. This imbalance is not necessarily due to hardware limitations, but rather to the absence of principled control frameworks that explain when and how additional legs improve locomotion performance. In multi-legged systems, coordinating many simultaneous contacts introduces a severe curse of dimensionality that challenges existing modeling and control approaches. As an alternative, multi-legged robots are typically controlled using low-dimensional gaits originally developed for bipeds or quadrupeds. These strategies fail to exploit the new symmetries and control opportunities that emerge in higher-dimensional systems. In this work, we develop a principled framework for discovering new control structures in multi-legged locomotion. We use geometric mechanics to reduce contact-rich locomotion planning to a graph optimization problem, and propose a spin model duality framework from statistical mechanics to exploit symmetry breaking and guide optimal gait reorganization. Using this approach, we identify an asymmetric locomotion strategy for a hexapod robot that achieves a forward speed of 0.61 body lengths per cycle (a 50% improvement over conventional gaits). The resulting asymmetry appears at both the control and hardware levels. At the control level, the body orientation oscillates asymmetrically between fast clockwise and slow counterclockwise turning phases for forward locomotion. At the hardware level, two legs on the same side remain unactuated and can be replaced with rigid parts without degrading performance. Numerical simulations and robophysical experiments validate the framework and reveal novel locomotion behaviors that emerge from symmetry reforming in high-dimensional embodied systems.

</details>


### [4] [Elements of Robot Morphology: Supporting Designers in Robot Form Exploration](https://arxiv.org/abs/2602.09203)
*Amy Koike,Ge,Guo,Xinning He,Callie Y. Kim,Dakota Sullivan,Bilge Mutlu*

Main category: cs.RO

TL;DR: 该论文提出了一种名为“机器人形态元素”的框架，用于指导系统地探索机器人的形式，并开发了实体工具包MEB来支持这种实践探索。通过案例研究和设计工作坊验证了这些工具在分析、构思、反思和协作机器人设计中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人形态对于人机交互非常重要，但在如何使用设计框架来指导系统化的形态探索方面知之甚少。为了解决这个问题，作者提出了一个新框架以识别构成机器人形态的基本元素，并提供一种方式促进对不同机器人形态的结构化探索。

Method: 首先，通过现有机器人的分析定义了一个包括感知、关节、末端执行器、移动性和结构五个基本元素的框架。接着，基于这个框架开发了‘形态探索积木’（MEB），这是一种实体工具，允许用户动手进行合作实验。最后，通过案例研究与设计工作坊的形式评估了该框架及工具包的有效性。

Result: 结果表明，所提出的框架和配套工具能够有效地支持对机器人形态的设计过程，包括但不限于分析现有设计、激发新的创意、鼓励反思以及促进团队间的协作。

Conclusion: 本研究为机器人形态设计领域提供了宝贵的见解，特别是通过提供一套具体的工具和方法论来辅助设计师们更好地理解和探索不同的机器人设计方案。

Abstract: Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.

</details>


### [5] [Risk-Aware Obstacle Avoidance Algorithm for Real-Time Applications](https://arxiv.org/abs/2602.09204)
*Ozan Kaya,Emir Cem Gezer,Roger Skjetne,Ingrid Bouwer Utne*

Main category: cs.RO

TL;DR: 本文提出了一种混合风险感知导航架构，该架构结合了路径上障碍物的概率建模与自主水面船只的平滑轨迹优化。通过构建概率风险地图并利用基于风险偏好的RRT规划器生成无碰撞路径，并使用B样条算法优化这些路径以确保连续性。实验结果表明，该系统能够在静态和动态障碍物环境中安全航行、保持平滑轨迹，并适应环境变化的风险。


<details>
  <summary>Details</summary>
Motivation: 在不断变化的海洋环境中实现鲁棒导航需要能够处理不确定性的自主系统。这项研究旨在开发一种新的风险感知导航框架，以提高自主水面船只的安全性和自主性，尤其是在面对不确定性及动态变化时。

Method: 该方法包括创建一个集成有障碍物路径概率模型和平滑轨迹优化技术的导航架构。首先建立捕捉障碍物接近度和动态对象行为的概率风险地图；然后采用一种考虑风险因素的RRT规划器来生成避免碰撞的路径；最后，使用B样条算法对生成的路径进行细化处理，保证其连续性。此外，还实现了三种不同的RRT*重布线模式：最小化路径长度、最小化风险以及同时优化路径长度和总风险。

Result: 实验评估显示，在包含静态和动态障碍物的不同场景中，所提出的系统能够有效导航、维持平滑轨迹，并且能够根据环境中的风险变化做出相应调整。相比传统的仅依赖LIDAR或视觉信息的导航方法，本方法在操作安全性和自主性方面有所提升。

Conclusion: 提出的风险感知导航架构为在不确定和动态环境中执行任务的自主车辆提供了一个有前景的解决方案，它不仅提高了安全性，还增强了系统的适应能力。

Abstract: Robust navigation in changing marine environments requires autonomous systems capable of perceiving, reasoning, and acting under uncertainty. This study introduces a hybrid risk-aware navigation architecture that integrates probabilistic modeling of obstacles along the vehicle path with smooth trajectory optimization for autonomous surface vessels. The system constructs probabilistic risk maps that capture both obstacle proximity and the behavior of dynamic objects. A risk-biased Rapidly Exploring Random Tree (RRT) planner leverages these maps to generate collision-free paths, which are subsequently refined using B-spline algorithms to ensure trajectory continuity. Three distinct RRT* rewiring modes are implemented based on the cost function: minimizing the path length, minimizing risk, and optimizing a combination of the path length and total risk. The framework is evaluated in experimental scenarios containing both static and dynamic obstacles. The results demonstrate the system's ability to navigate safely, maintain smooth trajectories, and dynamically adapt to changing environmental risks. Compared with conventional LIDAR or vision-only navigation approaches, the proposed method shows improvements in operational safety and autonomy, establishing it as a promising solution for risk-aware autonomous vehicle missions in uncertain and dynamic environments.

</details>


### [6] [From Legible to Inscrutable Trajectories: (Il)legible Motion Planning Accounting for Multiple Observers](https://arxiv.org/abs/2602.09227)
*Ananya Yammanuru,Maria Lusardi,Nancy M. Amato,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: 本文提出了一种新的运动规划问题MMLO-LMP，旨在生成对于正面动机的观察者来说可读且对于负面动机的观察者来说难以理解的轨迹，并考虑了每个观察者的可见性限制。同时，介绍了DUBIOUS这一轨迹优化器来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 在合作环境中，机器人需要向其他观察者（人类或机器人）清楚地传达其意图；而在对抗环境中，则需要避免暴露意图。当存在多个观察者时，他们可能只能看到环境的一部分并且具有不同的动机，这就提出了一个复杂的问题：如何根据观察者的不同动机和有限视野生成合适的运动轨迹。

Method: 定义了Mixed-Motive Limited-Observability Legible Motion Planning (MMLO-LMP) 问题，并提出了DUBIOUS算法作为解决方案，该算法能够在考虑观察者动机与视域限制的情况下生成适当轨迹。

Result: 实验结果表明，DUBIOUS能够成功生成既满足正面动机观察者对清晰度要求又能迷惑负面动机观察者的轨迹。

Conclusion: 本研究为处理多观察者环境下、基于观察者动机与视线范围差异的运动规划提供了一个新框架及其实现方法。

Abstract: In cooperative environments, such as in factories or assistive scenarios, it is important for a robot to communicate its intentions to observers, who could be either other humans or robots. A legible trajectory allows an observer to quickly and accurately predict an agent's intention. In adversarial environments, such as in military operations or games, it is important for a robot to not communicate its intentions to observers. An illegible trajectory leads an observer to incorrectly predict the agent's intention or delays when an observer is able to make a correct prediction about the agent's intention. However, in some environments there are multiple observers, each of whom may be able to see only part of the environment, and each of whom may have different motives. In this work, we introduce the Mixed-Motive Limited-Observability Legible Motion Planning (MMLO-LMP) problem, which requires a motion planner to generate a trajectory that is legible to observers with positive motives and illegible to observers with negative motives while also considering the visibility limitations of each observer. We highlight multiple strategies an agent can take while still achieving the problem objective. We also present DUBIOUS, a trajectory optimizer that solves MMLO-LMP. Our results show that DUBIOUS can generate trajectories that balance legibility with the motives and limited visibility regions of the observers. Future work includes many variations of MMLO-LMP, including moving observers and observer teaming.

</details>


### [7] [STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory](https://arxiv.org/abs/2602.09255)
*Mingfeng Yuan,Hao Zhang,Mahan Mohammadi,Runhao Li,Jinjun Shan,Steven L. Waslander*

Main category: cs.RO

TL;DR: 本文提出了一种名为STaR的框架，用于移动机器人在长时间、多样化动态场景中的导航和推理。该框架构建了一个任务无关的多模态长期记忆，并引入了一种基于信息瓶颈原则的任务条件检索算法，以从长期记忆中提取紧凑、非冗余的信息丰富的候选记忆集。通过在NaVQA和WH-VQA数据集上的评估以及真实Husky轮式机器人上的部署，STaR表现优于现有方法，展示了其在长时推理、可扩展性和实用性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 移动机器人需要在仓库、制造设施、农业及道路等室内和室外环境中长时间运行，面对的核心挑战之一是如何建立一个可支持规划、检索与针对开放式指令进行推理的工作流程的记忆系统，同时为导航提供精确可行的答案。

Method: 提出了STaR框架，它不仅构建了能够泛化到未见过查询且保持细粒度环境语义（如物体属性、空间关系和动态事件）的任务无关多模态长期记忆，还引入了一种基于信息瓶颈原则的可扩展任务条件检索算法来从长期记忆中提取一组紧凑而非冗余的信息丰富候选记忆集合。

Result: 在NaVQA（混合室内/室外校园场景）和WH-VQA（使用Isaac Sim创建的定制化仓库基准测试，强调上下文推理能力）两个数据集上进行评估时，STaR相比强大的基线模型表现出色，实现了更高的成功率和显著更低的空间误差。此外，在实际Husky轮式机器人上的部署也证明了其在长时推理、可扩展性及实用价值方面的有效性。

Conclusion: 研究结果表明，所提出的STaR框架有效地解决了移动机器人在开放、动态场景中进行长期记忆管理与上下文推理的问题，为提高机器人的自主导航能力和任务执行效率提供了新的解决方案。

Abstract: Mobile robots are often deployed over long durations in diverse open, dynamic scenes, including indoor setting such as warehouses and manufacturing facilities, and outdoor settings such as agricultural and roadway operations. A core challenge is to build a scalable long-horizon memory that supports an agentic workflow for planning, retrieval, and reasoning over open-ended instructions at variable granularity, while producing precise, actionable answers for navigation. We present STaR, an agentic reasoning framework that (i) constructs a task-agnostic, multimodal long-term memory that generalizes to unseen queries while preserving fine-grained environmental semantics (object attributes, spatial relations, and dynamic events), and (ii) introduces a Scalable TaskConditioned Retrieval algorithm based on the Information Bottleneck principle to extract from long-term memory a compact, non-redundant, information-rich set of candidate memories for contextual reasoning. We evaluate STaR on NaVQA (mixed indoor/outdoor campus scenes) and WH-VQA, a customized warehouse benchmark with many visually similar objects built with Isaac Sim, emphasizing contextual reasoning. Across the two datasets, STaR consistently outperforms strong baselines, achieving higher success rates and markedly lower spatial error. We further deploy STaR on a real Husky wheeled robot in both indoor and outdoor environments, demonstrating robust longhorizon reasoning, scalability, and practical utility.

</details>


### [8] [Data-centric Design of Learning-based Surgical Gaze Perception Models in Multi-Task Simulation](https://arxiv.org/abs/2602.09259)
*Yizhou Li,Shuyuan Yang,Jiaji Su,Zonghe Chua*

Main category: cs.RO

TL;DR: 本研究探索了在机器人辅助微创手术中，不同经验和感知模式下的注视监督如何影响注意力模型的学习。通过收集主动执行和被动观察两种模式下的注视数据，研究发现被动注视可以在一定程度上替代主动注视作为监督信号，尤其是初学者的被动注视可以近似中级水平的被动注视，为可扩展的、众包的注视监督提供了实用路径。


<details>
  <summary>Details</summary>
Motivation: 在机器人辅助微创手术(RMIS)中，由于触觉反馈和深度线索减少，专家视觉感知变得更加重要，这促进了基于注视指导的训练和学习型外科感知模型的发展。然而，操作专家的注视数据收集成本高昂，并且不清楚注视监督来源（包括专业水平和感知模式）如何影响注意力模型的学习内容。

Method: 研究人员创建了一个配对的主动-被动多任务外科注视数据集，在达芬奇SimNow模拟器上进行了四个练习。主动注视是在执行任务时使用带有眼动追踪功能的VR头显记录下来的，而相同的视频被再次用作刺激来收集观察者的被动注视，从而能够进行控制下的相同视频比较。研究通过注视密度重叠分析和单帧显著性建模评估了被动注视代替操作监督的可能性。

Result: MSI-Net生成了稳定且可解释的预测，而SalGAN则不稳定且经常与人类注视点不一致。使用被动注视训练的模型恢复了相当一部分中级水平的主动注意，但存在预期的退化，并且从主动到被动目标之间的迁移是非对称的。值得注意的是，初学者的被动标签能够以有限的质量损失接近中级被动目标。

Conclusion: 研究表明，虽然存在一些性能下降，但被动注视可以作为一种有效的替代方法用于训练注意力模型，特别是在高质量演示的情况下。此外，利用初学者提供的被动注视数据可能为外科培训和感知模型构建中的可扩展、众包式注视监督提供了一条实际可行的道路。

Abstract: In robot-assisted minimally invasive surgery (RMIS), reduced haptic feedback and depth cues increase reliance on expert visual perception, motivating gaze-guided training and learning-based surgical perception models. However, operative expert gaze is costly to collect, and it remains unclear how the source of gaze supervision, both expertise level (intermediate vs. novice) and perceptual modality (active execution vs. passive viewing), shapes what attention models learn. We introduce a paired active-passive, multi-task surgical gaze dataset collected on the da Vinci SimNow simulator across four drills. Active gaze was recorded during task execution using a VR headset with eye tracking, and the corresponding videos were reused as stimuli to collect passive gaze from observers, enabling controlled same-video comparisons. We quantify skill- and modality-dependent differences in gaze organization and evaluate the substitutability of passive gaze for operative supervision using fixation density overlap analyses and single-frame saliency modeling. Across settings, MSI-Net produced stable, interpretable predictions, whereas SalGAN was unstable and often poorly aligned with human fixations. Models trained on passive gaze recovered a substantial portion of intermediate active attention, but with predictable degradation, and transfer was asymmetric between active and passive targets. Notably, novice passive labels approximated intermediate-passive targets with limited loss on higher-quality demonstrations, suggesting a practical path for scalable, crowd-sourced gaze supervision in surgical coaching and perception modeling.

</details>


### [9] [Disambiguating Anthropomorphism and Anthropomimesis in Human-Robot Interaction](https://arxiv.org/abs/2602.09287)
*Minja Axelsson,Henry Shevlin*

Main category: cs.RO

TL;DR: 本文初步区分了人机交互和社交机器人领域中的拟人化(anthropomorphism)和类人模仿(anthropomimesis)概念，旨在为未来研究提供清晰的理论基础。


<details>
  <summary>Details</summary>
Motivation: 作者希望澄清在人机交互（HRI）和社会机器人学中经常使用的两个术语——拟人化与类人模仿之间的区别，并指出各自的责任方：拟人化涉及的是机器人的感知者，而类人模仿则关联到机器人的设计者。

Method: 通过定义与对比分析方法，文中明确界定了拟人化是指用户感知到机器人具有类似人类的特质，而类人模仿则是指机器人开发者在设计时赋予机器人类似人类的特点。

Result: 成功地区分了拟人化和类人模仿的概念，强调了它们在责任主体上的不同，即拟人化侧重于机器人的使用者视角，而类人模仿关注于机器人的设计过程。

Conclusion: 该贡献有助于未来HRI领域的学者基于这些更加清晰的概念进行机器人设计与评估工作。

Abstract: In this preliminary work, we offer an initial disambiguation of the theoretical concepts anthropomorphism and anthropomimesis in Human-Robot Interaction (HRI) and social robotics. We define anthropomorphism as users perceiving human-like qualities in robots, and anthropomimesis as robot developers designing human-like features into robots. This contribution aims to provide a clarification and exploration of these concepts for future HRI scholarship, particularly regarding the party responsible for human-like qualities - robot perceiver for anthropomorphism, and robot designer for anthropomimesis. We provide this contribution so that researchers can build on these disambiguated theoretical concepts for future robot design and evaluation.

</details>


### [10] [CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments](https://arxiv.org/abs/2602.09367)
*Jinghan Yang,Jingyi Hou,Xinbo Yu,Wei He,Yifan Wu*

Main category: cs.RO

TL;DR: 提出了一种名为CAPER的新框架，用于机器人科学实验中的约束和程序化推理，通过分离任务级别的推理、中层的多模态基础以及底层的控制来提高执行的可控性、鲁棒性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 在科学实验室中，机器人助手需要进行长时间的过程正确操作、在有限监督下的可靠执行以及在低演示次数条件下的鲁棒性。然而，端到端视觉-语言-动作（VLA）模型在这些条件下常常面临挑战，因为它们对于可恢复错误的假设和基于数据的策略学习往往在对协议敏感的实验中失效。

Method: CAPER框架被设计成具有责任分离结构：任务级别推理生成符合明确约束的过程有效动作序列；中间层级的多模态基础实现实验子任务而无需将空间决策委派给大型语言模型；底层控制则通过强化学习适应物理不确定性，只需少量演示即可完成。

Result: 实验结果表明，在科学工作流基准测试和公共长时程操作数据集上，CAPER框架在成功率和过程正确性方面表现出一致的改进，特别是在低数据量和长时程设置下表现尤为突出。

Conclusion: 通过使用CAPER框架，可以防止实验逻辑在执行时间上的违反，从而提高了整个系统的可控性、鲁棒性和数据效率。

Abstract: Robotic assistance in scientific laboratories requires procedurally correct long-horizon manipulation, reliable execution under limited supervision, and robustness in low-demonstration regimes. Such conditions greatly challenge end-to-end vision-language-action (VLA) models, whose assumptions of recoverable errors and data-driven policy learning often break down in protocol-sensitive experiments. We propose CAPER, a framework for Constrained And ProcEdural Reasoning for robotic scientific experiments, which explicitly restricts where learning and reasoning occur in the planning and control pipeline. Rather than strengthening end-to-end policies, CAPER enforces a responsibility-separated structure: task-level reasoning generates procedurally valid action sequences under explicit constraints, mid-level multimodal grounding realizes subtasks without delegating spatial decision-making to large language models, and low-level control adapts to physical uncertainty via reinforcement learning with minimal demonstrations. By encoding procedural commitments through interpretable intermediate representations, CAPER prevents execution-time violations of experimental logic, improving controllability, robustness, and data efficiency. Experiments on a scientific workflow benchmark and a public long-horizon manipulation dataset demonstrate consistent improvements in success rate and procedural correctness, particularly in low-data and long-horizon settings.

</details>


### [11] [Certified Gradient-Based Contact-Rich Manipulation via Smoothing-Error Reachable Tubes](https://arxiv.org/abs/2602.09368)
*Wei-Chen Li,Glen Chou*

Main category: cs.RO

TL;DR: 本文提出了一种新的可微模拟器，通过平滑接触动力学和几何形状来解决接触丰富的操作中的梯度问题，并提供了对真实混合动力学约束满足和目标可达性的正式保证。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的方法可以使用物理先验和可微模拟器有效地优化控制器，但在处理接触丰富的操作时会遇到困难，因为混合接触动力学导致的梯度不连续或消失。尽管平滑动力学能够产生连续梯度，但由此产生的模型偏差可能导致在实际系统上执行控制器时出现故障。

Method: 研究者们通过一种新颖的基于凸优化的可微模拟器同时平滑了接触动力学和几何形状，从而能够将与真实动力学之间的差异表征为集合值偏差。这种偏差通过分析系统可达集的边界来限制时间变化仿射反馈策略的优化，使得仅依赖于平滑动力学提供的信息梯度即可实现对真实闭环混合动力学的鲁棒约束满足保证。

Result: 该方法在包括平面推动、物体旋转以及手内灵巧操作等多个接触丰富任务上进行了评估，相较于基线方法实现了更低的安全违规率和目标误差，并且确保了约束条件得到满足。

Conclusion: 本研究首次提出了针对接触丰富操作的认证型基于梯度的策略合成方法，它结合了可微物理与集合值鲁棒控制，能够在保持对真实混合动力学形式上的约束满足和目标可达性的同时，利用平滑动力学带来的好处。

Abstract: Gradient-based methods can efficiently optimize controllers using physical priors and differentiable simulators, but contact-rich manipulation remains challenging due to discontinuous or vanishing gradients from hybrid contact dynamics. Smoothing the dynamics yields continuous gradients, but the resulting model mismatch can cause controller failures when executed on real systems. We address this trade-off by planning with smoothed dynamics while explicitly quantifying and compensating for the induced errors, providing formal guarantees of constraint satisfaction and goal reachability on the true hybrid dynamics. Our method smooths both contact dynamics and geometry via a novel differentiable simulator based on convex optimization, which enables us to characterize the discrepancy from the true dynamics as a set-valued deviation. This deviation constrains the optimization of time-varying affine feedback policies through analytical bounds on the system's reachable set, enabling robust constraint satisfaction guarantees for the true closed-loop hybrid dynamics, while relying solely on informative gradients from the smoothed dynamics. We evaluate our method on several contact-rich tasks, including planar pushing, object rotation, and in-hand dexterous manipulation, achieving guaranteed constraint satisfaction with lower safety violation and goal error than baselines. By bridging differentiable physics with set-valued robust control, our method is the first certifiable gradient-based policy synthesis method for contact-rich manipulation.

</details>


### [12] [LLM-Grounded Dynamic Task Planning with Hierarchical Temporal Logic for Human-Aware Multi-Robot Collaboration](https://arxiv.org/abs/2602.09472)
*Shuyuan Hu,Tao Lin,Kai Ye,Yang Yang,Tianwei Zhang*

Main category: cs.RO

TL;DR: 提出了一种神经符号框架，将大型语言模型的推理与线性时序逻辑规范相结合，以解决同时任务分配和规划问题，并通过实时感知动态调整计划，实现实验表明该方法在成功率、交互流畅性和减少规划延迟方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽然能够让非专家指定开放世界的多机器人任务，但生成的计划往往缺乏运动学可行性且效率低下，特别是在长期场景中。而形式化方法如线性时序逻辑（LTL）虽能提供正确性和最优保证，却通常局限于静态、离线环境，难以处理计算可扩展性的问题。

Method: 本文提出了一种神经符号框架，该框架将LLM推理扎根于分层LTL规范中，并解决了相应的同时任务分配和规划（STAP）问题。系统通过包含实时感知的滚动视界规划（RHP）循环来应对随机环境变化，例如移动用户或更新指令，并通过一个分层状态空间动态地完善计划。

Result: 广泛的现实世界实验显示，所提出的方法在成功率和交互流畅度方面显著优于基线方法，同时也最小化了规划延迟。

Conclusion: 本研究成功地结合了LLM的灵活性和LTL的形式化优点，为多机器人系统中的开放世界任务指定了一个既高效又灵活的解决方案，能够在动态环境中实现高效的规划和执行。

Abstract: While Large Language Models (LLM) enable non-experts to specify open-world multi-robot tasks, the generated plans often lack kinematic feasibility and are not efficient, especially in long-horizon scenarios. Formal methods like Linear Temporal Logic (LTL) offer correctness and optimal guarantees, but are typically confined to static, offline settings and struggle with computational scalability. To bridge this gap, we propose a neuro-symbolic framework that grounds LLM reasoning into hierarchical LTL specifications and solves the corresponding Simultaneous Task Allocation and Planning (STAP) problem. Unlike static approaches, our system resolves stochastic environmental changes, such as moving users or updated instructions via a receding horizon planning (RHP) loop with real-time perception, which dynamically refines plans through a hierarchical state space. Extensive real-world experiments demonstrate that our approach significantly outperforms baseline methods in success rate and interaction fluency while minimizing planning latency.

</details>


### [13] [Optimal Control of Microswimmers for Trajectory Tracking Using Bayesian Optimization](https://arxiv.org/abs/2602.09563)
*Lucas Palazzolo,Mickaël Binois,Laëtitia Giraldi*

Main category: cs.RO

TL;DR: 该论文提出了一种结合B样条参数化和贝叶斯优化的方法来解决微泳者轨迹跟踪问题，该方法不仅能够处理高计算成本的问题，而且不需要复杂的梯度计算。通过应用到鞭毛磁性游泳者上，此方法可以重现多种目标轨迹，并且对于不同保真度的模型都具有鲁棒性和通用性。


<details>
  <summary>Details</summary>
Motivation: 在微机器人学中，由于低雷诺数动力学的影响，设计微型游泳者的轨迹跟踪控制方案是一项关键挑战。

Method: 将轨迹跟踪问题表述为最优控制问题，并采用B样条参数化与贝叶斯优化相结合的方式来求解，这种方法能够在不需进行复杂梯度计算的情况下处理高昂的计算成本。

Result: 所提方法能够成功地让鞭毛磁性游泳者重现包括实验研究中观察到的生物启发路径在内的各种目标轨迹；此外，在三球体游泳者模型上的测试表明，该方法还可以适应并部分补偿壁面引起的流体力学效应。

Conclusion: 提出的优化策略展现出了跨越从基于常微分方程的低维度模型到基于偏微分方程的高保真度模拟的不同模型时的一致适用性，突显了其鲁棒性和普遍性。这些成果强调了贝叶斯优化作为处理复杂数值-结构相互作用下微观尺度运动中最佳控制策略工具的潜力。

Abstract: Trajectory tracking for microswimmers remains a key challenge in microrobotics, where low-Reynolds-number dynamics make control design particularly complex. In this work, we formulate the trajectory tracking problem as an optimal control problem and solve it using a combination of B-spline parametrization with Bayesian optimization, allowing the treatment of high computational costs without requiring complex gradient computations. Applied to a flagellated magnetic swimmer, the proposed method reproduces a variety of target trajectories, including biologically inspired paths observed in experimental studies. We further evaluate the approach on a three-sphere swimmer model, demonstrating that it can adapt to and partially compensate for wall-induced hydrodynamic effects. The proposed optimization strategy can be applied consistently across models of different fidelity, from low-dimensional ODE-based models to high-fidelity PDE-based simulations, showing its robustness and generality. These results highlight the potential of Bayesian optimization as a versatile tool for optimal control strategies in microscale locomotion under complex fluid-structure interactions.

</details>


### [14] [Sample-Efficient Real-World Dexterous Policy Fine-Tuning via Action-Chunked Critics and Normalizing Flows](https://arxiv.org/abs/2602.09580)
*Chenyu Yang,Denis Tarasov,Davide Liconti,Hehui Zheng,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 介绍了SOFT-FLOW，一种结合了正态流策略和基于动作块的批评者的学习框架，旨在解决现实世界中灵巧操作政策微调时遇到的挑战。通过提供多模态动作块的确切似然性，并对整个动作序列进行评估，以改善长周期信用分配，从而提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的灵巧操作政策微调面临有限的实际互动预算以及高度多模态的动作分布带来的挑战。扩散型政策虽然表达力强，但因动作概率难以处理而不允许在微调期间进行保守的可能性更新；而传统的高斯政策在多模态情况下会失效，特别是在分块执行动作时，标准的每步批评者无法与这种分块执行相匹配，导致信用分配不佳。

Method: 提出了SOFT-FLOW框架，它利用正态流（NF）来实现高效样本外政策微调。该框架下的正态流政策能够为多模态动作块提供精确的概率值，支持通过可能性正则化来进行保守且稳定的政策更新。此外，还引入了一种基于动作块的批评者，用于评估整个动作序列，使得价值估计与政策的时间结构相一致，进而改进了长时间范围内的信用分配问题。

Result: 在两个具有挑战性的现实世界灵巧操作任务上测试了SOFT-FLOW：从盒子中取出剪刀剪断胶带，以及手掌朝下抓握立方体旋转。在这两项任务中，相比于标准方法，SOFT-FLOW展示了更加稳定且高效的适应能力。

Conclusion: SOFT-FLOW作为首个将基于可能性的多模态生成式政策与块级价值学习相结合并在实际机器人硬件上得到验证的方法，在需要长期精确控制的任务上表现出了显著优势。

Abstract: Real-world fine-tuning of dexterous manipulation policies remains challenging due to limited real-world interaction budgets and highly multimodal action distributions. Diffusion-based policies, while expressive, do not permit conservative likelihood-based updates during fine-tuning because action probabilities are intractable. In contrast, conventional Gaussian policies collapse under multimodality, particularly when actions are executed in chunks, and standard per-step critics fail to align with chunked execution, leading to poor credit assignment. We present SOFT-FLOW, a sample-efficient off-policy fine-tuning framework with normalizing flow (NF) to address these challenges. The normalizing flow policy yields exact likelihoods for multimodal action chunks, allowing conservative, stable policy updates through likelihood regularization and thereby improving sample efficiency. An action-chunked critic evaluates entire action sequences, aligning value estimation with the policy's temporal structure and improving long-horizon credit assignment. To our knowledge, this is the first demonstration of a likelihood-based, multimodal generative policy combined with chunk-level value learning on real robotic hardware. We evaluate SOFT-FLOW on two challenging dexterous manipulation tasks in the real world: cutting tape with scissors retrieved from a case, and in-hand cube rotation with a palm-down grasp -- both of which require precise, dexterous control over long horizons. On these tasks, SOFT-FLOW achieves stable, sample-efficient adaptation where standard methods struggle.

</details>


### [15] [Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation](https://arxiv.org/abs/2602.09583)
*Marco Moletta,Michael C. Welle,Danica Kragic*

Main category: cs.RO

TL;DR: 本研究提出了一种新的偏好对齐方法RKO，旨在通过有限的演示来调整预训练的视觉运动扩散策略以反映用户偏好的行为。实验结果表明，特别是在处理可变形物体（如衣物和织物）时，偏好对齐策略尤其是RKO，在性能和样本效率方面优于标准扩散策略微调，强调了结构化偏好学习在实现个性化机器人行为中的重要性和可行性。


<details>
  <summary>Details</summary>
Motivation: 人类对于如何执行操作任务有自然形成的偏好，这些偏好往往是微妙、个人化的且难以明确表达。尽管让机器人考虑到这些偏好对于提高个性化和用户满意度非常重要，但在机器人操作领域，特别是涉及到可变形物体（例如衣物和织物）的情况下，这一领域仍处于探索初期。

Method: 研究人员开发了名为RKO的新偏好对齐方法，该方法结合了两种最近框架RPO与KTO的优点，旨在利用有限示例调整预先训练好的视觉-运动扩散策略来体现用户偏好的行为表现。

Result: 通过对多种衣物及偏好设置下的真实世界布料折叠任务进行评估发现，相比传统的扩散策略微调，偏好对齐策略（特别是RKO）展现出更优的表现以及更高的样本使用效率。

Conclusion: 研究表明，通过采用结构化的偏好学习方法可以有效地规模化地为复杂可变形物体操作任务提供更加个性化的机器人行为，这不仅展示了其实现的可能性也强调了其重要性。

Abstract: Humans naturally develop preferences for how manipulation tasks should be performed, which are often subtle, personal, and difficult to articulate. Although it is important for robots to account for these preferences to increase personalization and user satisfaction, they remain largely underexplored in robotic manipulation, particularly in the context of deformable objects like garments and fabrics. In this work, we study how to adapt pretrained visuomotor diffusion policies to reflect preferred behaviors using limited demonstrations. We introduce RKO, a novel preference-alignment method that combines the benefits of two recent frameworks: RPO and KTO. We evaluate RKO against common preference learning frameworks, including these two, as well as a baseline vanilla diffusion policy, on real-world cloth-folding tasks spanning multiple garments and preference settings. We show that preference-aligned policies (particularly RKO) achieve superior performance and sample efficiency compared to standard diffusion policy fine-tuning. These results highlight the importance and feasibility of structured preference learning for scaling personalized robot behavior in complex deformable object manipulation tasks.

</details>


### [16] [AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception](https://arxiv.org/abs/2602.09617)
*Ruoxuan Feng,Yuxuan Zhou,Siyu Mei,Dongzhan Zhou,Pengwei Wang,Shaowei Cui,Bin Fang,Guocai Yao,Di Hu*

Main category: cs.RO

TL;DR: 本文提出了一种大规模层次触觉数据集ToucHD和一个通用的触觉表示学习框架AnyTouch 2，旨在提高机器人在接触丰富环境中的动态触觉感知能力。实验结果表明该模型在不同传感器和任务上都表现出一致且强大的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中接触丰富的操作要求机器人能够感知时间上的触觉反馈、捕捉细微的表面变形，并推理物体属性以及力的动力学。虽然光学触觉传感器能够提供这样的丰富信息，但现有的触觉数据集和模型仍然有限，主要集中在物体级别的属性上而忽视了物理交互过程中精细的时间动力学。为了推进动态触觉感知的发展，需要一个系统化的动态感知能力层级来指导数据收集和模型设计。

Method: 提出了ToucHD，这是一个大规模的层次触觉数据集，覆盖了从基础的触觉原子动作到真实世界的操作以及与触摸-力量配对的数据。此外还提出了AnyTouch 2，一种适用于多种光学触觉传感器的一般触觉表示学习框架，它统一了物体级别理解与细粒度、力量感知的动态感知。

Result: 实验评估涵盖了静态物体属性和动态物理特性，还包括了跨越多个动态感知能力层级的真实世界操作任务。结果显示，在不同的传感器和任务上，所提出的模型表现出了持续且强大的性能。

Conclusion: 通过引入ToucHD数据集和AnyTouch 2框架，本研究为促进机器人在实际应用场景下实现更高级别的动态触觉感知提供了新的方法论支持。

Abstract: Real-world contact-rich manipulation demands robots to perceive temporal tactile feedback, capture subtle surface deformations, and reason about object properties as well as force dynamics. Although optical tactile sensors are uniquely capable of providing such rich information, existing tactile datasets and models remain limited. These resources primarily focus on object-level attributes (e.g., material) while largely overlooking fine-grained tactile temporal dynamics during physical interactions. We consider that advancing dynamic tactile perception requires a systematic hierarchy of dynamic perception capabilities to guide both data collection and model design. To address the lack of tactile data with rich dynamic information, we present ToucHD, a large-scale hierarchical tactile dataset spanning tactile atomic actions, real-world manipulations, and touch-force paired data. Beyond scale, ToucHD establishes a comprehensive tactile dynamic data ecosystem that explicitly supports hierarchical perception capabilities from the data perspective. Building on it, we propose AnyTouch 2, a general tactile representation learning framework for diverse optical tactile sensors that unifies object-level understanding with fine-grained, force-aware dynamic perception. The framework captures both pixel-level and action-specific deformations across frames, while explicitly modeling physical force dynamics, thereby learning multi-level dynamic perception capabilities from the model perspective. We evaluate our model on benchmarks that covers static object properties and dynamic physical attributes, as well as real-world manipulation tasks spanning multiple tiers of dynamic perception capabilities-from basic object-level understanding to force-aware dexterous manipulation. Experimental results demonstrate consistent and strong performance across sensors and tasks.

</details>


### [17] [AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild](https://arxiv.org/abs/2602.09657)
*Xiaolou Sun,Wufei Si,Wenhui Ni,Yuntian Li,Dongming Wu,Fei Xie,Runwei Guan,He-Yang Xu,Henghui Ding,Yuan Wu,Yutao Yue,Yongming Huang,Hui Xiong*

Main category: cs.RO

TL;DR: 提出了一种名为AutoFly的端到端视觉-语言-动作模型，用于无人机自主导航。该模型结合了伪深度编码器以从RGB输入中提取深度感知特征，并采用两阶段训练策略来对齐视觉、深度和语言表示与动作策略。此外，还构建了一个新的自主导航数据集，强调连续避障、自主规划和识别工作流程以及全面整合现实世界的数据。实验结果显示AutoFly在模拟和真实环境中的成功率比最先进的VLA基线高出3.9%。


<details>
  <summary>Details</summary>
Motivation: 当前针对无人机（UAVs）的视觉-语言导航（VLN）研究依赖于详细的预先指定的指令来指导沿预定路线飞行。然而，在未知环境中进行的真实世界户外探索通常无法获得详细的导航指令。因此，需要一种方法让无人机能够根据粗粒度的位置或方向指引自主地通过连续规划和障碍物避免来进行导航。此外，现有的VLN数据集由于过于依赖明确的指令跟踪而缺乏自主决策能力，且缺乏足够的现实世界数据，这限制了其在实际应用中的有效性。

Method: 提出了AutoFly，这是一个端到端的视觉-语言-动作（VLA）模型，专为无人机自主导航设计。该模型引入了一个伪深度编码器，可以从RGB输入中推导出深度感知特征，从而增强空间推理能力。同时，采用了一种渐进式的两阶段训练策略，有效地将视觉、深度和语言表征与行动策略结合起来。此外，为了应对现有VLN数据集的局限性，创建了一个新的自主导航数据集，该数据集侧重于连续障碍物避免、自主规划及识别工作流，并且全面整合了现实世界的数据。

Result: 实验结果表明，与最先进的VLA基准相比，AutoFly在成功完成任务方面提高了3.9%，并且在模拟环境和真实环境中表现一致。

Conclusion: AutoFly通过创新性的模型架构和训练策略，有效提升了无人机在未知环境下的自主导航能力。此外，新构建的数据集为未来的研究提供了更贴近现实挑战的基础。

Abstract: Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.

</details>


### [18] [RANT: Ant-Inspired Multi-Robot Rainforest Exploration Using Particle Filter Localisation and Virtual Pheromone Coordination](https://arxiv.org/abs/2602.09661)
*Ameer Alhashemi,Layan Abdulhadi,Karam Abuodeh,Tala Baghdadi,Suryanarayana Datla*

Main category: cs.RO

TL;DR: 本文提出了一种受蚂蚁启发的多机器人探索框架RANT，适用于嘈杂和不确定的环境。该系统结合了粒子滤波定位、基于行为的控制器以及一种轻量级的虚拟信息素阻塞协调机制，并通过实验分析了团队规模、定位精度和协调对覆盖范围、热点召回率及冗余的影响。


<details>
  <summary>Details</summary>
Motivation: 为了在充满噪声和不确定性的环境中实现有效的多机器人探索，作者们受到了自然界中蚂蚁协同工作模式的启发，开发了RANT框架来解决这一问题。

Method: RANT框架整合了粒子滤波技术用于定位；采用了基于梯度驱动热点利用的行为控制器；并引入了一个基于虚拟信息素阻止的轻量级不重复访问协调机制。此外，还通过调整团队规模、定位准确性等因素进行了实验分析，以评估这些因素如何影响整体性能。

Result: 实验结果显示，粒子滤波对于可靠地识别热点至关重要；所提出的协调机制显著减少了机器人的重叠探索；随着团队规模增加虽然可以提高覆盖面积，但由于相互干扰效应导致收益递减。

Conclusion: 综上所述，RANT提供了一种有效的方法来促进多机器人在复杂环境下进行协作探索。研究结果表明，在保持适当队伍规模的同时采用精准定位技术和良好的协调策略是实现高效探索的关键。

Abstract: This paper presents RANT, an ant-inspired multi-robot exploration framework for noisy, uncertain environments. A team of differential-drive robots navigates a 10 x 10 m terrain, collects noisy probe measurements of a hidden richness field, and builds local probabilistic maps while the supervisor maintains a global evaluation. RANT combines particle-filter localisation, a behaviour-based controller with gradient-driven hotspot exploitation, and a lightweight no-revisit coordination mechanism based on virtual pheromone blocking. We experimentally analyse how team size, localisation fidelity, and coordination influence coverage, hotspot recall, and redundancy. Results show that particle filtering is essential for reliable hotspot engagement, coordination substantially reduces overlap, and increasing team size improves coverage but yields diminishing returns due to interference.

</details>


### [19] [Fast Motion Planning for Non-Holonomic Mobile Robots via a Rectangular Corridor Representation of Structured Environments](https://arxiv.org/abs/2602.09714)
*Alejandro Gonzalez-Garcia,Sebastiaan Wyns,Sonia De Santis,Jan Swevers,Wilm Decré*

Main category: cs.RO

TL;DR: 提出了一种针对复杂但有结构环境下的非完整自主移动机器人的快速运动规划框架，通过确定性的自由空间分解创建紧凑的重叠矩形走廊图，从而大幅减少搜索空间并保持路径分辨率。该框架在线进行运动规划，生成近乎时间最优且符合运动学约束的轨迹。经过广泛模拟和物理机器人测试验证了该方法的有效性，并已公开为开源软件。


<details>
  <summary>Details</summary>
Motivation: 传统的基于网格的规划器在可扩展性方面存在问题，而许多考虑运动学可行性的规划器由于搜索空间复杂而导致计算负担过重。本文旨在解决这些问题，提供一个能够有效减少搜索空间同时不牺牲路径精度的方法，适用于大规模导航场景。

Method: 采用一种确定性的自由空间分解技术来构建由重叠矩形走廊组成的紧凑图。基于此图，在线执行运动规划，首先找到一系列矩形，然后使用分析规划器生成接近时间最优且满足运动学约束的轨迹。

Result: 实验结果表明，所提出的框架能够在保持高效率的同时实现精确的大规模导航任务。此外，该解决方案已经过大量仿真测试及真实机器人平台上的验证。

Conclusion: 本文介绍的框架为非完整移动机器人在复杂环境中提供了高效、可扩展的运动规划解决方案。通过创新的空间分解技术和优化的轨迹生成算法，成功克服了传统方法存在的局限性。

Abstract: We present a complete framework for fast motion planning of non-holonomic autonomous mobile robots in highly complex but structured environments. Conventional grid-based planners struggle with scalability, while many kinematically-feasible planners impose a significant computational burden due to their search space complexity. To overcome these limitations, our approach introduces a deterministic free-space decomposition that creates a compact graph of overlapping rectangular corridors. This method enables a significant reduction in the search space, without sacrificing path resolution. The framework then performs online motion planning by finding a sequence of rectangles and generating a near-time-optimal, kinematically-feasible trajectory using an analytical planner. The result is a highly efficient solution for large-scale navigation. We validate our framework through extensive simulations and on a physical robot. The implementation is publicly available as open-source software.

</details>


### [20] [Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization](https://arxiv.org/abs/2602.09722)
*Ye Wang,Sipeng Zheng,Hao Luo,Wanpeng Zhang,Haoqi Yuan,Chaoyi Xu,Haiweng Xu,Yicheng Feng,Mingyang Yu,Zhiyu Kang,Zongqing Lu,Qin Jin*

Main category: cs.RO

TL;DR: 本研究对视觉-语言-动作（VLA）模型在机器人控制中的扩展性进行了系统性的分析，通过实验挑战了关于实体化扩展的一些常见假设，并为从多样化机器人数据中训练大规模VLA策略提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言-动作（VLA）模型在通用机器人控制方面显示出强大的潜力，但尚不清楚标准的“扩大数据规模”方法是否以及在什么条件下适用于机器人学，在此领域内训练数据本质上跨实体、传感器和动作空间是异质的。

Method: 采用代表性的VLA框架结合视觉-语言主干与流匹配技术，研究者们在匹配条件下对关键设计决策进行了消融研究，并通过广泛的模拟及真实机器人实验进行了评估。此外，还引入了一种分组盲合集协议来提高现实世界结果的可靠性。

Result: 研究表明统一的末端执行器（EEF）相对动作表示对于稳健的跨实体转移至关重要；简单地汇集异构机器人数据集往往会导致负迁移而非增益；直观策略如感知丢弃和多阶段微调并不总能一致地提升性能。

Conclusion: 这项研究挑战了一些关于实体化扩展的普遍假设，并为从多样化机器人数据中训练大规模VLA策略提供了实际指导。

Abstract: While Vision-Language-Action (VLA) models show strong promise for generalist robot control, it remains unclear whether -- and under what conditions -- the standard "scale data" recipe translates to robotics, where training data is inherently heterogeneous across embodiments, sensors, and action spaces. We present a systematic, controlled study of VLA scaling that revisits core training choices for pretraining across diverse robots. Using a representative VLA framework that combines a vision-language backbone with flow-matching, we ablate key design decisions under matched conditions and evaluate in extensive simulation and real-robot experiments. To improve the reliability of real-world results, we introduce a Grouped Blind Ensemble protocol that blinds operators to model identity and separates policy execution from outcome judgment, reducing experimenter bias. Our analysis targets three dimensions of VLA scaling. (1) Physical alignment: we show that a unified end-effector (EEF)-relative action representation is critical for robust cross-embodiment transfer. (2) Embodiment mixture: we find that naively pooling heterogeneous robot datasets often induces negative transfer rather than gains, underscoring the fragility of indiscriminate data scaling. (3) Training regularization: we observe that intuitive strategies, such as sensory dropout and multi-stage fine-tuning, do not consistently improve performance at scale. Together, this study challenge some common assumptions about embodied scaling and provide practical guidance for training large-scale VLA policies from diverse robotic data. Project website: https://research.beingbeyond.com/rethink_vla

</details>


### [21] [NavDreamer: Video Models as Zero-Shot 3D Navigators](https://arxiv.org/abs/2602.09765)
*Xijie Huang,Weiqi Gai,Tianyue Wu,Congyu Wang,Zhiyang Liu,Xin Zhou,Yuze Wu,Fei Gao*

Main category: cs.RO

TL;DR: 提出了NavDreamer，一种基于视频的3D导航框架，利用生成式视频模型作为语言指令和导航轨迹之间的通用接口，通过引入基于采样的优化方法及逆动力学模型来提高零样本泛化能力，并在多个方面进行了系统评估。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有视觉-语言-动作模型在导航中面临的挑战，如数据收集劳动密集、静态表示无法捕捉时间动态和物理规律等问题。

Method: 开发了NavDreamer框架，该框架使用生成式视频模型作为语言指令与导航路径之间的桥梁，同时提出了一种基于采样的优化方法以减少预测随机性，并采用逆动力学模型从生成的视频计划中解码可执行路径点。

Result: 实验结果表明，NavDreamer能够很好地泛化到新的物体和未见过的环境上，特别是在高级决策制定方面表现突出。

Conclusion: NavDreamer展示了视频在编码时空信息和物理动态方面的潜力，结合互联网规模的数据可用性，能够在导航任务中实现强大的零样本泛化性能。

Abstract: Previous Vision-Language-Action models face critical limitations in navigation: scarce, diverse data from labor-intensive collection and static representations that fail to capture temporal dynamics and physical laws. We propose NavDreamer, a video-based framework for 3D navigation that leverages generative video models as a universal interface between language instructions and navigation trajectories. Our main hypothesis is that video's ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation. To mitigate the stochasticity of generative predictions, we introduce a sampling-based optimization method that utilizes a VLM for trajectory scoring and selection. An inverse dynamics model is employed to decode executable waypoints from generated video plans for navigation. To systematically evaluate this paradigm in several video model backbones, we introduce a comprehensive benchmark covering object navigation, precise navigation, spatial grounding, language control, and scene reasoning. Extensive experiments demonstrate robust generalization across novel objects and unseen environments, with ablation studies revealing that navigation's high-level decision-making nature makes it particularly suited for video-based planning.

</details>


### [22] [Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning](https://arxiv.org/abs/2602.09767)
*Ruopeng Cui,Yifei Bi,Haojie Luo,Wei Li*

Main category: cs.RO

TL;DR: 本文提出了一种正交混合专家(OMoE)架构和多判别器框架，以解决无监督技能发现中的学习效率低和奖励欺骗问题，通过在12-DOF Unitree A1四足机器人上进行实验，展示了该方法能够提高训练效率并增加状态空间覆盖率。


<details>
  <summary>Details</summary>
Motivation: 当前无监督技能发现的方法存在学习效率低以及容易受到奖励欺骗的问题，导致虽然奖励信号迅速增加并收敛，但学到的技能实际上缺乏多样性。

Method: 研究者们提出了一种正交混合专家(Orthogonal Mixture-of-Experts, OMoE)架构来防止多样化行为落入重叠表示，并且设计了一个多判别器框架，其中不同的判别器作用于不同的观察空间，以此来有效缓解奖励欺骗问题。

Result: 实验是在12自由度Unitree A1四足机器人上实施的，结果显示该方法不仅促进了训练效率，还相对于基线增加了18.3%的状态空间覆盖。

Conclusion: 通过引入OMoE架构与多判别器策略，本研究成功地提高了无监督技能发现过程中技能多样性和学习效率，为未来的研究提供了新的方向。

Abstract: Reinforcement learning necessitates meticulous reward shaping by specialists to elicit target behaviors, while imitation learning relies on costly task-specific data. In contrast, unsupervised skill discovery can potentially reduce these burdens by learning a diverse repertoire of useful skills driven by intrinsic motivation. However, existing methods exhibit two key limitations: they typically rely on a single policy to master a versatile repertoire of behaviors without modeling the shared structure or distinctions among them, which results in low learning efficiency; moreover, they are susceptible to reward hacking, where the reward signal increases and converges rapidly while the learned skills display insufficient actual diversity. In this work, we introduce an Orthogonal Mixture-of-Experts (OMoE) architecture that prevents diverse behaviors from collapsing into overlapping representations, enabling a single policy to master a wide spectrum of locomotion skills. In addition, we design a multi-discriminator framework in which different discriminators operate on distinct observation spaces, effectively mitigating reward hacking. We evaluated our method on the 12-DOF Unitree A1 quadruped robot, demonstrating a diverse set of locomotion skills. Our experiments demonstrate that the proposed framework boosts training efficiency and yields an 18.3\% expansion in state-space coverage compared to the baseline.

</details>


### [23] [BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation](https://arxiv.org/abs/2602.09849)
*Yucheng Hu,Jianke Zhang,Yuanfei Luo,Yanjiang Guo,Xiaoyu Chen,Xinshu Sun,Kun Feng,Qingzhou Lu,Sheng Chen,Yangang Zhang,Wei Li,Jianyu Chen*

Main category: cs.RO

TL;DR: 本文提出了一种名为BagelVLA的统一模型，它将语言规划、视觉预测和动作生成整合到一个框架中，通过残差流引导（RFG）技术提高了复杂操作任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作(Vision-Language-Action, VLA)模型通常只专注于语言规划或视觉预测之一，很少同时结合两者来指导动作生成，这导致在复杂的长时间跨度操作任务中表现不佳。为了解决这个问题，研究者们提出了BagelVLA模型。

Method: BagelVLA是一种统一模型，它从预训练的综合理解和生成模型初始化，并被训练成直接将文本推理与视觉预测交织进动作执行循环中。此外，引入了残差流引导(RFG)技术，该技术基于当前观察并通过单步去噪提取预测性视觉特征，以最小延迟指导动作生成。

Result: 广泛的实验表明，在多个模拟和现实世界的基准测试中，特别是在需要多阶段推理的任务上，BagelVLA相比现有基线有显著的优势。

Conclusion: 通过整合语言规划、视觉预测以及动作生成能力，BagelVLA展示出处理复杂、长时序操控任务的强大潜力。

Abstract: Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.

</details>


### [24] [TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback](https://arxiv.org/abs/2602.09888)
*Zihao Li,Yanan Zhou,Ranpeng Qiu,Hangyu Wu,Guoqiang Ren,Weiming Zhi*

Main category: cs.RO

TL;DR: 本文介绍了一种名为TriPilot-FF的开源全身遥控操作系统，该系统专为定制的双手机器人设计，并引入了基于激光雷达驱动的脚踏板触觉反馈，结合上身双臂主从遥控操作。此外，还展示了通过加入遥控反馈信号到ACT策略中来提高性能的可能性。


<details>
  <summary>Details</summary>
Motivation: 现有的移动操纵机器人的遥控操作系统主要依赖手部控制（如VR控制器和摇杆），而较少探索脚部控制通道用于连续基座控制。为了改善这一情况，研究者们提出了一个新颖的遥控操作系统TriPilot-FF，旨在通过增加脚踏板输入及触觉反馈来辅助操作者更好地协调轮式基座与双臂之间的动作，并在考虑障碍物的情况下实现更安全的操作。

Method: 开发了一个名为TriPilot-FF的系统，该系统利用低成本的基座安装激光雷达来提供障碍物接近信号，进而产生阻力脚踏提示，引导操作者避免碰撞。此外，系统支持手臂侧力反馈以增强接触感知，并提供实时力和视觉指导来促进双臂可操作性，从而建议移动基座重新定位。研究者还将遥控操作反馈信号整合到了一个基于变换器的动作分块（ACT）策略中。

Result: 实验证明了TriPilot-FF能够有效地“共驾”人类操作者执行长时间范围内的任务以及需要精确移动基座运动和协调的任务。当额外的信息可用时，通过将远程操作反馈信号纳入ACT策略中，显示出了性能上的改进。

Conclusion: TriPilot-FF系统通过引入创新的脚踏板输入机制及其与上身双臂遥控操作相结合的方式，为移动操纵机器人提供了一种新的遥控解决方案。该方法不仅提高了操作安全性，也增强了任务执行效率。同时，通过将遥控反馈信息融入到学习算法中，进一步提升了系统的整体表现。

Abstract: Mobile manipulators broaden the operational envelope for robot manipulation. However, the whole-body teleoperation of such robots remains a problem: operators must coordinate a wheeled base and two arms while reasoning about obstacles and contact. Existing interfaces are predominantly hand-centric (e.g., VR controllers and joysticks), leaving foot-operated channels underexplored for continuous base control. We present TriPilot-FF, an open-source whole-body teleoperation system for a custom bimanual mobile manipulator that introduces a foot-operated pedal with lidar-driven pedal haptics, coupled with upper-body bimanual leader-follower teleoperation. Using only a low-cost base-mounted lidar, TriPilot-FF renders a resistive pedal cue from proximity-to-obstacle signals in the commanded direction, shaping operator commands toward collision-averse behaviour without an explicit collision-avoidance controller. The system also supports arm-side force reflection for contact awareness and provides real-time force and visual guidance of bimanual manipulability to prompt mobile base repositioning, thereby improving reach. We demonstrate the capability of TriPilot-FF to effectively ``co-pilot'' the human operator over long time-horizons and tasks requiring precise mobile base movement and coordination. Finally, we incorporate teleoperation feedback signals into an Action Chunking with Transformers (ACT) policy and demonstrate improved performance when the additional information is available. We release the pedal device design, full software stack, and conduct extensive real-world evaluations on a bimanual wheeled platform. The project page of TriPilot-FF is http://bit.ly/46H3ZJT.

</details>


### [25] [TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data](https://arxiv.org/abs/2602.09893)
*Zhengxue Cheng,Yan Zhao,Keyu Wang,Hengdi Zhang,Li Song*

Main category: cs.RO

TL;DR: 本文提出了TaCo，这是首个针对触觉数据编解码器的综合基准，评估了30种压缩方法在不同传感器类型五个多样化数据集上的表现。研究结果表明，专门训练用于触觉数据的TaCo-LL（无损）和TaCo-L（有损）编解码器性能优越。


<details>
  <summary>Details</summary>
Motivation: 触觉感知对于实体智能至关重要，但在严格的带宽限制下实现高效触觉数据压缩以支持实时机器人应用仍是一个未充分探索的问题。由于触觉数据固有的异质性和时空复杂性，这一挑战变得更加复杂。

Method: 研究人员创建了TaCo，一个包含30种压缩方法（包括现成的压缩算法和神经编解码器）的评测基准，跨越了来自不同类型传感器的五个多样化数据集，并系统地评估了这些方法在四个关键任务上的表现：无损存储、人类可视化、材料与物体分类以及灵巧的机器人抓握。此外，还开发了专门针对触觉数据训练的数据驱动型编解码器TaCo-LL（无损）和TaCo-L（有损）。

Result: 实验结果验证了TaCo-LL和TaCo-L编解码器相对于其他方法具有更优的表现。

Conclusion: 通过建立TaCo基准，本研究为理解压缩效率与任务性能之间的重要权衡提供了基础框架，为未来触觉感知领域的进步铺平了道路。

Abstract: Tactile sensing is crucial for embodied intelligence, providing fine-grained perception and control in complex environments. However, efficient tactile data compression, which is essential for real-time robotic applications under strict bandwidth constraints, remains underexplored. The inherent heterogeneity and spatiotemporal complexity of tactile data further complicate this challenge. To bridge this gap, we introduce TaCo, the first comprehensive benchmark for Tactile data Codecs. TaCo evaluates 30 compression methods, including off-the-shelf compression algorithms and neural codecs, across five diverse datasets from various sensor types. We systematically assess both lossless and lossy compression schemes on four key tasks: lossless storage, human visualization, material and object classification, and dexterous robotic grasping. Notably, we pioneer the development of data-driven codecs explicitly trained on tactile data, TaCo-LL (lossless) and TaCo-L (lossy). Results have validated the superior performance of our TaCo-LL and TaCo-L. This benchmark provides a foundational framework for understanding the critical trade-offs between compression efficiency and task performance, paving the way for future advances in tactile perception.

</details>


### [26] [RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation](https://arxiv.org/abs/2602.09973)
*Hao Li,Ziqin Wang,Zi-han Ding,Shuai Yang,Yilun Chen,Yang Tian,Xiaolin Hu,Tai Wang,Dahua Lin,Feng Zhao,Si Liu,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本文介绍了RoboInter Manipulation Suite，这是一个统一的资源，包括用于操作的中间表示的数据、基准和模型。它旨在解决现有数据集成本高、特定性强、覆盖范围和多样性不足的问题，通过提供大规模高质量注释的数据集来支持机器人学习，并通过引入RoboInter-VQA和RoboInter-VLA增强VLMs的具身推理能力及实现从高层次计划到低层次执行的有效过渡。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作（VLA）系统在机器人操作领域的应用受到限制，主要因为现有的操作数据集成本高昂、高度依赖于具体实现且在覆盖面与多样性方面存在不足，阻碍了VLA模型的泛化能力。此外，尽管一些方法尝试通过先规划后执行的方式缓解这些问题，但它们严重依赖额外的中间监督信息，而这些信息在现有数据集中通常是缺失的。

Method: 为了解决上述挑战，作者们提出了RoboInter Manipulation Suite，其中包括：1) RoboInter-Tool，一个轻量级图形用户界面，支持半自动标注多种表征；2) RoboInter-Data，一个大型数据集，包含超过23万个跨571个不同场景的片段，提供了超过10种类别的密集逐帧注释；3) RoboInter-VQA，定义了9种空间类型和20种时间类型的具身VQA类别以评估并加强VLMs的具身推理能力；4) RoboInter-VLA，提供了一个集成式的先规划后执行框架，支持模块化和端到端VLA变体，通过中间监督将高级别规划与低级别执行连接起来。

Result: RoboInter Manipulation Suite显著提升了可用于机器人操作任务的数据规模和注释质量，为促进鲁棒性和可泛化的机器人学习奠定了实用基础。特别是，通过对多种类别的中间表征进行细致且多样化的注释，以及提出新的评估标准和执行框架，该套件不仅增强了VLMs的具身推理能力，还促进了从高层策略到实际操作之间更加平滑有效的转换。

Conclusion: RoboInter Manipulation Suite通过提供大量高质量标注的操作数据集、新型评估标准以及集成式执行框架，在促进更强大且通用的机器人学习方面迈出了重要一步。

Abstract: Advances in large vision-language models (VLMs) have stimulated growing interest in vision-language-action (VLA) systems for robot manipulation. However, existing manipulation datasets remain costly to curate, highly embodiment-specific, and insufficient in coverage and diversity, thereby hindering the generalization of VLA models. Recent approaches attempt to mitigate these limitations via a plan-then-execute paradigm, where high-level plans (e.g., subtasks, trace) are first generated and subsequently translated into low-level actions, but they critically rely on extra intermediate supervision, which is largely absent from existing datasets. To bridge this gap, we introduce the RoboInter Manipulation Suite, a unified resource including data, benchmarks, and models of intermediate representations for manipulation. It comprises RoboInter-Tool, a lightweight GUI that enables semi-automatic annotation of diverse representations, and RoboInter-Data, a large-scale dataset containing over 230k episodes across 571 diverse scenes, which provides dense per-frame annotations over more than 10 categories of intermediate representations, substantially exceeding prior work in scale and annotation quality. Building upon this foundation, RoboInter-VQA introduces 9 spatial and 20 temporal embodied VQA categories to systematically benchmark and enhance the embodied reasoning capabilities of VLMs. Meanwhile, RoboInter-VLA offers an integrated plan-then-execute framework, supporting modular and end-to-end VLA variants that bridge high-level planning with low-level execution via intermediate supervision. In total, RoboInter establishes a practical foundation for advancing robust and generalizable robotic learning via fine-grained and diverse intermediate representations.

</details>


### [27] [A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging](https://arxiv.org/abs/2602.10007)
*Bharathkumar Hegde,Melanie Bouroche*

Main category: cs.RO

TL;DR: 本文提出了一种多智能体安全防护（MASS）机制，通过控制屏障函数来实现联网自动驾驶车辆在密集交通中安全且协作地变道。结合先进的多智能体强化学习车道变换控制器与自定义奖励函数，形成名为MARL-MASS的系统，旨在平衡安全性与交通效率。实验结果表明该方法能有效保证安全并提高交通效率。


<details>
  <summary>Details</summary>
Motivation: 现有的车道变换控制器要么侧重于确保安全，要么是协同提升交通效率，但很少同时考虑这两个相互冲突的目标。为了解决这一问题，作者提出了一个能够在保证安全的同时促进合作的车道变换解决方案。

Method: 使用控制屏障函数设计了多智能体安全防护（MASS），并通过构建图结构表示的交互拓扑来捕捉CAVs之间的多智能体互动。此外，将最先进的多智能体强化学习（MARL）车道变换控制器与MASS集成，并定义了一个定制的奖励函数以优先考虑效率改进。

Result: 实验结果表明，MASS能够通过严格遵守安全约束来实现协作式车道变换。而且，所提出的定制奖励函数提高了带有安全保护措施训练的MARL策略的稳定性。

Conclusion: 通过鼓励探索一种尊重安全约束的合作车道变换策略，MARL-MASS有效地平衡了保障安全与提高拥堵交通中的交通效率之间的权衡。

Abstract: Lane changing in dense traffic is a significant challenge for Connected and Autonomous Vehicles (CAVs). Existing lane change controllers primarily either ensure safety or collaboratively improve traffic efficiency, but do not consider these conflicting objectives together. To address this, we propose the Multi-Agent Safety Shield (MASS), designed using Control Barrier Functions (CBFs) to enable safe and collaborative lane changes. The MASS enables collaboration by capturing multi-agent interactions among CAVs through interaction topologies constructed as a graph using a simple algorithm. Further, a state-of-the-art Multi-Agent Reinforcement Learning (MARL) lane change controller is extended by integrating MASS to ensure safety and defining a customised reward function to prioritise efficiency improvements. As a result, we propose a lane change controller, known as MARL-MASS, and evaluate it in a congested on-ramp merging simulation. The results demonstrate that MASS enables collaborative lane changes with safety guarantees by strictly respecting the safety constraints. Moreover, the proposed custom reward function improves the stability of MARL policies trained with a safety shield. Overall, by encouraging the exploration of a collaborative lane change policy while respecting safety constraints, MARL-MASS effectively balances the trade-off between ensuring safety and improving traffic efficiency in congested traffic. The code for MARL-MASS is available with an open-source licence at https://github.com/hkbharath/MARL-MASS

</details>


### [28] [Learning Force-Regulated Manipulation with a Low-Cost Tactile-Force-Controlled Gripper](https://arxiv.org/abs/2602.10013)
*Xuhui Kang,Tongxuan Tian,Sung-Wook Lee,Binghao Huang,Yunzhu Li,Yen-Ling Kuo*

Main category: cs.RO

TL;DR: Researchers developed TF-Gripper, a cost-effective force-controlled gripper with tactile sensing, and RETAF, a framework for reactive tactile adaptation of force, to improve robotic manipulation of delicate objects. The system outperforms standard position control and can be integrated with various base policies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable robots to precisely manipulate everyday, force-sensitive objects by adapting the force from tactile feedback, similar to human capabilities, while overcoming the limitations of commercial grippers in terms of cost and minimum force requirements.

Method: The method involves the creation of TF-Gripper, an affordable force-controlled parallel-jaw gripper, and the development of the RETAF framework, which separates grasping force control from arm pose prediction, allowing for high-frequency force regulation using tactile feedback and wrist images. A teleoperation device was also designed to record human-applied forces for training data.

Result: The results show that direct force control via RETAF significantly enhances grasp stability and task performance over position control. Tactile feedback proved to be critical for effective force regulation, and RETAF demonstrated consistent superiority over baseline methods and compatibility with different base policies.

Conclusion: The work introduces a viable solution for learning force-controlled policies in robotics, paving the way for more dexterous and adaptable robotic manipulation of delicate objects.

Abstract: Successfully manipulating many everyday objects, such as potato chips, requires precise force regulation. Failure to modulate force can lead to task failure or irreversible damage to the objects. Humans can precisely achieve this by adapting force from tactile feedback, even within a short period of physical contact. We aim to give robots this capability. However, commercial grippers exhibit high cost or high minimum force, making them unsuitable for studying force-controlled policy learning with everyday force-sensitive objects. We introduce TF-Gripper, a low-cost (~$150) force-controlled parallel-jaw gripper that integrates tactile sensing as feedback. It has an effective force range of 0.45-45N and is compatible with different robot arms. Additionally, we designed a teleoperation device paired with TF-Gripper to record human-applied grasping forces. While standard low-frequency policies can be trained on this data, they struggle with the reactive, contact-dependent nature of force regulation. To overcome this, we propose RETAF (REactive Tactile Adaptation of Force), a framework that decouples grasping force control from arm pose prediction. RETAF regulates force at high frequency using wrist images and tactile feedback, while a base policy predicts end-effector pose and gripper open/close action. We evaluate TF-Gripper and RETAF across five real-world tasks requiring precise force regulation. Results show that compared to position control, direct force control significantly improves grasp stability and task performance. We further show that tactile feedback is essential for force regulation, and that RETAF consistently outperforms baselines and can be integrated with various base policies. We hope this work opens a path for scaling the learning of force-controlled policies in robotic manipulation. Project page: https://force-gripper.github.io .

</details>


### [29] [A Collision-Free Sway Damping Model Predictive Controller for Safe and Reactive Forestry Crane Navigation](https://arxiv.org/abs/2602.10035)
*Marc-Philip Ecker,Christoph Fröhlich,Johannes Huemer,David Gruber,Bernhard Bischof,Tobias Glück,Wolfgang Kemmetmüller*

Main category: cs.RO

TL;DR: 本文提出了一种适用于林业起重机的防碰撞和减摆模型预测控制器，该控制器能够实时适应环境变化，并在实验中成功展示了其有效性和安全性。


<details>
  <summary>Details</summary>
Motivation: 目前的方法分别处理碰撞避免和负载摇摆控制问题，但没有一种方法可以同时解决这两个问题。为了提高林业起重机在动态、非结构化户外环境中的安全导航能力，有必要开发一种能够统一实现避碰和减摆控制的新型控制器。

Method: 本文采用了一种基于激光雷达环境映射与在线欧几里得距离场(EDF)结合的模型预测控制(MPC)方法。通过将EDF直接集成到MPC中，使得控制器能够在单个控制框架内同时执行碰撞约束和减少负载摆动的任务。

Result: 实验验证表明，所提出的控制器不仅能在静态环境变化时重新规划路径，还能在受到干扰的情况下保持无碰撞操作，并且当不存在绕行路径时提供安全停止功能。此外，在真实林业起重机上的测试显示了良好的减摆效果及障碍物规避性能。

Conclusion: 本研究首次实现了针对林业起重机的集成了避碰与减摆功能于一体的模型预测控制器，为复杂户外作业条件下的安全高效操作提供了新方案。

Abstract: Forestry cranes operate in dynamic, unstructured outdoor environments where simultaneous collision avoidance and payload sway control are critical for safe navigation. Existing approaches address these challenges separately, either focusing on sway damping with predefined collision-free paths or performing collision avoidance only at the global planning level. We present the first collision-free, sway-damping model predictive controller (MPC) for a forestry crane that unifies both objectives in a single control framework. Our approach integrates LiDAR-based environment mapping directly into the MPC using online Euclidean distance fields (EDF), enabling real-time environmental adaptation. The controller simultaneously enforces collision constraints while damping payload sway, allowing it to (i) replan upon quasi-static environmental changes, (ii) maintain collision-free operation under disturbances, and (iii) provide safe stopping when no bypass exists. Experimental validation on a real forestry crane demonstrates effective sway damping and successful obstacle avoidance. A video can be found at https://youtu.be/tEXDoeLLTxA.

</details>


### [30] [UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking](https://arxiv.org/abs/2602.10093)
*Baijun Chen,Weijie Wan,Tianxing Chen,Xianda Guo,Congsheng Xu,Yuanyang Qi,Haojie Zhang,Longyan Wu,Tianling Xu,Zixuan Li,Yizhe Wu,Rui Li,Xiaokang Yang,Ping Luo,Wei Sui,Yao Mu*

Main category: cs.RO

TL;DR: 提出了一种基于仿真的视觉-触觉数据合成平台UniVTAC，支持三种常用的视觉-触觉传感器，并能生成信息丰富的接触交互。基于此平台，还引入了UniVTAC编码器和UniVTAC基准测试，用于改进下游操作任务的表现并评估触觉驱动策略。实验结果表明，集成UniVTAC编码器可以显著提高机器人操作的成功率。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作（VLA）政策在机器人操作领域取得了快速进展，但仅靠视觉难以稳健完成如插入等需要丰富触觉的任务。同时，在物理世界中获取大规模且可靠的触觉数据成本高昂且具有挑战性，缺乏统一的评估平台也限制了策略学习与系统分析。

Method: 开发了UniVTAC平台，一个能够支持三种常用视觉-触觉传感器的数据合成仿真环境，允许大规模及可控地生成富含信息的接触互动。基于该平台训练了UniVTAC编码器，利用大量模拟合成数据及其设计的监督信号来提供以触觉为中心的视觉-触觉表示。此外，创建了包含八个代表性视觉-触觉操作任务的UniVTAC基准，用以评价触觉导向策略。

Result: 实验显示，将UniVTAC编码器集成后，在UniVTAC基准测试中的平均成功率提高了17.1%，而实际机器人实验进一步证明任务成功率提升了25%。

Conclusion: 通过引入UniVTAC平台、编码器以及基准测试集，本文为解决机器人操作中依赖于触觉感知的问题提供了新的方法论，并展示了其对于提高操作任务性能的有效性。

Abstract: Robotic manipulation has seen rapid progress with vision-language-action (VLA) policies. However, visuo-tactile perception is critical for contact-rich manipulation, as tasks such as insertion are difficult to complete robustly using vision alone. At the same time, acquiring large-scale and reliable tactile data in the physical world remains costly and challenging, and the lack of a unified evaluation platform further limits policy learning and systematic analysis. To address these challenges, we propose UniVTAC, a simulation-based visuo-tactile data synthesis platform that supports three commonly used visuo-tactile sensors and enables scalable and controllable generation of informative contact interactions. Based on this platform, we introduce the UniVTAC Encoder, a visuo-tactile encoder trained on large-scale simulation-synthesized data with designed supervisory signals, providing tactile-centric visuo-tactile representations for downstream manipulation tasks. In addition, we present the UniVTAC Benchmark, which consists of eight representative visuo-tactile manipulation tasks for evaluating tactile-driven policies. Experimental results show that integrating the UniVTAC Encoder improves average success rates by 17.1% on the UniVTAC Benchmark, while real-world robotic experiments further demonstrate a 25% improvement in task success. Our webpage is available at https://univtac.github.io/.

</details>


### [31] [VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model](https://arxiv.org/abs/2602.10098)
*Jingwen Sun,Wenyao Zhang,Zekun Qi,Shaojie Ren,Zezhi Liu,Hanxin Zhu,Guangzhong Sun,Xin Jin,Zhibo Chen*

Main category: cs.RO

TL;DR: 本文提出了一种名为VLA-JEPA的预训练框架，它通过无泄漏状态预测来避免现有方法中的问题。该框架在LIBERO、LIBERO-Plus、SimplerEnv和真实世界操作任务上的实验表明，与现有方法相比，在泛化性和鲁棒性方面取得了持续的进步。


<details>
  <summary>Details</summary>
Motivation: 当前基于潜行动作的目标往往学到的是错误的内容：它们更关注像素变化而不是动作相关状态转换，导致容易受到外观偏差、干扰运动和信息泄露的影响。因此，需要一种新的方法来克服这些缺点。

Method: 提出了VLA-JEPA，这是一种JEPA风格的预训练框架，其核心思想是进行无泄漏的状态预测。目标编码器从未来帧生成潜在表示，而学生路径仅看到当前观察——未来信息只作为监督目标使用，从未作为输入。通过在潜在空间而不是像素空间中进行预测，VLA-JEPA学习到对摄像机移动和无关背景变化具有鲁棒性的动态抽象。

Result: 实验结果表明，在LIBERO, LIBERO-Plus, SimplerEnv以及现实世界的操控任务上，VLA-JEPA相比现有方法在泛化能力和鲁棒性方面获得了显著提升。

Conclusion: VLA-JEPA提供了一个简单两阶段的方法（JEPA预训练后接动作头微调），去除了先前潜行动作管道的多阶段复杂性，并且在多个任务上展示了更好的泛化性能和鲁棒性。

Abstract: Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.

</details>


### [32] [Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2602.10101)
*Sizhe Yang,Linning Xu,Hao Li,Juncheng Mu,Jia Zeng,Dahua Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本文介绍了一种新的3D重建模型Robo3R，它能够从RGB图像和机器人状态实时预测准确的、按比例缩放的场景几何。该模型通过合成数据集进行训练，在多种下游任务中表现出色，优于现有的重建方法和深度传感器，为机器人操作提供了一个有前景的3D感知解决方案。


<details>
  <summary>Details</summary>
Motivation: 3D空间感知对于通用机器人操作至关重要，但获取可靠且高质量的3D几何形状仍然具有挑战性。深度传感器存在噪声及材料敏感性问题，而现有的重建模型缺乏物理交互所需的精度和度量一致性。

Method: 提出了Robo3R，一种前馈式的3D重建模型，能直接从RGB图像和机器人状态实时预测精确的、度量级场景几何。此模型结合了尺度不变局部几何与相对摄像机姿态，并通过学习到的整体相似变换将这些信息统一到以机器人为基准的场景表示中。为了满足操作的精度需求，Robo3R使用了掩码点头部生成清晰细致的点云，并采用基于关键点的透视n点（PnP）公式来优化摄像机外部参数和全局对齐。

Result: 在包含四百万高保真标注帧的大规模合成数据集Robo3R-4M上训练后，Robo3R在模仿学习、仿真到现实迁移、抓取合成以及无碰撞运动规划等下游任务中持续超越现有最先进的重建方法和深度传感器。

Conclusion: Robo3R作为一种替代性的3D感知模块，在支持机器人操作方面展示了巨大的潜力。

Abstract: 3D spatial perception is fundamental to generalizable robotic manipulation, yet obtaining reliable, high-quality 3D geometry remains challenging. Depth sensors suffer from noise and material sensitivity, while existing reconstruction models lack the precision and metric consistency required for physical interaction. We introduce Robo3R, a feed-forward, manipulation-ready 3D reconstruction model that predicts accurate, metric-scale scene geometry directly from RGB images and robot states in real time. Robo3R jointly infers scale-invariant local geometry and relative camera poses, which are unified into the scene representation in the canonical robot frame via a learned global similarity transformation. To meet the precision demands of manipulation, Robo3R employs a masked point head for sharp, fine-grained point clouds, and a keypoint-based Perspective-n-Point (PnP) formulation to refine camera extrinsics and global alignment. Trained on Robo3R-4M, a curated large-scale synthetic dataset with four million high-fidelity annotated frames, Robo3R consistently outperforms state-of-the-art reconstruction methods and depth sensors. Across downstream tasks including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning, we observe consistent gains in performance, suggesting the promise of this alternative 3D sensing module for robotic manipulation.

</details>


### [33] [DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos](https://arxiv.org/abs/2602.10105)
*Juncheng Mu,Sizhe Yang,Yiming Bao,Hojin Bae,Tianming Wei,Linning Xu,Boyi Li,Huazhe Xu,Jiangmiao Pang*

Main category: cs.RO

TL;DR: DexImit是一个自动化框架，可以将单目人类操作视频转换为物理上合理的机器人数据，无需任何额外信息。它通过四个阶段的生成流程来缩小人手和机器人灵巧手之间的实质差距，从而释放大规模人类操作视频数据的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于灵巧手在现实世界中的数据收集成本高且劳动密集，导致双手机器人灵巧操作的数据稀缺问题严重限制了其通用性。而人类操作视频作为操作知识的直接载体，具有极大的潜力来扩展机器人学习规模。但是，人手与机器人灵巧手之间存在巨大的实体差异，使得直接从人类视频中进行预训练极具挑战。

Method: 提出了DexImit，一种可以从任意视角重建手-物体交互、执行子任务分解和双手调度、合成与演示交互一致的机器人轨迹以及全面数据增强以实现零样本现实部署的四阶段生成流水线。

Result: DexImit能够基于互联网或视频生成模型的人类视频生成大规模机器人数据，并处理包括工具使用（如切苹果）、长期任务（如制作饮料）和精细操控（如堆叠杯子）在内的多种操作任务。

Conclusion: 通过DexImit，研究人员成功地利用人类操作视频来生成适合于机器人学习的大规模数据集，这有助于克服数据稀缺的问题，并促进双手机器人灵巧操作能力的发展。

Abstract: Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning. However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information. DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment. Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).

</details>


### [34] [EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration](https://arxiv.org/abs/2602.10106)
*Modi Shi,Shijia Peng,Jin Chen,Haoran Jiang,Yinghui Li,Di Huang,Ping Luo,Hongyang Li,Li Chen*

Main category: cs.RO

TL;DR: EgoHumanoid框架利用大量第一人称人类演示数据和少量机器人数据共同训练视觉-语言-动作策略，使类人机器人能够在多样的现实环境中执行移动操作任务。通过视图对齐和动作对齐来解决人类与机器人之间的形态差异问题。实验表明，这种方法在未见过的环境中的表现明显优于仅使用机器人数据的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管人类演示为机器人手臂操作提供了丰富的环境多样性和自然扩展性，但对于更复杂、需要更多数据支持的人形机器人移动操作而言，其潜力尚未得到充分探索。研究旨在开发一种新的框架，能够结合大量来自人类的第一人称视角演示数据及有限的机器人自身数据，以促进该领域的发展。

Method: 提出了一种名为EgoHumanoid的新框架，该框架通过整合大量的以人类为中心的演示视频以及少量直接从机器人收集的数据来联合训练一个视觉-语言-行动模型。为了克服人类与机器人之间存在的物理形态及视角差异，研究人员设计了一个系统性的校准流程，包括硬件层面的设计考量到后期的数据处理步骤。此外，还特别强调了视角一致化与动作转换两项关键技术，前者用于减少因摄像头高度和角度不同导致的视觉域差异；后者则致力于将人类的动作映射至一个统一且符合机器人运动学特性的动作空间内。

Result: 广泛的实地测试证明，相较于只依赖于机器人产生的数据的传统方法，加入无须实际操控机器人的第一人称视角信息后，新方法在处理陌生场景时性能提升了51%。同时，研究也揭示了哪些行为模式更容易迁移成功，并探讨了扩大人类演示样本规模的可能性。

Conclusion: EgoHumanoid框架通过有效利用大规模人类演示数据显著提高了类人机器人在多样化真实世界环境中执行loco-manipulation任务的能力，特别是在未知环境下表现出色。研究表明了该方法对于提高机器人适应性和泛化能力的巨大潜力。

Abstract: Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.

</details>


### [35] [Learning Agile Quadrotor Flight in the Real World](https://arxiv.org/abs/2602.10111)
*Yunfan Ren,Zhiyuan Zhu,Jiaxu Xing,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 提出了一种自适应框架，通过在线残差学习和自适应时间缩放（ATS）来主动探索平台物理极限，并使用现实锚定短时回溯传播（RASH-BPTT）技术实现飞行中策略的有效更新。实验表明该四旋翼无人机能够在接近执行器饱和限制的情况下可靠地执行敏捷动作，系统在大约100秒的飞行时间内将保守的基础策略从峰值速度1.9米/秒提升至7.3米/秒。


<details>
  <summary>Details</summary>
Motivation: 基于学习的控制器虽然能在敏捷四旋翼飞行中表现出色，但通常需要大量模拟训练以及精确的系统识别以完成有效的Sim2Real迁移。然而，固定策略面对超出分布范围的情况如外部气动干扰或内部硬件退化时表现不佳。为保证安全，这些控制器不得不采用保守的安全裕度，这限制了它们在非受控环境下的灵活性。

Method: 提出一个包含自适应时间缩放（ATS）和在线残差学习的自适应框架，无需精准的系统识别或离线Sim2Real转移。基于所学得的混合模型，进一步提出了现实锚定短时回溯传播（RASH-BPTT），以实现高效且鲁棒的飞行中策略更新。

Result: 实验结果证明，该四旋翼无人机系统能够可靠地在接近致动器饱和极限下执行敏捷操作，并且在约100秒飞行时间内，基础策略的最高速度从1.9 m/s提升到了7.3 m/s。

Conclusion: 实验证明，通过真实世界中的适应性调整不仅能够补偿建模误差，而且是维持在激进飞行状态下持续性能改进的一个实用机制。

Abstract: Learning-based controllers have achieved impressive performance in agile quadrotor flight but typically rely on massive training in simulation, necessitating accurate system identification for effective Sim2Real transfer. However, even with precise modeling, fixed policies remain susceptible to out-of-distribution scenarios, ranging from external aerodynamic disturbances to internal hardware degradation. To ensure safety under these evolving uncertainties, such controllers are forced to operate with conservative safety margins, inherently constraining their agility outside of controlled settings. While online adaptation offers a potential remedy, safely exploring physical limits remains a critical bottleneck due to data scarcity and safety risks. To bridge this gap, we propose a self-adaptive framework that eliminates the need for precise system identification or offline Sim2Real transfer. We introduce Adaptive Temporal Scaling (ATS) to actively explore platform physical limits, and employ online residual learning to augment a simple nominal model. {Based on the learned hybrid model, we further propose Real-world Anchored Short-horizon Backpropagation Through Time (RASH-BPTT) to achieve efficient and robust in-flight policy updates. Extensive experiments demonstrate that our quadrotor reliably executes agile maneuvers near actuator saturation limits. The system evolves a conservative base policy with a peak speed of 1.9 m/s to 7.3 m/s within approximately 100 seconds of flight time. These findings underscore that real-world adaptation serves not merely to compensate for modeling errors, but as a practical mechanism for sustained performance improvement in aggressive flight regimes.

</details>


### [36] [Decoupled MPPI-Based Multi-Arm Motion Planning](https://arxiv.org/abs/2602.10114)
*Dan Evron,Elias Goldsztejn,Ronen I. Brafman*

Main category: cs.RO

TL;DR: 本文提出了MR-STORM算法，这是一种基于STORM改进的分布式多机器人运动规划方法，能够有效处理静态和动态障碍物，并且在性能上优于现有的最先进算法。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的基于采样的运动规划算法可以用于控制多个机械臂，但这种方法的扩展性较差。为了解决这个问题，作者们希望开发一种更有效的多机器人系统解决方案。

Method: 首先对STORM算法进行了修改以处理动态障碍；然后让每个机械臂计算自己的运动计划前缀，并将其与其他机械臂共享，后者将此视为动态障碍来处理；最后引入了一种动态优先级方案。

Result: 通过实验验证了新提出的MR-STORM算法相比现有最先进的算法，在面对静态及动态障碍时都展现出了明显的实际优势。

Conclusion: 本研究成功地将STORM扩展到了多机器人场景下，形成了名为MR-STORM的新算法，该算法不仅支持动态避障还具有良好的可扩展性。

Abstract: Recent advances in sampling-based motion planning algorithms for high DOF arms leverage GPUs to provide SOTA performance. These algorithms can be used to control multiple arms jointly, but this approach scales poorly. To address this, we extend STORM, a sampling-based model-predictive-control (MPC) motion planning algorithm, to handle multiple robots in a distributed fashion. First, we modify STORM to handle dynamic obstacles. Then, we let each arm compute its own motion plan prefix, which it shares with the other arms, which treat it as a dynamic obstacle. Finally, we add a dynamic priority scheme. The new algorithm, MR-STORM, demonstrates clear empirical advantages over SOTA algorithms when operating with both static and dynamic obstacles.

</details>
