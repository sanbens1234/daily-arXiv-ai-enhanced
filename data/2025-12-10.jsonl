{"id": "2512.07969", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07969", "abs": "https://arxiv.org/abs/2512.07969", "authors": ["Alan Papalia", "Nikolas Sanderson", "Haoyu Han", "Heng Yang", "Hanumant Singh", "Michael Everett"], "title": "Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization", "comment": "8 pages, submitted for review", "summary": "Robotic perception often requires solving large nonlinear least-squares (NLS) problems. While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \\emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution. Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties. However, VarPro has seen limited use in robotic perception; a major challenge arises from gauge symmetries (e.g., cost invariance to global shifts and rotations), which are common in perception and induce specific computational challenges in standard VarPro approaches. We present a VarPro scheme designed for problems with gauge symmetries that jointly exploits separability and sparsity. Our method can be applied as a one-time preprocessing step to construct a \\emph{matrix-free Schur complement operator}. This operator allows efficient evaluation of costs, gradients, and Hessian-vector products of the reduced problem and readily integrates with standard iterative NLS solvers. We provide precise conditions under which our method applies, and describe extensions when these conditions are only partially met. Across synthetic and real benchmarks in SLAM, SNL, and SfM, our approach achieves up to \\textbf{2$\\times$--35$\\times$ faster runtimes} than state-of-the-art methods while maintaining accuracy. We release an open-source C++ implementation and all datasets from our experiments."}
{"id": "2512.07976", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07976", "abs": "https://arxiv.org/abs/2512.07976", "authors": ["Lazar Milikic", "Manthan Patel", "Jonas Frey"], "title": "VLD: Visual Language Goal Distance for Reinforcement Learning Navigation", "comment": null, "summary": "Training end-to-end policies from image data to directly predict navigation actions for robotic systems has proven inherently difficult. Existing approaches often suffer from either the sim-to-real gap during policy transfer or a limited amount of training data with action labels. To address this problem, we introduce Vision-Language Distance (VLD) learning, a scalable framework for goal-conditioned navigation that decouples perception learning from policy learning. Instead of relying on raw sensory inputs during policy training, we first train a self-supervised distance-to-goal predictor on internet-scale video data. This predictor generalizes across both image- and text-based goals, providing a distance signal that can be minimized by a reinforcement learning (RL) policy. The RL policy can be trained entirely in simulation using privileged geometric distance signals, with injected noise to mimic the uncertainty of the trained distance predictor. At deployment, the policy consumes VLD predictions, inheriting semantic goal information-\"where to go\"-from large-scale visual training while retaining the robust low-level navigation behaviors learned in simulation. We propose using ordinal consistency to assess distance functions directly and demonstrate that VLD outperforms prior temporal distance approaches, such as ViNT and VIP. Experiments show that our decoupled design achieves competitive navigation performance in simulation while supporting flexible goal modalities, providing an alternative and, most importantly, scalable path toward reliable, multimodal navigation policies."}
{"id": "2512.07998", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07998", "abs": "https://arxiv.org/abs/2512.07998", "authors": ["Mostafa Kamali Tabrizi", "Mingshi Chi", "Bir Bikram Dey", "Yu Qing Yuan", "Markus D. Solbach", "Yiqian Liu", "Michael Jenkin", "John K. Tsotsos"], "title": "DIJIT: A Robotic Head for an Active Observer", "comment": null, "summary": "We present DIJIT, a novel binocular robotic head expressly designed for mobile agents that behave as active observers. DIJIT's unique breadth of functionality enables active vision research and the study of human-like eye and head-neck motions, their interrelationships, and how each contributes to visual ability. DIJIT is also being used to explore the differences between how human vision employs eye/head movements to solve visual tasks and current computer vision methods. DIJIT's design features nine mechanical degrees of freedom, while the cameras and lenses provide an additional four optical degrees of freedom. The ranges and speeds of the mechanical design are comparable to human performance. Our design includes the ranges of motion required for convergent stereo, namely, vergence, version, and cyclotorsion. The exploration of the utility of these to both human and machine vision is ongoing. Here, we present the design of DIJIT and evaluate aspects of its performance. We present a new method for saccadic camera movements. In this method, a direct relationship between camera orientation and motor values is developed. The resulting saccadic camera movements are close to human movements in terms of their accuracy."}
{"id": "2512.08028", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08028", "abs": "https://arxiv.org/abs/2512.08028", "authors": ["Lampis Papakostas", "Aristeidis Geladaris", "Athanasios Mastrogeorgiou", "Jim Sharples", "Gautier Hattenberger", "Panagiotis Chatzakos", "Panagiotis Polygerinos"], "title": "Optimized Area Coverage in Disaster Response Utilizing Autonomous UAV Swarm Formations", "comment": null, "summary": "This paper presents a UAV swarm system designed to assist first responders in disaster scenarios like wildfires. By distributing sensors across multiple agents, the system extends flight duration and enhances data availability, reducing the risk of mission failure due to collisions. To mitigate this risk further, we introduce an autonomous navigation framework that utilizes a local Euclidean Signed Distance Field (ESDF) map for obstacle avoidance while maintaining swarm formation with minimal path deviation. Additionally, we incorporate a Traveling Salesman Problem (TSP) variant to optimize area coverage, prioritizing Points of Interest (POIs) based on preassigned values derived from environmental behavior and critical infrastructure. The proposed system is validated through simulations with varying swarm sizes, demonstrating its ability to maximize coverage while ensuring collision avoidance between UAVs and obstacles."}
{"id": "2512.08052", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08052", "abs": "https://arxiv.org/abs/2512.08052", "authors": ["Pedro Santana"], "title": "An Introduction to Deep Reinforcement and Imitation Learning", "comment": null, "summary": "Embodied agents, such as robots and virtual characters, must continuously select actions to execute tasks effectively, solving complex sequential decision-making problems. Given the difficulty of designing such controllers manually, learning-based approaches have emerged as promising alternatives, most notably Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL). DRL leverages reward signals to optimize behavior, while DIL uses expert demonstrations to guide learning. This document introduces DRL and DIL in the context of embodied agents, adopting a concise, depth-first approach to the literature. It is self-contained, presenting all necessary mathematical and machine learning concepts as they are needed. It is not intended as a survey of the field; rather, it focuses on a small set of foundational algorithms and techniques, prioritizing in-depth understanding over broad coverage. The material ranges from Markov Decision Processes to REINFORCE and Proximal Policy Optimization (PPO) for DRL, and from Behavioral Cloning to Dataset Aggregation (DAgger) and Generative Adversarial Imitation Learning (GAIL) for DIL."}
{"id": "2512.08145", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08145", "abs": "https://arxiv.org/abs/2512.08145", "authors": ["Haoran Wang", "Zhuohang Chen", "Guang Li", "Bo Ma", "Chuanghuang Li"], "title": "Chat with UAV -- Human-UAV Interaction Based on Large Language Models", "comment": null, "summary": "The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs."}
{"id": "2512.08170", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.08170", "abs": "https://arxiv.org/abs/2512.08170", "authors": ["Haoxin Zhang", "Shuaixin Li", "Xiaozhou Zhu", "Hongbo Chen", "Wen Yao"], "title": "RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features", "comment": null, "summary": "In this paper, we present a user-friendly LiDAR-camera calibration toolkit that is compatible with various LiDAR and camera sensors and requires only a single pair of laser points and a camera image in targetless environments. Our approach eliminates the need for an initial transform and remains robust even with large positional and rotational LiDAR-camera extrinsic parameters. We employ the Gluestick pipeline to establish 2D-3D point and line feature correspondences for a robust and automatic initial guess. To enhance accuracy, we quantitatively analyze the impact of feature distribution on calibration results and adaptively weight the cost of each feature based on these metrics. As a result, extrinsic parameters are optimized by filtering out the adverse effects of inferior features. We validated our method through extensive experiments across various LiDAR-camera sensors in both indoor and outdoor settings. The results demonstrate that our method provides superior robustness and accuracy compared to SOTA techniques. Our code is open-sourced on GitHub to benefit the community."}
{"id": "2512.08186", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08186", "abs": "https://arxiv.org/abs/2512.08186", "authors": ["Meng Wei", "Chenyang Wan", "Jiaqi Peng", "Xiqian Yu", "Yuqiang Yang", "Delin Feng", "Wenzhe Cai", "Chenming Zhu", "Tai Wang", "Jiangmiao Pang", "Xihui Liu"], "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation", "comment": null, "summary": "While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, \"grounds slowly\" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, \"moves fast\" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments."}
{"id": "2512.08188", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.08188", "abs": "https://arxiv.org/abs/2512.08188", "authors": ["Wenjiang Xu", "Cindy Wang", "Rui Fang", "Mingkang Zhang", "Lusong Li", "Jing Xu", "Jiayuan Gu", "Zecui Zeng", "Rui Chen"], "title": "Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model", "comment": "Website at https://embodied-tree-of-thoughts.github.io", "summary": "World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io ."}
{"id": "2512.08206", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08206", "abs": "https://arxiv.org/abs/2512.08206", "authors": ["Duo Zhang", "Junshan Huang", "Jingjin Yu"], "title": "High-Performance Dual-Arm Task and Motion Planning for Tabletop Rearrangement", "comment": "ICRA 2026 Submission", "summary": "We propose Synchronous Dual-Arm Rearrange- ment Planner (SDAR), a task and motion planning (TAMP) framework for tabletop rearrangement, where two robot arms equipped with 2-finger grippers must work together in close proximity to rearrange objects whose start and goal config- urations are strongly entangled. To tackle such challenges, SDAR tightly knit together its dependency-driven task planner (SDAR-T) and synchronous dual-arm motion planner (SDAR- M), to intelligently sift through a large number of possible task and motion plans. Specifically, SDAR-T applies a simple yet effective strategy to decompose the global object dependency graph induced by the rearrangement task, to produce more optimal dual-arm task plans than solutions derived from optimal task plans for a single arm. Leveraging state-of-the-art GPU SIMD-based motion planning tools, SDAR-M employs a layered motion planning strategy to sift through many task plans for the best synchronous dual-arm motion plan while ensuring high levels of success rate. Comprehensive evaluation demonstrates that SDAR delivers a 100% success rate in solving complex, non-monotone, long-horizon tabletop rearrangement tasks with solution quality far exceeding the previous state- of-the-art. Experiments on two UR-5e arms further confirm SDAR directly and reliably transfers to robot hardware."}
{"id": "2512.08233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08233", "abs": "https://arxiv.org/abs/2512.08233", "authors": ["Timothy Chen", "Marcus Dominguez-Kuhne", "Aiden Swann", "Xu Liu", "Mac Schwager"], "title": "Semantic-Metric Bayesian Risk Fields: Learning Robot Safety from Human Videos with a VLM Prior", "comment": null, "summary": "Humans interpret safety not as a binary signal but as a continuous, context- and spatially-dependent notion of risk. While risk is subjective, humans form rational mental models that guide action selection in dynamic environments. This work proposes a framework for extracting implicit human risk models by introducing a novel, semantically-conditioned and spatially-varying parametrization of risk, supervised directly from safe human demonstration videos and VLM common sense. Notably, we define risk through a Bayesian formulation. The prior is furnished by a pretrained vision-language model. In order to encourage the risk estimate to be more human aligned, a likelihood function modulates the prior to produce a relative metric of risk. Specifically, the likelihood is a learned ViT that maps pretrained features, to pixel-aligned risk values. Our pipeline ingests RGB images and a query object string, producing pixel-dense risk images. These images that can then be used as value-predictors in robot planning tasks or be projected into 3D for use in conventional trajectory optimization to produce human-like motion. This learned mapping enables generalization to novel objects and contexts, and has the potential to scale to much larger training datasets. In particular, the Bayesian framework that is introduced enables fast adaptation of our model to additional observations or common sense rules. We demonstrate that our proposed framework produces contextual risk that aligns with human preferences. Additionally, we illustrate several downstream applications of the model; as a value learner for visuomotor planners or in conjunction with a classical trajectory optimization algorithm. Our results suggest that our framework is a significant step toward enabling autonomous systems to internalize human-like risk. Code and results can be found at https://riskbayesian.github.io/bayesian_risk/."}
{"id": "2512.08248", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08248", "abs": "https://arxiv.org/abs/2512.08248", "authors": ["Ahan Basu", "Ratnangshu Das", "Pushpak Jagtap"], "title": "Learning Spatiotemporal Tubes for Temporal Reach-Avoid-Stay Tasks using Physics-Informed Neural Networks", "comment": null, "summary": "This paper presents a Spatiotemporal Tube (STT)-based control framework for general control-affine MIMO nonlinear pure-feedback systems with unknown dynamics to satisfy prescribed time reach-avoid-stay tasks under external disturbances. The STT is defined as a time-varying ball, whose center and radius are jointly approximated by a Physics-Informed Neural Network (PINN). The constraints governing the STT are first formulated as loss functions of the PINN, and a training algorithm is proposed to minimize the overall violation. The PINN being trained on certain collocation points, we propose a Lipschitz-based validity condition to formally verify that the learned PINN satisfies the conditions over the continuous time horizon. Building on the learned STT representation, an approximation-free closed-form controller is defined to guarantee satisfaction of the T-RAS specification. Finally, the effectiveness and scalability of the framework are validated through two case studies involving a mobile robot and an aerial vehicle navigating through cluttered environments."}
{"id": "2512.08271", "categories": ["cs.RO", "cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.08271", "abs": "https://arxiv.org/abs/2512.08271", "authors": ["Srijan Dokania", "Dharini Raghavan"], "title": "Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation", "comment": "Published and Presented at 3rd Workshop on Human-Centric Multilateral Teleoperation in ICRA 2025", "summary": "We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup."}
{"id": "2512.08280", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.08280", "abs": "https://arxiv.org/abs/2512.08280", "authors": ["Haldun Balim", "Na Li", "Yilun Du"], "title": "Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making", "comment": null, "summary": "Offline decision-making requires synthesizing reliable behaviors from fixed datasets without further interaction, yet existing generative approaches often yield trajectories that are dynamically infeasible. We propose Model Predictive Diffuser (MPDiffuser), a compositional model-based diffusion framework consisting of: (i) a planner that generates diverse, task-aligned trajectories; (ii) a dynamics model that enforces consistency with the underlying system dynamics; and (iii) a ranker module that selects behaviors aligned with the task objectives. MPDiffuser employs an alternating diffusion sampling scheme, where planner and dynamics updates are interleaved to progressively refine trajectories for both task alignment and feasibility during the sampling process. We also provide a theoretical rationale for this procedure, showing how it balances fidelity to data priors with dynamics consistency. Empirically, the compositional design improves sample efficiency, as it leverages even low-quality data for dynamics learning and adapts seamlessly to novel dynamics. We evaluate MPDiffuser on both unconstrained (D4RL) and constrained (DSRL) offline decision-making benchmarks, demonstrating consistent gains over existing approaches. Furthermore, we present a preliminary study extending MPDiffuser to vision-based control tasks, showing its potential to scale to high-dimensional sensory inputs. Finally, we deploy our method on a real quadrupedal robot, showcasing its practicality for real-world control."}
{"id": "2512.08333", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08333", "abs": "https://arxiv.org/abs/2512.08333", "authors": ["Yajat Yadav", "Zhiyuan Zhou", "Andrew Wagenmaker", "Karl Pertsch", "Sergey Levine"], "title": "Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging", "comment": null, "summary": "Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities."}
{"id": "2512.08405", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08405", "abs": "https://arxiv.org/abs/2512.08405", "authors": ["Fan Zhang", "Michael Gienger"], "title": "Learning Robot Manipulation from Audio World Models", "comment": null, "summary": "World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns."}
{"id": "2512.08476", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08476", "abs": "https://arxiv.org/abs/2512.08476", "authors": ["Po-An Shih", "Shao-Hua Wang", "Yung-Che Li", "Chia-Heng Tu", "Chih-Han Chang"], "title": "A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems", "comment": null, "summary": "Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems."}
{"id": "2512.08481", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08481", "abs": "https://arxiv.org/abs/2512.08481", "authors": ["Yixiang Lin", "Tiancheng Yang", "Jonathan Eden", "Ying Tan"], "title": "Prospect Theory in Physical Human-Robot Interaction: A Pilot Study of Probability Perception", "comment": "9 pages, 6 figures", "summary": "Understanding how humans respond to uncertainty is critical for designing safe and effective physical human-robot interaction (pHRI), as physically working with robots introduces multiple sources of uncertainty, including trust, comfort, and perceived safety. Conventional pHRI control frameworks typically build on optimal control theory, which assumes that human actions minimize a cost function; however, human behavior under uncertainty often departs from such optimal patterns. To address this gap, additional understanding of human behavior under uncertainty is needed. This pilot study implemented a physically coupled target-reaching task in which the robot delivered assistance or disturbances with systematically varied probabilities (10\\% to 90\\%). Analysis of participants' force inputs and decision-making strategies revealed two distinct behavioral clusters: a \"trade-off\" group that modulated their physical responses according to disturbance likelihood, and an \"always-compensate\" group characterized by strong risk aversion irrespective of probability. These findings provide empirical evidence that human decision-making in pHRI is highly individualized and that the perception of probability can differ to its true value. Accordingly, the study highlights the need for more interpretable behavioral models, such as cumulative prospect theory (CPT), to more accurately capture these behaviors and inform the design of future adaptive robot controllers."}
{"id": "2512.08518", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.08518", "abs": "https://arxiv.org/abs/2512.08518", "authors": ["Nadezhda Kushina", "Ko Watanabe", "Aarthi Kannan", "Ashita Ashok", "Andreas Dengel", "Karsten Berns"], "title": "SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking", "comment": null, "summary": "Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot \"Ameca\" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic."}
{"id": "2512.08541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08541", "abs": "https://arxiv.org/abs/2512.08541", "authors": ["Nils Gehrke", "David Brecht", "Dominik Kulmer", "Dheer Patel", "Frank Diermeyer"], "title": "vEDGAR - Can CARLA Do HiL?", "comment": null, "summary": "Simulation offers advantages throughout the development process of automated driving functions, both in research and product development. Common open-source simulators like CARLA are extensively used in training, evaluation, and software-in-the-loop testing of new automated driving algorithms. However, the CARLA simulator lacks an evaluation where research and automated driving vehicles are simulated with their entire sensor and actuation stack in real time. The goal of this work is therefore to create a simulation framework for testing the automation software on its dedicated hardware and identifying its limits. Achieving this goal would greatly benefit the open-source development workflow of automated driving functions, designating CARLA as a consistent evaluation tool along the entire development process. To achieve this goal, in a first step, requirements are derived, and a simulation architecture is specified and implemented. Based on the formulated requirements, the proposed vEDGAR software is evaluated, resulting in a final conclusion on the applicability of CARLA for HiL testing of automated vehicles. The tool is available open source: Modified CARLA fork: https://github.com/TUMFTM/carla, vEDGAR Framework: https://github.com/TUMFTM/vEDGAR"}
{"id": "2512.08548", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08548", "abs": "https://arxiv.org/abs/2512.08548", "authors": ["Yuchi Zhang", "Churui Sun", "Shiqi Liang", "Diyuan Liu", "Chao Ji", "Wei-Nan Zhang", "Ting Liu"], "title": "Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations", "comment": null, "summary": "Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks."}
{"id": "2512.08574", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08574", "abs": "https://arxiv.org/abs/2512.08574", "authors": ["Vit Kratky", "Robert Penicka", "Parakh M. Gupta", "Ondrej Prochazka", "Martin Saska"], "title": "RVC-NMPC: Nonlinear Model Predictive Control with Reciprocal Velocity Constraints for Mutual Collision Avoidance in Agile UAV Flight", "comment": "8 pages, 8 figures", "summary": "This paper presents an approach to mutual collision avoidance based on Nonlinear Model Predictive Control (NMPC) with time-dependent Reciprocal Velocity Constraints (RVCs). Unlike most existing methods, the proposed approach relies solely on observable information about other robots, eliminating the necessity of excessive communication use. The computationally efficient algorithm for computing RVCs, together with the direct integration of these constraints into NMPC problem formulation on a controller level, allows the whole pipeline to run at 100 Hz. This high processing rate, combined with modeled nonlinear dynamics of the controlled Uncrewed Aerial Vehicles (UAVs), is a key feature that facilitates the use of the proposed approach for an agile UAV flight. The proposed approach was evaluated through extensive simulations emulating real-world conditions in scenarios involving up to 10 UAVs and velocities of up to 25 m/s, and in real-world experiments with accelerations up to 30 m/s$^2$. Comparison with state of the art shows 31% improvement in terms of flight time reduction in challenging scenarios, while maintaining a collision-free navigation in all trials."}
{"id": "2512.08580", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08580", "abs": "https://arxiv.org/abs/2512.08580", "authors": ["Peijun Tang", "Shangjin Xie", "Binyan Sun", "Baifu Huang", "Kuncheng Luo", "Haotian Yang", "Weiqi Jin", "Jianan Wang"], "title": "Mind to Hand: Purposeful Robotic Control via Embodied Reasoning", "comment": "49 pages, 25 figures", "summary": "Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (\"mind\") with robot action (\"hand\"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space."}
{"id": "2512.08630", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.08630", "abs": "https://arxiv.org/abs/2512.08630", "authors": ["Marta Manzoni", "Alessandro Nazzari", "Roberto Rubinacci", "Marco Lovera"], "title": "Multi-Task Bayesian Optimization for Tuning Decentralized Trajectory Generation in Multi-UAV Systems", "comment": null, "summary": "This paper investigates the use of Multi-Task Bayesian Optimization for tuning decentralized trajectory generation algorithms in multi-drone systems. We treat each task as a trajectory generation scenario defined by a specific number of drone-to-drone interactions. To model relationships across scenarios, we employ Multi-Task Gaussian Processes, which capture shared structure across tasks and enable efficient information transfer during optimization. We compare two strategies: optimizing the average mission time across all tasks and optimizing each task individually. Through a comprehensive simulation campaign, we show that single-task optimization leads to progressively shorter mission times as swarm size grows, but requires significantly more optimization time than the average-task approach."}
{"id": "2512.08653", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08653", "abs": "https://arxiv.org/abs/2512.08653", "authors": ["Doumegna Mawuto Koudjo Felix", "Xianjia Yu", "Zhuo Zou", "Tomi Westerlund"], "title": "A Sensor-Aware Phenomenological Framework for Lidar Degradation Simulation and SLAM Robustness Evaluation", "comment": null, "summary": "Lidar-based SLAM systems are highly sensitive to adverse conditions such as occlusion, noise, and field-of-view (FoV) degradation, yet existing robustness evaluation methods either lack physical grounding or do not capture sensor-specific behavior. This paper presents a sensor-aware, phenomenological framework for simulating interpretable lidar degradations directly on real point clouds, enabling controlled and reproducible SLAM stress testing. Unlike image-derived corruption benchmarks (e.g., SemanticKITTI-C) or simulation-only approaches (e.g., lidarsim), the proposed system preserves per-point geometry, intensity, and temporal structure while applying structured dropout, FoV reduction, Gaussian noise, occlusion masking, sparsification, and motion distortion. The framework features autonomous topic and sensor detection, modular configuration with four severity tiers (light--extreme), and real-time performance (less than 20 ms per frame) compatible with ROS workflows. Experimental validation across three lidar architectures and five state-of-the-art SLAM systems reveals distinct robustness patterns shaped by sensor design and environmental context. The open-source implementation provides a practical foundation for benchmarking lidar-based SLAM under physically meaningful degradation scenarios."}
{"id": "2512.08656", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08656", "abs": "https://arxiv.org/abs/2512.08656", "authors": ["Lauritz Rismark Fosso", "Herman Bi√∏rn Amundsen", "Marios Xanthidis", "Sveinung Johan Ohrem"], "title": "Sim2Swim: Zero-Shot Velocity Control for Agile AUV Maneuvering in 3 Minutes", "comment": "7 pages, 4 figures", "summary": "Holonomic autonomous underwater vehicles (AUVs) have the hardware ability for agile maneuvering in both translational and rotational degrees of freedom (DOFs). However, due to challenges inherent to underwater vehicles, such as complex hydrostatics and hydrodynamics, parametric uncertainties, and frequent changes in dynamics due to payload changes, control is challenging. Performance typically relies on carefully tuned controllers targeting unique platform configurations, and a need for re-tuning for deployment under varying payloads and hydrodynamic conditions. As a consequence, agile maneuvering with simultaneous tracking of time-varying references in both translational and rotational DOFs is rarely utilized in practice. To the best of our knowledge, this paper presents the first general zero-shot sim2real deep reinforcement learning-based (DRL) velocity controller enabling path following and agile 6DOF maneuvering with a training duration of just 3 minutes. Sim2Swim, the proposed approach, inspired by state-of-the-art DRL-based position control, leverages domain randomization and massively parallelized training to converge to field-deployable control policies for AUVs of variable characteristics without post-processing or tuning. Sim2Swim is extensively validated in pool trials for a variety of configurations, showcasing robust control for highly agile motions."}
{"id": "2512.08661", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08661", "abs": "https://arxiv.org/abs/2512.08661", "authors": ["Ziyue Zheng", "Yongce Liu", "Hesheng Wang", "Zhongqiang Ren"], "title": "Ergodic Trajectory Planning with Dynamic Sensor Footprints", "comment": "12 figures", "summary": "This paper addresses the problem of trajectory planning for information gathering with a dynamic and resolution-varying sensor footprint. Ergodic planning offers a principled framework that balances exploration (visiting all areas) and exploitation (focusing on high-information regions) by planning trajectories such that the time spent in a region is proportional to the amount of information in that region. Existing ergodic planning often oversimplifies the sensing model by assuming a point sensor or a footprint with constant shape and resolution. In practice, the sensor footprint can drastically change over time as the robot moves, such as aerial robots equipped with downward-facing cameras, whose field of view depends on the orientation and altitude. To overcome this limitation, we propose a new metric that accounts for dynamic sensor footprints, analyze the theoretic local optimality conditions, and propose numerical trajectory optimization algorithms. Experimental results show that the proposed approach can simultaneously optimize both the trajectories and sensor footprints, with up to an order of magnitude better ergodicity than conventional methods. We also deploy our approach in a multi-drone system to ergodically cover an object in 3D space."}
{"id": "2512.08688", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08688", "abs": "https://arxiv.org/abs/2512.08688", "authors": ["Mark Pustilnik", "Francesco Borrelli"], "title": "Non Normalized Shared-Constraint Dynamic Games for Human-Robot Collaboration with Asymmetric Responsibility", "comment": null, "summary": "This paper proposes a dynamic game formulation for cooperative human-robot navigation in shared workspaces with obstacles, where the human and robot jointly satisfy shared safety constraints while pursuing a common task. A key contribution is the introduction of a non-normalized equilibrium structure for the shared constraints. This structure allows the two agents to contribute different levels of effort towards enforcing safety requirements such as collision avoidance and inter-players spacing. We embed this non-normalized equilibrium into a receding-horizon optimal control scheme."}
{"id": "2512.08754", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08754", "abs": "https://arxiv.org/abs/2512.08754", "authors": ["Jason Hughes", "Marcel Hussing", "Edward Zhang", "Shenbagaraj Kannapiran", "Joshua Caswell", "Kenneth Chaney", "Ruichen Deng", "Michaela Feehery", "Agelos Kratimenos", "Yi Fan Li", "Britny Major", "Ethan Sanchez", "Sumukh Shrote", "Youkang Wang", "Jeremy Wang", "Daudi Zein", "Luying Zhang", "Ruijun Zhang", "Alex Zhou", "Tenzi Zhouga", "Jeremy Cannon", "Zaffir Qasim", "Jay Yelon", "Fernando Cladera", "Kostas Daniilidis", "Camillo J. Taylor", "Eric Eaton"], "title": "A Multi-Robot Platform for Robotic Triage Combining Onboard Sensing and Foundation Models", "comment": "Technical Report for the DARPA Triage Challenge PRONTO team", "summary": "This report presents a heterogeneous robotic system designed for remote primary triage in mass-casualty incidents (MCIs). The system employs a coordinated air-ground team of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to locate victims, assess their injuries, and prioritize medical assistance without risking the lives of first responders. The UAV identify and provide overhead views of casualties, while UGVs equipped with specialized sensors measure vital signs and detect and localize physical injuries. Unlike previous work that focused on exploration or limited medical evaluation, this system addresses the complete triage process: victim localization, vital sign measurement, injury severity classification, mental status assessment, and data consolidation for first responders. Developed as part of the DARPA Triage Challenge, this approach demonstrates how multi-robot systems can augment human capabilities in disaster response scenarios to maximize lives saved."}
{"id": "2512.08767", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08767", "abs": "https://arxiv.org/abs/2512.08767", "authors": ["Mohammed Elseiagy", "Tsige Tadesse Alemayoh", "Ranulfo Bezerra", "Shotaro Kojima", "Kazunori Ohno"], "title": "Data-Driven Dynamic Parameter Learning of manipulator robots", "comment": "Accepted for publication at SII 2026. 6 pages, 7 figures. Code is available at: https://github.com/MohamedAlsiagy/dynamic_parameter_est", "summary": "Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems"}
{"id": "2512.08813", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08813", "abs": "https://arxiv.org/abs/2512.08813", "authors": ["Connor York", "Zachary R Madin", "Paul O'Dowd", "Edmund R Hunt"], "title": "Heterogeneity in Multi-Robot Environmental Monitoring for Resolving Time-Conflicting Tasks", "comment": "Accepted to SAC '26. To appear, DOI: https://doi.org/10.1145/3748522.3779970", "summary": "Multi-robot systems performing continuous tasks face a performance trade-off when interrupted by urgent, time-critical sub-tasks. We investigate this trade-off in a scenario where a team must balance area patrolling with locating an anomalous radio signal. To address this trade-off, we evaluate both behavioral heterogeneity through agent role specialization (\"patrollers\" and \"searchers\") and sensing heterogeneity (i.e., only the searchers can sense the radio signal). Through simulation, we identify the Pareto-optimal trade-offs under varying team compositions, with behaviorally heterogeneous teams demonstrating the most balanced trade-offs in the majority of cases. When sensing capability is restricted, heterogeneous teams with half of the sensing-capable agents perform comparably to homogeneous teams, providing cost-saving rationale for restricting sensor payload deployment. Our findings demonstrate that pre-deployment role and sensing specialization are powerful design considerations for multi-robot systems facing time-conflicting tasks, where varying the degree of behavioral heterogeneity can tune system performance toward either task."}
{"id": "2512.08877", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.08877", "abs": "https://arxiv.org/abs/2512.08877", "authors": ["Ryan LeRoy", "Jack Kolb"], "title": "IPPO Learns the Game, Not the Team: A Study on Generalization in Heterogeneous Agent Teams", "comment": "4 pages, 3 figures, appendix", "summary": "Multi-Agent Reinforcement Learning (MARL) is commonly deployed in settings where agents are trained via self-play with homogeneous teammates, often using parameter sharing and a single policy architecture. This opens the question: to what extent do self-play PPO agents learn general coordination strategies grounded in the underlying game, compared to overfitting to their training partners' behaviors? This paper investigates the question using the Heterogeneous Multi-Agent Challenge (HeMAC) environment, which features distinct Observer and Drone agents with complementary capabilities. We introduce Rotating Policy Training (RPT), an approach that rotates heterogeneous teammate policies of different learning algorithms during training, to expose the agent to a broader range of partner strategies. When playing alongside a withheld teammate policy (DDQN), we find that RPT achieves similar performance to a standard self-play baseline, IPPO, where all agents were trained sharing a single PPO policy. This result indicates that in this heterogeneous multi-agent setting, the IPPO baseline generalizes to novel teammate algorithms despite not experiencing teammate diversity during training. This shows that a simple IPPO baseline may possess the level of generalization to novel teammates that a diverse training regimen was designed to achieve."}
{"id": "2512.08920", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08920", "abs": "https://arxiv.org/abs/2512.08920", "authors": ["Jessica Yin", "Haozhi Qi", "Youngsun Wi", "Sayantan Kundu", "Mike Lambeta", "William Yang", "Changhao Wang", "Tingfan Wu", "Jitendra Malik", "Tess Hellebrekers"], "title": "OSMO: Open-Source Tactile Glove for Human-to-Robot Skill Transfer", "comment": "Project website: https://jessicayin.github.io/osmo_tactile_glove/", "summary": "Human video demonstrations provide abundant training data for learning robot policies, but video alone cannot capture the rich contact signals critical for mastering manipulation. We introduce OSMO, an open-source wearable tactile glove designed for human-to-robot skill transfer. The glove features 12 three-axis tactile sensors across the fingertips and palm and is designed to be compatible with state-of-the-art hand-tracking methods for in-the-wild data collection. We demonstrate that a robot policy trained exclusively on human demonstrations collected with OSMO, without any real robot data, is capable of executing a challenging contact-rich manipulation task. By equipping both the human and the robot with the same glove, OSMO minimizes the visual and tactile embodiment gap, enabling the transfer of continuous shear and normal force feedback while avoiding the need for image inpainting or other vision-based force inference. On a real-world wiping task requiring sustained contact pressure, our tactile-aware policy achieves a 72% success rate, outperforming vision-only baselines by eliminating contact-related failure modes. We release complete hardware designs, firmware, and assembly instructions to support community adoption."}
